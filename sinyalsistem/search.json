[
  {
    "objectID": "ss_prompt/signal_systems_prompt_2.html",
    "href": "ss_prompt/signal_systems_prompt_2.html",
    "title": "Signal and Systems",
    "section": "",
    "text": "Objective:\nCreate an engaging, interactive slide deck for an undergraduate Signals and Systems course in Electrical and Computer Engineering (ECE), blending theoretical concepts with practical, hands-on demonstrations using Quarto and reveal.js.\nTask:\nDevelop an interactive Quarto-based presentation using reveal.js tailored for undergraduate ECE students. Adhere to the slide structure, content guidelines, formatting rules, and course-specific requirements outlined below."
  },
  {
    "objectID": "ss_prompt/signal_systems_prompt_2.html#slide-structure",
    "href": "ss_prompt/signal_systems_prompt_2.html#slide-structure",
    "title": "Signal and Systems",
    "section": "1. Slide Structure",
    "text": "1. Slide Structure\n\nSlide Separator: Use --- to separate slides.\n\nTitle Slide: Use a first-level heading (#) and/or second-level heading (##).\n\nStandard Slides: Begin with a second-level heading (##) followed by content."
  },
  {
    "objectID": "ss_prompt/signal_systems_prompt_2.html#content-guidelines",
    "href": "ss_prompt/signal_systems_prompt_2.html#content-guidelines",
    "title": "Signal and Systems",
    "section": "2. Content Guidelines",
    "text": "2. Content Guidelines\n\nWrite text in markdown format, ending each sentence with double spaces.\n\nEnhance slides with:\n\nDiagrams (using Mermaid.js or Graphviz).\n\nExecutable Python code blocks (using Pyodide).\n\nInteractive visualizations (using Python, Pyodide, Observable.js, and Plotly).\n\nMath formulas (using LaTeX).\n\nMulti-column layouts.\n\nSpeaker notes for additional context.\nCallout for drawing extra attention."
  },
  {
    "objectID": "ss_prompt/signal_systems_prompt_2.html#diagrams",
    "href": "ss_prompt/signal_systems_prompt_2.html#diagrams",
    "title": "Signal and Systems",
    "section": "3. Diagrams",
    "text": "3. Diagrams\n\nCreate diagrams using Mermaid.js or Graphviz\nFormat for Mermaid.js:\n\n\\`\\`\\`\\{mermaid\\}\nMermaid code here\n\\`\\`\\`\\\n\nFormat for Graphviz:\n\n\\`\\`\\`\\{dot\\}\nGraphviz code here\n\\`\\`\\`\\"
  },
  {
    "objectID": "ss_prompt/signal_systems_prompt_2.html#executable-code-blocks",
    "href": "ss_prompt/signal_systems_prompt_2.html#executable-code-blocks",
    "title": "Signal and Systems",
    "section": "4. Executable Code Blocks",
    "text": "4. Executable Code Blocks\n\nInclude executable Python code blocks using Pyodide, formatted as:\n\n\\`\\`\\`\\{pyodide\\}\n#| max-lines: 10\nPython code here\n\\`\\`\\`\\"
  },
  {
    "objectID": "ss_prompt/signal_systems_prompt_2.html#interactive-code-blocks",
    "href": "ss_prompt/signal_systems_prompt_2.html#interactive-code-blocks",
    "title": "Signal and Systems",
    "section": "5. Interactive Code Blocks",
    "text": "5. Interactive Code Blocks\n\nCreate interactive visualizations using Python, Pyodide, Observable.js, and Plotly, formatted as:\n\n\\`\\`\\`\\{ojs\\}\nObservable.js code here\n```{pyodide} #| echo: false #| input: Python code here ```\n\n\n## 6. Math Formulas  \n- Use LaTeX for mathematical expressions:  \n  - **Inline:** \n    ```\n    $E=mc^2$\n    ```\n  - **Block:**  \n    ```\n    $$ E=mc^2 $$\n    ```\n\n## 7. Multi-Column Layout  \n- Use multi-column layouts for balanced content presentation, formatted as:  \n\n:::: {.columns} ::: {.column width=“50%”} Left Column\n- Item L1\n- Item L2\n::: ::: {.column width=“50%”} Right Column\n- Item R1\n- Item R2\n- Item R3\n::: :::: ```"
  },
  {
    "objectID": "ss_prompt/signal_systems_prompt_2.html#speaker-notes",
    "href": "ss_prompt/signal_systems_prompt_2.html#speaker-notes",
    "title": "Signal and Systems",
    "section": "8. Speaker Notes",
    "text": "8. Speaker Notes\n\nInclude speaker notes for additional explanations, formatted as:\n**Slide Content:**  \n- Point 1  \n- Point 2  \n\n::: {.notes}\nSpeaker notes here.\n:::"
  },
  {
    "objectID": "ss_prompt/signal_systems_prompt_2.html#callout",
    "href": "ss_prompt/signal_systems_prompt_2.html#callout",
    "title": "Signal and Systems",
    "section": "9. Callout",
    "text": "9. Callout\n\nThere are five different types of callouts available: callout-note, callout-warning, callout-important, callout-tip, and callout-caution.\nInclude callout for extra attention, formatted as:\n\n**Slide Content:**  \n- Point 1  \n- Point 2  \n\n::: {.callout-note}\nCallout notes here.\n:::"
  },
  {
    "objectID": "ss_prompt/signal_systems_prompt_2.html#course-specific-requirements",
    "href": "ss_prompt/signal_systems_prompt_2.html#course-specific-requirements",
    "title": "Signal and Systems",
    "section": "10. Course-Specific Requirements",
    "text": "10. Course-Specific Requirements\n\nContent Source: Use provided text and linked images as primary resources.\n\nAcademic Context: Ensure explanations, examples, and terminology align with ECE standards for Signals and Systems.\n\nInteractivity:\n\nIncorporate Python-based interactive elements (e.g., signal plots, spectrum analysis, convolution demos).\n\nEmbed charts, plots, and simulations directly within slides.\n\n\nEnhancement: Include real-world engineering applications, analogies, and problem-solving examples to contextualize concepts.\n\nClarity & Engagement: Maintain a logical structure with engaging visuals and concise slide text.\n\nConciseness: Limit text on slides; elaborate in speaker notes. Split dense slides into multiple slides for clarity."
  },
  {
    "objectID": "ss_prompt/signal_systems_prompt_2.html#presentation-yaml",
    "href": "ss_prompt/signal_systems_prompt_2.html#presentation-yaml",
    "title": "Signal and Systems",
    "section": "11. Presentation YAML",
    "text": "11. Presentation YAML\nUse the following YAML header, replacing {Lecture Title} with the specific lecture subtitle:\n---\ntitle: \"Signals and Systems\"\nsubtitle: \"{Lecture Title}\"\nauthor: \"Imron Rosyadi\"\nformat:\n  live-revealjs:\n    logo: \"qrjs_assets/unsoed_logo.png\"\n    footer: \"[irosyadi-2025](https://imron-slide.vercel.app)\"\n    slide-number: true\n    chalkboard: true\n    scrollable: true\n    controls: true\n    progress: true\n    transition: fade\n    theme: [default, qrjs_assets/ir_style.scss]\n    mermaid:\n        theme: neutral\npyodide:\n  packages:\n    - numpy\n    - plotly\n    - nbformat\n---"
  },
  {
    "objectID": "ss_47.html#systems-characterized-by-linear-constant-coefficient-differential-equations",
    "href": "ss_47.html#systems-characterized-by-linear-constant-coefficient-differential-equations",
    "title": "Signals and Systems",
    "section": "Systems Characterized by Linear Constant-Coefficient Differential Equations",
    "text": "Systems Characterized by Linear Constant-Coefficient Differential Equations\nIntroduction to LCCDEs in LTI Systems\n\nLinear Constant-Coefficient Differential Equations (LCCDEs) are fundamental to describing many continuous-time LTI systems.\nThey model physical systems such as electrical circuits, mechanical systems, and more.\nUnderstanding their behavior is crucial for analyzing and designing ECE systems.\n\n\nLCCDEs are a cornerstone for understanding continuous-time LTI systems. They provide a mathematical framework that directly relates the input and output of a system through derivatives. This structure is powerful because it allows us to model a vast array of real-world phenomena, from simple RC circuits to complex control systems. Our goal today is to see how Fourier analysis simplifies working with these equations."
  },
  {
    "objectID": "ss_47.html#general-form-of-an-lccde",
    "href": "ss_47.html#general-form-of-an-lccde",
    "title": "Signals and Systems",
    "section": "General Form of an LCCDE",
    "text": "General Form of an LCCDE\n\nAn LTI system’s input \\(x(t)\\) and output \\(y(t)\\) can be related by the following general form:\n\n\\[\n\\sum_{k=0}^{N} a_{k} \\frac{d^{k} y(t)}{d t^{k}}=\\sum_{k=0}^{M} b_{k} \\frac{d^{k} x(t)}{d t^{k}} \\quad (4.72)\n\\]\n\nHere, \\(a_k\\) and \\(b_k\\) are constant coefficients.\n\\(N\\) represents the order of the differential equation, corresponding to the highest derivative of the output.\n\n\nEquation 4.72 is the canonical form for an LCCDE. The left side involves derivatives of the output \\(y(t)\\), and the right side involves derivatives of the input \\(x(t)\\). The coefficients \\(a_k\\) and \\(b_k\\) are constants, which is why it’s called “constant-coefficient.” The order \\(N\\) is particularly important as it often dictates the complexity and characteristics of the system, such as the number of energy storage elements in a circuit."
  },
  {
    "objectID": "ss_47.html#frequency-response-and-the-fourier-transform",
    "href": "ss_47.html#frequency-response-and-the-fourier-transform",
    "title": "Signals and Systems",
    "section": "Frequency Response and the Fourier Transform",
    "text": "Frequency Response and the Fourier Transform\n\nFor an LTI system, the output’s Fourier Transform \\(Y(j\\omega)\\) relates to the input’s Fourier Transform \\(X(j\\omega)\\) through the frequency response \\(H(j\\omega)\\).\n\n\\[\nY(j \\omega)=H(j \\omega) X(j \\omega)\n\\]\n\nThis implies that the frequency response can be expressed as:\n\n\\[\nH(j \\omega)=\\frac{Y(j \\omega)}{X(j \\omega)} \\quad (4.73)\n\\]\n\nRecall the convolution property: convolution in the time domain becomes multiplication in the frequency domain. This is incredibly powerful for LTI systems. If we know the Fourier transform of the input \\(X(j\\omega)\\) and the system’s frequency response \\(H(j\\omega)\\), we can immediately find the Fourier transform of the output \\(Y(j\\omega)\\) by simple multiplication. Conversely, if we know \\(X(j\\omega)\\) and \\(Y(j\\omega)\\), we can determine \\(H(j\\omega)\\)."
  },
  {
    "objectID": "ss_47.html#deriving-hjomega-from-lccdes-step-1",
    "href": "ss_47.html#deriving-hjomega-from-lccdes-step-1",
    "title": "Signals and Systems",
    "section": "Deriving \\(H(j\\omega)\\) from LCCDEs: Step 1",
    "text": "Deriving \\(H(j\\omega)\\) from LCCDEs: Step 1\n\nTo find \\(H(j\\omega)\\), we apply the Fourier Transform to both sides of the LCCDE (4.72):\n\n\\[\n\\mathcal{F}\\left\\{\\sum_{k=0}^{N} a_{k} \\frac{d^{k} y(t)}{d t^{k}}\\right\\}=\\mathfrak{F}\\left\\{\\sum_{k=0}^{M} b_{k} \\frac{d^{k} x(t)}{d t^{k}}\\right\\} \\quad (4.74)\n\\]\n\nUsing the linearity property of the Fourier Transform, we can move the sum and constants outside:\n\n\\[\n\\sum_{k=0}^{N} a_{k} \\mathcal{F}\\left\\{\\frac{d^{k} y(t)}{d t^{k}}\\right\\}=\\sum_{k=0}^{M} b_{k} \\mathcal{F}\\left\\{\\frac{d^{k} x(t)}{d t^{k}}\\right\\} \\quad (4.75)\n\\]\n\nThe linearity property of the Fourier Transform is key here. It allows us to transform each term in the sum independently and then combine them. This simplifies the process significantly, as we don’t need to transform the entire sum at once."
  },
  {
    "objectID": "ss_47.html#deriving-hjomega-from-lccdes-step-2",
    "href": "ss_47.html#deriving-hjomega-from-lccdes-step-2",
    "title": "Signals and Systems",
    "section": "Deriving \\(H(j\\omega)\\) from LCCDEs: Step 2",
    "text": "Deriving \\(H(j\\omega)\\) from LCCDEs: Step 2\n\nNow, apply the differentiation property of the Fourier Transform, which states \\(\\mathcal{F}\\left\\{\\frac{d^k f(t)}{d t^k}\\right\\} = (j\\omega)^k F(j\\omega)\\):\n\n\\[\n\\sum_{k=0}^{N} a_{k}(j \\omega)^{k} Y(j \\omega)=\\sum_{k=0}^{M} b_{k}(j \\omega)^{k} X(j \\omega)\n\\]\n\nFactor out \\(Y(j\\omega)\\) and \\(X(j\\omega)\\):\n\n\\[\nY(j \\omega)\\left[\\sum_{k=0}^{N} a_{k}(j \\omega)^{k}\\right]=X(j \\omega)\\left[\\sum_{k=0}^{M} b_{k}(j \\omega)^{k}\\right]\n\\]\n\nFinally, solve for \\(H(j\\omega) = Y(j\\omega)/X(j\\omega)\\):\n\n\\[\nH(j \\omega)=\\frac{\\sum_{k=0}^{M} b_{k}(j \\omega)^{k}}{\\sum_{k=0}^{N} a_{k}(j \\omega)^{k}} \\quad (4.76)\n\\]\n\n\n\n\n\n\nTip\n\n\nThe frequency response \\(H(j\\omega)\\) for an LCCDE can be written directly by inspection from the differential equation coefficients!\n\n\n\n\nThis is the most crucial step. The differentiation property transforms time-domain derivatives into simple multiplications by powers of \\(j\\omega\\) in the frequency domain. This converts a differential equation into an algebraic equation. The resulting \\(H(j\\omega)\\) is a rational function, meaning a ratio of polynomials in \\(j\\omega\\). The coefficients of these polynomials are precisely the coefficients \\(b_k\\) and \\(a_k\\) from the original differential equation. This direct relationship is incredibly powerful for analysis."
  },
  {
    "objectID": "ss_47.html#lccde-to-frequency-response-flowchart",
    "href": "ss_47.html#lccde-to-frequency-response-flowchart",
    "title": "Signals and Systems",
    "section": "LCCDE to Frequency Response: Flowchart",
    "text": "LCCDE to Frequency Response: Flowchart\n\nVisualizing the process of converting an LCCDE into its frequency response.\n\n\n\n\n\n\ngraph LR\n    A[\"Linear Constant-Coefficient Differential Equation (LCCDE)\"] --&gt; B{Apply Fourier Transform to both sides};\n    B --&gt; C{Use Linearity Property};\n    C --&gt; D{Use Differentiation Property};\n    D --&gt; E{Rearrange Algebraically};\n    E --&gt; F[\"Frequency Response H(jω) = Y(jω)/X(jω)\"];\n\n\n\n\n\n\n\nThis flowchart summarizes the elegant path from a time-domain differential equation to its frequency-domain representation. Each step relies on fundamental properties of the Fourier Transform, transforming a complex calculus problem into a straightforward algebraic one. This is why Fourier analysis is so powerful in Signals and Systems."
  },
  {
    "objectID": "ss_47.html#example-4.24-first-order-system",
    "href": "ss_47.html#example-4.24-first-order-system",
    "title": "Signals and Systems",
    "section": "Example 4.24: First-Order System",
    "text": "Example 4.24: First-Order System\n\nConsider a stable LTI system described by:\n\n\\[\n\\frac{d y(t)}{d t}+a y(t)=x(t) \\quad (4.77)\n\\]\n\nUsing equation (4.76), we can directly find the frequency response.\nComparing with the general form, \\(N=1, M=0\\).\nCoefficients: \\(a_1=1, a_0=a\\), and \\(b_0=1\\).\nSubstitute into (4.76):\n\n\\[\nH(j \\omega)=\\frac{b_0 (j\\omega)^0}{a_1 (j\\omega)^1 + a_0 (j\\omega)^0} = \\frac{1}{j \\omega+a} \\quad (4.78)\n\\]\n\nThe impulse response \\(h(t)\\) is the inverse Fourier Transform of \\(H(j\\omega)\\).\nRecognizing this form (from Example 4.1), the impulse response is:\n\n\\[\nh(t)=e^{-a t} u(t)\n\\]\n\nFor stability, we require \\(a&gt;0\\).\n\n\nThis example demonstrates the direct application of the formula. For a first-order system, we have derivatives up to order 1 for the output and order 0 for the input. By simply plugging in the coefficients, we instantly get the frequency response. This particular form of \\(H(j\\omega)\\) is very common and its inverse Fourier transform, \\(e^{-at}u(t)\\), is a fundamental signal in ECE. The condition \\(a&gt;0\\) ensures that the impulse response decays over time, leading to a stable system."
  },
  {
    "objectID": "ss_47.html#interactive-plot-first-order-impulse-response",
    "href": "ss_47.html#interactive-plot-first-order-impulse-response",
    "title": "Signals and Systems",
    "section": "Interactive Plot: First-Order Impulse Response",
    "text": "Interactive Plot: First-Order Impulse Response\n\nExplore how the parameter ‘a’ affects the impulse response \\(h(t) = e^{-at}u(t)\\).\n\n\nviewof a_slider = Inputs.range([0.1, 5.0], {label: \"Parameter 'a'\", step: 0.1, value: 1.0})\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive plot helps visualize the impact of the parameter ‘a’ on the system’s impulse response. As you increase ‘a’, the exponential decay becomes faster, meaning the system responds more quickly and settles sooner. This corresponds to a wider bandwidth in the frequency domain. Conversely, a smaller ‘a’ leads to a slower, more spread-out response in the time domain, indicating a narrower bandwidth. This directly relates to the system’s time constant, which is \\(1/a\\)."
  },
  {
    "objectID": "ss_47.html#example-4.25-second-order-system",
    "href": "ss_47.html#example-4.25-second-order-system",
    "title": "Signals and Systems",
    "section": "Example 4.25: Second-Order System",
    "text": "Example 4.25: Second-Order System\n\nConsider a stable LTI system characterized by:\n\n\\[\n\\frac{d^{2} y(t)}{d t^{2}}+4 \\frac{d y(t)}{d t}+3 y(t)=\\frac{d x(t)}{d t}+2 x(t)\n\\]\n\nFrom eq. (4.76), the frequency response is:\n\n\\[\nH(j \\omega)=\\frac{(j \\omega)+2}{(j \\omega)^{2}+4(j \\omega)+3} \\quad (4.79)\n\\]\n\nTo find the impulse response \\(h(t)\\), we use partial-fraction expansion.\nFirst, factor the denominator:\n\n\\[\nH(j \\omega)=\\frac{j \\omega+2}{(j \\omega+1)(j \\omega+3)} \\quad (4.80)\n\\]\n\nHere we have a second-order system because the highest derivative of \\(y(t)\\) is two. The process to find \\(H(j\\omega)\\) remains the same: identify coefficients and plug them into the formula. However, finding the inverse Fourier Transform of this \\(H(j\\omega)\\) is not as direct as the first-order case. This is where partial-fraction expansion becomes an indispensable tool. It allows us to break down complex rational functions into simpler terms whose inverse transforms are known. Factoring the denominator is the first critical step in this process."
  },
  {
    "objectID": "ss_47.html#partial-fraction-expansion-process",
    "href": "ss_47.html#partial-fraction-expansion-process",
    "title": "Signals and Systems",
    "section": "Partial-Fraction Expansion Process",
    "text": "Partial-Fraction Expansion Process\n\nA visual representation of how partial-fraction expansion helps find \\(h(t)\\) from \\(H(j\\omega)\\).\n\n\n\n\n\n\ngraph TD\n    A[\"H(jω) as Ratio of Polynomials\"] --&gt; B{Factor Denominator Polynomial};\n    B --&gt; C{Apply Partial-Fraction Expansion};\n    C --&gt; D[\"Sum of Simpler Terms (e.g., 1/(jω+a))\"];\n    D --&gt; E{Find Inverse Fourier Transform for Each Term};\n    E --&gt; F[\"Impulse Response h(t)\"];\n\n\n\n\n\n\n\nThis flowchart outlines the general procedure for finding the impulse response when \\(H(j\\omega)\\) is a rational function. Partial-fraction expansion is a technique taught in calculus and is heavily used in signals and systems to simplify complex expressions into forms that are easily invertible using a table of Fourier Transform pairs."
  },
  {
    "objectID": "ss_47.html#impulse-response-for-example-4.25",
    "href": "ss_47.html#impulse-response-for-example-4.25",
    "title": "Signals and Systems",
    "section": "Impulse Response for Example 4.25",
    "text": "Impulse Response for Example 4.25\n\nApplying partial-fraction expansion to \\(H(j\\omega)\\):\n\n\\[\nH(j \\omega)=\\frac{\\frac{1}{2}}{j \\omega+1}+\\frac{\\frac{1}{2}}{j \\omega+3} .\n\\]\n\nRecognizing the inverse transforms of each term:\n\n\\[\nh(t)=\\frac{1}{2} e^{-t} u(t)+\\frac{1}{2} e^{-3 t} u(t)\n\\]\n\n\n\n\n\n\nImportant\n\n\nPartial-fraction expansion is a critical technique for transforming frequency-domain expressions into their time-domain equivalents!\n\n\n\n\nAfter performing the partial-fraction expansion, we get two simpler terms, each resembling the frequency response from Example 4.24. We can then easily find the inverse Fourier transform of each term and sum them up to get the total impulse response \\(h(t)\\). This illustrates the power of breaking down a complex problem into simpler, manageable parts."
  },
  {
    "objectID": "ss_47.html#example-4.26-system-response-to-an-input",
    "href": "ss_47.html#example-4.26-system-response-to-an-input",
    "title": "Signals and Systems",
    "section": "Example 4.26: System Response to an Input",
    "text": "Example 4.26: System Response to an Input\n\nConsider the system from Example 4.25 with the input:\n\n\\[\nx(t)=e^{-t} u(t)\n\\]\n\nIts Fourier Transform is \\(X(j\\omega) = \\frac{1}{j\\omega+1}\\).\nThe output’s Fourier Transform \\(Y(j\\omega) = H(j\\omega)X(j\\omega)\\):\n\n\\[\n\\begin{align*}\nY(j \\omega) & =H(j \\omega) X(j \\omega)=\\left[\\frac{j \\omega+2}{(j \\omega+1)(j \\omega+3)}\\right]\\left[\\frac{1}{j \\omega+1}\\right] \\\\\n& =\\frac{j \\omega+2}{(j \\omega+1)^{2}(j \\omega+3)} \\quad (4.81)\n\\end{align*}\n\\]\n\nNow we’re moving from finding the impulse response to finding the system’s output for a specific input. The process is similar: multiply \\(H(j\\omega)\\) by \\(X(j\\omega)\\) to get \\(Y(j\\omega)\\). Notice that the denominator now has a repeated pole at \\(j\\omega = -1\\). This requires a slightly modified partial-fraction expansion technique, as discussed in the appendix."
  },
  {
    "objectID": "ss_47.html#partial-fraction-for-yjomega",
    "href": "ss_47.html#partial-fraction-for-yjomega",
    "title": "Signals and Systems",
    "section": "Partial-Fraction for \\(Y(j\\omega)\\)",
    "text": "Partial-Fraction for \\(Y(j\\omega)\\)\n\nFor repeated poles, the partial-fraction expansion takes a specific form:\n\n\\[\nY(j \\omega)=\\frac{A_{11}}{j \\omega+1}+\\frac{A_{12}}{(j \\omega+1)^{2}}+\\frac{A_{21}}{j \\omega+3} \\quad (4.82)\n\\]\n\nThe constants are found to be:\n\n\\(A_{11}=\\frac{1}{4}\\)\n\\(A_{12}=\\frac{1}{2}\\)\n\\(A_{21}=-\\frac{1}{4}\\)\n\nSo, \\(Y(j\\omega)\\) becomes:\n\n\\[\nY(j \\omega)=\\frac{\\frac{1}{4}}{j \\omega+1}+\\frac{\\frac{1}{2}}{(j \\omega+1)^{2}}-\\frac{\\frac{1}{4}}{j \\omega+3} \\quad (4.83)\n\\]\n\nThe presence of a repeated pole \\((j\\omega+1)^2\\) means we need two terms in the partial fraction expansion for that pole: one with \\((j\\omega+1)\\) in the denominator and one with \\((j\\omega+1)^2\\). The coefficients \\(A_{11}, A_{12}, A_{21}\\) are determined using standard partial-fraction expansion techniques, usually involving solving a system of linear equations or specific evaluation methods."
  },
  {
    "objectID": "ss_47.html#output-signal-yt-for-example-4.26",
    "href": "ss_47.html#output-signal-yt-for-example-4.26",
    "title": "Signals and Systems",
    "section": "Output Signal \\(y(t)\\) for Example 4.26",
    "text": "Output Signal \\(y(t)\\) for Example 4.26\n\nTaking the inverse Fourier Transform of each term in \\(Y(j\\omega)\\) (Eq. 4.83):\n\n\\[\ny(t)=\\left[\\frac{1}{4} e^{-t}+\\frac{1}{2} t e^{-t}-\\frac{1}{4} e^{-3 t}\\right] u(t)\n\\]\n\nThe term \\(\\frac{1}{(j\\omega+1)^2}\\) corresponds to \\(t e^{-t} u(t)\\) in the time domain.\n\n\nEach term in the partial fraction expansion of \\(Y(j\\omega)\\) has a known inverse Fourier transform. The first and third terms are simple exponentials. The second term, \\(\\frac{1/2}{(j\\omega+1)^2}\\), is a common Fourier transform pair that can be derived using the differentiation property in the frequency domain or found in transform tables. The \\(t e^{-t} u(t)\\) term indicates a response that initially grows before decaying, a characteristic often seen in systems with repeated poles or specific damping conditions."
  },
  {
    "objectID": "ss_47.html#interactive-plot-input-impulse-response-and-output",
    "href": "ss_47.html#interactive-plot-input-impulse-response-and-output",
    "title": "Signals and Systems",
    "section": "Interactive Plot: Input, Impulse Response, and Output",
    "text": "Interactive Plot: Input, Impulse Response, and Output\n\nVisualize \\(x(t)\\), \\(h(t)\\), and \\(y(t)\\) for Example 4.26.\n\n\n\n\n\n\n\n\nThis plot visually connects the input, the system’s impulse response, and the resulting output in the time domain. Notice how the output \\(y(t)\\) is a ‘smoothed’ or ‘filtered’ version of the input, influenced by the characteristics of the impulse response. The shape of \\(y(t)\\) shows the system’s transient behavior, demonstrating how the system responds to the sudden onset of the input. This interaction highlights the convolution operation in the time domain, which is simplified to multiplication in the frequency domain."
  },
  {
    "objectID": "ss_47.html#summary-and-key-takeaways",
    "href": "ss_47.html#summary-and-key-takeaways",
    "title": "Signals and Systems",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\n\nAlgebraic Simplification: Fourier analysis transforms LCCDEs into algebraic equations in the frequency domain.\nDirect Derivation: \\(H(j\\omega)\\) can be directly obtained by inspection from the LCCDE coefficients.\nPartial-Fraction Expansion: A powerful technique for finding time-domain responses from rational frequency-domain expressions.\nFrequency-Domain Insights: Analyzing \\(H(j\\omega)\\) provides deep understanding of system characteristics (e.g., stability, filtering properties).\n\n\nIn conclusion, the Fourier Transform provides an incredibly powerful framework for analyzing LTI systems described by LCCDEs. It converts complex differential equations into simple algebraic ones, allowing us to easily determine frequency responses, impulse responses, and system outputs. This frequency-domain perspective is invaluable for understanding system behavior and forms the basis for many design principles in ECE."
  },
  {
    "objectID": "ss_44.html#the-convolution-property",
    "href": "ss_44.html#the-convolution-property",
    "title": "Signals and Systems",
    "section": "The Convolution Property",
    "text": "The Convolution Property\n4.4 The Convolution Property: Frequency Domain Perspective\nUnderstanding how LTI systems interact with signals is fundamental in Electrical and Computer Engineering. The convolution property establishes a powerful link between the time and frequency domains.\n\nThis lecture introduces one of the most important properties in Signals and Systems: the convolution property. It’s a cornerstone for analyzing LTI systems, allowing us to simplify complex time-domain operations by moving to the frequency domain. We’ll start by building intuition from Fourier series and then move to a formal derivation."
  },
  {
    "objectID": "ss_44.html#from-fourier-series-to-fourier-transform",
    "href": "ss_44.html#from-fourier-series-to-fourier-transform",
    "title": "Signals and Systems",
    "section": "From Fourier Series to Fourier Transform",
    "text": "From Fourier Series to Fourier Transform\nRecall that for periodic signals, the output of an LTI system is simply the Fourier series coefficients of the input multiplied by the system’s frequency response.\n\\[\n\\begin{equation*}\nx(t)=\\frac{1}{2 \\pi} \\int_{-\\infty}^{+\\infty} X(j \\omega) e^{j \\omega t} d \\omega=\\lim _{\\omega_{0} \\rightarrow 0} \\frac{1}{2 \\pi} \\sum_{k=-\\infty}^{+\\infty} X\\left(j k \\omega_{0}\\right) e^{j k \\omega_{0} t} \\omega_{0} \\tag{4.47}\n\\end{equation*}\n\\]\nThe response of an LTI system to a complex exponential \\(e^{j k \\omega_{0} t}\\) is \\(H\\left(j k \\omega_{0}\\right) e^{j k \\omega_{0} t}\\). Where \\(H(j \\omega)\\) is the Fourier Transform of the impulse response \\(h(t)\\).\n\nWe’re building on the intuition we developed for periodic signals in Chapter 3. Remember that complex exponentials are eigenfunctions of LTI systems. This means if you put \\(e^{j\\omega t}\\) into an LTI system, the output is simply \\(H(j\\omega)e^{j\\omega t}\\), where \\(H(j\\omega)\\) is the system’s frequency response. Equation 4.47 shows how an aperiodic signal \\(x(t)\\) can be represented as a continuous sum (an integral) of these complex exponentials. This is essentially the Fourier Transform synthesis equation. The key idea here is that because an LTI system scales each individual exponential component, we can extend this concept to the continuous spectrum of aperiodic signals."
  },
  {
    "objectID": "ss_44.html#intuitive-derivation-of-the-convolution-property",
    "href": "ss_44.html#intuitive-derivation-of-the-convolution-property",
    "title": "Signals and Systems",
    "section": "Intuitive Derivation of the Convolution Property",
    "text": "Intuitive Derivation of the Convolution Property\nApplying the superposition principle, each exponential component in the input \\(x(t)\\) is scaled by the frequency response.\n\\[\n\\frac{1}{2 \\pi} \\sum_{k=-\\infty}^{+\\infty} X\\left(j k \\omega_{0}\\right) e^{j k \\omega_{0} t} \\omega_{0} \\longrightarrow \\frac{1}{2 \\pi} \\sum_{k=-\\infty}^{+\\infty} X\\left(j k \\omega_{0}\\right) H\\left(j k \\omega_{0}\\right) e^{j k \\omega_{0} t} \\omega_{0}\n\\]\nTaking the limit as \\(\\omega_0 \\rightarrow 0\\), the output \\(y(t)\\) is:\n\\[\n\\begin{align*}\ny(t) & =\\lim _{\\omega_{0} \\rightarrow 0} \\frac{1}{2 \\pi} \\sum_{k=-\\infty}^{+\\infty} X\\left(j k \\omega_{0}\\right) H\\left(j k \\omega_{0}\\right) e^{j k \\omega_{0} t} \\omega_{0}  \\tag{4.49}\\\\\n& =\\frac{1}{2 \\pi} \\int_{-\\infty}^{+\\infty} X(j \\omega) H(j \\omega) e^{j \\omega t} d \\omega .\n\\end{align*}\n\\]\nBy definition, the inverse Fourier Transform of \\(Y(j\\omega)\\) is \\(y(t)\\):\n\\[\n\\begin{equation*}\ny(t)=\\frac{1}{2 \\pi} \\int_{-\\infty}^{+\\infty} Y(j \\omega) e^{j \\omega t} d \\omega \\tag{4.50}\n\\end{equation*}\n\\]\nComparing these, we arrive at the convolution property:\n\\[\n\\begin{equation*}\nY(j \\omega)=X(j \\omega) H(j \\omega) \\tag{4.51}\n\\end{equation*}\n\\]\n\nThis slide shows the intuitive leap from discrete Fourier series to the continuous Fourier Transform. The first equation represents the input signal as a sum of scaled exponentials. The arrow indicates that an LTI system scales each of these components by its frequency response \\(H(j k \\omega_0)\\). As the spacing between frequencies \\(\\omega_0\\) approaches zero, the sum becomes an integral. By comparing the resulting expression for \\(y(t)\\) (Equation 4.49) with the general definition of an inverse Fourier Transform (Equation 4.50), we can directly infer that the Fourier Transform of the output, \\(Y(j\\omega)\\), must be the product of the input’s Fourier Transform \\(X(j\\omega)\\) and the system’s frequency response \\(H(j\\omega)\\)."
  },
  {
    "objectID": "ss_44.html#formal-derivation",
    "href": "ss_44.html#formal-derivation",
    "title": "Signals and Systems",
    "section": "Formal Derivation",
    "text": "Formal Derivation\nStarting from the convolution integral, we can formally derive the property.\n\\[\n\\begin{equation*}\ny(t)=\\int_{-\\infty}^{+\\infty} x(\\tau) h(t-\\tau) d \\tau \\tag{4.52}\n\\end{equation*}\n\\]\nTaking the Fourier Transform of \\(y(t)\\):\n\\[\n\\begin{equation*}\nY(j \\omega)=\\mathcal{F}\\{y(t)\\}=\\int_{-\\infty}^{+\\infty}\\left[\\int_{-\\infty}^{+\\infty} x(\\tau) h(t-\\tau) d \\tau\\right] e^{-j \\omega t} d t \\tag{4.53}\n\\end{equation*}\n\\]\nInterchanging integration order and using the time-shift property \\(\\mathcal{F}\\{h(t-\\tau)\\} = H(j\\omega)e^{-j\\omega\\tau}\\):\n\\[\n\\begin{equation*}\nY(j \\omega)=\\int_{-\\infty}^{+\\infty} x(\\tau)\\left[\\int_{-\\infty}^{+\\infty} h(t-\\tau) e^{-j \\omega t} d t\\right] d \\tau = \\int_{-\\infty}^{+\\infty} x(\\tau) e^{-j \\omega \\tau} H(j \\omega) d \\tau \\tag{4.54}\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "ss_44.html#formal-derivation-1",
    "href": "ss_44.html#formal-derivation-1",
    "title": "Signals and Systems",
    "section": "Formal Derivation",
    "text": "Formal Derivation\nFactoring out \\(H(j\\omega)\\):\n\\[\n\\begin{equation*}\nY(j \\omega)=H(j \\omega) \\int_{-\\infty}^{+\\infty} x(\\tau) e^{-j \\omega \\tau} d \\tau = H(j \\omega) X(j \\omega) \\tag{4.55}\n\\end{equation*}\n\\]\nThus, the convolution property is:\n\\[\n\\begin{equation*}\ny(t)=h(t) * x(t) \\stackrel{\\mathfrak{F}}{\\longleftrightarrow} Y(j \\omega)=H(j \\omega) X(j \\omega) . \\tag{4.56}\n\\end{equation*}\n\\]\n\n\n\n\n\n\nImportant\n\n\nThis property is fundamental! It transforms complex convolution operations in the time domain into simple multiplication in the frequency domain.\n\n\n\n\nThe formal derivation provides a rigorous proof, starting directly from the definition of convolution.\nFirst, we apply the Fourier Transform definition to the output \\(y(t)\\), substituting the convolution integral.\nNext, we interchange the order of integration, which is a common technique in Fourier analysis.\nThe crucial step is recognizing that the inner integral, \\(\\int_{-\\infty}^{+\\infty} h(t-\\tau) e^{-j \\omega t} d t\\), is the Fourier Transform of a time-shifted impulse response, \\(h(t-\\tau)\\). By the time-shift property of the Fourier Transform, this equals \\(H(j\\omega)e^{-j\\omega\\tau}\\).\nFinally, we factor out \\(H(j\\omega)\\), leaving the Fourier Transform of \\(x(\\tau)\\), which is \\(X(j\\omega)\\). This directly leads to \\(Y(j\\omega) = H(j\\omega)X(j\\omega)\\). This derivation solidifies the intuitive understanding and provides a powerful tool for analyzing LTI systems."
  },
  {
    "objectID": "ss_44.html#significance-of-the-convolution-property",
    "href": "ss_44.html#significance-of-the-convolution-property",
    "title": "Signals and Systems",
    "section": "Significance of the Convolution Property",
    "text": "Significance of the Convolution Property\nThe convolution property is a cornerstone of LTI system analysis. \\(H(j \\omega)\\), the Fourier transform of the impulse response \\(h(t)\\), is the frequency response of the system.\n\nIt completely characterizes an LTI system, just like \\(h(t)\\).\nIt describes how the system scales and shifts the phase of different frequency components.\nCrucial for frequency-selective filtering, where certain frequency bands are passed and others are attenuated.\n\nThe overall frequency response of cascaded LTI systems is the product of their individual frequency responses, regardless of order.\n\n\n\n\n\ngraph LR\n    subgraph System 1\n        A[\"Input X(jω)\"] --&gt; B(\"H1(jω)\")\n    end\n    subgraph System 2\n        C[\"Output Y1(jω)\"] --&gt; D(\"H2(jω)\")\n    end\n    B --&gt; C\n    D --&gt; E[\"Output Y(jω)\"]\n\n\n\n\n\n\n\nThe frequency response \\(H(j\\omega)\\) is a complex-valued function. Its magnitude, \\(|H(j\\omega)|\\), tells us how much the amplitude of a sinusoidal component at frequency \\(\\omega\\) is scaled by the system. Its phase, \\(\\angle H(j\\omega)\\), tells us how much the phase of that component is shifted.\nUnderstanding \\(H(j\\omega)\\) is essential for designing filters to pass or block specific frequency ranges.\nThe Mermaid diagram illustrates two LTI systems connected in cascade. If the first system has frequency response \\(H_1(j\\omega)\\) and the second has \\(H_2(j\\omega)\\), then the output of the first is \\(Y_1(j\\omega) = X(j\\omega)H_1(j\\omega)\\). This becomes the input to the second system, so \\(Y(j\\omega) = Y_1(j\\omega)H_2(j\\omega) = X(j\\omega)H_1(j\\omega)H_2(j\\omega)\\).\nThis means the overall frequency response of the cascade is \\(H_1(j\\omega)H_2(j\\omega)\\). Since multiplication is commutative (\\(H_1H_2 = H_2H_1\\)), the order in which LTI systems are cascaded does not affect the overall frequency response. This is a powerful consequence of the convolution property."
  },
  {
    "objectID": "ss_44.html#stability-and-existence-of-hjomega",
    "href": "ss_44.html#stability-and-existence-of-hjomega",
    "title": "Signals and Systems",
    "section": "Stability and Existence of \\(H(j\\omega)\\)",
    "text": "Stability and Existence of \\(H(j\\omega)\\)\nThe Fourier Transform \\(H(j\\omega)\\) does not exist for every LTI system.\nFor a stable LTI system, its impulse response \\(h(t)\\) must be absolutely integrable:\n\\[\n\\begin{equation*}\n\\int_{-\\infty}^{+\\infty}|h(t)| d t&lt;\\infty \\tag{4.57}\n\\end{equation*}\n\\]\nThis condition is one of the Dirichlet conditions required for the existence of the Fourier Transform.\n\n\n\n\n\n\nNote\n\n\nA stable LTI system always has a frequency response \\(H(j\\omega)\\).\nThis is a crucial link between time-domain stability and frequency-domain analysis.\n\n\n\nFor unstable LTI systems, a generalization called the Laplace Transform is used.\n\nThis slide addresses an important theoretical point regarding the applicability of Fourier Transforms to LTI systems.\nThe absolute integrability condition (Equation 4.57) ensures that the integral defining the Fourier Transform converges. If an LTI system is stable, its impulse response will decay over time, making it absolutely integrable.\nTherefore, for any stable LTI system, we are guaranteed to be able to find its frequency response \\(H(j\\omega)\\).\nThis means that Fourier analysis is a suitable tool for analyzing the behavior of stable LTI systems. For systems that are not stable, we need a more general transform, such as the Laplace Transform, which allows for poles in the right-half plane."
  },
  {
    "objectID": "ss_44.html#example-4.15-time-shift",
    "href": "ss_44.html#example-4.15-time-shift",
    "title": "Signals and Systems",
    "section": "Example 4.15: Time Shift",
    "text": "Example 4.15: Time Shift\nConsider an LTI system with impulse response \\(h(t)=\\delta\\left(t-t_{0}\\right)\\).\n\\[\n\\begin{equation*}\nh(t)=\\delta\\left(t-t_{0}\\right) \\tag{4.58}\n\\end{equation*}\n\\]\nThe frequency response \\(H(j\\omega)\\) is the Fourier Transform of \\(h(t)\\):\n\\[\n\\begin{equation*}\nH(j \\omega)=e^{-j \\omega t_{0}} . \\tag{4.59}\n\\end{equation*}\n\\]\nThus, the Fourier Transform of the output \\(y(t)\\) is:\n\\[\n\\begin{align*}\nY(j \\omega) & =H(j \\omega) X(j \\omega)  \\tag{4.60}\\\\\n& =e^{-j \\omega t_{0}} X(j \\omega) .\n\\end{align*}\n\\]\nThis is consistent with the time-shift property: \\(y(t) = x(t-t_0)\\).\nA pure time shift system has unity magnitude response \\(|H(j\\omega)|=1\\) and a linear phase \\(-\\omega t_0\\).\n\nThis example is a perfect illustration of the convolution property in action.\nA system that simply shifts its input by \\(t_0\\) has an impulse response that is a shifted Dirac delta function.\nWhen we take the Fourier Transform of this impulse response, we get \\(e^{-j\\omega t_0}\\).\nApplying the convolution property, \\(Y(j\\omega) = X(j\\omega)e^{-j\\omega t_0}\\), which perfectly matches the time-shift property we learned earlier.\nNotice that the magnitude of \\(H(j\\omega)\\) is always 1, meaning all frequency components are passed without attenuation. The phase, however, is linear with frequency, \\(-\\omega t_0\\). This linear phase ensures that all frequency components are delayed by the same amount, preserving the signal’s shape without distortion."
  },
  {
    "objectID": "ss_44.html#interactive-time-shift",
    "href": "ss_44.html#interactive-time-shift",
    "title": "Signals and Systems",
    "section": "Interactive Time Shift",
    "text": "Interactive Time Shift\nAdjust the time shift \\(t_0\\) and observe its effect on the signal and its frequency response.\n\nviewof t0_shift = Inputs.range([-2, 2], {step: 0.1, value: 0.5, label: \"Time Shift (t0)\"})"
  },
  {
    "objectID": "ss_44.html#example-4.16-differentiator",
    "href": "ss_44.html#example-4.16-differentiator",
    "title": "Signals and Systems",
    "section": "Example 4.16: Differentiator",
    "text": "Example 4.16: Differentiator\nFor a differentiator, \\(y(t) = \\frac{d x(t)}{d t}\\).\nFrom the differentiation property:\n\\[\n\\begin{equation*}\nY(j \\omega)=j \\omega X(j \\omega) \\tag{4.61}\n\\end{equation*}\n\\]\nTherefore, the frequency response of a differentiator is:\n\\[\n\\begin{equation*}\nH(j \\omega)=j \\omega \\tag{4.62}\n\\end{equation*}\n\\]\n\nThis example shows that the differentiation operation in the time domain corresponds to a simple multiplication by \\(j\\omega\\) in the frequency domain.\nThe magnitude response \\(|H(j\\omega)| = |\\omega|\\) indicates that a differentiator amplifies higher frequencies linearly. This is why differentiation can make a signal “noisier” if high-frequency noise is present, as it enhances those components.\nThe phase response \\(\\angle H(j\\omega)\\) is \\(\\pi/2\\) for \\(\\omega &gt; 0\\) and \\(-\\pi/2\\) for \\(\\omega &lt; 0\\). This constant phase shift means that all frequency components are phase-shifted by 90 degrees, which is characteristic of differentiation."
  },
  {
    "objectID": "ss_44.html#interactive-differentiator-frequency-response",
    "href": "ss_44.html#interactive-differentiator-frequency-response",
    "title": "Signals and Systems",
    "section": "Interactive Differentiator Frequency Response",
    "text": "Interactive Differentiator Frequency Response\nObserve the magnitude and phase response of an ideal differentiator."
  },
  {
    "objectID": "ss_44.html#example-4.17-integrator",
    "href": "ss_44.html#example-4.17-integrator",
    "title": "Signals and Systems",
    "section": "Example 4.17: Integrator",
    "text": "Example 4.17: Integrator\nAn integrator is an LTI system where \\(y(t)=\\int_{-\\infty}^{t} x(\\tau) d \\tau\\).\nIts impulse response is the unit step \\(u(t)\\).\nThe frequency response \\(H(j\\omega)\\) is:\n\\[\nH(j \\omega)=\\frac{1}{j \\omega}+\\pi \\delta(\\omega)\n\\]\nUsing the convolution property, the output transform is:\n\\[\n\\begin{aligned}\nY(j \\omega) & =H(j \\omega) X(j \\omega) \\\\\n& =\\frac{1}{j \\omega} X(j \\omega)+\\pi X(j \\omega) \\delta(\\omega) \\\\\n& =\\frac{1}{j \\omega} X(j \\omega)+\\pi X(0) \\delta(\\omega)\n\\end{aligned}\n\\]\nThis is consistent with the integration property.\n\nIn contrast to differentiation, integration in the time domain corresponds to division by \\(j\\omega\\) in the frequency domain, along with a term related to the input’s DC component.\nThe magnitude response \\(|H(j\\omega)| = 1/|\\omega|\\) shows that an integrator attenuates higher frequencies and amplifies lower frequencies. This makes it a lowpass-like filter, often used for smoothing signals.\nThe phase response \\(\\angle H(j\\omega)\\) is \\(-\\pi/2\\) for \\(\\omega &gt; 0\\) and \\(\\pi/2\\) for \\(\\omega &lt; 0\\), a constant -90 degree phase shift.\nThe \\(\\pi X(0) \\delta(\\omega)\\) term is important because the integral of a signal can have a DC component even if the original signal has none. This term ensures that the DC value of the output is correctly handled, especially when \\(X(0)\\) is non-zero."
  },
  {
    "objectID": "ss_44.html#interactive-integrator-frequency-response",
    "href": "ss_44.html#interactive-integrator-frequency-response",
    "title": "Signals and Systems",
    "section": "Interactive Integrator Frequency Response",
    "text": "Interactive Integrator Frequency Response\nObserve the magnitude and phase response of an ideal integrator."
  },
  {
    "objectID": "ss_44.html#example-4.18-ideal-lowpass-filter",
    "href": "ss_44.html#example-4.18-ideal-lowpass-filter",
    "title": "Signals and Systems",
    "section": "Example 4.18: Ideal Lowpass Filter",
    "text": "Example 4.18: Ideal Lowpass Filter\nAn ideal lowpass filter passes frequencies below a cutoff \\(\\omega_c\\) and blocks others.\n\\[\nH(j \\omega)= \\begin{cases}1 & |\\omega|&lt;\\omega_{c}  \\tag{4.63}\\\\ 0 & |\\omega|&gt;\\omega_{c}\\end{cases}\n\\]\nThe impulse response \\(h(t)\\) of this filter is the inverse Fourier Transform of \\(H(j\\omega)\\):\n\\[\n\\begin{equation*}\nh(t)=\\frac{\\sin \\omega_{c} t}{\\pi t} \\tag{4.64}\n\\end{equation*}\n\\]\n\n\n\n\n\n\nWarning\n\n\nThe ideal lowpass filter is non-causal (\\(h(t)\\) is not zero for \\(t&lt;0\\)) and its impulse response has oscillatory behavior that extends infinitely in time.\nThis makes it impossible to realize perfectly in practice.\n\n\n\n\nThe ideal lowpass filter is a theoretical concept that is highly desired for its perfect frequency selectivity—it completely passes frequencies within a band and completely blocks those outside.\nHowever, when we find its impulse response, \\(h(t)\\), it turns out to be a sinc function. This sinc function has two major practical drawbacks:\n\nNon-causality: It is non-zero for \\(t &lt; 0\\), meaning the filter output would depend on future input values, which is impossible for real-time physical systems.\nInfinite Duration and Oscillations: The impulse response extends infinitely in time and exhibits oscillatory behavior (ringing). This can cause undesirable artifacts in the time domain, such as overshoot and undershoot, especially when processing signals with sharp transitions. This example highlights a fundamental trade-off in filter design between ideal frequency-domain characteristics and practical time-domain constraints."
  },
  {
    "objectID": "ss_44.html#interactive-ideal-lowpass-filter",
    "href": "ss_44.html#interactive-ideal-lowpass-filter",
    "title": "Signals and Systems",
    "section": "Interactive Ideal Lowpass Filter",
    "text": "Interactive Ideal Lowpass Filter\nAdjust the cutoff frequency \\(\\omega_c\\) and observe the frequency and impulse responses.\n\nviewof wc_ideal = Inputs.range([1, 10], {step: 0.1, value: 3, label: \"Cutoff Frequency (ωc)\"})"
  },
  {
    "objectID": "ss_44.html#example-4.18-practical-lowpass-filter",
    "href": "ss_44.html#example-4.18-practical-lowpass-filter",
    "title": "Signals and Systems",
    "section": "Example 4.18: Practical Lowpass Filter",
    "text": "Example 4.18: Practical Lowpass Filter\nConsider a more practical LTI system with impulse response \\(h(t)=e^{-at}u(t)\\).\n\\[\n\\begin{equation*}\nh(t)=e^{-t} u(t) \\tag{4.65}\n\\end{equation*}\n\\]\nThe frequency response of this system (an RC circuit) is:\n\\[\n\\begin{equation*}\nH(j \\omega)=\\frac{1}{j \\omega+1} \\tag{4.66}\n\\end{equation*}\n\\]\nCharacteristics:\n\nCausal: \\(h(t)=0\\) for \\(t&lt;0\\).\nNo Oscillations: Impulse response decays monotonically.\nEasier to implement: Simple RC circuit (first-order system).\nTrade-off: Less sharp frequency selectivity compared to ideal."
  },
  {
    "objectID": "ss_44.html#interactive-rc-filter",
    "href": "ss_44.html#interactive-rc-filter",
    "title": "Signals and Systems",
    "section": "Interactive RC Filter",
    "text": "Interactive RC Filter\nAdjust the parameter ‘a’ and observe the impulse and magnitude responses.\n\nviewof a_rc = Inputs.range([0.1, 5], {step: 0.1, value: 1, label: \"Parameter 'a'\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis example contrasts the ideal lowpass filter with a practical, realizable filter. The system with impulse response \\(h(t) = e^{-at}u(t)\\) is causal because it is zero for \\(t &lt; 0\\). Its impulse response decays monotonically without oscillations, which is often desirable in applications to avoid ringing artifacts. The frequency response, \\(H(j\\omega) = 1/(a+j\\omega)\\), is that of a first-order lowpass filter, commonly implemented with an RC circuit. While it doesn’t have the “brick-wall” cutoff of an ideal filter, its practical advantages often outweigh the less-than-ideal frequency selectivity. The parameter ‘a’ controls the filter’s bandwidth: a larger ‘a’ means a faster decay in the time domain and a wider bandwidth (higher cutoff frequency) in the frequency domain. This demonstrates the trade-offs engineers often face in filter design."
  },
  {
    "objectID": "ss_44.html#example-4.19-convolution-of-exponentials",
    "href": "ss_44.html#example-4.19-convolution-of-exponentials",
    "title": "Signals and Systems",
    "section": "Example 4.19: Convolution of Exponentials",
    "text": "Example 4.19: Convolution of Exponentials\nLet’s find the response of an LTI system with \\(h(t)=e^{-at}u(t)\\) to an input \\(x(t)=e^{-bt}u(t)\\).\nFrom previous examples, their Fourier Transforms are:\n\\(X(j \\omega)=\\frac{1}{b+j \\omega}\\)\n\\(H(j \\omega)=\\frac{1}{a+j \\omega}\\)\nUsing the convolution property \\(Y(j\\omega) = X(j\\omega)H(j\\omega)\\):\n\\[\n\\begin{equation*}\nY(j \\omega)=\\frac{1}{(a+j \\omega)(b+j \\omega)} \\tag{4.67}\n\\end{equation*}\n\\]\nTo find \\(y(t)\\), we use partial fraction expansion (assuming \\(a \\neq b\\)):\n\\[\n\\begin{equation*}\nY(j \\omega)=\\frac{A}{a+j \\omega}+\\frac{B}{b+j \\omega}, \\tag{4.68}\n\\end{equation*}\n\\]\nwhere \\(A=\\frac{1}{b-a}\\) and \\(B=\\frac{1}{a-b} = -A\\).\n\\[\n\\begin{equation*}\nY(j \\omega)=\\frac{1}{b-a}\\left[\\frac{1}{a+j \\omega}-\\frac{1}{b+j \\omega}\\right] \\tag{4.69}\n\\end{equation*}\n\\]\nTaking the inverse Fourier Transform (using linearity): \\(y(t)=\\frac{1}{b-a}\\left[e^{-a t} u(t)-e^{-b t} u(t)\\right]\\).\n\nThis example highlights the practical utility of the convolution property for solving convolution integrals that would be tedious in the time domain.\nInstead of direct time-domain convolution, we transform \\(x(t)\\) and \\(h(t)\\) into their respective Fourier Transforms, multiply them in the frequency domain to get \\(Y(j\\omega)\\), and then find the inverse Fourier Transform of \\(Y(j\\omega)\\).\nThe key step in finding the inverse transform of complex rational functions like \\(Y(j\\omega)\\) is often partial fraction expansion. This technique decomposes the complex fraction into simpler terms whose inverse transforms are known (e.g., \\(e^{-at}u(t)\\)).\nThe resulting \\(y(t)\\) shows how the system’s exponential decay and the input’s exponential decay combine to form the output."
  },
  {
    "objectID": "ss_44.html#example-4.19-convolution-of-exponentials-1",
    "href": "ss_44.html#example-4.19-convolution-of-exponentials-1",
    "title": "Signals and Systems",
    "section": "Example 4.19: Convolution of Exponentials",
    "text": "Example 4.19: Convolution of Exponentials\nSymbolic Partial Fraction Expansion\nVerify the partial fraction expansion using SymPy."
  },
  {
    "objectID": "ss_44.html#example-4.19-special-case-ab",
    "href": "ss_44.html#example-4.19-special-case-ab",
    "title": "Signals and Systems",
    "section": "Example 4.19: Special Case (\\(a=b\\))",
    "text": "Example 4.19: Special Case (\\(a=b\\))\nWhen \\(b=a\\), the partial fraction expansion is different:\n\\[\nY(j \\omega)=\\frac{1}{(a+j \\omega)^{2}} .\n\\]\nWe can recognize this form using the dual of the differentiation property:\n\\(\\frac{1}{(a+j \\omega)^{2}}=j \\frac{d}{d \\omega}\\left[\\frac{1}{a+j \\omega}\\right]\\).\nRecall the Fourier Transform pair:\n\\(e^{-a t} u(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\frac{1}{a+j \\omega}\\).\nUsing the differentiation in frequency property (\\(\\mathcal{F}\\{t x(t)\\} = j \\frac{d}{d\\omega} X(j\\omega)\\)):\n\\(t e^{-a t} u(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} j \\frac{d}{d \\omega}\\left[\\frac{1}{a+j \\omega}\\right]=\\frac{1}{(a+j \\omega)^{2}}\\).\nTherefore, for \\(a=b\\):\n\\(y(t)=t e^{-a t} u(t)\\).\n\nThe case where the parameters \\(a\\) and \\(b\\) are equal leads to a repeated pole in the frequency domain.\nDirect partial fraction expansion for repeated poles is slightly different, but the Fourier Transform properties provide a very elegant way to find the inverse transform.\nSpecifically, the differentiation in frequency property states that multiplying a signal by \\(t\\) in the time domain corresponds to differentiating its Fourier Transform with respect to \\(\\omega\\) and multiplying by \\(j\\).\nSince we know the Fourier Transform of \\(e^{-at}u(t)\\) is \\(1/(a+j\\omega)\\), applying this property directly gives us the inverse transform of \\(1/(a+j\\omega)^2\\) as \\(t e^{-at}u(t)\\).\nThis highlights the power and interconnectedness of Fourier Transform properties in solving complex signal processing problems."
  },
  {
    "objectID": "ss_44.html#interactive-convolution-of-exponentials",
    "href": "ss_44.html#interactive-convolution-of-exponentials",
    "title": "Signals and Systems",
    "section": "Interactive Convolution of Exponentials",
    "text": "Interactive Convolution of Exponentials\nAdjust parameters \\(a\\) and \\(b\\) to observe \\(x(t)\\), \\(h(t)\\), and \\(y(t)\\).\n\nviewof param_a = Inputs.range([0.1, 5], {step: 0.1, value: 1, label: \"Parameter 'a'\"})\nviewof param_b = Inputs.range([0.1, 5], {step: 0.1, value: 2, label: \"Parameter 'b'\"})"
  },
  {
    "objectID": "ss_44.html#example-4.20-convolution-of-sinc-functions",
    "href": "ss_44.html#example-4.20-convolution-of-sinc-functions",
    "title": "Signals and Systems",
    "section": "Example 4.20: Convolution of Sinc Functions",
    "text": "Example 4.20: Convolution of Sinc Functions\nConsider the response of an ideal lowpass filter (sinc impulse response) to a sinc input signal.\nInput: \\(x(t)=\\frac{\\sin \\omega_{i} t}{\\pi t}\\)\nImpulse Response: \\(h(t)=\\frac{\\sin \\omega_{c} t}{\\pi t}\\)\nTheir Fourier Transforms are rectangular pulses:\n\\(X(j \\omega)= \\begin{cases}1 & |\\omega| \\leq \\omega_{i} \\\\ 0 & \\text { elsewhere }\\end{cases}\\)\n\\(H(j \\omega)= \\begin{cases}1 & |\\omega| \\leq \\omega_{c} \\\\ 0 & \\text { elsewhere }\\end{cases}\\)\nThe output in the frequency domain is \\(Y(j\\omega)=X(j\\omega)H(j\\omega)\\):\n\\[\nY(j \\omega)= \\begin{cases}1 & |\\omega| \\leq \\omega_{0} \\\\ 0 & \\text { elsewhere }\\end{cases}\n\\]\nwhere \\(\\omega_{0} = \\min(\\omega_i, \\omega_c)\\).\nThe inverse Fourier Transform gives the output \\(y(t)\\):\n\\[\ny(t)=\\left\\{\\begin{array}{ll}\n\\frac{\\sin \\omega_{c} t}{\\pi t} & \\text { if } \\omega_{c} \\leq \\omega_{i} \\\\\n\\frac{\\sin \\omega_{i} t}{\\pi t} & \\text { if } \\omega_{i} \\leq \\omega_{c}\n\\end{array} .\\right.\n\\]\nThat is, the output is a sinc function with the smaller of the two cutoff frequencies.\n\nThis final example beautifully illustrates the filtering action. Both the input signal \\(x(t)\\) and the filter’s impulse response \\(h(t)\\) are sinc functions, which means their Fourier Transforms \\(X(j\\omega)\\) and \\(H(j\\omega)\\) are rectangular pulses.\nWhen these two rectangular pulses are multiplied in the frequency domain, the result \\(Y(j\\omega)\\) is another rectangular pulse whose width is determined by the narrower of the two input pulses.\nThis means that if the input signal’s bandwidth (\\(\\omega_i\\)) is wider than the filter’s bandwidth (\\(\\omega_c\\)), the filter truncates the signal’s spectrum, and the output signal’s bandwidth is limited by the filter. If the input’s bandwidth is narrower, the signal passes through the filter’s passband largely unchanged in its spectral shape.\nThe output \\(y(t)\\) is therefore a sinc function corresponding to this narrower bandwidth. This is a fundamental concept in filtering, spectrum shaping, and is integral to understanding processes like ideal sampling and reconstruction."
  },
  {
    "objectID": "ss_44.html#interactive-convolution-of-sinc-functions",
    "href": "ss_44.html#interactive-convolution-of-sinc-functions",
    "title": "Signals and Systems",
    "section": "Interactive Convolution of Sinc Functions",
    "text": "Interactive Convolution of Sinc Functions\nAdjust \\(\\omega_i\\) (input bandwidth) and \\(\\omega_c\\) (filter bandwidth) and observe the spectra and signals.\n\nviewof wi_sinc = Inputs.range([1, 10], {step: 0.1, value: 5, label: \"Input Bandwidth (ωi)\"})\nviewof wc_sinc = Inputs.range([1, 10], {step: 0.1, value: 3, label: \"Filter Bandwidth (ωc)\"})"
  },
  {
    "objectID": "ss_42.html#the-fourier-transform",
    "href": "ss_42.html#the-fourier-transform",
    "title": "The Fourier Transform",
    "section": "The Fourier Transform",
    "text": "The Fourier Transform\n4.2 The Fourier Transform for Periodic Signals\nWelcome to the fascinating world of Signals and Systems!\nToday, we’ll extend our understanding of the Fourier Transform (FT) to periodic signals.\nThis allows us to analyze both periodic and aperiodic signals within a unified framework.\nWe’ll discover how the Fourier Transform of a periodic signal reveals itself as a train of impulses in the frequency domain.\nThese impulses are directly related to the signal’s Fourier Series coefficients."
  },
  {
    "objectID": "ss_42.html#unifying-periodic-and-aperiodic-signals",
    "href": "ss_42.html#unifying-periodic-and-aperiodic-signals",
    "title": "The Fourier Transform",
    "section": "Unifying Periodic and Aperiodic Signals",
    "text": "Unifying Periodic and Aperiodic Signals\nTraditionally, Fourier Series describes periodic signals, while Fourier Transform handles aperiodic signals. Today, we bridge this gap.\n\n\n\n\n\n\nKey Idea\n\n\nThe Fourier Transform of a periodic signal is a train of impulses in the frequency domain. The areas of these impulses are proportional to the signal’s Fourier Series coefficients.\n\n\n\nThis approach provides a powerful, unified tool for signal analysis across ECE disciplines.\n\nExplain that prior to this, students learned Fourier Series for periodic signals and Fourier Transform for aperiodic signals. Emphasize that this section unites these concepts, providing a more general framework. Highlight the conceptual leap: continuous spectrum for aperiodic signals vs. discrete spectrum (impulses) for periodic signals."
  },
  {
    "objectID": "ss_42.html#building-intuition-a-single-impulse-in-frequency",
    "href": "ss_42.html#building-intuition-a-single-impulse-in-frequency",
    "title": "The Fourier Transform",
    "section": "Building Intuition: A Single Impulse in Frequency",
    "text": "Building Intuition: A Single Impulse in Frequency\nLet’s start with a simple Fourier Transform: a single impulse in the frequency domain. \\[ X(j \\omega)=2 \\pi \\delta\\left(\\omega-\\omega_{0}\\right) \\] What time-domain signal \\(x(t)\\) does this represent? We use the inverse Fourier Transform: \\[ x(t) = \\frac{1}{2 \\pi} \\int_{-\\infty}^{+\\infty} X(j \\omega) e^{j \\omega t} d \\omega \\] Substituting \\(X(j\\omega)\\): \\[ x(t) = \\frac{1}{2 \\pi} \\int_{-\\infty}^{+\\infty} 2 \\pi \\delta\\left(\\omega-\\omega_{0}\\right) e^{j \\omega t} d \\omega \\] \\[ x(t) = e^{j \\omega_{0} t} \\] This shows that a single impulse in the frequency domain corresponds to a complex exponential in the time domain.\n\nWalk through the derivation step-by-step. Explain the role of the \\(2\\pi\\) factor in the definition of the impulse. Emphasize that the sifting property of the Dirac delta function is crucial here. This is the foundational block for understanding the FT of periodic signals."
  },
  {
    "objectID": "ss_42.html#generalizing-to-a-train-of-impulses",
    "href": "ss_42.html#generalizing-to-a-train-of-impulses",
    "title": "The Fourier Transform",
    "section": "Generalizing to a Train of Impulses",
    "text": "Generalizing to a Train of Impulses\nIf we have a linear combination of impulses in the frequency domain: \\[ X(j \\omega)=\\sum_{k=-\\infty}^{+\\infty} 2 \\pi a_{k} \\delta\\left(\\omega-k \\omega_{0}\\right) \\] Applying the inverse Fourier Transform, as before: \\[ x(t) = \\frac{1}{2 \\pi} \\int_{-\\infty}^{+\\infty} \\left( \\sum_{k=-\\infty}^{+\\infty} 2 \\pi a_{k} \\delta\\left(\\omega-k \\omega_{0}\\right) \\right) e^{j \\omega t} d \\omega \\] \\[ x(t) = \\sum_{k=-\\infty}^{+\\infty} a_{k} \\left( \\frac{1}{2 \\pi} \\int_{-\\infty}^{+\\infty} 2 \\pi \\delta\\left(\\omega-k \\omega_{0}\\right) e^{j \\omega t} d \\omega \\right) \\] \\[ x(t) = \\sum_{k=-\\infty}^{+\\infty} a_{k} e^{j k \\omega_{0} t} \\]\n\n\n\n\n\n\nThe Core Relationship\n\n\nThis is precisely the Fourier Series representation of a periodic signal! The Fourier Transform of a periodic signal with Fourier Series coefficients \\(\\{a_k\\}\\) is a train of impulses. The impulse at the \\(k\\)-th harmonic frequency \\(k\\omega_0\\) has an area of \\(2\\pi a_k\\). \\[ \\mathcal{F} \\left\\{ \\sum_{k=-\\infty}^{+\\infty} a_{k} e^{j k \\omega_{0} t} \\right\\} = \\sum_{k=-\\infty}^{+\\infty} 2 \\pi a_{k} \\delta\\left(\\omega-k \\omega_{0}\\right) \\]\n\n\n\n\nHighlight the direct connection between the Fourier Series coefficients (\\(a_k\\)) and the impulse areas in the Fourier Transform. This is the main theoretical result of this section. Explain that the factor \\(2\\pi\\) arises from the definition of the inverse Fourier Transform and the Fourier Series."
  },
  {
    "objectID": "ss_42.html#example-4.6-the-periodic-square-wave",
    "href": "ss_42.html#example-4.6-the-periodic-square-wave",
    "title": "The Fourier Transform",
    "section": "Example 4.6: The Periodic Square Wave",
    "text": "Example 4.6: The Periodic Square Wave\nConsider a symmetric periodic square wave.\nIts Fourier Series coefficients are given by: \\[ a_{k}=\\frac{\\sin k \\omega_{0} T_{1}}{\\pi k} \\] Therefore, its Fourier Transform is: \\[ X(j \\omega)=\\sum_{k=-\\infty}^{+\\infty} \\frac{2 \\sin k \\omega_{0} T_{1}}{k} \\delta\\left(\\omega-k \\omega_{0}\\right) \\]"
  },
  {
    "objectID": "ss_42.html#example-4.6-the-periodic-square-wave-cont.",
    "href": "ss_42.html#example-4.6-the-periodic-square-wave-cont.",
    "title": "The Fourier Transform",
    "section": "Example 4.6: The Periodic Square Wave (cont.)",
    "text": "Example 4.6: The Periodic Square Wave (cont.)\n\n\nThe transform consists of impulses located at integer multiples of the fundamental frequency \\(\\omega_0\\).\nThe amplitude of each impulse is proportional to the corresponding Fourier Series coefficient.\nNotice the sinc function envelope, characteristic of rectangular pulses.\nThis illustrates how the discrete spectrum of a periodic signal maps to impulses in the frequency domain.\n\n Figure 4.12: Fourier transform of a symmetric periodic square wave (\\(T=4T_1\\)).\n\n\nPoint out the sinc function envelope and its relation to the rectangular pulse in the time domain. Explain that \\(T_1\\) is the pulse width and \\(T\\) is the period. Discuss how the spacing of the impulses is determined by \\(\\omega_0 = 2\\pi/T\\). Compare this to the bar graph of Fourier Series coefficients, noting the \\(2\\pi\\) scaling and the use of impulses."
  },
  {
    "objectID": "ss_42.html#interactive-demo-square-wave-spectrum",
    "href": "ss_42.html#interactive-demo-square-wave-spectrum",
    "title": "The Fourier Transform",
    "section": "Interactive Demo: Square Wave Spectrum",
    "text": "Interactive Demo: Square Wave Spectrum\nUse the sliders to change the square wave’s properties:\n\nPeriod (T): How does it affect the spacing of the impulses?\nPulse Width (T1): How does it affect the envelope of the spectrum?\n\n\nviewof T_val = Inputs.range([1, 10], {label: \"Period (T)\", step: 0.1, value: 4})\nviewof T1_val = Inputs.range([0.1, T_val/2 - 0.1], {label: \"Pulse Width (T1)\", step: 0.05, value: 1})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGuide students to observe:\n\nEffect of T: As T increases, \\(\\omega_0\\) decreases, and the impulses get closer together.\nEffect of T1: As T1 changes, the sinc envelope stretches or compresses, affecting the relative magnitudes of the impulses. When \\(T_1\\) approaches \\(T/2\\), the square wave becomes a pulse train with 50% duty cycle, and even harmonics vanish. When \\(T_1\\) is very small, the spectrum broadens. This interactive element reinforces the inverse relationship between time and frequency domains."
  },
  {
    "objectID": "ss_42.html#example-4.7-sinusoidal-signals",
    "href": "ss_42.html#example-4.7-sinusoidal-signals",
    "title": "The Fourier Transform",
    "section": "Example 4.7: Sinusoidal Signals",
    "text": "Example 4.7: Sinusoidal Signals\nThe Fourier Transform provides a concise representation for simple sinusoids.\nSine Wave: \\(x(t)=\\sin \\omega_{0} t\\)\nRecall its Fourier Series coefficients: \\[ a_{1} = \\frac{1}{2 j} \\] \\[ a_{-1} = -\\frac{1}{2 j} \\] \\[ a_{k}=0, \\quad k \\neq 1 \\text{ or } -1 \\] The Fourier Transform consists of two impulses at \\(\\pm \\omega_0\\): \\[ X(j \\omega) = 2\\pi \\left( \\frac{1}{2j} \\delta(\\omega - \\omega_0) - \\frac{1}{2j} \\delta(\\omega + \\omega_0) \\right) \\] \\[ X(j \\omega) = \\frac{\\pi}{j} \\delta(\\omega - \\omega_0) - \\frac{\\pi}{j} \\delta(\\omega + \\omega_0) \\] \\[ X(j \\omega) = -j\\pi \\delta(\\omega - \\omega_0) + j\\pi \\delta(\\omega + \\omega_0) \\]\nCosine Wave: \\(x(t)=\\cos \\omega_{0} t\\)\nIts Fourier Series coefficients are: \\[ a_{1}=a_{-1}=\\frac{1}{2} \\] \\[ a_{k}=0, \\quad k \\neq 1 \\text{ or } -1 \\] Its Fourier Transform also has two impulses at \\(\\pm \\omega_0\\): \\[ X(j \\omega) = 2\\pi \\left( \\frac{1}{2} \\delta(\\omega - \\omega_0) + \\frac{1}{2} \\delta(\\omega + \\omega_0) \\right) \\] \\[ X(j \\omega) = \\pi \\delta(\\omega - \\omega_0) + \\pi \\delta(\\omega + \\omega_0) \\]\n\nEmphasize the difference in phase (imaginary vs. real impulses) between sine and cosine, corresponding to their odd and even symmetry. Discuss the importance of these basic transforms as building blocks for more complex signals, especially in modulation systems."
  },
  {
    "objectID": "ss_42.html#visualizing-sinusoidal-spectra",
    "href": "ss_42.html#visualizing-sinusoidal-spectra",
    "title": "The Fourier Transform",
    "section": "Visualizing Sinusoidal Spectra",
    "text": "Visualizing Sinusoidal Spectra\n\n\nThese transforms are fundamental in understanding modulation systems.\nThey clearly show that sinusoids are “pure” frequencies, represented by single points in the spectrum.\nThe impulses indicate the presence of specific frequency components.\n\n Figure 4.13: Fourier transforms of (a) \\(\\sin \\omega_{0} t\\) and (b) \\(\\cos \\omega_{0} t\\).\n\n\nDirectly relate the visual representation to the derived impulse equations. Explain why sine has impulses pointing in opposite directions (due to imaginary coefficients) and cosine has impulses pointing in the same direction (due to real coefficients), although typically only magnitude spectra are plotted."
  },
  {
    "objectID": "ss_42.html#interactive-demo-sine-and-cosine-spectra",
    "href": "ss_42.html#interactive-demo-sine-and-cosine-spectra",
    "title": "The Fourier Transform",
    "section": "Interactive Demo: Sine and Cosine Spectra",
    "text": "Interactive Demo: Sine and Cosine Spectra\nUse the slider to change \\(\\omega_0\\):\n\nFundamental Frequency (\\(\\omega_0\\)):\n\nHow does this affect the position of the impulses in the spectrum? Note that for real signals, if there is a positive frequency component, there must be a corresponding negative frequency component.\n\nviewof omega0_val = Inputs.range([1, 10], {label: \"ω₀ (rad/s)\", step: 0.5, value: 2})\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoint out that changing \\(\\omega_0\\) simply shifts the impulses along the frequency axis. Reinforce that the magnitude of the impulses remains constant for a pure sinusoid. Discuss the physical meaning: a sinusoid contains energy only at its specific frequency."
  },
  {
    "objectID": "ss_42.html#example-4.8-the-impulse-train",
    "href": "ss_42.html#example-4.8-the-impulse-train",
    "title": "The Fourier Transform",
    "section": "Example 4.8: The Impulse Train",
    "text": "Example 4.8: The Impulse Train\nA signal of immense importance in sampling systems is the impulse train. \\[ x(t)=\\sum_{k=-\\infty}^{+\\infty} \\delta(t-k T) \\] This signal is periodic with period \\(T\\).\n\n\nIts Fourier Series coefficients were previously found to be: \\[ a_{k}=\\frac{1}{T} \\] This means every harmonic component has the same amplitude! Substituting this into our Fourier Transform definition for periodic signals: \\[ X(j \\omega)=\\sum_{k=-\\infty}^{+\\infty} 2 \\pi \\left(\\frac{1}{T}\\right) \\delta\\left(\\omega-k \\omega_{0}\\right) \\] \\[ X(j \\omega)=\\frac{2 \\pi}{T} \\sum_{k=-\\infty}^{+\\infty} \\delta\\left(\\omega-\\frac{2 \\pi k}{T}\\right) \\] The Fourier Transform of an impulse train in the time domain is also an impulse train in the frequency domain!\n\n (a) Periodic impulse train in time domain.  (b) Its Fourier transform in frequency domain.\n\n\n\n\n\n\n\nInverse Relationship\n\n\nAs the spacing between impulses in the time domain (\\(T\\)) gets longer, the spacing between impulses in the frequency domain (\\(2\\pi/T\\)) gets smaller. This is a profound illustration of the inverse relationship between time and frequency domains.\n\n\n\n\nHighlight that this is a unique signal whose FT has the same form as itself. Emphasize its critical role in sampling theory (Chapter 7), where it models the sampling process. Discuss the inverse relationship: a widely spaced time-domain signal has a narrowly spaced frequency-domain signal, and vice-versa."
  },
  {
    "objectID": "ss_42.html#interactive-demo-impulse-train-spectrum",
    "href": "ss_42.html#interactive-demo-impulse-train-spectrum",
    "title": "The Fourier Transform",
    "section": "Interactive Demo: Impulse Train Spectrum",
    "text": "Interactive Demo: Impulse Train Spectrum\nUse the slider to change the period T:\n\nPeriod (T):\n\nSee how changing the time-domain period directly impacts the frequency-domain spacing.\nThis phenomenon is central to understanding aliasing and the Nyquist-Shannon sampling theorem.\n\nviewof T_impulse_val = Inputs.range([0.5, 5], {label: \"Period (T) of Time-Domain Impulse Train\", step: 0.1, value: 1})\n\n\n\n\n\n\n\n\n\n\n\n\n\nEncourage students to actively manipulate the T slider and voice their observations. This direct visual feedback is crucial for grasping the inverse relationship. Connect this to the concept of sampling rate: a faster sampling rate (smaller T) means a wider frequency spectrum (larger \\(\\omega_F\\)), which is good for avoiding aliasing."
  },
  {
    "objectID": "ss_42.html#summary-and-key-takeaways",
    "href": "ss_42.html#summary-and-key-takeaways",
    "title": "The Fourier Transform",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\nWe’ve explored the powerful connection between Fourier Series and Fourier Transform for periodic signals.\n\nUnified View: Both periodic and aperiodic signals can be analyzed using the Fourier Transform.\nImpulse Representation: The Fourier Transform of a periodic signal is a train of impulses in the frequency domain.\nCoefficient Link: The area of each impulse is \\(2\\pi\\) times the corresponding Fourier Series coefficient \\(a_k\\).\nInverse Relationship: A wider spacing in one domain implies a narrower spacing in the other (e.g., impulse train).\n\n\n\n\n\n\n\nWhy is this important?\n\n\nThis unified framework is foundational for:\n\nSampling Theory: Understanding how continuous signals are converted to discrete samples.\nModulation: Analyzing how information is carried on carrier waves in communication systems.\nFilter Design: Designing systems that selectively process frequency components.\n\n\n\n\n\nSummarize the main points concisely. Reiterate the practical applications listed in the callout, emphasizing that these theoretical tools are directly applied in real-world ECE systems. Encourage questions and further discussion."
  },
  {
    "objectID": "ss_3y.html#understanding-the-core-differences",
    "href": "ss_3y.html#understanding-the-core-differences",
    "title": "Signals and Systems",
    "section": "Understanding the Core Differences",
    "text": "Understanding the Core Differences\n\nWelcome back! Today, we’ll consolidate our understanding of Fourier series by directly comparing its continuous-time and discrete-time versions. While they share a common goal of decomposing periodic signals into a sum of harmonically related exponentials, their mathematical forms and inherent properties differ significantly. This comparison will highlight the unique characteristics of each and reinforce why these distinctions are important in ECE."
  },
  {
    "objectID": "ss_3y.html#continuous-time-fourier-series-ctfs-recap",
    "href": "ss_3y.html#continuous-time-fourier-series-ctfs-recap",
    "title": "Signals and Systems",
    "section": "Continuous-Time Fourier Series (CTFS) Recap",
    "text": "Continuous-Time Fourier Series (CTFS) Recap\nFor a periodic CT signal \\(x(t)\\) with fundamental period \\(T_0\\) and fundamental frequency \\(\\omega_0 = 2\\pi/T_0\\).\n\n\nSynthesis Equation:\n(Reconstruction from coefficients)\n\\[ x(t) = \\sum_{k=-\\infty}^{\\infty} a_k e^{j k \\omega_0 t} \\]\n\nAnalysis Equation:\n(Calculation of coefficients)\n\\[ a_k = \\frac{1}{T_0} \\int_{T_0} x(t) e^{-j k \\omega_0 t} dt \\]\n\n\n\n\n\n\n\nNote\n\n\nKey Characteristics:\n\nInfinite series: Requires summing an infinite number of terms.\nConvergence issues: Partial sums may not perfectly reconstruct \\(x(t)\\), especially at discontinuities.\nGibbs phenomenon: Overshoots/undershoots near discontinuities in partial sums.\n\n\n\n\n\nLet’s quickly review the CTFS. The synthesis equation shows that any periodic continuous-time signal can be expressed as an infinite sum of complex exponentials. The analysis equation provides the formula to calculate the coefficients \\(a_k\\) by integrating the signal over one period, weighted by a complex exponential. The key takeaways here are the infinite nature of the series, which leads to potential convergence issues and the infamous Gibbs phenomenon at discontinuities when only a finite number of terms are used for reconstruction."
  },
  {
    "objectID": "ss_3y.html#discrete-time-fourier-series-dtfs-recap",
    "href": "ss_3y.html#discrete-time-fourier-series-dtfs-recap",
    "title": "Signals and Systems",
    "section": "Discrete-Time Fourier Series (DTFS) Recap",
    "text": "Discrete-Time Fourier Series (DTFS) Recap\nFor a periodic DT signal \\(x[n]\\) with fundamental period \\(N\\) and fundamental frequency \\(\\omega_0 = 2\\pi/N\\).\n\n\nSynthesis Equation:\n(Reconstruction from coefficients)\n\\[ x[n] = \\sum_{k=\\langle N\\rangle} a_k e^{j k \\omega_0 n} = \\sum_{k=\\langle N\\rangle} a_k e^{j k(2\\pi/N)n} \\]\n\nAnalysis Equation:\n(Calculation of coefficients)\n\\[ a_k = \\frac{1}{N} \\sum_{n=\\langle N\\rangle} x[n] e^{-j k \\omega_0 n} = \\frac{1}{N} \\sum_{n=\\langle N\\rangle} x[n] e^{-j k(2\\pi/N)n} \\]\n\n\n\n\n\n\n\nNote\n\n\nKey Characteristics:\n\nFinite series: Sums only \\(N\\) distinct terms.\nNo convergence issues: The finite sum perfectly reconstructs \\(x[n]\\).\nNo Gibbs phenomenon: Exact reconstruction with all \\(N\\) terms.\nCoefficients are periodic: \\(a_k = a_{k+N}\\).\n\n\n\n\n\nNow, for the DTFS. Again, we have synthesis and analysis equations, but with crucial differences. The summation in the synthesis equation is explicitly shown as a finite sum over \\(N\\) consecutive values of \\(k\\). The analysis equation involves a summation over \\(N\\) samples of the signal, rather than an integral. The most important characteristics are the finite nature of the series, which means no convergence issues and no Gibbs phenomenon. Also, remember that the coefficients \\(a_k\\) themselves are periodic with period \\(N\\)."
  },
  {
    "objectID": "ss_3y.html#ctfs-vs.-dtfs-a-side-by-side-comparison",
    "href": "ss_3y.html#ctfs-vs.-dtfs-a-side-by-side-comparison",
    "title": "Signals and Systems",
    "section": "CTFS vs. DTFS: A Side-by-Side Comparison",
    "text": "CTFS vs. DTFS: A Side-by-Side Comparison\n\n\nContinuous-Time Fourier Series (CTFS)\n\nSignal: \\(x(t)\\) (continuous-time)\nPeriod: \\(T_0\\)\nFundamental Freq: \\(\\omega_0 = 2\\pi/T_0\\)\nSeries Type: Infinite sum \\[ x(t) = \\sum_{k=-\\infty}^{\\infty} a_k e^{j k \\omega_0 t} \\]\nCoefficient Calculation: Integral \\[ a_k = \\frac{1}{T_0} \\int_{T_0} x(t) e^{-j k \\omega_0 t} dt \\]\nConvergence: Requires conditions, partial sums approximate \\(x(t)\\).\nGibbs Phenomenon: Present at discontinuities for partial sums.\nCoefficients periodicity: \\(a_k\\) are generally not periodic.\n\n\nDiscrete-Time Fourier Series (DTFS)\n\nSignal: \\(x[n]\\) (discrete-time)\nPeriod: \\(N\\)\nFundamental Freq: \\(\\omega_0 = 2\\pi/N\\)\nSeries Type: Finite sum (N terms) \\[ x[n] = \\sum_{k=\\langle N\\rangle} a_k e^{j k(2\\pi/N)n} \\]\nCoefficient Calculation: Summation \\[ a_k = \\frac{1}{N} \\sum_{n=\\langle N\\rangle} x[n] e^{-j k(2\\pi/N)n} \\]\nConvergence: Always exact with \\(N\\) terms.\nGibbs Phenomenon: Not present.\nCoefficients periodicity: \\(a_k\\) are periodic with period \\(N\\).\n\n\n\nThis slide provides a concise summary of the key differences. Notice the fundamental distinction between integrals and summations, and the implications of infinite versus finite series. The periodicity of coefficients in DTFS is also a unique and important property."
  },
  {
    "objectID": "ss_3y.html#example-periodic-square-wave-ctfs",
    "href": "ss_3y.html#example-periodic-square-wave-ctfs",
    "title": "Signals and Systems",
    "section": "Example: Periodic Square Wave (CTFS)",
    "text": "Example: Periodic Square Wave (CTFS)\nConsider a CT periodic square wave \\(x(t)\\) with period \\(T_0\\), amplitude \\(A=1\\), and pulse width \\(T_1\\).\nLet \\(T_0=1\\) and \\(T_1=0.5\\).\nThe Fourier series coefficients are given by:\n\\[ a_k = \\frac{A T_1}{T_0} \\text{sinc}\\left(k \\frac{T_1}{T_0}\\right) = \\frac{A T_1}{T_0} \\frac{\\sin(\\pi k T_1/T_0)}{\\pi k T_1/T_0} \\]\nFor \\(k=0\\), \\(a_0 = \\frac{A T_1}{T_0}\\).\n\nTo illustrate these concepts, let’s use a common signal: the periodic square wave. For continuous time, the coefficients are given by a sinc function. This means the spectrum is continuous-like, decaying as \\(k\\) increases, with zero crossings. Let’s visualize this."
  },
  {
    "objectID": "ss_3y.html#ctfs-square-wave-visualization",
    "href": "ss_3y.html#ctfs-square-wave-visualization",
    "title": "Signals and Systems",
    "section": "CTFS: Square Wave Visualization",
    "text": "CTFS: Square Wave Visualization\nHere we visualize the CT square wave and its Fourier coefficients.\n\nviewof A_ct = Inputs.number([0, 5], {\n  label: \"A (Amplitude):\",\n  step: 0.1,\n  value: 1\n})\n\nviewof T0_ct = Inputs.number([0.1, 5], {\n  label: \"T0 (Fundamental Period):\",\n  step: 0.1,\n  value: 1\n})\n\nviewof T1_ct = Inputs.number([0.1, 2], {\n  label: \"T1 (Pulse Width):\",\n  step: 0.1,\n  value: 0.5\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe top plot shows the continuous-time periodic square wave. The bottom plot shows the magnitudes of its Fourier series coefficients. Notice that the coefficients form a continuous envelope (the sinc function), but are only defined at discrete harmonic frequencies \\(k\\omega_0\\). This spectrum extends to infinity, with the magnitudes decreasing as \\(k\\) increases."
  },
  {
    "objectID": "ss_3y.html#example-periodic-square-wave-dtfs",
    "href": "ss_3y.html#example-periodic-square-wave-dtfs",
    "title": "Signals and Systems",
    "section": "Example: Periodic Square Wave (DTFS)",
    "text": "Example: Periodic Square Wave (DTFS)\nConsider a DT periodic square wave \\(x[n]\\) with period \\(N\\), amplitude \\(A=1\\), and pulse width \\(P\\).\nLet \\(N=10\\) and \\(P=5\\) (i.e., \\(2N_1+1=5 \\implies N_1=2\\)).\nThe Fourier series coefficients are given by:\n\\[ a_k = \\frac{A}{N} \\frac{\\sin(\\pi k P/N)}{\\sin(\\pi k/N)}, \\quad k \\neq 0, \\pm N, \\pm 2N, \\ldots \\]\nFor \\(k=0, \\pm N, \\ldots\\):\n\\[ a_k = \\frac{A P}{N} \\]\n\nNow, let’s look at the discrete-time equivalent. We define a square wave in discrete time, with period \\(N\\) and pulse width \\(P\\). The formula for its coefficients is a ratio of sines, which is the discrete-time equivalent of the sinc function. Let’s visualize this."
  },
  {
    "objectID": "ss_3y.html#dtfs-square-wave-visualization",
    "href": "ss_3y.html#dtfs-square-wave-visualization",
    "title": "Signals and Systems",
    "section": "DTFS: Square Wave Visualization",
    "text": "DTFS: Square Wave Visualization\nHere we visualize the DT square wave and its Fourier coefficients.\n\nviewof A_dt = Inputs.number([0, 5], {\n  label: \"A (Amplitude):\",\n  step: 0.1,\n  value: 1\n})\n\nviewof N_dt = Inputs.number([1, 20], {\n  label: \"N (Period):\",\n  step: 1,\n  value: 10\n})\n\nviewof P_dt = Inputs.number([1, 15], {\n  label: \"P (Pulse Width, 2N1+1):\",\n  step: 1,\n  value: 5\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we see the discrete-time square wave and its Fourier coefficients. Notice that the signal \\(x[n]\\) is a sequence of points. The coefficients \\(|a_k|\\) are also a sequence of points, and there are only \\(N\\) distinct coefficients (from \\(k=0\\) to \\(N-1\\)). This spectrum is periodic, meaning the pattern of coefficients from \\(0\\) to \\(N-1\\) repeats for \\(N\\) to \\(2N-1\\), and so on. This is a direct consequence of the discrete nature of the time signal."
  },
  {
    "objectID": "ss_3y.html#visual-comparison-ctfs-vs.-dtfs-coefficients",
    "href": "ss_3y.html#visual-comparison-ctfs-vs.-dtfs-coefficients",
    "title": "Signals and Systems",
    "section": "Visual Comparison: CTFS vs. DTFS Coefficients",
    "text": "Visual Comparison: CTFS vs. DTFS Coefficients\n\n\nCTFS Coefficients (k continuous envelope)\n\nviewof A_ct_comp = Inputs.number([0, 5], {\n  label: \"A (Amplitude):\",\n  step: 0.1,\n  value: 1\n})\n\nviewof T0_ct_comp = Inputs.number([0.1, 5], {\n  label: \"T0 (Fundamental Period):\",\n  step: 0.1,\n  value: 1\n})\n\nviewof T1_ct_comp = Inputs.number([0.1, 2], {\n  label: \"T1 (Pulse Width):\",\n  step: 0.1,\n  value: 0.5\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDTFS Coefficients (k sampled points)\n\nviewof A_dt_comp = Inputs.number([0, 5], {\n  label: \"A (Amplitude):\",\n  step: 0.1,\n  value: 1\n})\n\nviewof N_dt_comp = Inputs.number([1, 20], {\n  label: \"N (Period):\",\n  step: 1,\n  value: 10\n})\n\nviewof P_dt_comp = Inputs.number([1, 15], {\n  label: \"P (Pulse Width, 2N1+1):\",\n  step: 1,\n  value: 5\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere’s a direct visual comparison of the coefficient magnitudes. The CTFS coefficients follow a continuous sinc envelope, meaning that if we were to take more and more harmonics, they would fill out this shape. The DTFS coefficients, however, are discrete samples of a similar envelope, but they are also periodic. This “sampling” of the frequency domain is a direct consequence of the time-domain signal being discrete."
  },
  {
    "objectID": "ss_3y.html#reconstruction-gibbs-phenomenon-ctfs",
    "href": "ss_3y.html#reconstruction-gibbs-phenomenon-ctfs",
    "title": "Signals and Systems",
    "section": "Reconstruction & Gibbs Phenomenon (CTFS)",
    "text": "Reconstruction & Gibbs Phenomenon (CTFS)\nUsing only a finite number of terms to reconstruct a CT square wave will show the Gibbs phenomenon.\n\n\n\nviewof A_ct_rec = Inputs.number([0, 5], {\n  label: \"A (Amplitude):\",\n  step: 0.1,\n  value: 1\n})\nviewof T0_ct_rec = Inputs.number([0.1, 5], {\n  label: \"T0 (Fundamental Period):\",\n  step: 0.1,\n  value: 1\n})\nviewof T1_ct_rec = Inputs.number([0.1, 2], {\n  label: \"T1 (Pulse Width):\",\n  step: 0.1,\n  value: 0.5\n})\nviewof K_rec_ct = Inputs.range([1, 20], {\n  label: \"Number of Harmonics (K):\",\n  step: 1,\n  value: 1\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive plot demonstrates the reconstruction of a CT square wave. Adjust the K slider to change the number of harmonics used in the reconstruction. As you increase K, the reconstructed signal gets closer to the original, but notice the prominent overshoots and undershoots near the discontinuities. This is the Gibbs phenomenon, which persists even with a large number of terms, though it becomes narrower. It highlights the difficulty of representing sharp transitions with a finite sum of smooth sinusoids."
  },
  {
    "objectID": "ss_3y.html#reconstruction-no-gibbs-phenomenon-dtfs",
    "href": "ss_3y.html#reconstruction-no-gibbs-phenomenon-dtfs",
    "title": "Signals and Systems",
    "section": "Reconstruction & No Gibbs Phenomenon (DTFS)",
    "text": "Reconstruction & No Gibbs Phenomenon (DTFS)\nUsing all \\(N\\) terms perfectly reconstructs a DT periodic square wave.\n\n\n\nviewof A_dt_rec = Inputs.number([0, 5], {\n  label: \"A (Amplitude):\",\n  step: 0.1,\n  value: 1\n})\n\nviewof N_dt_rec = Inputs.number([1, 20], {\n  label: \"N (Period):\",\n  step: 1,\n  value: 10\n})\nviewof P_dt_rec = Inputs.number([1, 15], {\n  label: \"P (Pulse Width, 2N1+1):\",\n  step: 1,\n  value: 5\n})\n\nviewof M_rec_dt = Inputs.range([0, 4], {\n  label: \"Number of Terms (M):\",\n  step: 1,\n  value: 0\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn contrast to CTFS, the DTFS reconstruction behaves very differently. Adjust the M slider, which controls the number of terms used (from 1 to \\(N\\)). As you increase the number of terms, the reconstructed signal quickly converges. Once you include all \\(N\\) distinct terms (e.g., when the slider is at its maximum value for \\(N=10\\)), the reconstructed signal perfectly matches the original signal. There are no Gibbs phenomena or approximation errors. This exact reconstruction is a hallmark of the discrete-time Fourier series."
  },
  {
    "objectID": "ss_3y.html#conclusion-and-key-takeaways",
    "href": "ss_3y.html#conclusion-and-key-takeaways",
    "title": "Signals and Systems",
    "section": "Conclusion and Key Takeaways",
    "text": "Conclusion and Key Takeaways\n\n\nCTFS: The Infinite Spectroscope\n\nInfinite number of harmonic components.\nContinuous spectrum envelope (sinc for square wave).\nApproximation when using finite terms.\nGibbs phenomenon at discontinuities.\nUsed for analyzing analog signals in continuous systems.\n\n\nDTFS: The Finite Spectrum Analyzer\n\nFinite number (\\(N\\)) of distinct harmonic components.\nDiscrete spectrum (sampled sinc for square wave), which is periodic.\nExact reconstruction with all \\(N\\) terms.\nNo Gibbs phenomenon.\nUsed for analyzing sampled signals and discrete systems.\n\n\n\n\n\n\n\n\nTip\n\n\nAnalogy:\nThink of CTFS as trying to describe a smooth painting with an infinite palette of colors, while DTFS is like describing a pixelated image with a finite, repeating set of colors.\n\n\n\n\nTo wrap up, remember these core differences. CTFS deals with continuous signals and infinite spectra, leading to approximations and Gibbs phenomenon with finite terms. DTFS deals with discrete signals, has a finite, periodic spectrum, and provides exact reconstruction with all its terms. These distinctions are fundamental to understanding signal processing in both analog and digital domains. Thank you for your attention!"
  },
  {
    "objectID": "ss_37.html#section-3.7",
    "href": "ss_37.html#section-3.7",
    "title": "Signals and Systems",
    "section": "Section 3.7",
    "text": "Section 3.7\n\nToday, we delve into the properties of the Discrete-Time Fourier Series, or DTFS. Understanding these properties is crucial for analyzing and manipulating discrete-time signals efficiently. Many of these properties will seem familiar, as they have strong parallels with the Continuous-Time Fourier Series properties we’ve already covered. However, we’ll also highlight some key differences that are unique to the discrete-time domain. These properties not only offer conceptual insights but also simplify the computation of Fourier series coefficients for complex signals."
  },
  {
    "objectID": "ss_37.html#overview-of-dtfs-properties",
    "href": "ss_37.html#overview-of-dtfs-properties",
    "title": "Signals and Systems",
    "section": "Overview of DTFS Properties",
    "text": "Overview of DTFS Properties\nDTFS properties share strong similarities with CTFS properties.\nThey are essential tools for signal analysis and manipulation.\n\n\n\n\n\n\nNote\n\n\nShorthand Notation:\nWe use the notation \\(x[n] \\stackrel{\\mathcal{FS}}{\\longleftrightarrow} a_k\\) to indicate that \\(x[n]\\) is a periodic signal with period \\(N\\) and \\(a_k\\) are its Fourier series coefficients.\n\n\n\n\nThe table on the next slide summarizes the most important properties. It’s important to remember this shorthand notation, as it’s commonly used in textbooks and research to express the Fourier series relationship."
  },
  {
    "objectID": "ss_37.html#table-3.2-properties-of-discrete-time-fourier-series",
    "href": "ss_37.html#table-3.2-properties-of-discrete-time-fourier-series",
    "title": "Signals and Systems",
    "section": "Table 3.2: Properties of Discrete-Time Fourier Series",
    "text": "Table 3.2: Properties of Discrete-Time Fourier Series\n\n\n\n\n\n\n\n\nProperty\nPeriodic Signal\nFourier Series Coefficients\n\n\n\n\nPeriodicity\n\\(x[n]\\) periodic with period \\(N\\)\n\\(a_k\\) periodic with period \\(N\\)\n\n\nLinearity\n\\(A x[n]+B y[n]\\)\n\\(A a_{k}+B b_{k}\\)\n\n\nTime Shifting\n\\(x\\left[n-n_{0}\\right]\\)\n\\(a_{k} e^{-j k(2 \\pi / N) n_{0}}\\)\n\n\nFrequency Shifting\n\\(e^{j M(2 \\pi / N) n} x[n]\\)\n\\(a_{k-M}\\)\n\n\nConjugation\n\\(x^{*}[n]\\)\n\\(a_{-k}^{*}\\)\n\n\nTime Reversal\n\\(x[-n]\\)\n\\(a_{-k}\\)\n\n\nPeriodic Convolution\n\\(\\sum_{r=\\langle N\\rangle} x[r] y[n-r]\\)\n\\(N a_{k} b_{k}\\)\n\n\nMultiplication\n\\(x[n] y[n]\\)\n\\(\\sum_{l=\\langle N\\rangle} a_{l} b_{k-l}\\)\n\n\nFirst Difference\n\\(x[n]-x[n-1]\\)\n\\(\\left(1-e^{-j k(2 \\pi / N)}\\right) a_{k}\\)\n\n\nRunning Sum\n\\(\\sum_{m=-\\infty}^{n} x[m]\\) (\\(a_0=0\\) for periodicity)\n\\(\\frac{1}{\\left(1-e^{-j k(2 \\pi / N)}\\right)} a_{k}\\) (for \\(k \\neq 0, \\pm N, \\ldots\\))\n\n\nConjugate Symmetry (Real \\(x[n]\\))\n\\(x[n]\\) real\n\\(a_{k}=a_{-k}^{*}\\), \\(|a_k|=|a_{-k}|\\), \\(\\angle a_k = -\\angle a_{-k}\\)\n\n\nReal & Even Signals\n\\(x[n]\\) real and even\n\\(a_k\\) real and even\n\n\nReal & Odd Signals\n\\(x[n]\\) real and odd\n\\(a_k\\) purely imaginary and odd\n\n\n\n\nThis table summarizes the key properties. I’ve slightly reordered some for logical flow, but the content is the same as in the textbook. We’ll discuss some of the most important properties in more detail, especially those with distinct discrete-time characteristics, and walk through examples."
  },
  {
    "objectID": "ss_37.html#time-shifting-property",
    "href": "ss_37.html#time-shifting-property",
    "title": "Signals and Systems",
    "section": "Time Shifting Property",
    "text": "Time Shifting Property\nIf \\(x[n] \\stackrel{\\mathcal{FS}}{\\longleftrightarrow} a_k\\), then \\(x[n-n_0] \\stackrel{\\mathcal{FS}}{\\longleftrightarrow} a_k e^{-j k(2\\pi/N)n_0}\\).\nA time shift in the signal domain corresponds to a phase shift in the frequency domain.\nLet’s visualize this with a simple square wave.\n\n\n\nviewof N_shift = Inputs.number([1, 20], {\n  label: \"N (Period):\",\n  step: 1,\n  value: 10\n})\n\nviewof P_shift = Inputs.number([1, 15], {\n  label: \"P (Pulse Width, 2N1+1):\",\n  step: 1,\n  value: 3\n})\n\nviewof n0_shift = Inputs.range([0, N_shift - 1], {\n  label: \"Shift (n0):\",\n  step: 1,\n  value: 0\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObserve the top plot: as you adjust the n0 slider, the original square wave x[n] shifts to x[n-n0]. Now look at the bottom plot for the Fourier coefficients. The magnitudes of the coefficients do not change with a time shift. This makes sense, as shifting a signal doesn’t change its energy distribution across frequencies. However, the phases of the coefficients change linearly with k. This linear phase shift is directly proportional to the amount of time shift n0. This property is fundamental for understanding how delays affect the frequency content of signals."
  },
  {
    "objectID": "ss_37.html#multiplication-property",
    "href": "ss_37.html#multiplication-property",
    "title": "Signals and Systems",
    "section": "Multiplication Property",
    "text": "Multiplication Property\nIf \\(x[n] \\stackrel{\\mathcal{FS}}{\\longleftrightarrow} a_k\\) and \\(y[n] \\stackrel{\\mathcal{FS}}{\\longleftrightarrow} b_k\\), both periodic with period \\(N\\).\nThen \\(x[n] y[n] \\stackrel{\\mathcal{FS}}{\\longleftrightarrow} d_k\\), where \\(d_k\\) is given by periodic convolution:\n\\[ d_k = \\sum_{l=\\langle N\\rangle} a_l b_{k-l} \\]\n\n\n\n\n\n\nImportant\n\n\nKey Difference from CTFS:\nIn CTFS, multiplication in time domain corresponds to aperiodic convolution of coefficients.\nIn DTFS, it corresponds to periodic convolution of coefficients, meaning the summation is over \\(N\\) terms and coefficients \\(b_k\\) are considered periodic.\n\n\n\n\nThe multiplication property is one of the most important distinctions between CTFS and DTFS. While both involve convolution in the frequency domain, DTFS uses periodic convolution. This means that when we calculate \\(b_{k-l}\\), we treat \\(b\\) as a periodic sequence. This property is crucial in understanding how mixing signals in the time domain affects their spectral components, such as in modulation."
  },
  {
    "objectID": "ss_37.html#first-difference-property",
    "href": "ss_37.html#first-difference-property",
    "title": "Signals and Systems",
    "section": "First Difference Property",
    "text": "First Difference Property\nIf \\(x[n] \\stackrel{\\mathcal{FS}}{\\longleftrightarrow} a_k\\), then \\(x[n]-x[n-1] \\stackrel{\\mathcal{FS}}{\\longleftrightarrow} (1-e^{-j k(2\\pi/N)}) a_k\\).\nThis is the discrete-time parallel to the differentiation property in continuous time.\n\n\n\n\n\n\nTip\n\n\nThis property is useful when the first difference of a signal is simpler to analyze than the original signal itself, often simplifying coefficient calculation.\n\n\n\nLet’s see how the first difference affects the spectrum of a discrete-time ramp signal.\n\n\n\n// Define N for DT ramp\nviewof N_diff = Inputs.number([1, 20], {\n  label: \"N (Period):\",\n  step: 1,\n  value: 10\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the top plot, you see a periodic ramp signal and its first difference. The first difference of a ramp is a constant (except for the wrap-around point), which is a much simpler signal. Observe the bottom plot: the magnitudes of the coefficients for the first difference signal are significantly altered compared to the original ramp. This property effectively scales the coefficients by a frequency-dependent term, often leading to a “flatter” or simpler spectrum for signals with sharp transitions when differenced. For example, the DC component (\\(k=0\\)) of the difference signal is very small (ideally zero for a perfect step), which is reflected in its coefficient."
  },
  {
    "objectID": "ss_37.html#parsevals-relation-for-dt-periodic-signals",
    "href": "ss_37.html#parsevals-relation-for-dt-periodic-signals",
    "title": "Signals and Systems",
    "section": "Parseval’s Relation for DT Periodic Signals",
    "text": "Parseval’s Relation for DT Periodic Signals\n\\[ \\frac{1}{N} \\sum_{n=\\langle N\\rangle}|x[n]|^{2}=\\sum_{k=\\langle N\\rangle}\\left|a_{k}\\right|^{2} \\]\nThis relation states that the average power in one period of \\(x[n]\\) (LHS) equals the sum of the average powers in all its harmonic components (RHS).\n\n\n\n\n\n\nTip\n\n\nThis is a powerful conservation-of-energy principle, linking the time-domain energy (or power) of a signal to its frequency-domain representation. It applies to both CT and DT signals, with appropriate changes in summation/integration and scaling factors.\n\n\n\nLet’s verify Parseval’s relation for a discrete-time square wave.\n\n\n\nviewof N_parseval = Inputs.number([1, 20], {\n  label: \"N (Period):\",\n  step: 1,\n  value: 10\n})\n\nviewof P_parseval = Inputs.number([1, 15], {\n  label: \"P (Pulse Width, 2N1+1):\",\n  step: 1,\n  value: 3\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe output clearly shows that the calculated LHS and RHS are equal, verifying Parseval’s relation for this discrete-time square wave. This confirms that the total average power of the signal is conserved whether you measure it in the time domain or sum the powers of its frequency components. This is a fundamental concept in signal processing for power analysis."
  },
  {
    "objectID": "ss_37.html#example-3.13-linearity",
    "href": "ss_37.html#example-3.13-linearity",
    "title": "Signals and Systems",
    "section": "Example 3.13: Linearity",
    "text": "Example 3.13: Linearity\nFind the Fourier series coefficients \\(a_k\\) of \\(x[n]\\) shown below.\n\\(x[n]\\) can be decomposed into \\(x_1[n]\\) (square wave) and \\(x_2[n]\\) (DC component).\n\n\n\n\n\n\nTip\n\n\nStrategy:\n\nDecompose \\(x[n]\\) into simpler components whose coefficients are known or easy to find.\nUse the linearity property to sum the coefficients.\n\n\n\n\n\n\n\n\n\ngraph TD\n    A[\"x[n]\"] --&gt; B{Linearity Property}\n    B --&gt; C[\"x1[n]\"]\n    B --&gt; D[\"x2[n]\"]\n    C --&gt; E[\"b_k\"]\n    D --&gt; F[\"c_k (DC component)\"]\n    E & F --&gt; G[\"a_k = b_k + c_k\"]\n\n\n\n\n\n\n\nThis example demonstrates the power of the linearity property. Instead of directly calculating the Fourier series for the complex signal \\(x[n]\\), we can break it down into two simpler signals: a standard square wave \\(x_1[n]\\) and a DC offset \\(x_2[n]\\). We already know how to find the coefficients for these simpler signals from previous examples or basic definitions. Then, we just add their coefficients to get the coefficients for \\(x[n]\\)."
  },
  {
    "objectID": "ss_37.html#example-3.13-visualizing-components-and-coefficients",
    "href": "ss_37.html#example-3.13-visualizing-components-and-coefficients",
    "title": "Signals and Systems",
    "section": "Example 3.13: Visualizing Components and Coefficients",
    "text": "Example 3.13: Visualizing Components and Coefficients\n\\(x[n]\\) with \\(N=5\\). \\(x_1[n]\\) is a square wave with \\(N_1=1\\), and \\(x_2[n]\\) is a DC component.\n\n\n\n\n\n\n\n\n\n\nThe top plot shows the original signal \\(x[n]\\) (orange), its square wave component \\(x_1[n]\\) (blue), and its DC component \\(x_2[n]\\) (green). You can clearly see how \\(x[n]\\) is the sum of \\(x_1[n]\\) and \\(x_2[n]\\). The bottom plot shows the magnitudes of their respective Fourier coefficients. Notice how \\(|a_k|\\) (orange) is the sum of \\(|b_k|\\) (blue) and \\(|c_k|\\) (green) for each \\(k\\). Specifically, for \\(k=0\\), \\(a_0 = b_0 + c_0 = 3/5 + 1 = 8/5\\). For other \\(k\\), \\(c_k=0\\), so \\(a_k = b_k\\). This visual confirms the linearity property in action."
  },
  {
    "objectID": "ss_37.html#example-3.14-minimum-power-signal-reconstruction",
    "href": "ss_37.html#example-3.14-minimum-power-signal-reconstruction",
    "title": "Signals and Systems",
    "section": "Example 3.14: Minimum Power & Signal Reconstruction",
    "text": "Example 3.14: Minimum Power & Signal Reconstruction\nProblem: Determine \\(x[n]\\) given:\n\n\\(x[n]\\) is periodic with \\(N=6\\).\n\\(\\sum_{n=0}^{5} x[n]=2\\).\n\\(\\sum_{n=2}^{7}(-1)^{n} x[n]=1\\).\n\\(x[n]\\) has the minimum power per period among signals satisfying 1-3.\n\nSolution Steps:\n\nFrom Fact 2: \\(a_0 = \\frac{1}{N} \\sum x[n] = \\frac{1}{6}(2) = \\frac{1}{3}\\).\nFrom Fact 3: \\((-1)^n = e^{-j\\pi n} = e^{-j(2\\pi/6)3n}\\), so this term corresponds to \\(k=3\\). Thus, \\(a_3 = \\frac{1}{N} \\sum (-1)^n x[n] = \\frac{1}{6}(1) = \\frac{1}{6}\\).\nFrom Fact 4 (Minimum Power): Parseval’s relation states \\(P = \\sum_{k=\\langle N\\rangle} |a_k|^2\\). To minimize power, all other coefficients (\\(a_1, a_2, a_4, a_5\\)) must be zero.\n\nTherefore, \\(x[n]\\) only has \\(a_0\\) and \\(a_3\\) as non-zero coefficients.\n\\(x[n] = a_0 e^{j0(2\\pi/6)n} + a_3 e^{j3(2\\pi/6)n} = a_0 + a_3 e^{j\\pi n} = \\frac{1}{3} + \\frac{1}{6}(-1)^n\\).\n\nThis example is a classic problem that ties together several properties. We use the definition of the Fourier series coefficients to extract \\(a_0\\) and \\(a_3\\) from the given summations. Then, the critical insight comes from Parseval’s relation: to minimize the total average power, any coefficient not explicitly determined must be zero. This allows us to uniquely determine the signal \\(x[n]\\) from partial information about its spectrum. Let’s visualize this reconstructed signal."
  },
  {
    "objectID": "ss_37.html#example-3.14-visualizing-reconstructed-xn",
    "href": "ss_37.html#example-3.14-visualizing-reconstructed-xn",
    "title": "Signals and Systems",
    "section": "Example 3.14: Visualizing Reconstructed \\(x[n]\\)",
    "text": "Example 3.14: Visualizing Reconstructed \\(x[n]\\)\nThe reconstructed signal: \\(x[n] = \\frac{1}{3} + \\frac{1}{6}(-1)^n\\).\n\n\n\n\n\n\n\nHere’s the signal \\(x[n]\\) that satisfies all the given conditions. It’s a simple, alternating sequence, which makes sense given that only the DC component (\\(a_0\\)) and the highest frequency component (\\(a_3\\) for \\(N=6\\)) were non-zero. This demonstrates how spectral information, combined with power minimization, can uniquely define a signal."
  },
  {
    "objectID": "ss_37.html#example-3.15-periodic-convolution",
    "href": "ss_37.html#example-3.15-periodic-convolution",
    "title": "Signals and Systems",
    "section": "Example 3.15: Periodic Convolution",
    "text": "Example 3.15: Periodic Convolution\nProblem: Determine and sketch \\(w[n]\\) given its Fourier series coefficients \\(c_k\\). \\(w[n]\\) is periodic with \\(N=7\\), and \\(c_k = \\frac{\\sin^2(3\\pi k/7)}{7\\sin^2(\\pi k/7)}\\).\nStrategy:\n\nRecognize \\(c_k\\) as a product of known coefficient forms.\nUse the periodic convolution property to find \\(w[n]\\).\n\nWe observe \\(c_k = 7 d_k^2\\), where \\(d_k\\) are the coefficients of a square wave \\(x[n]\\) with \\(N=7\\) and \\(N_1=1\\) (pulse width \\(P=3\\)).\nSince \\(c_k = 7 d_k d_k\\), by the periodic convolution property (\\(N a_k b_k \\longleftrightarrow \\sum x[r]y[n-r]\\)), we have:\n\\[ w[n] = \\sum_{r=\\langle 7\\rangle} x[r] x[n-r] \\]\nThis means \\(w[n]\\) is the periodic convolution of the square wave \\(x[n]\\) with itself.\n\nThis example takes a different approach: starting from the Fourier coefficients and working back to the time-domain signal. The key is recognizing the structure of \\(c_k\\). It looks like the square of the coefficients of a standard discrete-time square wave. This immediately points us to the periodic convolution property. So, \\(w[n]\\) is the periodic convolution of a square wave with itself. This is a powerful technique for finding signals whose spectra are products of known spectra. Next, we’ll visualize the square wave and its self-convolution."
  },
  {
    "objectID": "ss_37.html#example-3.15-visualizing-periodic-convolution",
    "href": "ss_37.html#example-3.15-visualizing-periodic-convolution",
    "title": "Signals and Systems",
    "section": "Example 3.15: Visualizing Periodic Convolution",
    "text": "Example 3.15: Visualizing Periodic Convolution\n\\(x[n]\\) is a square wave with \\(N=7\\) and \\(P=3\\) (\\(N_1=1\\)). \\(w[n]\\) is its periodic convolution with itself: \\(w[n] = (x * x)[n]\\).\n\n\n\n\n\n\n\nThe top plot shows the original square wave \\(x[n]\\) with \\(N=7\\) and a pulse width of 3. The bottom plot shows the result of its periodic convolution with itself, \\(w[n]\\). Notice how the convolution operation “smoothes” the signal, creating a triangular shape (a discrete-time triangle wave). This is a common result when convolving a pulse with itself, and seeing it derived through the frequency domain (by squaring the coefficients) is a powerful illustration of the convolution property."
  },
  {
    "objectID": "ss_37.html#summary-of-dtfs-properties",
    "href": "ss_37.html#summary-of-dtfs-properties",
    "title": "Signals and Systems",
    "section": "Summary of DTFS Properties",
    "text": "Summary of DTFS Properties\n\nPeriodicity: Both \\(x[n]\\) and \\(a_k\\) are periodic with period \\(N\\).\nLinearity: \\(A x[n]+B y[n] \\longleftrightarrow A a_{k}+B b_{k}\\).\nTime Shift: \\(x[n-n_0] \\longleftrightarrow a_k e^{-j k(2\\pi/N)n_0}\\) (phase shift).\nMultiplication: \\(x[n] y[n] \\longleftrightarrow \\sum_{l=\\langle N\\rangle} a_l b_{k-l}\\) (periodic convolution of coefficients).\nPeriodic Convolution: \\(\\sum_{r=\\langle N\\rangle} x[r] y[n-r] \\longleftrightarrow N a_k b_k\\) (product of coefficients).\nFirst Difference: \\(x[n]-x[n-1] \\longleftrightarrow (1-e^{-j k(2\\pi/N)}) a_k\\).\nParseval’s Relation: Average power is conserved across domains. \\[ \\frac{1}{N} \\sum_{n=\\langle N\\rangle}|x[n]|^{2}=\\sum_{k=\\langle N\\rangle}\\left|a_{k}\\right|^{2} \\]\n\n\n\n\n\n\n\nCaution\n\n\nAlways remember the periodic nature of DTFS coefficients and the finite summation in periodic convolution, as these are key distinctions from CTFS.\n\n\n\n\nIn summary, the properties of the Discrete-Time Fourier Series provide a powerful toolbox for signal analysis. They allow us to understand how operations in the time domain affect the frequency domain, and vice-versa. The finite nature of the series and the periodicity of its coefficients are central to all these properties. Mastering these properties will be essential as we move on to more complex signal processing concepts. Thank you."
  },
  {
    "objectID": "ss_35.html#introduction-to-fourier-series-properties",
    "href": "ss_35.html#introduction-to-fourier-series-properties",
    "title": "Signals and Systems",
    "section": "Introduction to Fourier Series Properties",
    "text": "Introduction to Fourier Series Properties\nFourier series representations possess important properties useful for conceptual insights and simplifying calculations.\nThese properties help us understand how operations on a signal in the time domain affect its Fourier series coefficients.\n\nToday, we’ll dive into the fundamental properties of Continuous-Time Fourier Series. Understanding these properties is crucial because they not only provide deeper insights into signal behavior but also offer powerful shortcuts for analyzing complex signals, often avoiding direct, tedious calculations."
  },
  {
    "objectID": "ss_35.html#shorthand-notation-for-fourier-series",
    "href": "ss_35.html#shorthand-notation-for-fourier-series",
    "title": "Signals and Systems",
    "section": "Shorthand Notation for Fourier Series",
    "text": "Shorthand Notation for Fourier Series\nTo simplify discussion, we use a shorthand notation.\nIf \\(x(t)\\) is a periodic signal with period \\(T\\) and fundamental frequency \\(\\omega_0 = 2\\pi/T\\), and its Fourier series coefficients are \\(a_k\\), we write:\n\\[\nx(t) \\stackrel{\\mathcal{F S}}{\\longleftrightarrow} a_{k}\n\\]\n\nThis notation, \\(x(t)\\) double-arrow FS \\(a_k\\), simply means that \\(a_k\\) are the complex Fourier series coefficients corresponding to the periodic signal \\(x(t)\\). It’s a convenient way to express the Fourier series pair without writing out the full synthesis or analysis equations every time."
  },
  {
    "objectID": "ss_35.html#linearity",
    "href": "ss_35.html#linearity",
    "title": "Signals and Systems",
    "section": "Linearity",
    "text": "Linearity\nIf \\(x(t)\\) and \\(y(t)\\) are periodic with period \\(T\\), with coefficients \\(a_k\\) and \\(b_k\\) respectively:\n\\[\n\\begin{aligned}\n& x(t) \\stackrel{\\mathcal{F S}}{\\longleftrightarrow} a_{k}, \\\\\n& y(t) \\stackrel{\\mathcal{F S}}{\\longleftrightarrow} b_{k} .\n\\end{aligned}\n\\]\nThen, a linear combination of these signals, \\(z(t) = A x(t) + B y(t)\\), will have Fourier series coefficients \\(c_k\\):\n\\[\nz(t)=A x(t)+B y(t) \\stackrel{\\mathcal{F S}}{\\longleftrightarrow} c_{k}=A a_{k}+B b_{k}\n\\]\n\nLinearity is one of the most fundamental properties. It tells us that if we combine signals in a linear way, their Fourier coefficients combine in the exact same linear way. This is incredibly useful because it allows us to break down complex signals into simpler components, analyze each component separately, and then combine the results. The proof follows directly from the definition of the Fourier series analysis equation."
  },
  {
    "objectID": "ss_35.html#linearity-interactive-demonstration",
    "href": "ss_35.html#linearity-interactive-demonstration",
    "title": "Signals and Systems",
    "section": "Linearity: Interactive Demonstration",
    "text": "Linearity: Interactive Demonstration\nLet’s see linearity in action with two simple periodic signals.\n\n\n\n\n\n\n\nIn this Python example, we simulate the Fourier coefficients for two simple signals, a sine wave and a cosine wave. We then apply arbitrary scaling factors, A and B, to combine these signals. The output clearly shows that the coefficients of the combined signal are simply the scaled sum of the individual coefficients, confirming the linearity property. While this uses hypothetical coefficients for simplicity, it captures the essence."
  },
  {
    "objectID": "ss_35.html#time-shifting",
    "href": "ss_35.html#time-shifting",
    "title": "Signals and Systems",
    "section": "Time Shifting",
    "text": "Time Shifting\nWhen a periodic signal \\(x(t)\\) is shifted in time by \\(t_0\\) to become \\(y(t) = x(t-t_0)\\), its period \\(T\\) remains unchanged.\nThe new Fourier series coefficients \\(b_k\\) are related to the original coefficients \\(a_k\\) by:\n\\[\nx\\left(t-t_{0}\\right) \\stackrel{\\mathcal{F S}}{\\longleftrightarrow} e^{-j k \\omega_{0} t_{0}} a_{k}=e^{-j k(2 \\pi / T) t_{0}} a_{k}\n\\]\n\n\n\n\n\n\nTip\n\n\nKey Insight: Time shifting a signal only changes the phase of its Fourier coefficients, not their magnitudes.\n\n\n\n\nTime shifting is a very intuitive operation. If you take a periodic signal and just slide it along the time axis, its fundamental frequency and the ‘strength’ of its harmonics don’t change. What does change is the starting phase of each harmonic component. The exponential term, \\(e^{-j k \\omega_{0} t_{0}}\\), represents this phase shift, which is linear with the harmonic index \\(k\\). This property is crucial for understanding delays and propagation effects in systems."
  },
  {
    "objectID": "ss_35.html#time-shifting-interactive-visualization",
    "href": "ss_35.html#time-shifting-interactive-visualization",
    "title": "Signals and Systems",
    "section": "Time Shifting: Interactive Visualization",
    "text": "Time Shifting: Interactive Visualization\nObserve a square wave and its time-shifted version. Notice how the magnitude of coefficients remains the same, but the phase changes.\n\nviewof t0_slider = Inputs.range([-2, 2], {\n  label: \"Shift Amount (t0)\",\n  step: 0.1,\n  value: 0\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive plot demonstrates the time-shifting property. The top panel shows the original square wave and its shifted version. As you adjust the Shift Amount (t0) slider, you’ll see the red dashed line (the shifted signal) move. Crucially, look at the middle panel: the magnitudes of the Fourier coefficients for both signals are identical. However, in the bottom panel, the phases of the shifted signal’s coefficients exhibit a linear change with respect to the harmonic index \\(k\\), which is exactly what the formula \\(e^{-j k \\omega_{0} t_{0}}\\) predicts."
  },
  {
    "objectID": "ss_35.html#time-reversal",
    "href": "ss_35.html#time-reversal",
    "title": "Signals and Systems",
    "section": "Time Reversal",
    "text": "Time Reversal\nWhen a periodic signal \\(x(t)\\) undergoes time reversal to become \\(y(t) = x(-t)\\), its period \\(T\\) remains unchanged.\nThe new Fourier series coefficients \\(b_k\\) are related to the original coefficients \\(a_k\\) by:\n\\[\nx(-t) \\stackrel{\\mathcal{F S}}{\\longleftrightarrow} a_{-k}\n\\]\n\n\n\n\n\n\nNote\n\n\nSymmetry Implication:\nIf \\(x(t)\\) is even (\\(x(-t) = x(t)\\)), then \\(a_k = a_{-k}\\).\nIf \\(x(t)\\) is odd (\\(x(-t) = -x(t)\\)), then \\(a_k = -a_{-k}\\).\n\n\n\n\nTime reversal flips the signal about the vertical axis at \\(t=0\\). The property states that the Fourier coefficients are also ‘flipped’ in their index. So the \\(k\\)-th coefficient of \\(x(-t)\\) is the negative \\(k\\)-th coefficient of \\(x(t)\\). This has direct implications for symmetric signals. For an even signal, the coefficients are also even; for an odd signal, the coefficients are odd. This can save a lot of calculation if you recognize the symmetry."
  },
  {
    "objectID": "ss_35.html#time-reversal-interactive-visualization",
    "href": "ss_35.html#time-reversal-interactive-visualization",
    "title": "Signals and Systems",
    "section": "Time Reversal: Interactive Visualization",
    "text": "Time Reversal: Interactive Visualization\nObserve an asymmetric periodic signal and its time-reversed version.\n\n\n\n\n\n\n\nHere, we visualize time reversal using a sawtooth wave, which is an asymmetric signal. The top plot shows the original signal and its flipped version. In the coefficient plots, you’ll notice that the magnitudes are symmetric (\\(|a_k| = |a_{-k}|\\)), but the phases are anti-symmetric (\\(\\angle a_k = -\\angle a_{-k}\\)), reflecting the \\(a_{-k}\\) relationship. This clearly illustrates that reversing a signal in time corresponds to ‘reversing’ the indices of its Fourier coefficients."
  },
  {
    "objectID": "ss_35.html#time-scaling",
    "href": "ss_35.html#time-scaling",
    "title": "Signals and Systems",
    "section": "Time Scaling",
    "text": "Time Scaling\nIf \\(x(t)\\) is periodic with period \\(T\\) and fundamental frequency \\(\\omega_0\\), then \\(x(\\alpha t)\\) (for \\(\\alpha &gt; 0\\)) is periodic with period \\(T/\\alpha\\) and fundamental frequency \\(\\alpha \\omega_0\\).\nThe Fourier coefficients for \\(x(\\alpha t)\\) are the same as for \\(x(t)\\), i.e., \\(a_k\\).\n\\[\nx(\\alpha t)=\\sum_{k=-\\infty}^{+\\infty} a_{k} e^{j k\\left(\\alpha \\omega_{0}\\right) t}\n\\]\n\n\n\n\n\n\nCaution\n\n\nImportant: While the coefficients \\(a_k\\) remain the same, the series representation changes because the fundamental frequency is now \\(\\alpha \\omega_0\\).\n\n\n\n\nTime scaling stretches or compresses the signal along the time axis. If you compress it (alpha &gt; 1), it speeds up, and its period shortens. If you stretch it (alpha &lt; 1), it slows down, and its period lengthens. The key here is that the weights or amplitudes of the harmonic components (the \\(a_k\\)’s) don’t change. What changes is the frequency at which each harmonic occurs, effectively scaling the fundamental frequency by \\(\\alpha\\)."
  },
  {
    "objectID": "ss_35.html#time-scaling-interactive-visualization",
    "href": "ss_35.html#time-scaling-interactive-visualization",
    "title": "Signals and Systems",
    "section": "Time Scaling: Interactive Visualization",
    "text": "Time Scaling: Interactive Visualization\nObserve how scaling affects the signal’s period and fundamental frequency, while the coefficients themselves remain invariant.\n\nviewof alpha_slider = Inputs.range([0.5, 2], {\n  label: \"Scaling Factor (α)\",\n  step: 0.1,\n  value: 1\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive plot helps illustrate the effect of time scaling. As you adjust the Scaling Factor (α), you’ll see the dashed red line (the scaled signal) either compress or expand. Notice that the period of the scaled signal changes to \\(T/\\alpha\\). However, if you look at the bottom panel, the Fourier coefficients a_k themselves (their magnitudes) remain exactly the same. What changes is the mapping of these coefficients to the actual frequencies in the synthesis equation, as the fundamental frequency becomes \\(\\alpha \\omega_0\\)."
  },
  {
    "objectID": "ss_35.html#multiplication",
    "href": "ss_35.html#multiplication",
    "title": "Signals and Systems",
    "section": "Multiplication",
    "text": "Multiplication\nIf \\(x(t)\\) and \\(y(t)\\) are periodic with period \\(T\\), with coefficients \\(a_k\\) and \\(b_k\\) respectively:\n\\[\n\\begin{aligned}\n& x(t) \\stackrel{\\mathcal{F S}}{\\longleftrightarrow} a_{k}, \\\\\n& y(t) \\stackrel{\\mathcal{F S}}{\\longleftrightarrow} b_{k} .\n\\end{aligned}\n\\]\nThe Fourier series coefficients \\(h_k\\) of their product \\(x(t)y(t)\\) are given by the discrete-time convolution of \\(a_k\\) and \\(b_k\\):\n\\[\nx(t) y(t) \\stackrel{\\mathcal{F S}}{\\longleftrightarrow} h_{k}=\\sum_{l=-\\infty}^{\\infty} a_{l} b_{k-l}\n\\]\n\nThe multiplication property is often called the convolution property in the frequency domain. Just as convolution in the time domain corresponds to multiplication in the frequency domain for Fourier transforms, here, multiplication of two periodic signals in the time domain corresponds to the convolution of their discrete Fourier series coefficients. This is a powerful concept, especially when analyzing systems where signals are modulated or mixed."
  },
  {
    "objectID": "ss_35.html#conjugation-and-conjugate-symmetry",
    "href": "ss_35.html#conjugation-and-conjugate-symmetry",
    "title": "Signals and Systems",
    "section": "Conjugation and Conjugate Symmetry",
    "text": "Conjugation and Conjugate Symmetry\nTaking the complex conjugate of a periodic signal \\(x(t)\\) has the effect of complex conjugation and time reversal on its Fourier series coefficients:\n\\[\nx^{*}(t) \\stackrel{\\mathcal{F S}}{\\longleftrightarrow} a_{-k}^{*}\n\\]\nA significant consequence for real signals (\\(x(t) = x^*(t)\\)) is conjugate symmetry:\n\\[\na_{-k}=a_{k}^{*}\n\\]\nThis implies various symmetry properties for magnitudes, phases, real, and imaginary parts of coefficients for real signals.\n\nThis property is fundamental when dealing with real-world signals, which are always real-valued. If a signal is real, its Fourier coefficients must satisfy conjugate symmetry. This means \\(a_0\\) must be real, the magnitudes \\(|a_k|\\) are even (\\(|a_k| = |a_{-k}|\\)), and the phases \\(\\angle a_k\\) are odd (\\(\\angle a_k = -\\angle a_{-k}\\)). These symmetries simplify calculations and provide a way to check for errors."
  },
  {
    "objectID": "ss_35.html#parsevals-relation-for-continuous-time-periodic-signals",
    "href": "ss_35.html#parsevals-relation-for-continuous-time-periodic-signals",
    "title": "Signals and Systems",
    "section": "Parseval’s Relation for Continuous-Time Periodic Signals",
    "text": "Parseval’s Relation for Continuous-Time Periodic Signals\nParseval’s relation states that the average power of a periodic signal in the time domain equals the sum of the average powers of its harmonic components in the frequency domain.\n\\[\n\\frac{1}{T} \\int_{T}|x(t)|^{2} d t=\\sum_{k=-\\infty}^{+\\infty}\\left|a_{k}\\right|^{2}\n\\]\nThe left side is the average power in one period of \\(x(t)\\). The term \\(|a_k|^2\\) represents the average power in the \\(k\\)-th harmonic component.\n\n\n\n\n\n\nImportant\n\n\nConservation of Energy: This relation highlights the conservation of power between the time and frequency domains.\n\n\n\n\nParseval’s relation is a cornerstone of signal analysis. It tells us that we can quantify the total energy or power in a signal by either integrating its squared magnitude over time or by summing the squared magnitudes of its Fourier coefficients. This is incredibly useful for power calculations, noise analysis, and understanding how power is distributed across different frequency components."
  },
  {
    "objectID": "ss_35.html#parsevals-relation-interactive-demonstration",
    "href": "ss_35.html#parsevals-relation-interactive-demonstration",
    "title": "Signals and Systems",
    "section": "Parseval’s Relation: Interactive Demonstration",
    "text": "Parseval’s Relation: Interactive Demonstration\nLet’s verify Parseval’s relation for a simple square wave.\n\n\n\n\n\n\n\nIn this example, we calculate the average power of a square wave in two ways. First, directly from its time-domain definition by integrating its squared magnitude over one period. Second, by summing the squared magnitudes of its Fourier series coefficients. Notice how closely these two values match, especially as we increase the number of harmonics considered. This vividly demonstrates Parseval’s relation, confirming that power is conserved across domains."
  },
  {
    "objectID": "ss_35.html#summary-of-properties",
    "href": "ss_35.html#summary-of-properties",
    "title": "Signals and Systems",
    "section": "Summary of Properties",
    "text": "Summary of Properties\nMany important properties of continuous-time Fourier series are summarized in the table below.\n\n\n\nProperty\nPeriodic Signal\nFourier Series Coeffs\n\n\n\n\nLinearity\n\\(Ax(t) + By(t)\\)\n\\(Aa_k + Bb_k\\)\n\n\nTime Shift\n\\(x(t-t_0)\\)\n\\(a_k e^{-jk\\omega_0 t_0}\\)\n\n\nFreq Shift\n\\(e^{jM\\omega_0 t}x(t)\\)\n\\(a_{k-M}\\)\n\n\nConjugation\n\\(x^*(t)\\)\n\\(a_{-k}^*\\)\n\n\nTime Reversal\n\\(x(-t)\\)\n\\(a_{-k}\\)\n\n\nTime Scaling\n\\(x(\\alpha t)\\)\n\\(a_k\\) (with new \\(\\omega_0\\))"
  },
  {
    "objectID": "ss_35.html#summary-of-properties-1",
    "href": "ss_35.html#summary-of-properties-1",
    "title": "Signals and Systems",
    "section": "Summary of Properties",
    "text": "Summary of Properties\n\n\n\nProperty\nPeriodic Signal\nFourier Series Coeffs\n\n\n\n\nPeriodic Convolution\n\\(\\int_T x(\\tau)y(t-\\tau)d\\tau\\)\n\\(T a_k b_k\\)\n\n\nMultiplication\n\\(x(t)y(t)\\)\n\\(\\sum_{l=-\\infty}^\\infty a_l b_{k-l}\\)\n\n\nDifferentiation\n\\(dx(t)/dt\\)\n\\(jk\\omega_0 a_k\\)\n\n\nIntegration\n\\(\\int x(\\tau)d\\tau\\)\n\\((1/(jk\\omega_0)) a_k\\)\n\n\nReal Signal\n\\(x(t)\\) real\n\\(a_k = a_{-k}^*\\)\n\n\nReal & Even\n\\(x(t)\\) real & even\n\\(a_k\\) real & even\n\n\nReal & Odd\n\\(x(t)\\) real & odd\n\\(a_k\\) purely imag. & odd\n\n\n\n\\[\n\\frac{1}{T} \\int_{T}|x(t)|^{2} d t=\\sum_{k=-\\infty}^{+\\infty}\\left|a_{k}\\right|^{2} \\quad \\text{(Parseval's Relation)}\n\\]\n\nThis table provides a comprehensive overview of all the key properties we’ve discussed and some additional ones. It’s an invaluable reference for any Fourier series problem. Notice how operations in one domain (time) often have a dual or inverse effect in the other domain (frequency). For instance, differentiation in time corresponds to multiplication by \\(jk\\omega_0\\) in the frequency domain. These dualities are a recurring theme in signal processing."
  },
  {
    "objectID": "ss_35.html#example-3.6-using-linearity-and-time-shifting",
    "href": "ss_35.html#example-3.6-using-linearity-and-time-shifting",
    "title": "Signals and Systems",
    "section": "Example 3.6: Using Linearity and Time Shifting",
    "text": "Example 3.6: Using Linearity and Time Shifting\nConsider the signal \\(g(t)\\) with period 4.\nWe know \\(g(t) = x(t-1) - 1/2\\), where \\(x(t)\\) is a symmetric square wave with \\(T=4, T_1=1\\).\nThe coefficients \\(a_k\\) for \\(x(t)\\) are \\(\\frac{\\sin(\\pi k/2)}{k\\pi}\\) (for \\(k \\neq 0\\)) and \\(a_0 = 1/2\\).\n\n\n\n\n\ngraph LR\n    A[\"x(t) - Square Wave T=4, T1=1\"] --&gt; B{\"Shift by 1: x(t-1)\"}\n    C[\"Constant -1/2\"] --&gt; D{Add to B}\n    B --&gt; E[\"g(t) = x(t-1) - 1/2\"]\n    D --&gt; E\n\n\n\n\n\n\n\nThis example shows how to leverage properties to find Fourier coefficients without direct integration. We’re given a signal \\(g(t)\\) that looks like a shifted and DC-offset version of a standard square wave \\(x(t)\\). The square wave \\(x(t)\\) has easily derivable coefficients. By recognizing \\(g(t)\\) as a transformation of \\(x(t)\\), we can use the linearity and time-shifting properties."
  },
  {
    "objectID": "ss_35.html#example-3.6-deriving-coefficients",
    "href": "ss_35.html#example-3.6-deriving-coefficients",
    "title": "Signals and Systems",
    "section": "Example 3.6: Deriving Coefficients",
    "text": "Example 3.6: Deriving Coefficients\n\nTime Shifting: Coefficients \\(b_k\\) for \\(x(t-1)\\) are \\(a_k e^{-j k \\omega_0 t_0}\\).\nHere, \\(\\omega_0 = 2\\pi/T = 2\\pi/4 = \\pi/2\\) and \\(t_0 = 1\\).\nSo, \\(b_k = a_k e^{-j k (\\pi/2)(1)} = a_k e^{-j k \\pi/2}\\).\nDC Offset: The term \\(-1/2\\) has coefficients \\(c_k = -1/2\\) for \\(k=0\\) and \\(0\\) for \\(k \\neq 0\\).\nLinearity: Coefficients \\(d_k\\) for \\(g(t) = x(t-1) - 1/2\\) are \\(b_k + c_k\\).\n\\[\nd_{k}=\\left\\{\\begin{array}{ll}\n\\frac{\\sin (\\pi k / 2)}{k \\pi} e^{-j k \\pi / 2}, & \\text { for } k \\neq 0 \\\\\n0, & \\text { for } k=0\n\\end{array}\\right.\n\\]\n\n\nFirst, we apply the time-shifting property. We know the coefficients of the unshifted square wave, \\(x(t)\\). Shifting it by 1 in time simply multiplies each coefficient \\(a_k\\) by a phase factor \\(e^{-j k \\pi/2}\\). Next, we consider the DC offset. A constant value in the time domain only contributes to the \\(a_0\\) coefficient in the frequency domain. Finally, we use linearity: the coefficients of the sum are the sum of the coefficients. We combine the shifted square wave’s coefficients with the DC offset’s coefficients to get the coefficients for \\(g(t)\\). Notice how the \\(a_0\\) term for \\(x(t)\\) (\\(1/2\\)) combined with the DC offset (\\(-1/2\\)) to yield \\(d_0=0\\)."
  },
  {
    "objectID": "ss_35.html#example-3.7-using-differentiation",
    "href": "ss_35.html#example-3.7-using-differentiation",
    "title": "Signals and Systems",
    "section": "Example 3.7: Using Differentiation",
    "text": "Example 3.7: Using Differentiation\nConsider the triangular wave signal \\(x(t)\\) with period \\(T=4\\).\nIts derivative, \\(dx(t)/dt\\), is the square wave \\(g(t)\\) from Example 3.6.\nIf \\(e_k\\) are the coefficients for \\(x(t)\\) and \\(d_k\\) for \\(g(t)\\), the differentiation property states:\n\\[\nd_{k}=j k \\omega_{0} e_{k}\n\\]\nSince \\(\\omega_0 = \\pi/2\\):\n\\[\nd_{k}=j k (\\pi/2) e_{k}\n\\]\n\nThis example showcases the differentiation property. We have a triangular wave, and its derivative is a square wave, which we just analyzed in Example 3.6. Instead of directly calculating the Fourier series for the triangular wave (which involves integration by parts), we can use the differentiation property. This property states that differentiating in the time domain corresponds to multiplying the Fourier coefficients by \\(j k \\omega_0\\)."
  },
  {
    "objectID": "ss_35.html#example-3.7-deriving-coefficients",
    "href": "ss_35.html#example-3.7-deriving-coefficients",
    "title": "Signals and Systems",
    "section": "Example 3.7: Deriving Coefficients",
    "text": "Example 3.7: Deriving Coefficients\nFrom \\(d_k = j k (\\pi/2) e_k\\), we can find \\(e_k\\):\n\\[\ne_{k}=\\frac{2 d_{k}}{j k \\pi}, \\quad k \\neq 0\n\\]\nSubstituting \\(d_k\\) from Example 3.6:\n\\[\ne_{k}=\\frac{2 \\sin (\\pi k / 2)}{j(k \\pi)^{2}} e^{-j k \\pi / 2}, \\quad k \\neq 0\n\\]\nFor \\(k=0\\), \\(e_0\\) is the average value of \\(x(t)\\) over one period:\n\\[\ne_{0}=\\frac{1}{T} \\int_T x(t) dt = \\frac{1}{4} \\times (\\text{Area of triangle}) = \\frac{1}{4} \\times (2) = \\frac{1}{2}\n\\]\n\nWe rearrange the differentiation formula to solve for \\(e_k\\) in terms of \\(d_k\\). We already know \\(d_k\\) from the previous example. For the DC component, \\(e_0\\), we cannot use the differentiation formula because it would involve division by zero. Instead, \\(e_0\\) is simply the average value of the signal, which we can find by calculating the area under one period of the triangular wave and dividing by the period. This demonstrates how properties can be combined with basic definitions to solve problems efficiently."
  },
  {
    "objectID": "ss_35.html#example-3.8-impulse-train-coefficients",
    "href": "ss_35.html#example-3.8-impulse-train-coefficients",
    "title": "Signals and Systems",
    "section": "Example 3.8: Impulse Train Coefficients",
    "text": "Example 3.8: Impulse Train Coefficients\nThe impulse train \\(x(t) = \\sum_{k=-\\infty}^{\\infty} \\delta(t-kT)\\) with period \\(T\\).\nThe Fourier series coefficients \\(a_k\\) are found by integrating over one period (e.g., \\([-T/2, T/2]\\)):\n\\[\na_{k}=\\frac{1}{T} \\int_{-T / 2}^{T / 2} \\delta(t) e^{-j k 2 \\pi / T} d t=\\frac{1}{T}\n\\]\nAll coefficients are equal: \\(a_k = 1/T\\).\n\n\n\n\n\ngraph LR\n    A[…] --- B{…}\n    B -- T --&gt; C{\"δ(t)\"}\n    C -- T --&gt; D{\"δ(t-T)\"}\n    D -- T --&gt; E{\"δ(t-2T)\"}\n    E -- T --&gt; B\n    style C fill:#f9f,stroke:#333,stroke-width:2px\n\n\n\n\n\n\n\nThe impulse train is a very important signal in signal processing. It consists of a series of Dirac delta functions equally spaced in time. To find its Fourier coefficients, we use the analysis equation and integrate over a single period that contains one impulse, typically at \\(t=0\\). Due to the sifting property of the delta function, the integral simply evaluates to the value of the exponential at \\(t=0\\), which is 1. Thus, all Fourier coefficients for an impulse train are constant, \\(1/T\\). This also aligns with the property that a real and even signal (like the impulse train) has real and even coefficients."
  },
  {
    "objectID": "ss_35.html#example-3.8-relating-signals-using-properties",
    "href": "ss_35.html#example-3.8-relating-signals-using-properties",
    "title": "Signals and Systems",
    "section": "Example 3.8: Relating Signals using Properties",
    "text": "Example 3.8: Relating Signals using Properties\nThe derivative of a square wave \\(g(t)\\) is \\(q(t)\\), which can be expressed as a difference of two shifted impulse trains:\n\\[\nq(t)=x\\left(t+T_{1}\\right)-x\\left(t-T_{1}\\right)\n\\]\n\nCoefficients for \\(x(t)\\) are \\(a_k = 1/T\\).\nBy time-shifting and linearity, coefficients \\(b_k\\) for \\(q(t)\\) are:\n\\(b_k = a_k e^{j k \\omega_0 T_1} - a_k e^{-j k \\omega_0 T_1}\\)\n\\(b_k = \\frac{1}{T} (e^{j k \\omega_0 T_1} - e^{-j k \\omega_0 T_1}) = \\frac{2j \\sin(k \\omega_0 T_1)}{T}\\)\n\n\n\n\n\n\ngraph TD\n    A[\"Square Wave g(t)\"]\n    B[\"Derivative d/dt\"]\n    C[\"Impulse Train x(t)\"]\n    D[\"Shifted x(t+T1)\"]\n    E[\"Shifted x(t-T1)\"]\n    F[\"q(t) = D - E\"]\n\n    A -- B --&gt; F\n    C -- Shift --&gt; D\n    C -- Shift --&gt; E\n    D -- Subtract --&gt; F\n    E -- Subtract --&gt; F\n\n\n\n\n\n\n\nThis part of the example cleverly connects several properties. We observe that the derivative of a square wave is a series of positive and negative impulses. This q(t) can be seen as the difference between two time-shifted impulse trains. We already know the coefficients for a basic impulse train. By applying the time-shifting property to get the coefficients for \\(x(t+T_1)\\) and \\(x(t-T_1)\\), and then the linearity property for their difference, we can quickly derive the Fourier coefficients \\(b_k\\) for \\(q(t)\\)."
  },
  {
    "objectID": "ss_35.html#example-3.8-connecting-back-to-square-wave",
    "href": "ss_35.html#example-3.8-connecting-back-to-square-wave",
    "title": "Signals and Systems",
    "section": "Example 3.8: Connecting Back to Square Wave",
    "text": "Example 3.8: Connecting Back to Square Wave\nSince \\(q(t)\\) is the derivative of \\(g(t)\\), we use the differentiation property:\n\\[\nb_{k}=j k \\omega_{0} c_{k}\n\\]\nWhere \\(c_k\\) are the Fourier coefficients of \\(g(t)\\).\nSolving for \\(c_k\\) (for \\(k \\neq 0\\)):\n\\[\nc_{k}=\\frac{b_{k}}{j k \\omega_{0}}=\\frac{2 j \\sin \\left(k \\omega_{0} T_{1}\\right)}{j k \\omega_{0} T}=\\frac{\\sin \\left(k \\omega_{0} T_{1}\\right)}{k \\pi}, \\quad k \\neq 0\n\\]\nFor \\(c_0\\), the average value of \\(g(t)\\) from inspection of a square wave with width \\(2T_1\\):\n\\[\nc_{0}=\\frac{2 T_{1}}{T}\n\\]\nThese are the same coefficients derived directly for a square wave!\n\nFinally, we tie it all together. Since \\(q(t)\\) is the derivative of \\(g(t)\\), we can use the differentiation property in reverse. We divide the coefficients \\(b_k\\) (which we just found for \\(q(t)\\)) by \\(jk\\omega_0\\) to get the coefficients \\(c_k\\) for \\(g(t)\\). The \\(k=0\\) term (DC component) needs to be found separately by calculating the average value of the square wave. This confirms that the Fourier coefficients obtained using these properties match those found by direct integration, demonstrating the power and consistency of these properties."
  },
  {
    "objectID": "ss_35.html#example-3.9-characterizing-a-signal",
    "href": "ss_35.html#example-3.9-characterizing-a-signal",
    "title": "Signals and Systems",
    "section": "Example 3.9: Characterizing a Signal",
    "text": "Example 3.9: Characterizing a Signal\nGiven facts about \\(x(t)\\):\n\n\\(x(t)\\) is a real signal.\n\\(x(t)\\) is periodic with \\(T=4\\), coefficients \\(a_k\\).\n\\(a_k = 0\\) for \\(|k|&gt;1\\).\nThe signal with coefficients \\(b_k = e^{-j\\pi k/2} a_{-k}\\) is odd.\n\\(\\frac{1}{4} \\int_{4}|x(t)|^{2} d t = 1/2\\).\n\nLet’s determine \\(x(t)\\) to within a sign factor.\n\nThis is a comprehensive problem that requires applying multiple properties to deduce the form of an unknown signal. We are given five distinct facts about a signal \\(x(t)\\) and its Fourier coefficients. Our goal is to use these facts to uniquely identify \\(x(t)\\) up to a sign. This is a great example of how understanding these properties allows for powerful deductive reasoning in signal analysis."
  },
  {
    "objectID": "ss_35.html#example-3.9-step-by-step-deduction",
    "href": "ss_35.html#example-3.9-step-by-step-deduction",
    "title": "Signals and Systems",
    "section": "Example 3.9: Step-by-Step Deduction",
    "text": "Example 3.9: Step-by-Step Deduction\nFact 3: \\(a_k = 0\\) for \\(|k|&gt;1\\).\n\\(\\implies x(t) = a_0 + a_1 e^{j\\pi t/2} + a_{-1} e^{-j\\pi t/2}\\) (since \\(\\omega_0 = 2\\pi/4 = \\pi/2\\)).\nFact 1: \\(x(t)\\) is real.\n\\(\\implies a_0\\) is real, and \\(a_1 = a_{-1}^*\\).\nSo, \\(x(t) = a_0 + 2\\operatorname{Re}\\{a_1 e^{j\\pi t/2}\\}\\).\nFact 4: Signal with \\(b_k = e^{-j\\pi k/2} a_{-k}\\) is odd.\n\n\\(a_{-k}\\) corresponds to \\(x(-t)\\) (Time Reversal).\n\\(e^{-j\\pi k/2}\\) corresponds to a time shift of \\(t_0=1\\) (since \\(e^{-jk\\omega_0 t_0}\\) with \\(\\omega_0=\\pi/2, t_0=1\\) gives \\(e^{-jk\\pi/2}\\)).\n\n\\(\\implies b_k\\) correspond to \\(x(-(t-1)) = x(-t+1)\\).\nSince \\(x(-t+1)\\) is odd and real (Fact 1), its Fourier coefficients \\(b_k\\) must be purely imaginary and odd. \\(\\implies b_0 = 0\\) and \\(b_{-1} = -b_1\\).\n\nWe start by using Fact 3 to limit the number of non-zero coefficients to \\(a_0, a_1, a_{-1}\\). Then, Fact 1 (x(t) is real) tells us about the symmetry of these coefficients: \\(a_0\\) is real and \\(a_1\\) is the complex conjugate of \\(a_{-1}\\). This simplifies the time-domain expression for \\(x(t)\\). Fact 4 is where it gets interesting. We recognize \\(a_{-k}\\) as the coefficients for \\(x(-t)\\) (time reversal) and the exponential term as a time shift. So, \\(b_k\\) are the coefficients for \\(x(-t+1)\\). Since this signal is explicitly stated to be odd AND we know \\(x(t)\\) is real (making \\(x(-t+1)\\) also real), its coefficients \\(b_k\\) must be purely imaginary and odd. This immediately gives us \\(b_0=0\\) and \\(b_{-1}=-b_1\\)."
  },
  {
    "objectID": "ss_35.html#example-3.9-final-steps",
    "href": "ss_35.html#example-3.9-final-steps",
    "title": "Signals and Systems",
    "section": "Example 3.9: Final Steps",
    "text": "Example 3.9: Final Steps\nFrom \\(b_0 = 0\\):\n\\(b_0 = e^{-j\\pi (0)/2} a_{-0} = a_0 = 0\\).\nFrom \\(b_{-1} = -b_1\\):\nWe also know \\(b_k = e^{-j\\pi k/2} a_{-k}\\).\nFor \\(k=1\\): \\(b_1 = e^{-j\\pi /2} a_{-1} = -j a_{-1}\\).\nSince \\(a_{-1} = a_1^*\\), we have \\(b_1 = -j a_1^*\\).\nSince \\(b_k\\) are purely imaginary, \\(b_1\\) must be purely imaginary. This is consistent with \\(-j a_1^*\\)."
  },
  {
    "objectID": "ss_35.html#example-3.9-final-steps-1",
    "href": "ss_35.html#example-3.9-final-steps-1",
    "title": "Signals and Systems",
    "section": "Example 3.9: Final Steps",
    "text": "Example 3.9: Final Steps\nFact 5: Parseval’s Relation.\nThe average power of \\(x(-t+1)\\) is also \\(1/2\\).\n\\(\\frac{1}{4} \\int_{4}|x(-t+1)|^{2} d t=1/2\\).\nUsing Parseval’s: \\(\\sum_{k=-\\infty}^\\infty |b_k|^2 = |b_{-1}|^2 + |b_0|^2 + |b_1|^2 = 1/2\\).\nSince \\(b_0=0\\) and \\(b_{-1}=-b_1\\), we have \\(2|b_1|^2 = 1/2 \\implies |b_1|^2 = 1/4 \\implies |b_1| = 1/2\\).\nSince \\(b_1\\) is purely imaginary, \\(b_1 = j/2\\) or \\(b_1 = -j/2\\).\n\nWe use the deduced properties of \\(b_k\\). From \\(b_0 = 0\\), we find that \\(a_0\\) must be zero. This significantly simplifies \\(x(t)\\) to just two exponential terms. Next, we use Parseval’s relation. Since time reversal and time shifting don’t change the average power, the average power of \\(x(-t+1)\\) is also \\(1/2\\). Applying Parseval’s to the \\(b_k\\) coefficients, and recalling that \\(b_0=0\\) and \\(b_{-1}=-b_1\\), we find that \\(|b_1|\\) must be \\(1/2\\). Coupled with the fact that \\(b_1\\) is purely imaginary, this means \\(b_1\\) can only be \\(j/2\\) or \\(-j/2\\)."
  },
  {
    "objectID": "ss_35.html#example-3.9-determining-xt",
    "href": "ss_35.html#example-3.9-determining-xt",
    "title": "Signals and Systems",
    "section": "Example 3.9: Determining \\(x(t)\\)",
    "text": "Example 3.9: Determining \\(x(t)\\)\nWe have \\(a_0=0\\) and \\(b_1 = -j a_1^*\\).\nCase 1: \\(b_1 = j/2\\).\n\\(j/2 = -j a_1^* \\implies a_1^* = -1/2 \\implies a_1 = -1/2\\).\nThen \\(x(t) = 2\\operatorname{Re}\\{(-1/2) e^{j\\pi t/2}\\} = 2(-1/2 \\cos(\\pi t/2)) = -\\cos(\\pi t/2)\\).\nCase 2: \\(b_1 = -j/2\\).\n\\(-j/2 = -j a_1^* \\implies a_1^* = 1/2 \\implies a_1 = 1/2\\).\nThen \\(x(t) = 2\\operatorname{Re}\\{(1/2) e^{j\\pi t/2}\\} = 2(1/2 \\cos(\\pi t/2)) = \\cos(\\pi t/2)\\).\nThus, \\(x(t) = \\pm \\cos(\\pi t/2)\\).\n\nFinally, we substitute the possible values of \\(b_1\\) back into our relationship between \\(a_1\\) and \\(b_1\\). This allows us to determine the possible values for \\(a_1\\). With \\(a_0=0\\) and the determined \\(a_1\\), we can reconstruct \\(x(t)\\) using the synthesis equation. As the problem stated, we arrive at \\(x(t)\\) being \\(\\cos(\\pi t/2)\\) or \\(-\\cos(\\pi t/2)\\), demonstrating that the given facts are sufficient to determine the signal up to a sign factor. This is a powerful illustration of how properties can be used in reverse, from frequency domain characteristics to time domain signal reconstruction."
  },
  {
    "objectID": "ss_33.html#introduction-to-periodic-signals",
    "href": "ss_33.html#introduction-to-periodic-signals",
    "title": "Signals and Systems",
    "section": "1. Introduction to Periodic Signals",
    "text": "1. Introduction to Periodic Signals\nA signal \\(x(t)\\) is periodic if, for some positive value of \\(T\\),\n\\[\nx(t)=x(t+T) \\quad \\text { for all } t \\tag{3.21}\n\\]\nThe fundamental period \\(T\\) is the minimum positive, nonzero value satisfying this condition. The fundamental frequency \\(\\omega_{0}\\) is related by \\(\\omega_{0} = 2\\pi/T\\).\n\nRecall from Chapter 1 that periodic signals repeat themselves over a fixed interval. The fundamental period is the shortest time for one complete cycle, and the fundamental frequency tells us how often that cycle occurs in radians per second."
  },
  {
    "objectID": "ss_33.html#basic-periodic-signals",
    "href": "ss_33.html#basic-periodic-signals",
    "title": "Signals and Systems",
    "section": "Basic Periodic Signals",
    "text": "Basic Periodic Signals\nThe building blocks of Fourier Series are complex exponentials.\nSinusoidal Signal:\n\\[\nx(t)=\\cos \\omega_{0} t \\tag{3.22}\n\\]\nComplex Exponential:\n\\[\nx(t)=e^{j \\omega_{0} t} \\tag{3.23}\n\\]\nBoth are periodic with fundamental frequency \\(\\omega_{0}\\) and period \\(T=2\\pi/\\omega_{0}\\).\nHarmonically Related Complex Exponentials:\n\\[\n\\phi_{k}(t)=e^{j k \\omega_{0} t}=e^{j k(2 \\pi / T) t}, \\quad k=0, \\pm 1, \\pm 2, \\ldots \\tag{3.24}\n\\] Each \\(\\phi_k(t)\\) is periodic with period \\(T\\).\n\nWhile real sinusoids are intuitive, complex exponentials are mathematically more convenient for analysis. Notice that each harmonically related exponential φ_k(t) has a frequency that is an integer multiple of the fundamental frequency ω₀. This is key to building complex signals."
  },
  {
    "objectID": "ss_33.html#the-fourier-series-representation",
    "href": "ss_33.html#the-fourier-series-representation",
    "title": "Signals and Systems",
    "section": "2. The Fourier Series Representation",
    "text": "2. The Fourier Series Representation\nA periodic signal \\(x(t)\\) can be represented as a linear combination of harmonically related complex exponentials:\n\\[\nx(t)=\\sum_{k=-\\infty}^{+\\infty} a_{k} e^{j k \\omega_{0} t}=\\sum_{k=-\\infty}^{+\\infty} a_{k} e^{j k(2 \\pi / T) t} \\tag{3.25}\n\\]\n\nThe coefficients \\(a_k\\) are called Fourier Series Coefficients.\n\\(k=0\\): DC component (constant term).\n\\(k=\\pm 1\\): Fundamental components (first harmonic).\n\\(k=\\pm N\\): \\(N\\)th harmonic components.\n\n\nThis equation is the heart of the Fourier Series. It states that any periodic signal can be decomposed into a sum of simple sinusoids (or complex exponentials) at integer multiples of the fundamental frequency. The coefficients a_k tell us the amplitude and phase of each frequency component. Think of it like a recipe, where a_k are the ingredients and e^(jkω₀t) are the basic flavors."
  },
  {
    "objectID": "ss_33.html#example-3.2-constructing-a-signal-part-1",
    "href": "ss_33.html#example-3.2-constructing-a-signal-part-1",
    "title": "Signals and Systems",
    "section": "Example 3.2: Constructing a Signal (Part 1)",
    "text": "Example 3.2: Constructing a Signal (Part 1)\nConsider a periodic signal \\(x(t)\\) with fundamental frequency \\(2\\pi\\) (so \\(T=1\\)). It is expressed as:\n\\[\nx(t)=\\sum_{k=-3}^{+3} a_{k} e^{j k 2 \\pi t} \\tag{3.26}\n\\]\nWith coefficients:\n\n\\(a_{0}=1\\)\n\\(a_{1}=a_{-1}=\\frac{1}{4}\\)\n\\(a_{2}=a_{-2}=\\frac{1}{2}\\)\n\\(a_{3}=a_{-3}=\\frac{1}{3}\\)\n\n\nHere, we are given the Fourier series coefficients and are going to reconstruct the time-domain signal. The fundamental frequency is 2π, meaning the fundamental period is 1 second. We only have components up to the 3rd harmonic."
  },
  {
    "objectID": "ss_33.html#example-3.2-constructing-a-signal-part-2",
    "href": "ss_33.html#example-3.2-constructing-a-signal-part-2",
    "title": "Signals and Systems",
    "section": "Example 3.2: Constructing a Signal (Part 2)",
    "text": "Example 3.2: Constructing a Signal (Part 2)\nRewriting the sum and grouping terms:\n\\[\n\\begin{align*}\nx(t)= & 1+\\frac{1}{4}\\left(e^{j 2 \\pi t}+e^{-j 2 \\pi t}\\right)+\\frac{1}{2}\\left(e^{j 4 \\pi t}+e^{-j 4 \\pi t}\\right) \\tag{3.27} \\\\\n& +\\frac{1}{3}\\left(e^{j 6 \\pi t}+e^{-j 6 \\pi t}\\right)\n\\end{align*}\n\\]\nUsing Euler’s relation (\\(e^{j\\theta} + e^{-j\\theta} = 2\\cos\\theta\\)), this simplifies to:\n\\[\nx(t)=1+\\frac{1}{2} \\cos 2 \\pi t+\\cos 4 \\pi t+\\frac{2}{3} \\cos 6 \\pi t \\tag{3.28}\n\\]\n\nBy grouping the complex conjugate exponential terms, we can convert them into real cosine functions. This shows how complex exponentials combine to form real sinusoids, which are often easier to visualize in the time domain."
  },
  {
    "objectID": "ss_33.html#example-3.2-interactive-signal-synthesis",
    "href": "ss_33.html#example-3.2-interactive-signal-synthesis",
    "title": "Signals and Systems",
    "section": "Example 3.2: Interactive Signal Synthesis",
    "text": "Example 3.2: Interactive Signal Synthesis\nSee how a signal is built from its harmonics.\nThis interactive plot shows the individual harmonic components and their sum. Adjust the checkboxes to see how each harmonic contributes to the overall signal.\n\nviewof dc_on = Inputs.toggle({label: \"DC\", value: true})\nviewof fundamental_on = Inputs.toggle({label: \"Fund.\", value: true})\nviewof second_harmonic_on = Inputs.toggle({label: \"2nd Harm.\", value: true})\nviewof third_harmonic_on = Inputs.toggle({label: \"3rd Harm.\", value: true})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive plot helps visualize the concept of signal synthesis. You can enable or disable individual harmonic components to see how they add up to form the final signal x(t). Notice how the signal becomes more complex as higher harmonics are added. This demonstrates the power of Fourier series in representing any periodic signal."
  },
  {
    "objectID": "ss_33.html#properties-for-real-periodic-signals",
    "href": "ss_33.html#properties-for-real-periodic-signals",
    "title": "Signals and Systems",
    "section": "3. Properties for Real Periodic Signals",
    "text": "3. Properties for Real Periodic Signals\nIf \\(x(t)\\) is a real periodic signal, its Fourier series coefficients \\(a_k\\) have a specific property:\n\\[\na_{k}^{*}=a_{-k} \\tag{3.29}\n\\] This means the coefficients for negative frequencies are the complex conjugates of the coefficients for positive frequencies.\nThis property leads to alternative forms for the Fourier Series.\n\nThis property is very useful for simplifying calculations and understanding the symmetry of the spectrum for real-world signals. It implies that the magnitude spectrum is even, and the phase spectrum is odd."
  },
  {
    "objectID": "ss_33.html#alternative-forms-for-real-signals",
    "href": "ss_33.html#alternative-forms-for-real-signals",
    "title": "Signals and Systems",
    "section": "Alternative Forms for Real Signals",
    "text": "Alternative Forms for Real Signals\nIf \\(x(t)\\) is real, we can express its Fourier series in terms of real sinusoids:\nForm 1 (Amplitude-Phase): If \\(a_{k}=A_{k} e^{j \\theta_{k}}\\), then:\n\\[\nx(t)=a_{0}+2 \\sum_{k=1}^{\\infty} A_{k} \\cos \\left(k \\omega_{0} t+\\theta_{k}\\right) \\tag{3.31}\n\\]\nForm 2 (Cosine-Sine): If \\(a_{k}=B_{k}+j C_{k}\\), then:\n\\[\nx(t)=a_{0}+2 \\sum_{k=1}^{\\infty}\\left[B_{k} \\cos k \\omega_{0} t-C_{k} \\sin k \\omega_{0} t\\right] \\tag{3.32}\n\\]\n\n\n\n\n\n\nNote\n\n\nThe complex exponential form (Eq. 3.25) is generally more convenient for analysis and manipulation in ECE, despite these real-valued alternatives.\n\n\n\n\nThese trigonometric forms are often used in introductory texts, but the complex exponential form is more powerful for advanced analysis, especially when dealing with system responses. It unifies the representation and simplifies mathematical operations."
  },
  {
    "objectID": "ss_33.html#determination-of-fourier-series-coefficients",
    "href": "ss_33.html#determination-of-fourier-series-coefficients",
    "title": "Signals and Systems",
    "section": "4. Determination of Fourier Series Coefficients",
    "text": "4. Determination of Fourier Series Coefficients\nThe Analysis Equation\nTo find the coefficients \\(a_k\\) for a given \\(x(t)\\), we use the following derivation:\n\nMultiply \\(x(t)\\) by \\(e^{-j n \\omega_{0} t}\\): \\[\nx(t) e^{-j n \\omega_{0} t}=\\sum_{k=-\\infty}^{+\\infty} a_{k} e^{j k \\omega_{0} t} e^{-j n \\omega_{0} t} \\tag{3.33}\n\\]\nIntegrate both sides over one period \\(T\\): \\[\n\\int_{0}^{T} x(t) e^{-j n \\omega_{0} t} d t=\\int_{0}^{T} \\sum_{k=-\\infty}^{+\\infty} a_{k} e^{j(k-n) \\omega_{0} t} d t \\tag{3.34}\n\\]\nInterchange summation and integration: \\[\n\\int_{0}^{T} x(t) e^{-j n \\omega_{0} t} d t=\\sum_{k=-\\infty}^{+\\infty} a_{k}\\left[\\int_{0}^{T} e^{j(k-n) \\omega_{0} t} d t\\right]\n\\]\n\n\nThis derivation is crucial. It shows how we “extract” each individual coefficient a_k from the signal x(t). The key step is the orthogonality property of complex exponentials over a period, which makes all terms in the summation zero except when k=n."
  },
  {
    "objectID": "ss_33.html#the-orthogonality-of-complex-exponentials",
    "href": "ss_33.html#the-orthogonality-of-complex-exponentials",
    "title": "Signals and Systems",
    "section": "The Orthogonality of Complex Exponentials",
    "text": "The Orthogonality of Complex Exponentials\nThe integral \\(\\int_{0}^{T} e^{j(k-n) \\omega_{0} t} d t\\) evaluates as:\n\\[\n\\int_{0}^{T} e^{j(k-n) \\omega_{0} t} d t=\\left\\{\\begin{array}{ll}\nT, & k=n \\\\\n0, & k \\neq n\n\\end{array}\\right.\n\\]\nThis property is called orthogonality. Applying this to the summation yields \\(T a_n\\).\nTherefore, the formula for the Fourier Series coefficients is:\n\\[\na_{n}=\\frac{1}{T} \\int_{T} x(t) e^{-j n \\omega_{0} t} d t \\tag{3.37}\n\\] The integration can be over any interval of length \\(T\\).\n\nThe orthogonality property is what makes Fourier series analysis possible. It allows us to isolate each harmonic component. The integral effectively acts as a “filter” that picks out the n-th harmonic."
  },
  {
    "objectID": "ss_33.html#the-fourier-series-pair",
    "href": "ss_33.html#the-fourier-series-pair",
    "title": "Signals and Systems",
    "section": "The Fourier Series Pair",
    "text": "The Fourier Series Pair\nThe Fourier Series is defined by a pair of equations:\n\n\nSynthesis Equation (Time Domain to Frequency Domain):\nReconstructs \\(x(t)\\) from its coefficients.\n\\[\nx(t)=\\sum_{k=-\\infty}^{+\\infty} a_{k} e^{j k \\omega_{0} t} \\tag{3.38}\n\\]\n\nAnalysis Equation (Frequency Domain to Time Domain):\nDetermines the coefficients \\(a_k\\) from \\(x(t)\\).\n\\[\na_{k}=\\frac{1}{T} \\int_{T} x(t) e^{-j k \\omega_{0} t} d t \\tag{3.39}\n\\]\n\n\n\n\n\n\n\nImportant\n\n\nThese two equations are fundamental to understanding and applying Fourier Series. They represent the bridge between the time domain and the frequency domain.\n\n\n\n\nThe a_k coefficients are also called spectral coefficients. They describe the signal’s content at different frequencies. The coefficient a₀ (when k=0) is simply the average value of \\(x(t)\\) over one period. \\[\na_{0}=\\frac{1}{T} \\int_{T} x(t) d t \\tag{3.40}\n\\]"
  },
  {
    "objectID": "ss_33.html#example-3.3-fourier-series-of-a-simple-sinusoid",
    "href": "ss_33.html#example-3.3-fourier-series-of-a-simple-sinusoid",
    "title": "Signals and Systems",
    "section": "Example 3.3: Fourier Series of a Simple Sinusoid",
    "text": "Example 3.3: Fourier Series of a Simple Sinusoid\nConsider the signal \\(x(t)=\\sin \\omega_{0} t\\).\nWe can determine its Fourier series coefficients by inspection using Euler’s formula:\n\\[\n\\sin \\omega_{0} t=\\frac{1}{2 j} e^{j \\omega_{0} t}-\\frac{1}{2 j} e^{-j \\omega_{0} t}\n\\]\nComparing this to the synthesis equation (Eq. 3.38), we find:\n\n\\(a_{1}=\\frac{1}{2 j}\\)\n\\(a_{-1}=-\\frac{1}{2 j}\\)\n\\(a_{k}=0\\), for \\(k \\neq +1\\) or \\(-1\\).\n\n\nThis example shows that a pure sinusoid only has two non-zero frequency components: one at +ω₀ and one at -ω₀. This is a very clean spectrum. Notice the j in the denominator, indicating a phase shift."
  },
  {
    "objectID": "ss_33.html#example-3.4-more-complex-sum-of-sinusoids-part-1",
    "href": "ss_33.html#example-3.4-more-complex-sum-of-sinusoids-part-1",
    "title": "Signals and Systems",
    "section": "Example 3.4: More Complex Sum of Sinusoids (Part 1)",
    "text": "Example 3.4: More Complex Sum of Sinusoids (Part 1)\nLet \\(x(t)=1+\\sin \\omega_{0} t+2 \\cos \\omega_{0} t+\\cos \\left(2 \\omega_{0} t+\\frac{\\pi}{4}\\right)\\).\nExpanding into complex exponentials and collecting terms:\n\\[\nx(t)=1+\\left(1-\\frac{1}{2} j\\right) e^{j \\omega_{0} t}+\\left(1+\\frac{1}{2} j\\right) e^{-j \\omega_{0} t}+\\left(\\frac{1}{2} e^{j(\\pi / 4)}\\right) e^{j 2 \\omega_{0} t}+\\left(\\frac{1}{2} e^{-j(\\pi / 4)}\\right) e^{-j 2 \\omega_{0} t}\n\\]\nThe Fourier series coefficients are:\n\n\\(a_{0}=1\\)\n\\(a_{1}=1-\\frac{1}{2} j\\)\n\\(a_{-1}=1+\\frac{1}{2} j\\)\n\\(a_{2}=\\frac{1}{2} e^{j(\\pi / 4)}=\\frac{\\sqrt{2}}{4}(1+j)\\)\n\\(a_{-2}=\\frac{1}{2} e^{-j(\\pi / 4)}=\\frac{\\sqrt{2}}{4}(1-j)\\)\n\\(a_{k}=0\\), for \\(|k|&gt;2\\).\n\n\nHere, we’re combining different sinusoids with varying amplitudes and phases. The resulting a_k coefficients are complex, reflecting both the amplitude and phase contribution of each harmonic. Notice how a_k and a_-k are complex conjugates, as expected for a real signal."
  },
  {
    "objectID": "ss_33.html#example-3.4-interactive-spectrum-visualization",
    "href": "ss_33.html#example-3.4-interactive-spectrum-visualization",
    "title": "Signals and Systems",
    "section": "Example 3.4: Interactive Spectrum Visualization",
    "text": "Example 3.4: Interactive Spectrum Visualization\nMagnitude and Phase Spectrum\nThis plot shows the magnitude and phase of the Fourier coefficients \\(a_k\\) for Example 3.4. Interact with the plot to examine the spectral content.\n\n\n\n\n\n\n\nThe magnitude spectrum shows how much “energy” each harmonic contributes to the signal. The phase spectrum shows the relative phase shift of each harmonic. Notice the symmetry for real signals: magnitude is even (|a_k| = |a_-k|) and phase is odd (∠a_k = -∠a_-k). The DC component a₀ has zero phase, as it’s a real constant."
  },
  {
    "objectID": "ss_33.html#example-3.5-periodic-square-wave-part-1",
    "href": "ss_33.html#example-3.5-periodic-square-wave-part-1",
    "title": "Signals and Systems",
    "section": "Example 3.5: Periodic Square Wave (Part 1)",
    "text": "Example 3.5: Periodic Square Wave (Part 1)\nThe periodic square wave is a canonical signal in ECE.\nDefined over one period as:\n\\[\nx(t)= \\begin{cases}1, & |t|&lt;T_{1} \\\\ 0, & T_{1}&lt;|t|&lt;T / 2\\end{cases} \\tag{3.41}\n\\]\nThis signal is periodic with fundamental period \\(T\\) and fundamental frequency \\(\\omega_{0}=2 \\pi / T\\).\n\nThe square wave is a fundamental signal for understanding Fourier series because it’s simple yet contains an infinite number of harmonics. It’s used in digital communications, clock signals, and many other areas. We’ll derive its Fourier series coefficients."
  },
  {
    "objectID": "ss_33.html#example-3.5-periodic-square-wave-part-2",
    "href": "ss_33.html#example-3.5-periodic-square-wave-part-2",
    "title": "Signals and Systems",
    "section": "Example 3.5: Periodic Square Wave (Part 2)",
    "text": "Example 3.5: Periodic Square Wave (Part 2)\nCalculating \\(a_0\\) and \\(a_k\\)\nFor \\(k=0\\) (DC component): \\[\na_{0}=\\frac{1}{T} \\int_{-T_{1}}^{T_{1}} 1 \\, dt=\\frac{2 T_{1}}{T} \\tag{3.42}\n\\] \\(a_0\\) is the average value of \\(x(t)\\) over one period.\nFor \\(k \\neq 0\\): \\[\na_{k}=\\frac{1}{T} \\int_{-T_{1}}^{T_{1}} e^{-j k \\omega_{0} t} d t = \\frac{1}{T} \\left[ \\frac{e^{-j k \\omega_{0} t}}{-j k \\omega_{0}} \\right]_{-T_{1}}^{T_{1}}\n\\] \\[\na_{k}=\\frac{2}{k \\omega_{0} T}\\left[\\frac{e^{j k \\omega_{0} T_{1}}-e^{-j k \\omega_{0} T_{1}}}{2 j}\\right] \\tag{3.43}\n\\] Recognizing the term in brackets as \\(\\sin(k\\omega_0 T_1)\\), and using \\(\\omega_0 T = 2\\pi\\):\n\\[\na_{k}=\\frac{2 \\sin \\left(k \\omega_{0} T_{1}\\right)}{k \\omega_{0} T}=\\frac{\\sin \\left(k \\omega_{0} T_{1}\\right)}{k \\pi}, \\quad k \\neq 0 \\tag{3.44}\n\\]\n\nThe calculation of a₀ is straightforward: it’s simply the fraction of the period for which the signal is ‘on’ (at value 1). For a_k where k≠0, the integration results in a sinc function shape in the frequency domain. This is a very common and important result."
  },
  {
    "objectID": "ss_33.html#example-3.5-periodic-square-wave-part-3",
    "href": "ss_33.html#example-3.5-periodic-square-wave-part-3",
    "title": "Signals and Systems",
    "section": "Example 3.5: Periodic Square Wave (Part 3)",
    "text": "Example 3.5: Periodic Square Wave (Part 3)\nSpectrum for a 50% Duty Cycle\nConsider the case where \\(T=4T_1\\) (a 50% duty cycle, i.e., \\(x(t)=1\\) for half the period). In this case, \\(\\omega_0 T_1 = \\frac{2\\pi}{T} T_1 = \\frac{2\\pi}{4T_1} T_1 = \\frac{\\pi}{2}\\).\n\n\\(a_{0}=\\frac{1}{2}\\)\n\\(a_{k}=\\frac{\\sin (k \\pi / 2)}{k \\pi}, \\quad k \\neq 0\\)\n\nThis implies:\n\n\\(a_k = 0\\) for \\(k\\) even and non-zero.\n\\(a_k\\) alternates sign for odd \\(k\\):\n\n\\(a_1 = a_{-1} = \\frac{1}{\\pi}\\)\n\\(a_3 = a_{-3} = -\\frac{1}{3\\pi}\\)\n\\(a_5 = a_{-5} = \\frac{1}{5\\pi}\\)\n\n\n\nFor a symmetric square wave, all even harmonics (except DC) are zero. This is a characteristic of signals with odd symmetry around the midpoint of the pulse. The coefficients decrease in magnitude as 1/k, which is typical for signals with discontinuities."
  },
  {
    "objectID": "ss_33.html#example-3.5-interactive-square-wave-spectrum",
    "href": "ss_33.html#example-3.5-interactive-square-wave-spectrum",
    "title": "Signals and Systems",
    "section": "Example 3.5: Interactive Square Wave Spectrum",
    "text": "Example 3.5: Interactive Square Wave Spectrum\nObserve the effect of pulse width on the spectrum.\nAdjust the duty cycle (ratio of pulse width \\(2T_1\\) to period \\(T\\)) to see how the frequency spectrum changes. The envelope of the spectrum is a \\(\\text{sinc}\\) function.\n\nviewof duty_cycle = Inputs.range([0.1, 0.9], {step: 0.05, value: 0.5, label: \"Duty Cycle\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive plot vividly demonstrates the relationship between the time-domain characteristics (pulse width/duty cycle) and the frequency-domain spectrum. As you change the duty cycle, observe how the nulls of the sinc envelope shift. A narrower pulse spreads the energy across more frequencies (wider sinc), while a wider pulse concentrates energy (narrower sinc). This is a fundamental concept in signal analysis."
  },
  {
    "objectID": "ss_33.html#summary-applications",
    "href": "ss_33.html#summary-applications",
    "title": "Signals and Systems",
    "section": "Summary & Applications",
    "text": "Summary & Applications\nKey Takeaways:\n\nPeriodic signals can be uniquely represented by their Fourier series coefficients.\nThe Fourier Series provides a powerful tool to move between time and frequency domains.\nComplex exponentials are the fundamental building blocks.\n\nReal-World Engineering Applications:\n\nAudio Processing: Equalization, compression (MP3).\nImage Processing: JPEG compression, filtering.\nCommunication Systems: Modulation, multiplexing, channel analysis.\nFilter Design: Understanding how circuits respond to different frequencies.\nVibration Analysis: Identifying resonant frequencies in mechanical systems.\n\n\n\n\n\n\n\nTip\n\n\nPractice deriving Fourier Series for different signals and interpreting their spectra. Understanding the relationship between time-domain features and frequency-domain characteristics is crucial!\n\n\n\n\nFourier Series is not just a theoretical concept; it’s a cornerstone of modern engineering. From the music you listen to, to the images you see, and the wireless communication you use, Fourier analysis plays a critical role. Mastering this topic will give you a powerful lens through which to view and design systems."
  },
  {
    "objectID": "ss_24.html#causal-lti-systems-described-by-differential-and-difference-equations",
    "href": "ss_24.html#causal-lti-systems-described-by-differential-and-difference-equations",
    "title": "Signals and Systems",
    "section": "2.4 Causal LTI Systems Described by Differential and Difference Equations",
    "text": "2.4 Causal LTI Systems Described by Differential and Difference Equations"
  },
  {
    "objectID": "ss_24.html#system-description-using-equations",
    "href": "ss_24.html#system-description-using-equations",
    "title": "Signals and Systems",
    "section": "System Description using Equations",
    "text": "System Description using Equations\n\nContinuous-time systems are often described by Linear Constant-Coefficient Differential Equations (LCCDEs).\n\nExamples: RC circuits (Figure 1.1), vehicle motion (Figure 1.2), mechanical systems with restoring and damping forces.\n\nDiscrete-time systems are often described by Linear Constant-Coefficient Difference Equations (LCCDEs).\n\nExamples: Bank account accumulation (Example 1.10), digital simulations, signal processing filters (e.g., differencing, averaging filters).\n\n\n\nToday, we delve into a foundational aspect of Signals and Systems: how we mathematically model systems using differential and difference equations. These equations are not just abstract mathematical constructs; they are the language we use to describe a vast array of real-world engineering phenomena. Think of an RC circuit, where the capacitor’s voltage changes based on the input current and its own past charge, or a vehicle’s speed evolving under the influence of applied forces and friction. All these are elegantly captured by differential equations. Similarly, in the digital realm, from a simple moving average filter to complex control algorithms, difference equations provide the framework for understanding and designing discrete-time systems. Understanding how to interpret and solve these equations is crucial for analyzing system behavior."
  },
  {
    "objectID": "ss_24.html#implicit-system-specification",
    "href": "ss_24.html#implicit-system-specification",
    "title": "Signals and Systems",
    "section": "Implicit System Specification",
    "text": "Implicit System Specification\n\nDifferential and difference equations provide an implicit specification of system behavior.\n\nThey describe a relationship between the input (\\(x(t)\\) or \\(x[n]\\)) and the output (\\(y(t)\\) or \\(y[n]\\)), rather than an explicit expression.\n\nExample: First-order LCCDE \\[\n\\frac{d y(t)}{d t}+2 y(t)=x(t) \\tag{2.95}\n\\]\n\nThis equation relates the rate of change of \\(y(t)\\) and \\(y(t)\\) itself to \\(x(t)\\).\nIt doesn’t directly tell us what \\(y(t)\\) is for a given \\(x(t)\\).\n\n\n\nIt’s important to understand that these equations are not direct formulas for the output. They don’t simply say “y equals this function of x”. Instead, they describe a constraint or a rule that the input and output must satisfy together. For instance, the equation shown on the slide states that the sum of the output’s rate of change and twice the output itself must equal the input. To get an explicit formula for \\(y(t)\\), we need to solve this differential equation. This implicit nature is key to understanding why solving these equations requires specific techniques and additional information."
  },
  {
    "objectID": "ss_24.html#auxiliary-conditions-and-initial-rest",
    "href": "ss_24.html#auxiliary-conditions-and-initial-rest",
    "title": "Signals and Systems",
    "section": "Auxiliary Conditions and Initial Rest",
    "text": "Auxiliary Conditions and Initial Rest\n\nTo find an explicit expression for the output, we must solve the differential or difference equation.\nSolving requires more information than the equation alone: auxiliary conditions.\n\nE.g., initial speed of a car, initial voltage across a capacitor.\n\nDifferent auxiliary conditions lead to different input-output relationships.\nFor Causal LTI systems, the auxiliary conditions typically take the form of the condition of initial rest.\n\nIf the input \\(x(t)=0\\) for \\(t &lt; t_0\\), then the output \\(y(t)\\) must also be 0 for \\(t &lt; t_0\\).\nThis implies specific initial conditions at \\(t_0\\), e.g., \\(y(t_0)=0\\) (and its derivatives for higher-order systems).\n\n\n\nImagine trying to predict where a car will be after 10 seconds of constant acceleration. You can’t just know the acceleration; you also need to know its starting position and speed. Similarly, for an RC circuit, knowing the applied voltage isn’t enough; you need the initial capacitor voltage. These “starting values” are our auxiliary conditions. For most systems we study in Signals and Systems, particularly those that are Linear, Time-Invariant, and Causal – which we’ll call LTI-C systems for short – we employ a standard auxiliary condition called “initial rest.” This condition essentially states that if the system has not been acted upon by an input before a certain time, its output, and all its internal “memory” elements, must be zero up to that time. This condition is crucial because it makes the system unique, causal, and LTI. It ensures that the system’s response only begins when the input begins, and that the system “remembers” its past only via the input it has received, not through some arbitrary pre-existing state."
  },
  {
    "objectID": "ss_24.html#solving-first-order-lccde-example-2.14",
    "href": "ss_24.html#solving-first-order-lccde-example-2.14",
    "title": "Signals and Systems",
    "section": "Solving First-Order LCCDE: Example 2.14",
    "text": "Solving First-Order LCCDE: Example 2.14\nConsider the system described by: \\(\\frac{d y(t)}{d t}+2 y(t)=x(t)\\) Let the input signal be: \\(x(t)=K e^{3 t} u(t)\\)\n\nThe complete solution to a differential equation consists of two parts: \\[\ny(t)=y_{p}(t)+y_{h}(t) \\tag{2.97}\n\\]\n\n\\(y_p(t)\\): Particular solution (or forced response). This part satisfies the full differential equation with the given input.\n\\(y_h(t)\\): Homogeneous solution (or natural response). This part is a solution to the homogeneous differential equation (with input set to zero): \\[\n\\frac{d y(t)}{d t}+2 y(t)=0 \\tag{2.98}\n\\]\n\n\n\nLet’s walk through an example to see how we solve these. We have a first-order differential equation modeling a system, and we’re applying an exponential input x(t). The general approach to solving such an equation is to break the problem into two parts. First, we find the particular solution, which directly responds to the input signal. This is the forced part of the response. Second, we find the homogeneous solution, which describes the system’s inherent behavior—how it would react without any external input, or how it would ‘ring out’ if disturbed. This is often called the natural response. The total solution is simply the sum of these two components."
  },
  {
    "objectID": "ss_24.html#example-2.14-finding-the-particular-solution",
    "href": "ss_24.html#example-2.14-finding-the-particular-solution",
    "title": "Signals and Systems",
    "section": "Example 2.14: Finding the Particular Solution",
    "text": "Example 2.14: Finding the Particular Solution\n\nFor \\(t&gt;0\\), the input is \\(x(t)=K e^{3 t}\\).\nWe hypothesize that the particular solution \\(y_p(t)\\) will have the same exponential form as the input for \\(t&gt;0\\): \\[\ny_{p}(t)=Y e^{3 t} \\tag{2.99}\n\\] where \\(Y\\) is a coefficient to be determined.\nSubstitute \\(y_p(t)\\) and \\(x(t)\\) into the original differential equation (\\(\\frac{d y(t)}{d t}+2 y(t)=x(t)\\)) for \\(t&gt;0\\): \\[\n\\frac{d}{dt}(Y e^{3 t}) + 2 (Y e^{3 t}) = K e^{3 t}  \\quad \\implies \\quad 3 Y e^{3 t}+2 Y e^{3 t}=K e^{3 t} \\tag{2.100}\n\\]\nCancel \\(e^{3 t}\\) from both sides, then solve for \\(Y\\): \\[\n5 Y = K \\implies Y = \\frac{K}{5} \\tag{2.101, 2.102}\n\\]\nThus, the particular solution for \\(t&gt;0\\) is: \\[\ny_{p}(t)=\\frac{K}{5} e^{3 t}, \\quad t&gt;0 \\tag{2.103}\n\\]\n\n\nTo find the particular solution when the input is an exponential, a common and effective strategy is to assume that the particular solution itself will be an exponential of the same form. So, for an input K * e^(3t), we propose a particular solution Y * e^(3t). We then substitute this proposed solution and the input back into the original differential equation. The derivative of Y * e^(3t) is 3Y * e^(3t). After substitution, we can factor out e^(3t) and solve for the unknown coefficient Y. In this case, we find Y = K/5. This gives us the part of the output that is directly “forced” by the input signal."
  },
  {
    "objectID": "ss_24.html#example-2.14-finding-the-homogeneous-solution",
    "href": "ss_24.html#example-2.14-finding-the-homogeneous-solution",
    "title": "Signals and Systems",
    "section": "Example 2.14: Finding the Homogeneous Solution",
    "text": "Example 2.14: Finding the Homogeneous Solution\n\nNow, we need to find the homogeneous solution \\(y_h(t)\\), which satisfies the homogeneous differential equation: \\[\n\\frac{d y(t)}{d t}+2 y(t)=0 \\tag{2.98}\n\\]\nWe hypothesize an exponential form for the homogeneous solution: \\[\ny_{h}(t)=A e^{s t} \\tag{2.104}\n\\] where \\(A\\) and \\(s\\) are constants.\nSubstitute \\(y_h(t)\\) into the homogeneous equation: \\[\n\\frac{d}{dt}(A e^{s t}) + 2 (A e^{s t}) = 0 \\quad \\implies \\quad A s e^{s t}+2 A e^{s t}=0 \\tag{2.105}\n\\]\nFactor out \\(A e^{s t}\\): \\[\nA e^{s t}(s+2)=0\n\\]\nFor this to be true for all \\(t\\), we must have \\(s+2=0\\), which implies \\(s = -2\\).\nTherefore, the homogeneous solution is: \\[\ny_{h}(t)=A e^{-2 t}\n\\] where \\(A\\) is an arbitrary constant determined by auxiliary conditions.\n\n\nNext, let’s find the homogeneous solution. This represents the system’s “natural” way of responding, independent of the specific input. We again assume an exponential form, A * e^(st). When we substitute this into the homogeneous equation (where the input is zero), we find a condition for ‘s’. In this case, s must be -2. This means A * e^(-2t) is a valid solution to the homogeneous equation for any constant A. This A is the arbitrary constant that will be determined by our auxiliary or initial conditions. The -2t in the exponent indicates an exponentially decaying component, often related to the system’s stability or its natural frequency."
  },
  {
    "objectID": "ss_24.html#example-2.14-total-solution-and-initial-rest",
    "href": "ss_24.html#example-2.14-total-solution-and-initial-rest",
    "title": "Signals and Systems",
    "section": "Example 2.14: Total Solution and Initial Rest",
    "text": "Example 2.14: Total Solution and Initial Rest\n\nCombine particular and homogeneous solutions to get the general solution for \\(t&gt;0\\): \\[\ny(t)=A e^{-2 t}+\\frac{K}{5} e^{3 t}, \\quad t&gt;0 \\tag{2.106}\n\\]\nApply the condition of initial rest:\n\nSince the input \\(x(t)=K e^{3 t} u(t)\\) implies \\(x(t)=0\\) for \\(t&lt;0\\), a causal LTI system will have \\(y(t)=0\\) for \\(t&lt;0\\).\nThis means the initial condition is \\(y(0)=0\\).\n\nSolve for \\(A\\) using \\(y(0)=0\\) in the general solution: \\[\n0 = A e^{-2(0)} + \\frac{K}{5} e^{3(0)}  \\implies  0 = A + \\frac{K}{5}  \\implies  A = -\\frac{K}{5}\n\\]\nSubstitute \\(A\\) back into the general solution to obtain the final, complete response: \\[\ny(t)=\\frac{K}{5} e^{3 t} - \\frac{K}{5} e^{-2 t}, \\quad t&gt;0\n\\]\nCombining with \\(y(t)=0\\) for \\(t&lt;0\\): \\[\ny(t)=\\frac{K}{5}\\left[e^{3 t}-e^{-2 t}\\right] u(t) \\tag{2.108}\n\\]\n\n\nNow that we have both the particular and homogeneous solutions, we combine them to form the general solution. Notice that it still has an unknown constant A. This is where our auxiliary condition of “initial rest” comes into play. Since our input x(t) effectively starts at t=0 (due to the u(t)), the condition of initial rest implies that the output y(t) must also be zero before t=0. Thus, we can set y(0) = 0. Plugging this into our general solution allows us to solve for A, making the solution unique. In this example, A turns out to be -K/5. With A determined, we have the complete and unique output y(t) for the given input and initial rest condition. This final form includes the unit step u(t) to correctly represent the causality, meaning the response starts at t=0."
  },
  {
    "objectID": "ss_24.html#example-2.14-output-signal-plot",
    "href": "ss_24.html#example-2.14-output-signal-plot",
    "title": "Signals and Systems",
    "section": "Example 2.14: Output Signal Plot",
    "text": "Example 2.14: Output Signal Plot\nAdjust the input amplitude K and observe the output response. \n\nviewof K_slider = Inputs.range([0, 10], {\n  label: html`&lt;span style=\"font-size: 0.8em;\"&gt;Amplitude K&lt;/span&gt;`,\n  step: 1,\n  value: 5,\n});\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s visualize this. Here, you can change the amplitude ‘K’ of the input signal using the slider. Observe how the input signal x(t) starts at t=0 and grows exponentially. The output y(t) also starts at t=0, due to initial rest. It initially grows because of the e^(3t) term from the input, but also incorporates the decaying e^(-2t) term from the system’s natural response. Notice how if you double K, the output also doubles, confirming the linearity of the system under initial rest conditions. This interactive plot helps us gain an intuitive understanding of how the system processes the input and how the different components of the solution manifest over time."
  },
  {
    "objectID": "ss_24.html#general-nth-order-lccde",
    "href": "ss_24.html#general-nth-order-lccde",
    "title": "Signals and Systems",
    "section": "General Nth-Order LCCDE",
    "text": "General Nth-Order LCCDE\n\nA general \\(N\\)th-order linear constant-coefficient differential equation is given by: \\[\n\\sum_{k=0}^{N} a_{k} \\frac{d^{k} y(t)}{d t^{k}}=\\sum_{k=0}^{M} b_{k} \\frac{d^{k} x(t)}{d t^{k}} \\tag{2.109}\n\\]\n\n\\(N\\) is the order of the system (highest derivative of \\(y(t)\\)).\nIf \\(N=0\\), then \\(y(t)=\\frac{1}{a_{0}} \\sum_{k=0}^{M} b_{k} \\frac{d^{k} x(t)}{d t^{k}}\\), which is an explicit function of \\(x(t)\\) and its derivatives (no auxiliary conditions needed).\n\nSolution approach: Still a sum of particular and homogeneous solutions.\n\nHomogeneous equation: \\(\\sum_{k=0}^{N} a_{k} \\frac{d^{k} y(t)}{d t^{k}}=0\\).\n\nInitial Rest Condition: For \\(x(t)=0\\) for \\(t \\leq t_0\\), the output \\(y(t)=0\\) for \\(t \\leq t_0\\). This requires the initial conditions: \\[\ny\\left(t_{0}\\right)=\\frac{d y\\left(t_{0}\\right)}{d t}=\\ldots=\\frac{d^{N-1} y\\left(t_{0}\\right)}{d t^{N-1}}=0 \\tag{2.112}\n\\]\n\n\nThe concepts we just covered for a first-order differential equation extend directly to more complex, higher-order systems. Equation 2.109 is the general form for an Nth-order LCCDE. The ‘N’ here refers to the highest derivative of the output y(t). If N is zero, the equation simplifies, and y(t) becomes an explicit function of the input and its derivatives; in this highly simplified case, no auxiliary conditions are strictly needed. However, for N greater than or equal to 1, the output is implicitly defined, and we still follow the same strategy: finding particular and homogeneous solutions. For LTI-C systems, the initial rest condition applies, but for an Nth-order system, it means that a total of N initial conditions (the output and its first N-1 derivatives) must all be zero at the starting point t0 where the input becomes non-zero. This provides the N constants needed to uniquely determine the full solution. We will later introduce more advanced tools like Laplace Transforms to greatly simplify the solution of these higher-order equations."
  },
  {
    "objectID": "ss_24.html#linear-constant-coefficient-difference-equations-lccdes",
    "href": "ss_24.html#linear-constant-coefficient-difference-equations-lccdes",
    "title": "Signals and Systems",
    "section": "Linear Constant-Coefficient Difference Equations (LCCDEs)",
    "text": "Linear Constant-Coefficient Difference Equations (LCCDEs)\n\nThe discrete-time counterpart to continuous-time LCCDEs is the \\(N\\)th-order linear constant-coefficient difference equation: \\[\n\\sum_{k=0}^{N} a_{k} y[n-k]=\\sum_{k=0}^{M} b_{k} x[n-k] \\tag{2.113}\n\\]\nSolution approach: Analogous to differential equations – sum of a particular solution and a homogeneous solution.\n\nHomogeneous equation: \\(\\sum_{k=0}^{N} a_{k} y[n-k]=0\\).\n\nCondition of Initial Rest: If \\(x[n]=0\\) for \\(n&lt;n_0\\), then \\(y[n]=0\\) for \\(n&lt;n_0\\).\n\nUnder the condition of initial rest, the system described by the difference equation is LTI and causal.\n\n\n\nJust as differential equations describe continuous-time systems, difference equations are fundamental for discrete-time systems. Equation 2.113 is the general form. The solution methodology mirrors the continuous-time case: find a particular solution responding to the input, and a homogeneous solution representing the system’s natural behavior. And crucially, just as with differential equations, difference equations require auxiliary conditions for a unique solution. Again, for LTI and causal systems, we adopt the “initial rest” condition. This states that if the input is zero before a certain time n0, then the output must also be zero before n0. This ensures causality and preserves the LTI properties."
  },
  {
    "objectID": "ss_24.html#recursive-vs.-nonrecursive-systems",
    "href": "ss_24.html#recursive-vs.-nonrecursive-systems",
    "title": "Signals and Systems",
    "section": "Recursive vs. Nonrecursive Systems",
    "text": "Recursive vs. Nonrecursive Systems\nRecursive Equation (IIR)\n\nIf \\(N \\ge 1\\) (output depends on past outputs), the equation can be rearranged to directly compute \\(y[n]\\): \\[\ny[n]=\\frac{1}{a_{0}}\\left\\{\\sum_{k=0}^{M} b_{k} x[n-k]-\\sum_{k=1}^{N} a_{k} y[n-k]\\right\\} \\tag{2.115}\n\\]\nOutput \\(y[n]\\) depends on both current/past inputs and past outputs.\nRequires auxiliary conditions (e.g., \\(y[n_0-1], \\ldots, y[n_0-N]\\)) to start the recursion.\nOften leads to Infinite Impulse Response (IIR) systems.\n\nNonrecursive Equation (FIR)\n\nIn the special case when \\(N = 0\\): \\[\ny[n]=\\sum_{k=0}^{M}\\left(\\frac{b_{k}}{a_{0}}\\right) x[n-k] \\tag{2.116}\n\\]\nOutput \\(y[n]\\) depends only on present and past inputs.\nNo auxiliary conditions are directly needed, as output is explicit.\nAlways a Finite Impulse Response (FIR) system.\n\nIts impulse response \\(h[n]\\) has finite duration: \\[\nh[n]= \\begin{cases}\\frac{b_{n}}{a_{0}}, & 0 \\leq n \\leq M \\\\ 0, & \\text { otherwise }\\end{cases} \\tag{2.117}\n\\]\n\n\n\nA significant distinction in discrete-time systems is between recursive and nonrecursive equations. If the current output y[n] depends on past outputs (i.e., N is greater than or equal to 1), the equation is recursive. This forms a feedback loop where past results influence current calculations. Because of this feedback, initial conditions are essential to kickstart the process, and such systems typically have an impulse response that goes on indefinitely, hence “Infinite Impulse Response” or IIR.\nIn contrast, if N is zero, the current output y[n] depends only on current and past inputs. This is a nonrecursive equation. There’s no feedback from the output back into the system. As a result, no auxiliary conditions are needed, and the impulse response is always finite in duration, making these “Finite Impulse Response” or FIR systems. FIR systems are simpler to analyze and design in certain contexts."
  },
  {
    "objectID": "ss_24.html#solving-first-order-lccde-example-2.15",
    "href": "ss_24.html#solving-first-order-lccde-example-2.15",
    "title": "Signals and Systems",
    "section": "Solving First-Order LCCDE: Example 2.15",
    "text": "Solving First-Order LCCDE: Example 2.15\nConsider the difference equation: \\(y[n]-\\frac{1}{2} y[n-1]=x[n]\\)\nRearranging for recursive computation: \\(y[n]=x[n]+\\frac{1}{2} y[n-1]\\)\nLet the input be an impulse: \\(x[n]=K \\delta[n]\\)\n\nApply initial rest condition:\n\nSince \\(x[n]=0\\) for \\(n&lt;-1\\), initial rest implies \\(y[n]=0\\) for \\(n&lt;-1\\).\nTherefore, \\(y[-1]=0\\).\n\nIterative Solution for \\(n \\ge 0\\):\n\n\\(y[0] = x[0] + \\frac{1}{2} y[-1] = K \\delta[0] + \\frac{1}{2}(0) = K\\)\n\\(y[1] = x[1] + \\frac{1}{2} y[0] = K \\delta[1] + \\frac{1}{2} K = 0 + \\frac{1}{2} K = \\frac{1}{2} K\\)\n\\(y[2] = x[2] + \\frac{1}{2} y[1] = K \\delta[2] + \\frac{1}{2} \\left(\\frac{1}{2} K\\right) = 0 + \\left(\\frac{1}{2}\\right)^2 K = \\left(\\frac{1}{2}\\right)^2 K\\)\n\\(\\vdots\\)\nIn general, for \\(n \\ge 0\\): \\(y[n] = \\left(\\frac{1}{2}\\right)^n K\\).\n\nImpulse Response (\\(K=1\\)): \\[\nh[n]=\\left(\\frac{1}{2}\\right)^{n} u[n] \\tag{2.125}\n\\] This is an Infinite Impulse Response (IIR) system.\n\n\nLet’s see a discrete-time example. We have a first-order difference equation, rearranged into its recursive form to show that \\(y[n]\\) depends on \\(y[n-1]\\). We apply an impulse input, \\(K \\delta[n]\\). The initial rest condition means that before n=0, y[n] is zero, so y[-1] is zero. From there, we can iteratively calculate y[n] for n=0, 1, 2, ....\nAt n=0, y[0] is x[0] plus half of y[-1], which gives K. At n=1, y[1] is x[1] plus half of y[0], which gives (1/2)K. This pattern continues, leading to \\(y[n] = (1/2)^n K\\) for n greater than or equal to zero. If K=1, this directly gives us the impulse response h[n] = (1/2)^n u[n]. Since this response extends indefinitely, it confirms that this is an IIR system, characteristic of recursive difference equations."
  },
  {
    "objectID": "ss_24.html#example-2.15-impulse-response-of-an-iir-system",
    "href": "ss_24.html#example-2.15-impulse-response-of-an-iir-system",
    "title": "Signals and Systems",
    "section": "Example 2.15: Impulse Response of an IIR System",
    "text": "Example 2.15: Impulse Response of an IIR System\nObserve the decaying impulse response for different coefficients. \n\nviewof alpha_slider = Inputs.range([-0.9, 0.9], {\n  label: html`&lt;span style=\"font-size: 0.8em;\"&gt;Coefficient 'a'&lt;/span&gt;`,\n  step: 0.1,\n  value: 0.5,\n});\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere’s an interactive visualization of the impulse response we just derived. The difference equation is y[n] = x[n] + a*y[n-1]. The impulse response is h[n] = a^n u[n]. You can adjust the coefficient ‘a’ using the slider. Notice how the shape of the impulse response changes: - When a is between 0 and 1 (like 0.5 in our example), the response decays exponentially, indicating a stable system. - When a is negative (e.g., -0.5), it oscillates while decaying. - If the absolute value of a is greater than or equal to 1, the response will not decay, indicating an unstable or marginally stable system.\nThis helps visualize why these are called Infinite Impulse Response (IIR) systems—the response theoretically continues indefinitely, though it can decay quickly depending on the value of ‘a’."
  },
  {
    "objectID": "ss_24.html#block-diagram-representations",
    "href": "ss_24.html#block-diagram-representations",
    "title": "Signals and Systems",
    "section": "Block Diagram Representations",
    "text": "Block Diagram Representations\n\nRepresenting systems using block diagrams offers several advantages:\n\nProvides a pictorial representation for better intuitive understanding of system structure.\nUseful for simulation (analog or digital computers).\nGuides hardware implementation for physical systems.\n\nBasic Operations for Discrete-Time Systems:\n\nAdder: Sums multiple input signals.\nMultiplier: Scales a signal by a constant coefficient.\nUnit Delay: Outputs the input from the previous time step. This is the memory element.\n\n\n\nMoving beyond mathematical equations, block diagrams offer a powerful and intuitive way to visualize and implement systems. They break down a complex system into an interconnection of simple, fundamental operations. This not only enhances our understanding of how signals flow and are processed within the system but also provides a direct blueprint for how these systems could be simulated on computers or even built in hardware. For discrete-time systems, we use three basic building blocks: adders to combine signals, multipliers to scale them by coefficients, and crucially, a unit delay element, which serves as the system’s memory, holding onto a value from the previous time step."
  },
  {
    "objectID": "ss_24.html#discrete-time-first-order-system-block-diagram",
    "href": "ss_24.html#discrete-time-first-order-system-block-diagram",
    "title": "Signals and Systems",
    "section": "Discrete-Time First-Order System Block Diagram",
    "text": "Discrete-Time First-Order System Block Diagram\nSystem equation: \\(y[n]+a y[n-1]=b x[n]\\)\nRearranged for direct computation: \\(y[n] = -a y[n-1] + b x[n]\\)\n\n\n\n\n\ngraph LR\n    x_n(\"x[n]\") --&gt; mult_b{b}\n    mult_b --&gt; sum1(\"$$+$$\")\n    delay(\"$$z^{-1}$$\") -- y[n-1] --&gt; mult_a{-a}\n    mult_a --&gt; sum1\n    sum1 -- y[n] --&gt; delay\n    sum1 -- y[n] --&gt; y_n(\"y[n]\")\n\n    subgraph Operations\n        sum1\n        mult_b\n        mult_a\n        delay\n    end\n\n\n\n\n\n\n\nThe delay element (\\(z^{-1}\\)) represents the system’s memory. Its initial state corresponds to the auxiliary condition \\(y[-1]\\).\nThis diagram clearly shows feedback, characteristic of recursive (IIR) systems.\n\n\nLet’s construct a block diagram for our first-order discrete-time difference equation: y[n] = -a*y[n-1] + b*x[n]. The term b*x[n] is created by multiplying the input x[n] by b. The term -a*y[n-1] is generated from the output y[n]. The output y[n] goes into a delay element, which stores y[n-1]. This y[n-1] is then multiplied by -a. Finally, these two terms (b*x[n] and -a*y[n-1]) are summed to produce the current output y[n]. Notice the feedback loop: the output y[n] is fed back through the delay and multiplier to influence future outputs. The delay element is explicitly where the system’s “memory” resides, and its initial stored value y[-1] is the necessary auxiliary condition. This feedback structure is a hallmark of recursive systems."
  },
  {
    "objectID": "ss_24.html#basic-elements-of-block-diagram",
    "href": "ss_24.html#basic-elements-of-block-diagram",
    "title": "Signals and Systems",
    "section": "Basic Elements of Block Diagram",
    "text": "Basic Elements of Block Diagram\nFor equation \\(y[n] = -a*y[n-1] + b*x[n]\\) : (a) an adder; (b) multiplication by a coefficient; (c) a unit delay; (d) overall representation."
  },
  {
    "objectID": "ss_24.html#basic-operations-for-continuous-time-systems",
    "href": "ss_24.html#basic-operations-for-continuous-time-systems",
    "title": "Signals and Systems",
    "section": "Basic Operations for Continuous-Time Systems",
    "text": "Basic Operations for Continuous-Time Systems\n\nSimilar basic elements found in continuous-time block diagrams:\n\nAdder: Sums multiple input signals.\nMultiplier: Scales a signal by a constant coefficient.\n\nCrucially, for continuous-time, instead of a differentiator (which is difficult to implement and sensitive to noise), we use an integrator.\n\nIntegrator: \\(\\int_{-\\infty}^{t} (\\cdot) d\\tau\\). This is the memory storage element for continuous-time systems.\n\n\n\nFor continuous-time systems, the fundamental building blocks for block diagrams are similar: adders and multipliers. However, a key difference emerges when we deal with derivatives. While analytically we define systems with derivatives, practically, differentiators are very hard to build and are notoriously sensitive to noise amplification. Think about trying to build a circuit that perfectly differentiates a signal – any tiny bit of noise would be greatly exaggerated. So, for practical implementation and stable representations, we prefer to work with integrators instead. An integrator performs the inverse operation of differentiation, accumulating the input over time, and it naturally acts as the memory element in continuous-time systems, much like a capacitor storing charge."
  },
  {
    "objectID": "ss_24.html#continuous-time-first-order-system-block-diagram",
    "href": "ss_24.html#continuous-time-first-order-system-block-diagram",
    "title": "Signals and Systems",
    "section": "Continuous-Time First-Order System Block Diagram",
    "text": "Continuous-Time First-Order System Block Diagram\nSystem equation: \\(\\frac{d y(t)}{d t}+a y(t)=b x(t)\\)\nRearranged for integration: \\(\\frac{d y(t)}{d t}=b x(t)-a y(t)\\)\nIntegrate from \\(-\\infty\\) to \\(t\\): \\(y(t)=\\int_{-\\varkappa}^{t}[b x(\\tau)-a y(\\tau)] d \\tau\\)\n\n\n\n\n\ngraph LR\n    x_t(\"x(t)\") --&gt; mult_b{b}\n    mult_b --&gt; sum1(\"$$+$$\")\n    y_t(\"y(t)\") --&gt; mult_a{-a}\n    mult_a --&gt; sum1\n    sum1 --&gt; intg(\"∫dτ\")\n    intg --&gt; y_t\n\n\n\n\n\n\n\nThe integrator is the memory element, storing the accumulated signal.\nThe value \\(y(t_0)\\) represents the initial condition stored by the integrator.\nThis representation is the basis for analog computer simulations.\n\n\nLet’s visualize the first-order continuous-time LCCDE. Instead of expressing y(t) directly, we rearrange the equation to isolate the derivative of y(t). So, dy(t)/dt = b*x(t) - a*y(t). If we integrate both sides from negative infinity to t, the left side becomes y(t), and the right side becomes the integral of b*x(tau) - a*y(tau).\nIn the block diagram: x(t) is multiplied by b. y(t) is multiplied by -a. These two signals are added together. The sum is then fed into an integrator. The output of the integrator is y(t). Again, we see a feedback loop where y(t) is fed back to influence its own derivative. The integrator is the system’s memory, storing its past values. This type of block diagram directly forms the basis of historical analog computers and remains a fundamental way to understand continuous-time system implementation."
  },
  {
    "objectID": "ss_24.html#basic-elements-of-block-diagram-1",
    "href": "ss_24.html#basic-elements-of-block-diagram-1",
    "title": "Signals and Systems",
    "section": "Basic Elements of Block Diagram",
    "text": "Basic Elements of Block Diagram\n\nan adder; (b) multiplication by a coefficient; (c) a differentiator; (d) overall representation using differentiator; (d) overall representation using integrator"
  },
  {
    "objectID": "ss_24.html#continous-system-in-differential-equations",
    "href": "ss_24.html#continous-system-in-differential-equations",
    "title": "Signals and Systems",
    "section": "Continous System in Differential Equations",
    "text": "Continous System in Differential Equations\nMass–Spring System (Oscillator)\nA mass attached to a spring (ignoring friction first).\n\nHooke’s law: restoring force \\(F = -kx\\)\nNewton’s second law: \\(F = m \\dfrac{d^2x}{dt^2}\\)\n\nSo:\n\\[\nm \\frac{d^2x}{dt^2} + kx = 0\n\\]\nThis second-order ODE governs the oscillation. Its solution is sinusoidal:\n\\[\nx(t) = A \\cos(\\omega t) + B \\sin(\\omega t), \\quad \\omega = \\sqrt{\\tfrac{k}{m}}\n\\]"
  },
  {
    "objectID": "ss_24.html#continous-system-in-differential-equations-1",
    "href": "ss_24.html#continous-system-in-differential-equations-1",
    "title": "Signals and Systems",
    "section": "Continous System in Differential Equations",
    "text": "Continous System in Differential Equations\nRC Circuit (Charging a Capacitor)\nA resistor \\(R\\) and capacitor \\(C\\) in series with a voltage source \\(V\\). Kirchhoff’s law:\n\\[\nV = V_R + V_C = Ri(t) + \\frac{q(t)}{C}\n\\]\nSince \\(i(t) = \\dfrac{dq}{dt}\\):\n\\[\nR \\frac{dq}{dt} + \\frac{q}{C} = V\n\\]\nThis first-order ODE models how charge (and voltage across capacitor) evolves. The solution is exponential:\n\\[\nq(t) = CV \\left(1 - e^{-t/RC}\\right)\n\\]\nPendulum (Nonlinear System)\nFor a pendulum of length \\(L\\), angle \\(\\theta(t)\\):\n\\[\n\\frac{d^2 \\theta}{dt^2} + \\frac{g}{L}\\sin(\\theta) = 0\n\\]\nThis is a nonlinear ODE (due to \\(\\sin\\theta\\))."
  },
  {
    "objectID": "ss_24.html#discrete-system-in-difference-equations",
    "href": "ss_24.html#discrete-system-in-difference-equations",
    "title": "Signals and Systems",
    "section": "Discrete System in Difference Equations",
    "text": "Discrete System in Difference Equations\nDigital RC Circuit (Discrete Approximation)\nIf you sample the continuous RC circuit at time steps of length \\(\\Delta t\\), the capacitor voltage \\(v[n]\\) satisfies a first-order difference equation:\n\\[\nv[n+1] = \\left(1 - \\frac{\\Delta t}{RC}\\right) v[n] + \\frac{\\Delta t}{RC} V\n\\]\nThis models how the voltage changes step by step in a digital simulation."
  },
  {
    "objectID": "ss_24.html#spring-mass-system-with-numerical-integration",
    "href": "ss_24.html#spring-mass-system-with-numerical-integration",
    "title": "Signals and Systems",
    "section": "Spring-Mass System with Numerical Integration",
    "text": "Spring-Mass System with Numerical Integration\nDiscretizing Newton’s second law for a spring–mass:\n\\[\nm \\frac{d^2x}{dt^2} = -kx\n\\]\nUsing finite differences (\\(x_{n+1} - 2x_n + x_{n-1}\\)/\\(\\Delta t^2\\)):\n\\[\nx_{n+1} = 2x_n - x_{n-1} - \\frac{k}{m} \\Delta t^2 \\, x_n\n\\]\nThis is a second-order difference equation that simulates oscillations step by step.\nControl Systems (Z-Domain Models)\nDigital controllers (like in robotics or motor drives) are governed by difference equations. Example: A discrete-time first-order system:\n\\[\ny[n+1] = a y[n] + b u[n]\n\\]\nwhere \\(y[n]\\) is system output and \\(u[n]\\) is input at time step \\(n\\)."
  },
  {
    "objectID": "ss_24.html#conclusion",
    "href": "ss_24.html#conclusion",
    "title": "Signals and Systems",
    "section": "Conclusion",
    "text": "Conclusion\n\nLCCDEs are fundamental for describing causal LTI systems in both continuous and discrete domains.\nSolving involves finding a particular solution (forced response) and a homogeneous solution (natural response).\nAuxiliary conditions, particularly the condition of initial rest, are crucial for unique, causal, and LTI solutions.\nDiscrete-time systems are categorized as Recursive (IIR) or Nonrecursive (FIR) based on output dependence.\nBlock diagrams (using adders, multipliers, delays/integrators) provide valuable visual understanding and aid implementation.\nAhead: We will develop more powerful frequency-domain tools (e.g., Laplace and Z-transforms) to simplify solving these equations and further analyze complex system properties.\n\n\nTo summarize, differential and difference equations are the bedrock for modeling LTI causal systems. We’ve learned that solving them involves combining a particular solution, driven by the input, with a homogeneous solution, representing the system’s natural behavior. Initial rest is the key auxiliary condition for ensuring the crucial properties of causality and LTI. We also explored the distinction between FIR and IIR systems in discrete time and saw how block diagrams offer a powerful visual and practical representation. In the coming chapters, we’ll build on this foundation by introducing frequency-domain transforms that will provide even more efficient and insightful ways to analyze these complex and fascinating systems. Thank you."
  },
  {
    "objectID": "ss_22.html#continuous-time-lti-systems-the-convolution-integral",
    "href": "ss_22.html#continuous-time-lti-systems-the-convolution-integral",
    "title": "Signal and Systems",
    "section": "CONTINUOUS-TIME LTI SYSTEMS: THE CONVOLUTION INTEGRAL",
    "text": "CONTINUOUS-TIME LTI SYSTEMS: THE CONVOLUTION INTEGRAL\nECE Undergraduate Course\nImron Rosyadi\n\nWe’ve seen how the convolution sum governs discrete-time LTI systems. Now, we’ll develop the parallel concept for continuous-time systems. Our goal is the same: to find a way to express any continuous-time signal in terms of impulses, which will then let us find a system’s output for any input, just by knowing its impulse response."
  },
  {
    "objectID": "ss_22.html#representing-continuous-signals-with-impulses",
    "href": "ss_22.html#representing-continuous-signals-with-impulses",
    "title": "Signal and Systems",
    "section": "Representing Continuous Signals with Impulses",
    "text": "Representing Continuous Signals with Impulses\nWe start by approximating a continuous signal \\(x(t)\\) with a “staircase” of narrow pulses.\nEach pulse has a width \\(\\Delta\\) and height \\(x(k\\Delta)\\).\nThe approximation, \\(\\hat{x}(t)\\), is a sum of scaled and shifted rectangular pulses:\n\\[\n\\hat{x}(t) = \\sum_{k=-\\infty}^{\\infty} x(k\\Delta) \\delta_{\\Delta}(t-k\\Delta)\\Delta\n\\]\nwhere \\(\\delta_{\\Delta}(t)\\) is a rectangular pulse of width \\(\\Delta\\) and height \\(1/\\Delta\\).\n\n\n\n\n\n\n\nLook at the plot. The smooth blue line is our original signal, \\(x(t)\\). The orange bars represent the staircase approximation, \\(\\hat{x}(t)\\). Each bar is a narrow pulse. As you can imagine, if we make the width \\(\\Delta\\) of these pulses smaller and smaller, the orange staircase will become a better and better approximation of the blue curve."
  },
  {
    "objectID": "ss_22.html#from-summation-to-integration",
    "href": "ss_22.html#from-summation-to-integration",
    "title": "Signal and Systems",
    "section": "From Summation to Integration",
    "text": "From Summation to Integration\nAs we shrink the pulse width, \\(\\Delta \\rightarrow 0\\):\n\nThe staircase approximation \\(\\hat{x}(t)\\) becomes the signal \\(x(t)\\).\nThe narrow pulse \\(\\delta_{\\Delta}(t)\\) becomes the ideal impulse \\(\\delta(t)\\).\nThe summation becomes an integral.\n\n\\[\n\\lim_{\\Delta \\to 0} \\sum_{k=-\\infty}^{\\infty} x(k\\Delta) \\delta_{\\Delta}(t-k\\Delta)\\Delta \\quad \\longrightarrow \\quad \\int_{-\\infty}^{\\infty} x(\\tau) \\delta(t-\\tau) d\\tau\n\\]\nThis gives us the sifting property for continuous-time signals:\n\\[\nx(t) = \\int_{-\\infty}^{\\infty} x(\\tau) \\delta(t-\\tau) d\\tau\n\\]\n\nThis limiting process is the heart of the transition from discrete to continuous. The sum, which works with discrete points, transforms into an integral, which works over a continuum. The result is the continuous-time sifting property. Just like its discrete counterpart, it says we can represent any signal \\(x(t)\\) as a “continuous sum” (an integral) of weighted, shifted impulses. The weight for the impulse at time \\(\\tau\\) is the value of the signal at that time, \\(x(\\tau)\\)."
  },
  {
    "objectID": "ss_22.html#the-convolution-integral",
    "href": "ss_22.html#the-convolution-integral",
    "title": "Signal and Systems",
    "section": "The Convolution Integral",
    "text": "The Convolution Integral\nBy applying linearity and time-invariance, we arrive at the system output \\(y(t)\\):\n\nInput: \\(x(t) = \\int x(\\tau) \\delta(t-\\tau) d\\tau\\) (An integral of weighted impulses)\nLinearity: The output is the integral of the responses to those weighted impulses.\nTime-Invariance: The response to a shifted impulse \\(\\delta(t-\\tau)\\) is a shifted impulse response \\(h(t-\\tau)\\).\n\nCombining these gives the Convolution Integral:\n\\[\ny(t) = \\int_{-\\infty}^{+\\infty} x(\\tau) h(t-\\tau) d\\tau\n\\]\nThis is denoted as \\(y(t) = x(t) * h(t)\\). Once again, an LTI system is completely characterized by its impulse response \\(h(t)\\).\n\nThe logic is exactly the same as for the discrete-time case. Linearity means we can consider the response to each infinitesimal slice of the input, \\(x(\\tau)d\\tau\\), and then integrate (sum) them all up. Time-invariance means the response to a slice at time \\(\\tau\\) is just a shifted and scaled version of the impulse response, specifically \\(x(\\tau)h(t-\\tau)d\\tau\\). Integrating over all \\(\\tau\\) gives us the total output at time \\(t\\). The convolution integral is the cornerstone of continuous-time LTI system analysis."
  },
  {
    "objectID": "ss_22.html#the-flip-and-slide-method-continuous",
    "href": "ss_22.html#the-flip-and-slide-method-continuous",
    "title": "Signal and Systems",
    "section": "The “Flip-and-Slide” Method (Continuous)",
    "text": "The “Flip-and-Slide” Method (Continuous)\nWe evaluate \\(y(t) = \\int x(\\tau)h(t-\\tau)d\\tau\\) for each output time t.\nProcedure for a fixed t:\n\nPlot vs. \\(\\tau\\): Graph the input \\(x(\\tau)\\) and impulse response \\(h(\\tau)\\).\nFlip: Time-reverse \\(h(\\tau)\\) to get \\(h(-\\tau)\\).\nSlide: Shift \\(h(-\\tau)\\) by \\(t\\) to get \\(h(t-\\tau)\\).\nMultiply: Form the product signal \\(x(\\tau)h(t-\\tau)\\).\nIntegrate: Compute the total area under the product signal. This area is the value of \\(y(t)\\).\n\nRepeat for all t to find the entire output signal \\(y(t)\\).\n\nThe graphical method is also completely analogous. For any specific moment t where we want to find the output, we “flip” the impulse response and “slide” it into position. Then we multiply it by the input signal and calculate the area of the resulting shape. That area is our output value, \\(y(t)\\). We then slide to the next value of t and repeat."
  },
  {
    "objectID": "ss_22.html#example-rc-circuit-integrator",
    "href": "ss_22.html#example-rc-circuit-integrator",
    "title": "Signal and Systems",
    "section": "Example: RC Circuit (Integrator)",
    "text": "Example: RC Circuit (Integrator)\nLet’s find the response of a simple integrator to an exponential input.\nProblem\n\nInput: \\(x(t) = e^{-at}u(t)\\), for \\(a &gt; 0\\).\nImpulse Response: \\(h(t) = u(t)\\) (This is an ideal integrator).\n\nAnalysis\n\nFor \\(t &lt; 0\\), there’s no overlap, so \\(y(t) = 0\\).\nFor \\(t \\ge 0\\), the overlap is for \\(0 &lt; \\tau &lt; t\\). \\[\ny(t) = \\int_{0}^{t} e^{-a\\tau} d\\tau = \\frac{1}{a}(1 - e^{-at})\n\\]\n\nResult: \\(y(t) = \\frac{1}{a}(1 - e^{-at})u(t)\\). This is the classic charging curve of an RC circuit."
  },
  {
    "objectID": "ss_22.html#example-rc-circuit-integrator-1",
    "href": "ss_22.html#example-rc-circuit-integrator-1",
    "title": "Signal and Systems",
    "section": "Example: RC Circuit (Integrator)",
    "text": "Example: RC Circuit (Integrator)\n\n\n\n\n\n\n\nThis is the continuous-time version of the accumulator example we saw earlier. The impulse response \\(h(t)=u(t)\\) corresponds to an integrator. When we feed an exponential input into an integrator, we get the output shown on the right. Engineers will recognize this immediately as the voltage across a capacitor in a series RC circuit when a DC voltage is applied. The convolution integral mathematically derives this well-known physical behavior."
  },
  {
    "objectID": "ss_22.html#interactive-demo-convolving-two-pulses",
    "href": "ss_22.html#interactive-demo-convolving-two-pulses",
    "title": "Signal and Systems",
    "section": "Interactive Demo: Convolving Two Pulses",
    "text": "Interactive Demo: Convolving Two Pulses\n\n\\(x(t)\\) is a rectangle from \\(t=0\\) to \\(t=1\\).\n\\(h(t)\\) is a ramp from \\(t=0\\) to \\(t=2\\).\n\nUse the slider for t to see the “flip-and-slide” method in action.\n\nviewof t = Inputs.range([-0.5, 3.5], {label: \"t\", step: 0.1, value: 1.5});\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s see this in action. The top plot shows the fixed input \\(x(\\tau)\\) and the flipped, sliding impulse response \\(h(t-\\tau)\\). As you move the slider, you slide the red curve. The middle plot shows their product, and the green shaded region is the area we need to calculate. The bottom plot shows the full output signal \\(y(t)\\) built up from these areas. The cyan triangle shows the output value for the current t, which is precisely the green area in the plot above it. Watch how the shape of the overlapping area changes, creating the different segments of the output signal."
  },
  {
    "objectID": "ss_22.html#example-one-sided-exponential",
    "href": "ss_22.html#example-one-sided-exponential",
    "title": "Signal and Systems",
    "section": "Example: One-Sided Exponential",
    "text": "Example: One-Sided Exponential\nA left-sided exponential convolved with a shifted step function.\nProblem\n\n\\(x(t) = e^{2t}u(-t)\\) (left-sided)\n\\(h(t) = u(t-3)\\) (right-sided)\n\nAnalysis\n\nCase 1: \\(t-3 \\le 0\\) (i.e., \\(t \\le 3\\)) Overlap is for \\(\\tau &lt; t-3\\). \\(y(t) = \\int_{-\\infty}^{t-3} e^{2\\tau}d\\tau = \\frac{1}{2} e^{2(t-3)}\\)\nCase 2: \\(t-3 &gt; 0\\) (i.e., \\(t &gt; 3\\)) Overlap is for \\(\\tau &lt; 0\\). \\(y(t) = \\int_{-\\infty}^{0} e^{2\\tau} d\\tau = \\frac{1}{2}\\)"
  },
  {
    "objectID": "ss_22.html#example-one-sided-exponential-1",
    "href": "ss_22.html#example-one-sided-exponential-1",
    "title": "Signal and Systems",
    "section": "Example: One-Sided Exponential",
    "text": "Example: One-Sided Exponential\n\n\n\n\n\n\n\nThis example is interesting because the input signal is non-zero only for negative time. The impulse response is a step that starts at t=3. When we convolve them, we analyze the overlap in two cases. When \\(t\\) is less than 3, the overlap region depends on \\(t\\), resulting in an exponential rise. But once \\(t\\) moves past 3, the region of overlap becomes fixed (from minus infinity to zero), so the integral gives a constant value of 1/2."
  },
  {
    "objectID": "ss_22.html#summary",
    "href": "ss_22.html#summary",
    "title": "Signal and Systems",
    "section": "Summary",
    "text": "Summary\n\nSignal Representation: Any continuous signal \\(x(t)\\) can be represented by the sifting integral: \\(x(t) = \\int x(\\tau)\\delta(t-\\tau)d\\tau\\).\nLTI System Response: The output \\(y(t)\\) of a continuous-time LTI system is the input \\(x(t)\\) convolved with the system’s impulse response \\(h(t)\\).\nThe Convolution Integral: The core operation for continuous-time LTI systems is: \\[ y(t) = x(t) * h(t) = \\int_{-\\infty}^{\\infty} x(\\tau)h(t-\\tau) d\\tau \\]\nCalculation: The “flip-and-slide” method provides a graphical way to compute the convolution integral by finding the area under the product of the input and the flipped, shifted impulse response.\nKey Parallel: The theory and methods for continuous-time systems directly mirror those we learned for discrete-time systems, with sums replaced by integrals.\n\n\nTo summarize, we’ve built a complete framework for analyzing continuous-time LTI systems that is perfectly analogous to the discrete-time framework. We represent signals using impulses, which leads to the convolution integral. This integral, calculated via the “flip-and-slide” method, gives us the system’s output for any input, provided we know its unique “fingerprint”—the impulse response \\(h(t)\\)."
  },
  {
    "objectID": "ss_16.html#introduction-to-system-properties",
    "href": "ss_16.html#introduction-to-system-properties",
    "title": "Signal and Systems",
    "section": "Introduction to System Properties",
    "text": "Introduction to System Properties\n\nUnderstanding System Behavior: Basic system properties help categorize and analyze how systems behave.\nThese properties are crucial for:\n\nSimplifying system analysis and design.\nPredicting system responses to various inputs.\nDeveloping theoretical frameworks for signals and systems.\n\n\n\nWelcome everyone to this session on Basic System Properties. In Signals and Systems, understanding how systems react to signals is fundamental. But to truly understand them, we first need to characterize them based on certain intrinsic properties. These properties act like fundamental rules that govern a system’s behavior, allowing us to predict, design, and even troubleshoot complex systems with greater ease. They’ll be central to all our subsequent discussions in this course."
  },
  {
    "objectID": "ss_16.html#systems-with-and-without-memory",
    "href": "ss_16.html#systems-with-and-without-memory",
    "title": "Signal and Systems",
    "section": "1. Systems with and without Memory",
    "text": "1. Systems with and without Memory\nA system is memoryless if its output at any given time depends only on the input at that same time.\nMemoryless System Examples:\n\nDiscrete-Time: \\[\ny[n]=\\left(2 x[n]-x^{2}[n]\\right)^{2} \\quad \\text{(1.90)}\n\\]\nContinuous-Time (Resistor): \\[\ny(t)=R x(t) \\quad \\text{(1.91)}\n\\]"
  },
  {
    "objectID": "ss_16.html#systems-with-and-without-memory-1",
    "href": "ss_16.html#systems-with-and-without-memory-1",
    "title": "Signal and Systems",
    "section": "1. Systems with and without Memory",
    "text": "1. Systems with and without Memory\nSystems with Memory: Output depends on past or future input values.\n\nDiscrete-Time Accumulator (Summer): \\[\ny[n]=\\sum_{k=-\\infty}^{n} x[k] \\quad \\text{(1.92)}\n\\] This can also be expressed as \\(y[n]=y[n-1]+x[n]\\).\nDiscrete-Time Delay: \\[\ny[n]=x[n-1] \\quad \\text{(1.93)}\n\\]\nContinuous-Time Capacitor: \\[\ny(t)=\\frac{1}{C} \\int_{-\\infty}^{t} x(\\tau) d \\tau \\quad \\text{(1.94)}\n\\]\n\n\nThe first property we’ll discuss is memory. Think of it simply: does the system need to “remember” past (or anticipate future) inputs to produce its current output?\nA memoryless system is like an instantaneous black box. The resistor is a classic example: the voltage across it at time ‘t’ only depends on the current flowing through it at that exact same time ‘t’. It doesn’t care what the current was five seconds ago, or what it will be in the future.\nOn the other hand, systems with memory hold onto information. An accumulator, for instance, needs to sum all past inputs up to the current moment. Similarly, a delay system needs to remember the previous input value to output it at the current time. In physical systems, memory is often associated with energy storage, such as in capacitors (storing charge related to past current) or inductors (storing magnetic energy related to past voltage). Digital systems store information in registers. While we usually think of memory as related to the past, dependency on future values also qualifies as memory, as we’ll see with causality."
  },
  {
    "objectID": "ss_16.html#memory-interactive-demonstration",
    "href": "ss_16.html#memory-interactive-demonstration",
    "title": "Signal and Systems",
    "section": "Memory: Interactive Demonstration",
    "text": "Memory: Interactive Demonstration\nLet’s compare a memoryless system with a system with memory (an accumulator).\nMemoryless System: \\(y[n] = x[n]^2\\)"
  },
  {
    "objectID": "ss_16.html#memory-interactive-demonstration-1",
    "href": "ss_16.html#memory-interactive-demonstration-1",
    "title": "Signal and Systems",
    "section": "Memory: Interactive Demonstration",
    "text": "Memory: Interactive Demonstration\nSystem with Memory (Accumulator): \\(y[n] = \\sum_{k=-\\infty}^{n} x[k]\\)\n\n\n\n\n\n\n\nHere we have two interactive plots to illustrate the concept of memory.\nOn the first, for the memoryless system y[n] = x[n]^2, observe how y[n] at any point n (e.g., n=0) is simply the square of x[n] at that same n. There’s no dependency on x[-1] or x[1]. The output mirrors the input’s shape instantaneously.\nOn the second, we have an accumulator y[n] = sum(x[k]). If the input x[n] is an impulse at n=0 and n=1, the output y[n] at, say, n=2, is the sum of x[-infinity] to x[2]. It “remembers” the past impulses. Notice how the output value at a given n is influenced by all the input values that came before it. This continuous build-up is a clear sign of memory."
  },
  {
    "objectID": "ss_16.html#invertibility-and-inverse-systems",
    "href": "ss_16.html#invertibility-and-inverse-systems",
    "title": "Signal and Systems",
    "section": "2. Invertibility and Inverse Systems",
    "text": "2. Invertibility and Inverse Systems\nA system is invertible if distinct inputs always produce distinct outputs.\n\nIf a system is invertible, an inverse system exists.\nWhen cascaded with the original system, the inverse system yields an output identical to the original input.\n\n\n\n\n\n\ngraph LR\n    A[\"Input x[n]\"] --&gt; S1[\"System S\"]\n    S1 --&gt; B[\"Output y[n]\"]\n    B --&gt; S2[\"Inverse System S⁻¹\"]\n    S2 --&gt; C[\"Output w[n] = x[n]\"]\n\n\n\n\n\n\n(Figure 1.45(a) - Concept of an inverse system)"
  },
  {
    "objectID": "ss_16.html#invertibility-and-inverse-systems-1",
    "href": "ss_16.html#invertibility-and-inverse-systems-1",
    "title": "Signal and Systems",
    "section": "2. Invertibility and Inverse Systems",
    "text": "2. Invertibility and Inverse Systems\nInvertible System Example (Continuous-Time):\n\\[\ny(t)=2 x(t) \\quad \\text{(1.97)}\n\\] Inverse System: \\[\nw(t)=\\frac{1}{2} y(t) \\quad \\text{(1.98)}\n\\]\nInvertible System Example (Discrete-Time Accumulator):\nThe accumulator: \\(y[n]=\\sum_{k=-\\infty}^{n} x[k]\\) Inverse System: \\[\nw[n]=y[n]-y[n-1] \\quad \\text{(1.99)}\n\\]\n\nNext, we consider invertibility. An invertible system is one where you can uniquely determine the input signal if you know the output signal. Imagine a secret code: if you can decode the message perfectly back to its original form, the encoding process was invertible. If multiple different original messages could result in the same encoded message, then it’s not invertible and you can’t uniquely recover the original.\nMathematically, this means that if \\(x_1(t) \\neq x_2(t)\\), then their corresponding outputs \\(y_1(t) \\neq y_2(t)\\). If this condition holds, then an inverse system can be designed. The block diagram shows this concept: the original system followed by its inverse system effectively acts as an identity system, where the final output is just the original input.\nConsider the simple scaling system y(t) = 2x(t). If you know y(t), you can always find x(t) by dividing y(t) by 2. It’s perfectly invertible. The accumulator is also invertible, its inverse is the first difference operator."
  },
  {
    "objectID": "ss_16.html#non-invertible-systems",
    "href": "ss_16.html#non-invertible-systems",
    "title": "Signal and Systems",
    "section": "Non-Invertible Systems",
    "text": "Non-Invertible Systems\nExamples of Non-Invertible Systems:\n\nZero System: \\[\ny[n]=0 \\quad \\text{(1.100)}\n\\] Many different inputs (e.g., \\(x[n]=u[n]\\) or \\(x[n]=\\delta[n]\\)) produce the same zero output.\nSquaring System: \\[\ny(t)=x^{2}(t) \\quad \\text{(1.101)}\n\\] You cannot determine the sign of \\(x(t)\\) from \\(y(t)\\). For example, \\(x(t)=2\\) and \\(x(t)=-2\\) both yield \\(y(t)=4\\).\n\nPractical Application: Encoding Systems * In communications, encoders must be invertible for perfect signal recovery.\n\nNow for non-invertible systems: these are systems where multiple distinct inputs can lead to the same output. If this happens, you can’t uniquely reconstruct the input from the output, as you don’t know which of the multiple input possibilities was the original one.\nThe ‘zero system’ is a trivial but clear example: no matter what you input, the output is always zero. If the output is zero, you have no way of knowing what the original input was. Similarly, for the squaring system \\(y(t) = x^2(t)\\), consider inputs \\(x(t)=2\\) and \\(x(t)=-2\\). Both produce the output \\(y(t)=4\\). If you only see \\(y(t)=4\\), you can’t tell if the input was 2 or -2. Thus, the system is non-invertible.\nThis concept is vital in areas like data compression and encoding. For instance, lossless audio compression aims to be invertible, allowing you to perfectly reconstruct the original audio. Lossy compression, like MP3, is non-invertible because it discards information, making perfect reconstruction impossible, though often perceptually acceptable."
  },
  {
    "objectID": "ss_16.html#invertibility-interactive-demonstration",
    "href": "ss_16.html#invertibility-interactive-demonstration",
    "title": "Signal and Systems",
    "section": "Invertibility: Interactive Demonstration",
    "text": "Invertibility: Interactive Demonstration\nLet’s demonstrate the inverse for an accumulator.\n\n\n\n\n\n\n\nThis interactive plot helps us visualize the invertibility of the accumulator.\n\nOriginal Input x[n]: This is the signal we apply to our system.\nSystem Output y[n] (Accumulator): This shows x[n] after passing through the accumulator, where each y[n] is the running sum of x[k] up to n. Notice how the signal builds up or combines the past values.\nRecovered Signal w[n] (Inverse System): This is the output of the inverse system, which performs a “first difference” operation (\\(w[n] = y[n] - y[n-1]\\)). Observe how this operation effectively “undoes” the accumulation and recovers the original shape of x[n].\n\nThe printout below the graph confirms numerically that the recovered signal w[n] is indeed identical to the original input x[n]. This provides a concrete example of an invertible system and its inverse."
  },
  {
    "objectID": "ss_16.html#causality",
    "href": "ss_16.html#causality",
    "title": "Signal and Systems",
    "section": "3. Causality",
    "text": "3. Causality\nA system is causal if its output at any time depends only on values of the input at the present time and in the past.\n\nOften referred to as nonanticipative.\nIf \\(x_1(t) = x_2(t)\\) for \\(t \\le t_0\\), then \\(y_1(t) = y_2(t)\\) for \\(t \\le t_0\\).\nAll memoryless systems are causal.\n\nCausal System Examples:\n\nRC Circuit: Capacitor voltage responds to present and past source voltage.\nAccumulator: \\(y[n]=\\sum_{k=-\\infty}^{n} x[k]\\)\nDelay: \\(y[n]=x[n-1]\\)"
  },
  {
    "objectID": "ss_16.html#causality-1",
    "href": "ss_16.html#causality-1",
    "title": "Signal and Systems",
    "section": "3. Causality",
    "text": "3. Causality\nNon-Causal System Examples:\n\nFuture-Dependent: \\[\ny[n]=x[n]-x[n+1] \\quad \\text{(1.102)}\n\\] \\[\ny(t)=x(t+1) \\quad \\text{(1.103)}\n\\]\nTime Reversal: \\[\ny[n]=x[-n] \\quad \\text{(1.105)}\n\\] For \\(n &lt; 0\\), e.g., \\(y[-4]=x[4]\\), output depends on future input.\nAveraging System: \\[\ny[n]=\\frac{1}{2 M+1} \\sum_{k=-M}^{+M} x[n-k] \\quad \\text{(1.104)}\n\\] Includes future values like \\(x[n+M]\\).\n\n\nCausality is a crucial property, especially for real-time systems. Simply put, a causal system cannot “look into the future.” Its current output can only be influenced by the current input or inputs that have already occurred (i.e., past inputs). Think of hitting a drum: the sound (output) can only happen after you hit it (input), not before.\nAn RC circuit is causal because the capacitor voltage builds up based on the history of the current, not what current will flow in the future. All memoryless systems are inherently causal because their output only depends on the present input, which by definition means no future dependency.\nHowever, many systems are non-causal. For instance, a system that outputs x[n+1] outputs a future value of the input. This is impossible in real-time, but perfectly fine for processing recorded data, like in audio mastering or image processing, where the entire signal is available. The average system is an example, it takes inputs from a window that spans both past and future values relative to n. Another subtle non-causal system is y[n] = x[-n]. For \\(n &lt; 0\\), say \\(n=-5\\), \\(y[-5]\\) becomes \\(x[5]\\), which is a future value."
  },
  {
    "objectID": "ss_16.html#causality-interactive-demonstration",
    "href": "ss_16.html#causality-interactive-demonstration",
    "title": "Signal and Systems",
    "section": "Causality: Interactive Demonstration",
    "text": "Causality: Interactive Demonstration\nCompare a causal delay with a non-causal advance.\n\n\n\n\n\n\n\nThis interactive plot clearly demonstrates the difference between causal and non-causal systems using a delay and an advance.\n\nOriginal Input x[n]: Our reference signal.\nCausal System: y[n] = x[n-1] (Delay): Observe that the output y[n] is always the value of x[n] from the previous time step. For example, y[0] is x[-1], y[1] is x[0], and so on. The output signal is shifted to the right, appearing after the corresponding input values. This is how physical, real-time systems operate. For any given n, your output only depends on x[n] or x[n-k] where k is a positive integers.\nNon-Causal System: y[n] = x[n+1] (Advance): Notice how the output y[n] is the value of x[n] from the next time step. For example, y[0] is x[1], y[1] is x[2]. The output signal appears shifted to the left, meaning it anticipates future input values. While impossible in real-time, such systems are useful for processing pre-recorded data when all future samples are available."
  },
  {
    "objectID": "ss_16.html#stability",
    "href": "ss_16.html#stability",
    "title": "Signal and Systems",
    "section": "4. Stability",
    "text": "4. Stability\nA system is stable if small (bounded) inputs lead to responses that do not diverge (are also bounded).\n\nBounded-Input, Bounded-Output (BIBO) Stability.\nInformally: A stable system eventually settles down or remains within limits, given a reasonable input.\n\n\n\nStable System Analogy: Pendulum\n Gravity and friction provide restoring/dissipating forces.\n\nUnstable System Analogy: Inverted Pendulum  Gravity increases deviation; small perturbation leads to tipping."
  },
  {
    "objectID": "ss_16.html#stability-1",
    "href": "ss_16.html#stability-1",
    "title": "Signal and Systems",
    "section": "4. Stability",
    "text": "4. Stability\nUnstable System Example: Accumulator for Unit Step Input\nIf \\(x[n]=u[n]\\) (unit step, bounded, equal to 1 for \\(n \\ge 0\\)), the accumulator output is: \\[\ny[n]=\\sum_{k=-\\infty}^{n} u[k]=(n+1) u[n]\n\\] * \\(y[0]=1, y[1]=2, y[2]=3, \\ldots\\) * \\(y[n]\\) grows without bound, so the accumulator is unstable.\n\nStability is a critical property for practical systems. An unstable system, even with a small input, can produce an output that grows infinitely large, potentially leading to system failure or unwanted behavior. Think of a microphone feeding back into a speaker: a small sound can quickly escalate into a loud, ear-splitting screech. That’s instability.\nThe formal definition is BIBO stability: Bounded-Input, Bounded-Output. If you put a signal into the system whose amplitude never exceeds a certain finite maximum (it’s “bounded”), then the output signal must also have an amplitude that never exceeds some finite maximum. If the output grows without limit for any bounded input, the system is unstable.\nThe pendulum analogy is excellent. A normal pendulum is stable: give it a small push, and it oscillates for a while but eventually settles back to its equilibrium point due to damping (friction) and a restoring force (gravity). An inverted pendulum, however, is inherently unstable: a tiny push will cause it to fall over, with its deviation from vertical growing rapidly.\nThe accumulator, as we saw before, is an example of an unstable system. If you feed it a constant input like a unit step, its output just keeps growing indefinitely, accumulating the past values. This unbounded growth from a bounded input directly violates the BIBO stability criterion."
  },
  {
    "objectID": "ss_16.html#stability-interactive-demonstration-accumulator",
    "href": "ss_16.html#stability-interactive-demonstration-accumulator",
    "title": "Signal and Systems",
    "section": "Stability: Interactive Demonstration (Accumulator)",
    "text": "Stability: Interactive Demonstration (Accumulator)\nLet’s observe the accumulator’s response to a bounded input.\n\n\n\n\n\n\n\nThis interactive demonstration uses the accumulator system again, but specifically to show its instability.\n\nBounded Input x[n] = u[n]: The top plot shows a unit step function. This signal is definitely bounded; its maximum value is 1, and its minimum is 0.\nSystem Output (Accumulator): The bottom plot shows the output of the accumulator when u[n] is the input. You can clearly see that y[n] grows linearly with n for n &gt;= 0. For example, y[0]=1, y[1]=2, y[2]=3, and so on.\n\nSince the output y[n] grows indefinitely as n increases, even though the input x[n] remains bounded, the accumulator is an unstable system according to the BIBO definition. This illustrates why the formal definition of stability is vital, as intuitive “slow growth” can still lead to unboundedness over time."
  },
  {
    "objectID": "ss_16.html#examples-of-stability-check",
    "href": "ss_16.html#examples-of-stability-check",
    "title": "Signal and Systems",
    "section": "Examples of Stability Check",
    "text": "Examples of Stability Check\n\n\nSystem \\(S_1\\): \\(y(t) = t x(t)\\) (1.109)\n\nInput: \\(x(t) = 1\\) (A bounded input).\nOutput: \\(y(t) = t \\cdot 1 = t\\).\nAs \\(t \\to \\infty\\), \\(y(t)\\) grows without bound.\nConclusion: System \\(S_1\\) is unstable.\n\n\nSystem \\(S_2\\): \\(y(t) = e^{x(t)}\\) (1.110)\n\nConsider any bounded input: For some \\(B &gt; 0\\), \\(|x(t)| &lt; B\\).\n\nThis means \\(-B &lt; x(t) &lt; B\\).\n\nThen for the output:\n\n\\(e^{-B} &lt; y(t) &lt; e^{B}\\).\n\nThe output \\(y(t)\\) is bounded by \\(e^B\\).\nConclusion: System \\(S_2\\) is stable.\n\n\n\nThese two examples highlight how to formally check for stability.\nFor \\(S_1\\), the strategy for proving instability is to find just one counterexample. A simple constant input, like \\(x(t)=1\\), is bounded. But when passed through \\(y(t)=tx(t)\\), the output becomes \\(y(t)=t\\). This output clearly grows without limit as time progresses. Since we found a bounded input that yields an unbounded output, \\(S_1\\) is unstable.\nFor \\(S_2\\), it’s not immediately obvious to find a counterexample if it’s stable. So, the strategy is to prove it for any bounded input. If an input \\(x(t)\\) is bounded by \\(B\\), meaning its absolute value is always less than \\(B\\), then the exponential function \\(e^{x(t)}\\) will also be bounded. Since \\(e^x\\) is an monotonically increasing function, \\(e^{-B} &lt; e^{x(t)} &lt; e^B\\). This shows that if the input is bounded by \\(B\\), the output is guaranteed to be bounded by \\(e^B\\). Thus, \\(S_2\\) is stable. Always remember to check for all bounded inputs, not just specific ones, when proving stability."
  },
  {
    "objectID": "ss_16.html#time-invariance-interactive-demonstration-time-varying-gain",
    "href": "ss_16.html#time-invariance-interactive-demonstration-time-varying-gain",
    "title": "Signal and Systems",
    "section": "Time Invariance: Interactive Demonstration (Time-Varying Gain)",
    "text": "Time Invariance: Interactive Demonstration (Time-Varying Gain)\nLet’s illustrate that \\(y[n]=nx[n]\\) is time-varying.\n\n\n\n\n\n\n\nThis demonstration visually proves that the system \\(y[n]=nx[n]\\) is not time-invariant.\n\nTop Plot: \\(x_1[n]\\) and \\(y_1[n]\\): We start with an input \\(x_1[n]\\) which is an impulse at \\(n=0\\). The output \\(y_1[n]\\) is calculated as \\(n \\cdot x_1[n]\\), which for \\(n=0\\) is \\(0 \\cdot 1 = 0\\). So, \\(y_1[n]\\) is zero everywhere.\nMiddle Plot: \\(x_2[n]\\) and \\(y_2[n]\\): Now, we shift the input \\(x_1[n]\\) by 2 units to get \\(x_2[n]\\) (an impulse at \\(n=2\\)). The output \\(y_2[n]\\) is then calculated as \\(n \\cdot x_2[n]\\). For \\(n=2\\), this becomes \\(2 \\cdot 1 = 2\\). So, \\(y_2[n]\\) is an impulse of amplitude 2 at \\(n=2\\).\nBottom Plot: Comparison (\\(y_2[n]\\) vs. expected \\(y_1[n-2]\\)): If the system were time-invariant, the output \\(y_2[n]\\) (green stems) should simply be a shifted version of \\(y_1[n]\\) (meaning still all zeros, but shifted, so still all zeros – represented by the dashed blue stems for expected \\(y_1[n-2]\\)). However, y2[n] clearly has a non-zero value at n=2.\n\nSince \\(y_2[n]\\) is not equal to \\(y_1[n-2]\\), the system is time-varying. The gain n changes based on time, so the response to the same input (just shifted) will be different."
  },
  {
    "objectID": "ss_16.html#linearity",
    "href": "ss_16.html#linearity",
    "title": "Signal and Systems",
    "section": "6. Linearity",
    "text": "6. Linearity\nA system is linear if it possesses the property of superposition. This means it satisfies two conditions:\n\nAdditivity: If \\(x_1(t) \\to y_1(t)\\) and \\(x_2(t) \\to y_2(t)\\), then \\(x_1(t)+x_2(t) \\to y_1(t)+y_2(t)\\).\nHomogeneity (Scaling): If \\(x_1(t) \\to y_1(t)\\), then \\(a x_1(t) \\to a y_1(t)\\) for any complex constant \\(a\\).\n\nThese two properties can be combined: * Continuous Time: \\[\n    a x_1(t)+b x_2(t) \\rightarrow a y_1(t)+b y_2(t) \\quad \\text{(1.121)}\n    \\] * Discrete Time: \\[\n    a x_1[n]+b x_2[n] \\rightarrow a y_1[n]+b y_2[n] \\quad \\text{(1.122)}\n    \\]\nImportant Consequence: For linear systems, a zero input \\(x[n]=0\\) for all \\(n\\) must result in a zero output \\(y[n]=0\\) for all \\(n\\).\n\nLinearity is perhaps the most fundamental property in Signals and Systems, forming the basis for many powerful analysis techniques. A linear system obeys the principle of superposition.\nThis principle is broken down into two parts: 1. Additivity: If you have two different inputs, say \\(x_1\\) and \\(x_2\\), and you know their individual outputs, \\(y_1\\) and \\(y_2\\), then if you apply the sum of the inputs (\\(x_1+x_2\\)), the output will simply be the sum of their individual outputs (\\(y_1+y_2\\)). 2. Homogeneity/Scaling: If you scale an input by a factor ‘a’ (e.g., make it twice as strong), the output will also be scaled by the exact same factor ‘a’.\nBoth of these must hold for a system to be linear. If either one fails, the system is nonlinear. The combined statement is very powerful for checking linearity.\nA crucial consequence of linearity is that a linear system with no input should produce no output. This “zero-in/zero-out” check is often the quickest way to spot a nonlinear system. If you apply zero input and get a non-zero output (like a constant offset), the system is definitely not linear. We’ll explore examples now."
  },
  {
    "objectID": "ss_16.html#linearity-examples",
    "href": "ss_16.html#linearity-examples",
    "title": "Signal and Systems",
    "section": "Linearity: Examples",
    "text": "Linearity: Examples\nExample 1: System \\(S_A: y(t) = t x(t)\\) (Example 1.17)\n\nLet \\(x_3(t) = a x_1(t) + b x_2(t)\\).\n\\(y_3(t) = t x_3(t) = t(a x_1(t) + b x_2(t)) = a (t x_1(t)) + b (t x_2(t)) = a y_1(t) + b y_2(t)\\).\nConclusion: System \\(S_A\\) is linear. (It is also time-varying, as seen before!)\n\nExample 2: System \\(S_B: y(t) = x^2(t)\\) (Example 1.18)\n\nLet \\(x_3(t) = a x_1(t) + b x_2(t)\\).\n\\(y_3(t) = (a x_1(t) + b x_2(t))^2 = a^2 x_1^2(t) + b^2 x_2^2(t) + 2ab x_1(t) x_2(t)\\).\nThis is \\(a^2 y_1(t) + b^2 y_2(t) + 2ab x_1(t) x_2(t)\\) which is not \\(a y_1(t) + b y_2(t)\\).\nConclusion: System \\(S_B\\) is non-linear.\n\nExample 3: System \\(S_C: y[n] = \\text{Re}\\{x[n]\\}\\) (Example 1.19)\n\nIs additive, but fails homogeneity for complex scalars.\nIf \\(x_1[n] = r[n]+js[n]\\), then \\(y_1[n] = r[n]\\).\nLet \\(a=j\\). The input \\(x_2[n] = j x_1[n] = -s[n] + j r[n]\\).\n\\(y_2[n] = \\text{Re}\\{x_2[n]\\} = -s[n]\\).\nExpected \\(a y_1[n] = j r[n]\\). Since \\(-s[n] \\ne j r[n]\\), it fails homogeneity.\nConclusion: System \\(S_C\\) is non-linear.\n\n\nLet’s walk through these examples to solidify our understanding of linearity. For each, we apply the superposition test.\n\nSystem \\(S_A: y(t) = t x(t)\\): This is a direct application of the definition. When we substitute the combined input \\(ax_1(t)+bx_2(t)\\) into the system equation, we can factor out \\(a\\) and \\(b\\), showing that the output is indeed the linear combination of individual outputs. So, it’s linear. Note that this system is linear but not time-invariant, as we discussed previously. Systems can have one property without the other.\nSystem \\(S_B: y(t) = x^2(t)\\): Here, the squaring operation \\(x^2(t)\\) immediately suggests non-linearity because of the cross-term \\(2ab x_1(t) x_2(t)\\) that arises from expanding the square of \\((ax_1+bx_2)\\). This cross-term prevents the output from being a simple linear combination of \\(y_1\\) and \\(y_2\\). Therefore, it’s non-linear.\nSystem \\(S_C: y[n] = \\text{Re}\\{x[n]\\}\\): This one is tricky and highlights the importance of considering complex scalars for homogeneity. It is additive, but it fails homogeneity when scaled by a complex constant like \\(j\\). The real part of \\(j \\cdot x_1[n]\\) is not \\(j\\) times the real part of \\(x_1[n]\\). This demonstrates that both additivity AND homogeneity must hold for any complex scalar for a system to be linear."
  },
  {
    "objectID": "ss_16.html#linearity-interactive-demonstration-quadratic-system",
    "href": "ss_16.html#linearity-interactive-demonstration-quadratic-system",
    "title": "Signal and Systems",
    "section": "Linearity: Interactive Demonstration (Quadratic System)",
    "text": "Linearity: Interactive Demonstration (Quadratic System)\nLet’s test \\(y[n] = x^2[n]\\) for linearity.\n\n\n\n\n\n\n\nThis demonstration concretely shows why the system \\(y[n]=x^2[n]\\) is nonlinear. We’re testing the superposition property by comparing two results:\n\nExpected Linear Sum (blue dashed stems): This is what the output should be if the system were linear: \\(a \\cdot y_1[n] + b \\cdot y_2[n]\\). We compute \\(y_1[n]\\) from \\(x_1[n]\\) and \\(y_2[n]\\) from \\(x_2[n]\\) separately, then combine them.\nActual Output (red solid stems): This is the output we get when we first combine the inputs (\\(a \\cdot x_1[n] + b \\cdot x_2[n]\\)) and then pass this combined signal through the system.\n\nObserve: * At n=0: x1[0]=2, x2[0]=0. The combined input is 2*2 + 3*0 = 4. The actual output is 4^2=16. The expected linear sum is 2*(2^2) + 3*(0^2) = 2*4 + 0 = 8. They are different! * At n=1: x1[1]=0, x2[1]=1. The combined input is 2*0 + 3*1 = 3. The actual output is 3^2=9. The expected linear sum is 2*(0^2) + 3*(1^2) = 0 + 3*1 = 3. They are different!\nThe plots clearly show that the red solid stems (actual output) do not match the blue dashed stems (expected linear sum). The numerical checks at the bottom confirm this. This discrepancy, caused by the squaring operation, violates the superposition principle, proving the system is nonlinear."
  },
  {
    "objectID": "ss_16.html#linearity-incrementally-linear-systems",
    "href": "ss_16.html#linearity-incrementally-linear-systems",
    "title": "Signal and Systems",
    "section": "Linearity: Incrementally Linear Systems",
    "text": "Linearity: Incrementally Linear Systems\nExample: System \\(S_D: y[n] = 2x[n] + 3\\) (1.132)\n\nThis system often looks “linear” because it’s a linear equation.\nHowever, it violates the zero-input, zero-output property:\n\nIf \\(x[n]=0\\), then \\(y[n]=2(0)+3=3 \\ne 0\\).\n\nTherefore, System \\(S_D\\) is non-linear.\n\n\n\n\n\n\ngraph TD\n    A[\"Input x[n]\"] --&gt; S_linear[\"Linear System: $$y_L[n]=2x[n]$$\"]\n    S_linear --&gt; add[+]\n    S_const[Constant Input: 3] --&gt; add\n    add --&gt; B[\"Output y[n]\"]\n\n\n\n\n\n\n(Figure 1.48 - Structure of an incrementally linear system)\nThis system is incrementally linear: * The difference between two outputs is a linear function of the difference between their inputs. \\[\ny_1[n]-y_2[n] = (2x_1[n]+3) - (2x_2[n]+3) = 2(x_1[n]-x_2[n]) \\quad \\text{(1.136)}\n\\] * This means it can be viewed as a linear system (\\(2x[n]\\)) with a constant offset (3).\n\nThis final example for linearity, \\(y[n]=2x[n]+3\\), is a very common point of confusion. Many students, seeing a “linear equation”, assume it defines a linear system. However, this is not the case within the context of Signals and Systems.\nThe quickest way to check is the “zero-in, zero-out” property. If \\(x[n]\\) is zero for all \\(n\\), a truly linear system must output zero for all \\(n\\). But for \\(y[n]=2x[n]+3\\), if \\(x[n]=0\\), then \\(y[n]=3\\). Since the output is non-zero, this system is not linear. It fails the homogeneity property (try scaling by zero!).\nHowever, such systems are called “incrementally linear.” This means that while the system itself isn’t linear, changes in its output are linear with respect to changes in its input. The diagram illustrates this: the system can be seen as a linear part (\\(2x[n]\\)) combined with a constant offset (3), which is essentially the system’s “zero-input response.” This distinction is important for understanding more complex systems later."
  },
  {
    "objectID": "ss_16.html#conclusion-summary",
    "href": "ss_16.html#conclusion-summary",
    "title": "Signal and Systems",
    "section": "Conclusion & Summary",
    "text": "Conclusion & Summary\nWe’ve explored six fundamental properties of systems:\n\nMemory / Memoryless: Does output depend only on the current input?\nInvertibility: Can the input be uniquely recovered from the output?\nCausality: Does output depend only on present and past inputs? (Non-anticipative)\nStability: Do bounded inputs lead to bounded outputs? (BIBO)\nTime Invariance: Do a time shift in input cause an identical time shift in output?\nLinearity: Does the system satisfy superposition (additivity & homogeneity)?\n\nThese properties are essential tools for:\n\nClassifying systems.\nSimplifying analysis (especially for linear, time-invariant systems).\nDesigning systems with desired behaviors.\n\nUnderstanding these basics lays the groundwork for advanced topics in Signals and Systems!\n\nTo wrap up, we’ve covered six critical properties that define the behavior of systems in Signals and Systems.\n\nMemory speaks to whether a system “remembers” past inputs or predicts future ones.\nInvertibility is about whether we can uniquely reverse the system’s operation to deduce the original input.\nCausality is about real-time behavior - can the system react only to what has already happened, or does it anticipate?\nStability is crucial for reliability, ensuring that small inputs don’t lead to out-of-control outputs.\nTime Invariance means the system’s behavior is consistent over time, regardless of when an input is applied.\nLinearity is the bedrock for many advanced analysis techniques, allowing us to break down complex inputs into simpler components and superimpose their responses.\n\nThese definitions are not just academic. They directly influence how we design everything from audio filters to control systems, telecommunications networks, and medical imaging devices. Mastering these properties will significantly simplify your journey through the rest of this Signals and Systems course. Thank you!"
  },
  {
    "objectID": "ss_14.html#introduction",
    "href": "ss_14.html#introduction",
    "title": "Signal and Systems",
    "section": "Introduction",
    "text": "Introduction\nThe unit impulse and unit step functions are fundamental signals in Continuous and Discrete Time.\n\nThey serve as building blocks for representing other complex signals.\nCrucial for understanding System Responses (e.g., Impulse Response).\n\n\nToday, we’ll dive into these essential signals. We’ll define them for both discrete and continuous time, explore their relationships, and see how they are used, including their powerful sampling property."
  },
  {
    "objectID": "ss_14.html#the-discrete-time-unit-impulse-sequence-deltan",
    "href": "ss_14.html#the-discrete-time-unit-impulse-sequence-deltan",
    "title": "Signal and Systems",
    "section": "1.4.1 The Discrete-Time Unit Impulse Sequence (\\(\\delta[n]\\))",
    "text": "1.4.1 The Discrete-Time Unit Impulse Sequence (\\(\\delta[n]\\))\nThe discrete-time unit impulse (or unit sample) is defined as:\n\\[\n\\delta[n]= \\begin{cases}0, & n \\neq 0 \\\\ 1, & n=0\\end{cases} \\quad \\text{(Equation 1.63)}\n\\]\nIt represents a single, instantaneous event at \\(n=0\\).\nMathematical Concept\n\nA single point with value 1 at \\(n=0\\).\nAll other points are 0.\nAnalogous to a very short click or tap."
  },
  {
    "objectID": "ss_14.html#the-discrete-time-unit-impulse-sequence-deltan-1",
    "href": "ss_14.html#the-discrete-time-unit-impulse-sequence-deltan-1",
    "title": "Signal and Systems",
    "section": "1.4.1 The Discrete-Time Unit Impulse Sequence (\\(\\delta[n]\\))",
    "text": "1.4.1 The Discrete-Time Unit Impulse Sequence (\\(\\delta[n]\\))\nInteractive Plot\n\n\n\n\n\n\n\nThe discrete-time unit impulse, also known as the unit sample, is a very simple yet powerful signal. As you can see from the definition and the plot, it’s non-zero only at n=0, where its value is 1. Think of it as a single, isolated “click” in time. We often use it to represent instantaneous events or to probe a system’s behavior. We’ll soon see its importance as a building block for other signals."
  },
  {
    "objectID": "ss_14.html#the-discrete-time-unit-step-sequence-un",
    "href": "ss_14.html#the-discrete-time-unit-step-sequence-un",
    "title": "Signal and Systems",
    "section": "1.4.1 The Discrete-Time Unit Step Sequence (\\(u[n]\\))",
    "text": "1.4.1 The Discrete-Time Unit Step Sequence (\\(u[n]\\))\nThe discrete-time unit step is defined by:\n\\[\nu[n]= \\begin{cases}0, & n&lt;0 \\\\ 1, & n \\geq 0\\end{cases} \\quad \\text{(Equation 1.64)}\n\\]\nIt represents a signal that turns on at \\(n=0\\) and stays on.\nEngineering Application\n\nOften used to model a switch being turned on.\nRepresents the initiation of a process."
  },
  {
    "objectID": "ss_14.html#the-discrete-time-unit-step-sequence-un-1",
    "href": "ss_14.html#the-discrete-time-unit-step-sequence-un-1",
    "title": "Signal and Systems",
    "section": "1.4.1 The Discrete-Time Unit Step Sequence (\\(u[n]\\))",
    "text": "1.4.1 The Discrete-Time Unit Step Sequence (\\(u[n]\\))\n\n\n\n\n\n\n\nThe discrete-time unit step is equally important. It’s zero for all negative indices and then jumps to 1 and stays at 1 for all non-negative indices. This signal is often used to represent the moment something is “switched on” or the start of a process. For example, if you apply a constant voltage to a circuit at a specific time, that can be modeled with a step function."
  },
  {
    "objectID": "ss_14.html#relationship-between-deltan-and-un-difference",
    "href": "ss_14.html#relationship-between-deltan-and-un-difference",
    "title": "Signal and Systems",
    "section": "Relationship Between \\(\\delta[n]\\) and \\(u[n]\\) (Difference)",
    "text": "Relationship Between \\(\\delta[n]\\) and \\(u[n]\\) (Difference)\nThe discrete-time unit impulse is the first difference of the discrete-time step:\n\\[\n\\delta[n]=u[n]-u[n-1] \\quad \\text{(Equation 1.65)}\n\\]\nThis means you can generate an impulse from two shifted step functions.\nConcept Illustrated\n\n\n\n\n\ngraph LR\n    A[\"u[n]\"] --&gt; B{Delay by 1};\n    B --&gt; C[\"u[n-1]\"];\n    A --&gt; D{Subtract};\n    C --&gt; D;\n    D --&gt; E[\"$$\\delta[n]$$\"];"
  },
  {
    "objectID": "ss_14.html#relationship-between-deltan-and-un-difference-1",
    "href": "ss_14.html#relationship-between-deltan-and-un-difference-1",
    "title": "Signal and Systems",
    "section": "Relationship Between \\(\\delta[n]\\) and \\(u[n]\\) (Difference)",
    "text": "Relationship Between \\(\\delta[n]\\) and \\(u[n]\\) (Difference)\nInteractive Demonstration\n\n\n\n\n\n\n\nThis is a crucial relationship. If you take the unit step u[n] and subtract a delayed version of itself, u[n-1], you get the unit impulse delta[n]. Think about it: u[n] turns on at n=0. u[n-1] turns on at n=1. So, when you subtract them, the ’1’s cancel out everywhere except at n=0 (where u[n] is 1 and u[n-1] is 0), resulting in a single value of 1. This reinforces how these basic signals are interconnected."
  },
  {
    "objectID": "ss_14.html#relationship-between-deltan-and-un-running-sum",
    "href": "ss_14.html#relationship-between-deltan-and-un-running-sum",
    "title": "Signal and Systems",
    "section": "Relationship Between \\(\\delta[n]\\) and \\(u[n]\\) (Running Sum)",
    "text": "Relationship Between \\(\\delta[n]\\) and \\(u[n]\\) (Running Sum)\nConversely, the discrete-time unit step is the running sum of the unit impulse:\n\\[\nu[n]=\\sum_{m=-\\infty}^{n} \\delta[m] \\quad \\text{(Equation 1.66)}\n\\]\nThis means you can build up a step function by accumulating impulses.\nMathematical Form \\[\nu[n]=\\sum_{k=0}^{\\infty} \\delta[n-k] \\quad \\text{(Equation 1.67)}\n\\] - Or, a superposition of delayed impulses. - Emphasizes accumulation."
  },
  {
    "objectID": "ss_14.html#relationship-between-deltan-and-un-running-sum-1",
    "href": "ss_14.html#relationship-between-deltan-and-un-running-sum-1",
    "title": "Signal and Systems",
    "section": "Relationship Between \\(\\delta[n]\\) and \\(u[n]\\) (Running Sum)",
    "text": "Relationship Between \\(\\delta[n]\\) and \\(u[n]\\) (Running Sum)\nInteractive Demonstration: Running Sum\n\n\n\n\n\n\n\nJust as we can get an impulse by differencing, we can get a step by summing. The unit step u[n] is simply the running sum of the unit impulse. If you add up the values of delta[m] from negative infinity up to n, you’ll see that for any n &lt; 0, the sum is zero because delta[m] is zero. But once n reaches zero or goes positive, the delta[0] component of value 1 gets included, and the sum becomes 1. This also implies that the step function can be seen as an infinite sum of delayed impulses. This concept of summing impulses to form other signals is foundational to convolution, which we’ll cover in Chapter 2."
  },
  {
    "objectID": "ss_14.html#sampling-property-of-deltan",
    "href": "ss_14.html#sampling-property-of-deltan",
    "title": "Signal and Systems",
    "section": "Sampling Property of \\(\\delta[n]\\)",
    "text": "Sampling Property of \\(\\delta[n]\\)\nThe unit impulse can be used to sample the value of a signal at a specific point.\n\\[\nx[n] \\delta[n]=x[0] \\delta[n] \\quad \\text{(Equation 1.68)}\n\\]\nMore generally, for a delayed impulse: \\[\nx[n] \\delta\\left[n-n_{0}\\right]=x\\left[n_{0}\\right] \\delta\\left[n-n_{0}\\right] \\quad \\text{(Equation 1.69)}\n\\]\nThis property is extremely powerful for signal analysis.\nExample: What is \\(x[n]\\delta[n-3]\\) if \\(x[n] = \\cos(\\frac{\\pi n}{4})\\)? \\(x[n]\\delta[n-3] = x[3]\\delta[n-3] = \\cos(\\frac{3\\pi}{4})\\delta[n-3] = -\\frac{\\sqrt{2}}{2}\\delta[n-3]\\)\n\nThis is one of the most important properties of the unit impulse: its sampling property. Because the impulse is only non-zero at one specific point, when you multiply any signal x[n] by delta[n], the result is x[0] at n=0 and zero everywhere else. It “picks out” the value of the signal at that specific time. If the impulse is delayed to n_0, it picks out x[n_0]. This is incredibly useful for isolating specific values of a signal or for simplifying expressions in system analysis, especially when working with convolution."
  },
  {
    "objectID": "ss_14.html#the-continuous-time-unit-step-function-ut",
    "href": "ss_14.html#the-continuous-time-unit-step-function-ut",
    "title": "Signal and Systems",
    "section": "1.4.2 The Continuous-Time Unit Step Function (\\(u(t)\\))",
    "text": "1.4.2 The Continuous-Time Unit Step Function (\\(u(t)\\))\nThe continuous-time unit step function is defined as:\n\\[\nu(t)= \\begin{cases}0, & t&lt;0 \\\\ 1, & t&gt;0\\end{cases} \\quad \\text{(Equation 1.70)}\n\\]\nNote: \\(u(t)\\) is discontinuous at \\(t=0\\).\nPhysical Interpretation\n\nRepresents an abrupt change in a system.\nE.g., turning on a power supply, applying a force."
  },
  {
    "objectID": "ss_14.html#the-continuous-time-unit-step-function-ut-1",
    "href": "ss_14.html#the-continuous-time-unit-step-function-ut-1",
    "title": "Signal and Systems",
    "section": "1.4.2 The Continuous-Time Unit Step Function (\\(u(t)\\))",
    "text": "1.4.2 The Continuous-Time Unit Step Function (\\(u(t)\\))\nInteractive Plot\n\n\n\n\n\n\n\nThe continuous-time unit step u(t) is very similar to its discrete counterpart. It’s zero for negative time and rises to 1 for positive time. The key difference is the discontinuity at t=0. For practical purposes in engineering, we often define u(0) as 0.5 or leave it undefined, but its behavior surrounding that point is what’s important. It models a sharp ‘on’ transition, like closing a switch in an analog circuit."
  },
  {
    "objectID": "ss_14.html#the-continuous-time-unit-impulse-function-deltat",
    "href": "ss_14.html#the-continuous-time-unit-impulse-function-deltat",
    "title": "Signal and Systems",
    "section": "The Continuous-Time Unit Impulse Function (\\(\\delta(t)\\))",
    "text": "The Continuous-Time Unit Impulse Function (\\(\\delta(t)\\))\nThe continuous-time unit impulse is related to the unit step by: \\[\nu(t)=\\int_{-\\infty}^{t} \\delta(\\tau) d \\tau \\quad \\text{(Equation 1.71)}\n\\]\nThis implies: \\[\n\\delta(t)=\\frac{d u(t)}{d t} \\quad \\text{(Equation 1.72)}\n\\]\nChallenge: u(t) is not formally differentiable at \\(t=0\\).\n\nNow, let’s turn to the continuous-time unit impulse, delta(t). It’s fundamentally linked to the unit step. If you integrate the impulse, you get the step. This also means that the impulse can be thought of as the derivative of the unit step. However, u(t) has an abrupt jump at t=0, making it formally non-differentiable in the traditional sense. This is where the concept of the impulse becomes an idealization."
  },
  {
    "objectID": "ss_14.html#continuous-time-unit-impulse-an-idealization",
    "href": "ss_14.html#continuous-time-unit-impulse-an-idealization",
    "title": "Signal and Systems",
    "section": "Continuous-Time Unit Impulse: An Idealization",
    "text": "Continuous-Time Unit Impulse: An Idealization\nTo understand \\(\\delta(t)\\), we approximate \\(u(t)\\) with a smooth function \\(u_{\\Delta}(t)\\).\nApproximate Unit Step (\\(u_{\\Delta}(t)\\))\n\nRises from 0 to 1 over a small interval \\(\\Delta\\).\n\\(\\delta(t)=\\lim _{\\Delta \\rightarrow 0} \\delta_{\\Delta}(t)\\) (Equation 1.74)"
  },
  {
    "objectID": "ss_14.html#continuous-time-unit-impulse-an-idealization-1",
    "href": "ss_14.html#continuous-time-unit-impulse-an-idealization-1",
    "title": "Signal and Systems",
    "section": "Continuous-Time Unit Impulse: An Idealization",
    "text": "Continuous-Time Unit Impulse: An Idealization\nDerivative of Approximation (\\(\\delta_{\\Delta}(t)\\))\n\nA short pulse of duration \\(\\Delta\\).\nCrucially: Its area is always 1.\n\n\n\n\n\n\n\n\nTo properly understand delta(t), we use the concept of an idealization through a limiting process. Imagine an approximate step function, u_delta(t), that smoothly rises from 0 to 1 over a very short duration delta. As delta gets smaller and smaller, u_delta(t) approaches the ideal u(t).\nNow, consider the derivative of this approximate step, delta_delta(t). It’s a pulse of height 1/delta and duration delta, meaning its area is always (1/delta) * delta = 1, regardless of how small delta is. As delta approaches zero, this pulse becomes infinitely tall and infinitesimally narrow, but its area remains unity. This infinitely tall, infinitely narrow pulse with unit area is our continuous-time unit impulse delta(t). This unit area is its defining characteristic."
  },
  {
    "objectID": "ss_14.html#continuous-time-unit-impulse-graphical-representation",
    "href": "ss_14.html#continuous-time-unit-impulse-graphical-representation",
    "title": "Signal and Systems",
    "section": "Continuous-Time Unit Impulse (Graphical Representation)",
    "text": "Continuous-Time Unit Impulse (Graphical Representation)\nThe unit impulse \\(\\delta(t)\\) is graphically represented by an arrow at \\(t=0\\), with its height indicating the area (or strength) of the impulse.\n\n\nUnit Impulse\n\nInfinitely tall, infinitesimally narrow.\nArea = 1.\n\n \n\nScaled Impulse (\\(k\\delta(t)\\))\n\nArea = \\(k\\).\nThe height of the arrow is proportional to \\(k\\).\n\n \n\n\nSince sketching an infinitely narrow, infinitely tall function is impossible, we use a special graphical notation for the unit impulse: an arrow. The number next to the arrow, or its height, represents the area of the impulse, not its amplitude in the traditional sense. A “unit” impulse has an area of 1. A scaled impulse k*delta(t) has an area of k. This area property is crucial because it’s what determines the effect of the impulse on a system, not its theoretical “height.”"
  },
  {
    "objectID": "ss_14.html#relationship-between-deltat-and-ut-running-integral",
    "href": "ss_14.html#relationship-between-deltat-and-ut-running-integral",
    "title": "Signal and Systems",
    "section": "Relationship Between \\(\\delta(t)\\) and \\(u(t)\\) (Running Integral)",
    "text": "Relationship Between \\(\\delta(t)\\) and \\(u(t)\\) (Running Integral)\nThe continuous-time unit step is the running integral of the unit impulse:\n\\[\nu(t)=\\int_{-\\infty}^{t} \\delta(\\tau) d \\tau \\quad \\text{(Equation 1.71)}\n\\]\nThis highlights the accumulation of area.\nEquivalent Form \\[\nu(t)=\\int_{0}^{\\infty} \\delta(t-\\sigma) d \\sigma \\quad \\text{(Equation 1.75)}\n\\]\n\nSuperposition of delayed impulses."
  },
  {
    "objectID": "ss_14.html#relationship-between-deltat-and-ut-running-integral-1",
    "href": "ss_14.html#relationship-between-deltat-and-ut-running-integral-1",
    "title": "Signal and Systems",
    "section": "Relationship Between \\(\\delta(t)\\) and \\(u(t)\\) (Running Integral)",
    "text": "Relationship Between \\(\\delta(t)\\) and \\(u(t)\\) (Running Integral)\nConceptual Illustration (Integral of \\(\\delta(t)\\))\n\n\n\n\n\n\n\nThe integral relationship is the reverse of the derivative. If you integrate the continuous-time unit impulse from negative infinity up to time t, you will trace out the unit step function. The integral is zero until t crosses 0, at which point it captures the unit area of the impulse, and the integral becomes 1. This concept of integrating impulse areas is fundamental to understanding linear time-invariant systems and convolution, showing how systems accumulate the effect of an input over time."
  },
  {
    "objectID": "ss_14.html#sampling-property-of-deltat",
    "href": "ss_14.html#sampling-property-of-deltat",
    "title": "Signal and Systems",
    "section": "Sampling Property of \\(\\delta(t)\\)",
    "text": "Sampling Property of \\(\\delta(t)\\)\nSimilar to discrete-time, the continuous-time impulse has a powerful sampling property:\n\\[\nx(t) \\delta(t)=x(0) \\delta(t) \\quad \\text{(Equation 1.76)}\n\\]\nFor a delayed impulse: \\[\nx(t) \\delta\\left(t-t_{0}\\right)=x\\left(t_{0}\\right) \\delta\\left(t-t_{0}\\right)\n\\]\nThis means multiplying a function by an impulse isolates the function’s value at the impulse’s location."
  },
  {
    "objectID": "ss_14.html#sampling-property-of-deltat-1",
    "href": "ss_14.html#sampling-property-of-deltat-1",
    "title": "Signal and Systems",
    "section": "Sampling Property of \\(\\delta(t)\\)",
    "text": "Sampling Property of \\(\\delta(t)\\)\nDemonstration: x(t) = sin(t) multiplied by $\\delta(t - \\pi/2)$\n\n\n\n\n\n\n\nThe sampling property is just as vital in continuous-time as it is in discrete-time. When you multiply a well-behaved continuous signal x(t) by delta(t), the result is effectively the value of x(t) at t=0, scaled by the impulse itself. This means it’s x(0) multiplied by an impulse of unit area. If the impulse is shifted to t_0, it samples x(t_0). This property allows us to extract specific values from a continuous signal using the impulse function, which is critical in concepts like Fourier transforms and filtering."
  },
  {
    "objectID": "ss_14.html#example-1.7-derivative-of-a-discontinuous-signal",
    "href": "ss_14.html#example-1.7-derivative-of-a-discontinuous-signal",
    "title": "Signal and Systems",
    "section": "Example 1.7: Derivative of a Discontinuous Signal",
    "text": "Example 1.7: Derivative of a Discontinuous Signal\nConsider the signal \\(x(t)\\) (from Figure 1.40a). We want to find its derivative \\(\\dot{x}(t)\\).\nOriginal Signal \\(x(t)\\)\n\nPiecewise constant.\nContains jump discontinuities."
  },
  {
    "objectID": "ss_14.html#example-1.7-derivative-of-a-discontinuous-signal-1",
    "href": "ss_14.html#example-1.7-derivative-of-a-discontinuous-signal-1",
    "title": "Signal and Systems",
    "section": "Example 1.7: Derivative of a Discontinuous Signal",
    "text": "Example 1.7: Derivative of a Discontinuous Signal\nDerivative \\(\\dot{x}(t)\\)\n\nZero where \\(x(t)\\) is constant.\nImpulses at jump discontinuities.\nArea of impulse = size of jump.\n\n\n\n\n\n\n\n\nThis example beautifully demonstrates the power of the impulse function. The derivative of a constant signal is zero. However, when a signal has a ‘jump discontinuity’, its derivative at that point is not conventionally defined. Using the concept of the impulse, the derivative at a jump discontinuity becomes an impulse whose area is equal to the magnitude of the jump. For x(t): - At t=1, x(t) jumps from 0 to 2. This is a jump of +2, so x_dot(t) has an impulse of area +2 at t=1. - At t=2, x(t) jumps from 2 to -1. This is a jump of -3, so x_dot(t) has an impulse of area -3 at t=2. - At t=4, x(t) jumps from -1 to 1. This is a jump of +2, so x_dot(t) has an impulse of area +2 at t=4. Everywhere else, where the signal is flat, the derivative is zero."
  },
  {
    "objectID": "ss_14.html#example-1.7-recovering-xt-from-dotxt",
    "href": "ss_14.html#example-1.7-recovering-xt-from-dotxt",
    "title": "Signal and Systems",
    "section": "Example 1.7: Recovering \\(x(t)\\) from \\(\\dot{x}(t)\\)",
    "text": "Example 1.7: Recovering \\(x(t)\\) from \\(\\dot{x}(t)\\)\nWe can verify the derivative by integrating \\(\\dot{x}(t)\\) to recover \\(x(t)\\). \\[\nx(t)=\\int_{0}^{t} \\dot{x}(\\tau) d \\tau \\quad \\text{(Equation 1.77)}\n\\]\nEach impulse contributes its area to the running sum (integral).\nIntegral Steps\n\nFor \\(t&lt;1\\): \\(\\int_{0}^{t} \\dot{x}(\\tau) d \\tau = 0\\)\nFor \\(1&lt;t&lt;2\\): \\(\\int_{0}^{t} \\dot{x}(\\tau) d \\tau = (\\text{Area at } t=1) = 2\\)\nFor \\(2&lt;t&lt;4\\): \\(\\int_{0}^{t} \\dot{x}(\\tau) d \\tau = (\\text{Area at } t=1) + (\\text{Area at } t=2) = 2 + (-3) = -1\\)\nFor \\(t&gt;4\\): \\(\\int_{0}^{t} \\dot{x}(\\tau) d \\tau = (\\text{Area at } t=1) + (\\text{Area at } t=2) + (\\text{Area at } t=4) = 2 + (-3) + 2 = 1\\)"
  },
  {
    "objectID": "ss_14.html#example-1.7-recovering-xt-from-dotxt-1",
    "href": "ss_14.html#example-1.7-recovering-xt-from-dotxt-1",
    "title": "Signal and Systems",
    "section": "Example 1.7: Recovering \\(x(t)\\) from \\(\\dot{x}(t)\\)",
    "text": "Example 1.7: Recovering \\(x(t)\\) from \\(\\dot{x}(t)\\)\nInteractive Verification (Conceptual)\n\n\n\n\n\n\n\nTo confirm our derivative, we integrate x_dot(t). The integral accumulates the areas of the impulses. - For t less than 1, the integral is 0. - As t crosses 1, the integral captures the area of the impulse at t=1, which is +2. So, x(t) becomes 2. - As t crosses 2, the integral adds the area of the impulse at t=2, which is -3, resulting in 2 - 3 = -1. So, x(t) becomes -1. - Finally, as t crosses 4, the integral adds the area of the impulse at t=4, which is +2, resulting in -1 + 2 = +1. So, x(t) becomes 1. This exactly reconstructs the original signal x(t), demonstrating the complementary nature of differentiation and integration with respect to impulse functions."
  },
  {
    "objectID": "ss_14.html#key-takeaways",
    "href": "ss_14.html#key-takeaways",
    "title": "Signal and Systems",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nUnit Impulse\n\nDiscrete-Time (\\(\\delta[n]\\)): Non-zero only at \\(n=0\\) (value 1).\nContinuous-Time (\\(\\delta(t)\\)): Infinitely tall, infinitesimally narrow pulse with unit area. An idealization.\n\nUnit Step\n\nDiscrete-Time (\\(u[n]\\)): 0 for \\(n&lt;0\\), 1 for \\(n \\ge 0\\).\nContinuous-Time (\\(u(t)\\)): 0 for \\(t&lt;0\\), 1 for \\(t &gt; 0\\).\n\nRelationships\n\n\\(\\delta[n] = u[n] - u[n-1]\\)\n\\(u[n] = \\sum_{m=-\\infty}^{n} \\delta[m]\\)\n\\(\\delta(t) = \\frac{du(t)}{dt}\\) ; \\(u(t) = \\int_{-\\infty}^{t} \\delta(\\tau) d\\tau\\)"
  },
  {
    "objectID": "ss_14.html#key-takeaways-1",
    "href": "ss_14.html#key-takeaways-1",
    "title": "Signal and Systems",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nSampling Property\n\n\\(x[n]\\delta[n-n_0] = x[n_0]\\delta[n-n_0]\\)\n\\(x(t)\\delta(t-t_0) = x(t_0)\\delta(t-t_0)\\)\n\nThese functions are fundamental for signal representation and system analysis.\n\nTo summarize, the unit impulse and unit step functions are the bedrock of signals and systems analysis. They allow us to compactly represent abrupt changes, instantaneous events, and serve as basic building blocks for more complex signals. Their interconnected nature, represented by the difference/sum and derivative/integral relationships, is crucial. But perhaps most importantly, remember their sampling property, which allows us to extract signal values at specific points, a concept vital for future topics. Mastering these basic signals will greatly aid your understanding of system responses and signal processing."
  },
  {
    "objectID": "ss_12.html#introduction-why-transformations",
    "href": "ss_12.html#introduction-why-transformations",
    "title": "Signal and Systems",
    "section": "Introduction: Why Transformations?",
    "text": "Introduction: Why Transformations?\nA central concept in signal and system analysis is the transformation of a signal.\n\nAircraft Control Systems:\n\nPilot actions (signals) are transformed by electrical and mechanical systems.\nResulting in changes to thrust, control surface positions, aircraft velocity, and heading.\n\nHigh-Fidelity Audio Systems:\n\nInput signal (music) is modified.\nTo enhance desirable characteristics, remove noise, or balance components (e.g., treble & bass).\n\n\nThese transformations allow us to introduce basic properties of signals and systems, playing a crucial role in their definition and characterization.\n\nThe core idea is that signals rarely exist in isolation; they are continuously modified, processed, and affected by the systems they pass through. Understanding these transformations, especially simple ones involving the time axis, sets the foundation for more complex system analysis.\nFor instance, in aerospace, a pilot’s joystick movement (a signal) isn’t directly translated into rudder movement. It goes through sensors, analog-to-digital converters, flight control computers (performing complex transformations), digital-to-analog converters, and actuators before physically moving the rudder. Each stage is a transformation."
  },
  {
    "objectID": "ss_12.html#time-shift-continuous-time-signals",
    "href": "ss_12.html#time-shift-continuous-time-signals",
    "title": "Signal and Systems",
    "section": "Time Shift: Continuous-Time Signals",
    "text": "Time Shift: Continuous-Time Signals\nA fundamental transformation where the signal’s shape remains the same, but its position on the time t-axis changes.\n\n\n\nDefinition: \\(x(t-t_0)\\)\n\nDelayed version of \\(x(t)\\) if \\(t_0 &gt; 0\\).\nAdvanced version of \\(x(t)\\) if \\(t_0 &lt; 0\\).\n\n\n\n\n\nApplications: - Radar, sonar, and seismic signal processing: Signals arriving at different receivers at different times due to propagation delays.\n\nThink of \\(x(t)\\) as an original event. If you want some value of \\(x(t)\\) that occurred at time \\(t_{event}\\) to now appear at \\(t_{new}\\) in the transformed signal \\(x(t-t_0)\\), then \\((t_{new} - t_0) = t_{event}\\), which means \\(t_{new} = t_{event} + t_0\\). - If \\(t_0 &gt; 0\\), \\(t_{new}\\) is later than \\(t_{event}\\), hence it’s a delay. - If \\(t_0 &lt; 0\\), \\(t_{new}\\) is earlier than \\(t_{event}\\), hence it’s an advance. The provided figure 1.9 shows \\(t_0 &lt; 0\\), meaning \\(x(t-t_0)\\) is an advanced version of \\(x(t)\\)."
  },
  {
    "objectID": "ss_12.html#time-shift-discrete-time-signals",
    "href": "ss_12.html#time-shift-discrete-time-signals",
    "title": "Signal and Systems",
    "section": "Time Shift: Discrete-Time Signals",
    "text": "Time Shift: Discrete-Time Signals\nAnalogous to continuous-time, but for discrete samples.\n\n\n\nDefinition: \\(x[n-n_0]\\)\n\nThe signal \\(x[n]\\) is shifted by \\(n_0\\) samples.\nIf \\(n_0 &gt; 0\\), it’s a delay.\nIf \\(n_0 &lt; 0\\), it’s an advance.\n\n\n\n\n\nIn this figure, \\(n_0 &gt; 0\\), so \\(x[n-n_0]\\) is a delayed version of \\(x[n]\\). Each point in \\(x[n]\\) occurs later in \\(x[n-n_0]\\).\n\nDiscrete-time shifts are fundamental in digital signal processing, for example, in implementing delay lines, echo effects, or buffering data in communications systems. The variable n represents the sample index, typically integers."
  },
  {
    "objectID": "ss_12.html#time-reversal-reflection",
    "href": "ss_12.html#time-reversal-reflection",
    "title": "Signal and Systems",
    "section": "Time Reversal (Reflection)",
    "text": "Time Reversal (Reflection)\nThis transformation reflects the signal about the origin of the independent variable (\\(t=0\\) or \\(n=0\\)).\nAnalogy: If \\(x(t)\\) is an audio recording, then \\(x(-t)\\) is that recording played backward."
  },
  {
    "objectID": "ss_12.html#time-reversal-reflection-1",
    "href": "ss_12.html#time-reversal-reflection-1",
    "title": "Signal and Systems",
    "section": "Time Reversal (Reflection)",
    "text": "Time Reversal (Reflection)\n\n\nContinuous Time:\n\nDefinition: \\(x(-t)\\)\nObtained by reflection about \\(t=0\\).\n\n\n\n\\(x(t)\\)\n\n\n\n\\(x(-t)\\)\n\n\nDiscrete Time:\n\nDefinition: \\(x[-n]\\)\nObtained by reflection about \\(n=0\\).\n\n\n\n\\(x[n]\\)\n\n\n\n\\(x[-n]\\)\n\n\n\nTime reversal is key for understanding concepts like system causality, stability, and in operations like convolution, where one signal is often reversed. Visually, imagine folding the graph paper along the vertical axis at \\(t=0\\) or \\(n=0\\)."
  },
  {
    "objectID": "ss_12.html#time-scaling",
    "href": "ss_12.html#time-scaling",
    "title": "Signal and Systems",
    "section": "Time Scaling",
    "text": "Time Scaling\nThis transformation compresses or stretches the signal along the time axis.\n\nDefinition: \\(x(\\alpha t)\\) (or \\(x[\\alpha n]\\))\n\nLinearly Compressed if \\(|\\alpha| &gt; 1\\) (e.g., \\(x(2t)\\)). The signal plays faster.\nLinearly Stretched if \\(|\\alpha| &lt; 1\\) (e.g., \\(x(t/2)\\)). The signal plays slower.\nIf \\(\\alpha &lt; 0\\), it also involves a time reversal.\n\n\nAnalogy: Playing an audio recording at twice the speed (\\(x(2t)\\)) or half the speed (\\(x(t/2)\\))."
  },
  {
    "objectID": "ss_12.html#time-scaling-1",
    "href": "ss_12.html#time-scaling-1",
    "title": "Signal and Systems",
    "section": "Time Scaling",
    "text": "Time Scaling\n\n(Top: \\(x(t)\\); Middle: \\(x(2t)\\); Bottom: \\(x(t/2)\\))\n\nFor \\(x(\\alpha t)\\), the value of \\(x(t)\\) at \\(t=t_0\\) will appear in \\(x(\\alpha t)\\) at \\(t = t_0/\\alpha\\). - If \\(|\\alpha| &gt; 1\\), then \\(|t_0/\\alpha| &lt; |t_0|\\), meaning the signal duration is compressed. - If \\(|\\alpha| &lt; 1\\), then \\(|t_0/\\alpha| &gt; |t_0|\\), meaning the signal duration is stretched. This is a critical transformation in frequency analysis (e.g., Fourier Transform), as time scaling in one domain corresponds to frequency scaling in the other."
  },
  {
    "objectID": "ss_12.html#combined-transformations-xalpha-t-beta",
    "href": "ss_12.html#combined-transformations-xalpha-t-beta",
    "title": "Signal and Systems",
    "section": "Combined Transformations: \\(x(\\alpha t + \\beta)\\)",
    "text": "Combined Transformations: \\(x(\\alpha t + \\beta)\\)\nSuch transformations preserve the shape of \\(x(t)\\), but the resulting signal may be:\n\nLinearly stretched if \\(|\\alpha|&lt;1\\)\n\nLinearly compressed if \\(|\\alpha|&gt;1\\)\n\nReversed in time if \\(\\alpha&lt;0\\)\n\nShifted in time if \\(\\beta\\) is nonzero"
  },
  {
    "objectID": "ss_12.html#combined-transformations-xalpha-t-beta-1",
    "href": "ss_12.html#combined-transformations-xalpha-t-beta-1",
    "title": "Signal and Systems",
    "section": "Combined Transformations: \\(x(\\alpha t + \\beta)\\)",
    "text": "Combined Transformations: \\(x(\\alpha t + \\beta)\\)\nSystematic Approach to \\(x(\\alpha t + \\beta)\\):\n\nShift: First, delay or advance \\(x(t)\\) in accordance with the value of \\(\\beta\\). This gives an intermediate signal, e.g., \\(y(t) = x(t+\\beta)\\).\nScale/Reverse: Then, perform time scaling and/or time reversal on the resulting signal \\(y(t)\\) in accordance with the value of \\(\\alpha\\). This means replacing \\(t\\) with \\(\\alpha t\\) in \\(y(t)\\), resulting in \\(y(\\alpha t) = x(\\alpha t + \\beta)\\).\n\n\nIt’s critical to perform these operations in the correct order. The method described (shift first, then scale/reverse) is generally intuitive and widely used. An alternative is to factor out \\(\\alpha\\): \\(x(\\alpha(t + \\beta/\\alpha))\\). This implies a shift by \\(-\\beta/\\alpha\\) after scaling, which can be confusing. Sticking to the text’s method: shift \\(x(t)\\) by \\(\\beta\\) to get \\(x(t+\\beta)\\), then substitute \\(t \\rightarrow \\alpha t\\) to get \\(x(\\alpha t + \\beta)\\)."
  },
  {
    "objectID": "ss_12.html#example-1.1-shift-and-reversal",
    "href": "ss_12.html#example-1.1-shift-and-reversal",
    "title": "Signal and Systems",
    "section": "Example 1.1: Shift and Reversal",
    "text": "Example 1.1: Shift and Reversal\nGiven \\(x(t)\\) in Figure (a), let’s find \\(x(t+1)\\) and \\(x(-t+1)\\).\n\n\na. Original Signal: \\(x(t)\\)\nb. Transformation 1: \\(x(t+1)\\)\n\nThis corresponds to an advance (shift to the left) by one unit along the \\(t\\)-axis.\n\nc. Transformation 2: \\(x(-t+1)\\)\n\nThis signal is the time-reversed version of \\(x(t+1)\\).\nIt is obtained graphically by reflecting \\(x(t+1)\\) about the \\(t\\)-axis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWalk through the points: For \\(x(t+1)\\): - The point at \\(t=0\\) in \\(x(t)\\) moves to \\(t=-1\\) in \\(x(t+1)\\). - The point at \\(t=1\\) in \\(x(t)\\) moves to \\(t=0\\) in \\(x(t+1)\\). - The point at \\(t=2\\) in \\(x(t)\\) moves to \\(t=1\\) in \\(x(t+1)\\). This confirms the shift to the left (advance).\nFor \\(x(-t+1)\\): - Take the signal \\(x(t+1)\\) (from Figure b). - Reflect it about the vertical axis (\\(t=0\\)). - The point at \\(t=-1\\) in \\(x(t+1)\\) moves to \\(t=1\\) in \\(x(-t+1)\\). - The point at \\(t=0\\) in \\(x(t+1)\\) stays at \\(t=0\\) in \\(x(-t+1)\\). - The point at \\(t=1\\) in \\(x(t+1)\\) moves to \\(t=-1\\) in \\(x(-t+1)\\). This precisely matches Figure (c)."
  },
  {
    "objectID": "ss_12.html#example-1.2-time-scaling",
    "href": "ss_12.html#example-1.2-time-scaling",
    "title": "Signal and Systems",
    "section": "Example 1.2: Time Scaling",
    "text": "Example 1.2: Time Scaling\nGiven \\(x(t)\\) in Figure (a), let’s find \\(x(\\frac{3}{2} t)\\).\n\n\na. Original Signal: \\(x(t)\\)\nd. Transformation: \\(x(\\frac{3}{2} t)\\)\n\nThis corresponds to a linear compression of \\(x(t)\\) by a factor of \\(\\frac{2}{3}\\). Since \\(|\\alpha| = \\frac{3}{2} &gt; 1\\).\nThe value of \\(x(t)\\) at \\(t=t_0\\) occurs in \\(x(\\frac{3}{2} t)\\) at \\(t=\\frac{2}{3} t_0\\). E.g., \\(x(1)\\) is found in \\(x(\\frac{3}{2} t)\\) at \\(t=\\frac{2}{3}\\).\nSince \\(x(t)\\) is zero for \\(t&lt;0\\), \\(x(\\frac{3}{2} t)\\) is zero for \\(t&lt;0\\).\nSince \\(x(t)\\) is zero for \\(t&gt;2\\), \\(x(\\frac{3}{2} t)\\) is zero for \\(t &gt; \\frac{4}{3}\\).\n\n\n\n\n\n\n\n\n\n\n\n\nExplain intuitively what “compression by a factor of 2/3” means. It means the signal “finishes” in 2/3 of the original time. - Original signal \\(x(t)\\) starts at \\(t=0\\) and ends at \\(t=2\\). - The transformed signal \\(x(\\frac{3}{2} t)\\) starts when \\(\\frac{3}{2} t = 0 \\Rightarrow t=0\\). - It ends when \\(\\frac{3}{2} t = 2 \\Rightarrow t = 2 \\times \\frac{2}{3} = \\frac{4}{3}\\). The duration of the signal has indeed been compressed from \\(2\\) to \\(4/3\\)."
  },
  {
    "objectID": "ss_12.html#example-1.3-combined-shift-scale",
    "href": "ss_12.html#example-1.3-combined-shift-scale",
    "title": "Signal and Systems",
    "section": "Example 1.3: Combined Shift & Scale",
    "text": "Example 1.3: Combined Shift & Scale\nSuppose we want to determine \\(x(\\frac{3}{2} t+1)\\) for the signal \\(x(t)\\) from Figure (a).\n\nShift based on \\(\\beta=1\\):\n\nFirst, advance (shift to the left) \\(x(t)\\) by 1.\nThis yields the signal \\(x(t+1)\\), shown in Figure (b), which we analyzed in Example 1.1.\n\n\n\n\\(x(t+1)\\)"
  },
  {
    "objectID": "ss_12.html#example-1.3-combined-shift-scale-1",
    "href": "ss_12.html#example-1.3-combined-shift-scale-1",
    "title": "Signal and Systems",
    "section": "Example 1.3: Combined Shift & Scale",
    "text": "Example 1.3: Combined Shift & Scale\n\nScale based on \\(\\alpha=\\frac{3}{2}\\):\n\nNow, take \\(x(t+1)\\) and substitute \\(t \\rightarrow \\frac{3}{2}t\\) to get \\(x(\\frac{3}{2}t+1)\\).\nThis linearly compresses the signal \\(x(t+1)\\) by a factor of \\(\\frac{2}{3}\\).\nThe signal \\(x(t+1)\\) exists for \\(t \\in [-1, 1]\\).\nSo \\(x(\\frac{3}{2}t+1)\\) exists for \\(\\frac{3}{2}t \\in [-1, 1] \\implies t \\in [-\\frac{2}{3}, \\frac{2}{3}]\\).\n\n\n\n\\(x(\\frac{3}{2} t+1)\\)\n\n\n\nReinforce the general rule for \\(x(\\alpha t + \\beta)\\): 1. Apply the shift related to \\(\\beta\\) to the original signal’s time variable. So, \\(x(t) \\rightarrow x(t+\\beta)\\). 2. Apply the scaling/reversal related to \\(\\alpha\\) to the new time variable of the shifted signal. So, \\(x(t+\\beta) \\rightarrow x(\\alpha t + \\beta)\\).\nThis order (shift then scale) is common and easy to visualize. Alternative (factor \\(\\alpha\\) first): \\(x(\\alpha(t+\\beta/\\alpha))\\). This would imply scaling \\(x(t)\\) to \\(x(\\alpha t)\\), then shifting \\(x(\\alpha t)\\) by \\(-\\beta/\\alpha\\). Both methods yield the same result but require careful attention to the order and interpretation of the shift parameter."
  },
  {
    "objectID": "ss_12.html#interactive-demo-signal-transformations",
    "href": "ss_12.html#interactive-demo-signal-transformations",
    "title": "Signal and Systems",
    "section": "Interactive Demo: Signal Transformations",
    "text": "Interactive Demo: Signal Transformations\nExplore how time shifting, scaling, and reversal affect a signal.\nLet’s use a simple triangular pulse signal.\n\n\n\n\n\n\n\nExperiment with the parameters: - beta: Changes the horizontal position (shift). Positive beta means shifting left (advance), negative beta means shifting right (delay). - alpha: - Values greater than 1: Compression. - Values between 0 and 1: Stretching. - Negative values: Reversal combined with compression/stretching. - Try alpha = -1 and beta = 0 to see pure time reversal. - Try alpha = 0.5 and beta = 0 to see stretching. - Try alpha = 2 and beta = 0 to see compression."
  },
  {
    "objectID": "ss_12.html#periodic-signals-continuous-time",
    "href": "ss_12.html#periodic-signals-continuous-time",
    "title": "Signal and Systems",
    "section": "Periodic Signals: Continuous Time",
    "text": "Periodic Signals: Continuous Time\nAn important class of signals that repeat themselves over time.\n\nDefinition: A continuous-time signal \\(x(t)\\) is periodic if there is a positive value of \\(T\\) for which: \\[\nx(t)=x(t+T) \\quad \\text{for all } t \\tag{1.11}\n\\] We say \\(x(t)\\) is periodic with period \\(T\\)."
  },
  {
    "objectID": "ss_12.html#periodic-signals-continuous-time-1",
    "href": "ss_12.html#periodic-signals-continuous-time-1",
    "title": "Signal and Systems",
    "section": "Periodic Signals: Continuous Time",
    "text": "Periodic Signals: Continuous Time\n\nFundamental Period (\\(T_0\\)): The smallest positive value of \\(T\\) for which \\(x(t)=x(t+T)\\) holds.\n\nException: For a constant signal, the fundamental period is undefined, as it is periodic for any \\(T\\).\n\nAperiodic Signal: A signal that is not periodic.\n\nApplications: Natural responses of conserved energy systems (e.g., ideal LC circuits, frictionless mechanical systems) are often periodic.\n\nEmphasize “for all values of \\(t\\)” in the definition. This means the entire signal must repeat. If a signal repeats but has a unique feature (like a discontinuity) that doesn’t repeat, it’s not periodic. This differentiates from signals that might look “locally” periodic."
  },
  {
    "objectID": "ss_12.html#periodic-signals-discrete-time",
    "href": "ss_12.html#periodic-signals-discrete-time",
    "title": "Signal and Systems",
    "section": "Periodic Signals: Discrete Time",
    "text": "Periodic Signals: Discrete Time\nSimilar to continuous-time, but defined for discrete samples.\n\nDefinition: A discrete-time signal \\(x[n]\\) is periodic with period \\(N\\), where \\(N\\) is a positive integer, if: \\[\nx[n]=x[n+N] \\quad \\text{for all } n \\tag{1.12}\n\\] If this holds, \\(x[n]\\) is periodic with period \\(N\\)."
  },
  {
    "objectID": "ss_12.html#periodic-signals-discrete-time-1",
    "href": "ss_12.html#periodic-signals-discrete-time-1",
    "title": "Signal and Systems",
    "section": "Periodic Signals: Discrete Time",
    "text": "Periodic Signals: Discrete Time\n\nFundamental Period (\\(N_0\\)): The smallest positive integer value of \\(N\\) for which \\(x[n]=x[n+N]\\) holds.\n\n\nThe key difference for discrete-time is that the period \\(N\\) must be an integer. This has implications when dealing with discrete-time sinusoids, where their periodicity depends on the ratio of their frequency to \\(2\\pi\\) being a rational number."
  },
  {
    "objectID": "ss_12.html#example-1.4-checking-periodicity",
    "href": "ss_12.html#example-1.4-checking-periodicity",
    "title": "Signal and Systems",
    "section": "Example 1.4: Checking Periodicity",
    "text": "Example 1.4: Checking Periodicity\nConsider the signal given by: \\[\nx(t)=\\left\\{\\begin{array}{ll}\n\\cos (t) & \\text { if } t&lt;0 \\\\\n\\sin (t) & \\text { if } t \\geq 0\n\\end{array}\\right.\n\\]"
  },
  {
    "objectID": "ss_12.html#example-1.4-checking-periodicity-1",
    "href": "ss_12.html#example-1.4-checking-periodicity-1",
    "title": "Signal and Systems",
    "section": "Example 1.4: Checking Periodicity",
    "text": "Example 1.4: Checking Periodicity\nAnalysis:\n\nWe know \\(\\cos(t+2\\pi)=\\cos(t)\\) and \\(\\sin(t+2\\pi)=\\sin(t)\\).\nIndividually, both cosine (for \\(t&lt;0\\)) and sine (for \\(t \\ge 0\\)) parts are periodic with period \\(2\\pi\\).\nHowever, observe the discontinuity at \\(t=0\\).\nFor \\(x(t)\\) to be periodic, every feature in its shape must recur periodically.\nThe discontinuity at \\(t=0\\) does not recur at \\(t=2\\pi, 4\\pi, \\ldots\\) or \\(t=-2\\pi, -4\\pi, \\ldots\\).\n\nConclusion: The signal \\(x(t)\\) is not periodic.\n\nThis example highlights a common mistake. Just because components of a signal are periodic doesn’t mean the composite signal is. The definition \\(x(t)=x(t+T)\\) for all t is strict. The jump at \\(t=0\\) is unique. For \\(x(t)\\) to be periodic with period \\(2\\pi\\), we would need \\(x(0) = x(2\\pi)\\). \\(x(0) = \\sin(0) = 0\\). But for \\(t&lt;0\\), values approaching \\(0\\) from the left are \\(\\cos(t)\\), so \\(\\lim_{t \\to 0^-} x(t) = \\cos(0) = 1\\). And for \\(t \\ge 0\\), values approaching \\(0\\) from the right are \\(\\sin(t)\\), so \\(\\lim_{t \\to 0^+} x(t) = \\sin(0) = 0\\). The mismatch at \\(t=0\\) (specifically, \\(x(0)=0\\) but immediately to its left it’s 1) must repeat if it were periodic. But it clearly doesn’t."
  },
  {
    "objectID": "ss_12.html#interactive-demo-constructing-periodic-signals",
    "href": "ss_12.html#interactive-demo-constructing-periodic-signals",
    "title": "Signal and Systems",
    "section": "Interactive Demo: Constructing Periodic Signals",
    "text": "Interactive Demo: Constructing Periodic Signals\nLet’s visualize how periodicity works for continuous and discrete-time signals.\n\n\n\n\n\n\n\nContinuous-Time Demo: - Observe how changing \\(T_0\\) stretches or compresses the repeating pattern. - The dashed red lines indicate the start of each period.\nDiscrete-Time Demo: - Notice that \\(N_0\\) must be an integer. - See how the sequence repeats after every \\(N_0\\) samples. - If you were to set \\(N_0\\) to a value that is not a multiple of the base signal’s inherent periodicity (e.g., the base_signal has internal period 4), you’ll see how only the first \\(N_0\\) samples are truly repeated. For proper fundamental period, \\(N_0\\) should be the smallest integer for which the specific signal repeats. In the stem plot, the red dashed lines show the declared \\(N_0\\)."
  },
  {
    "objectID": "ss_12.html#even-and-odd-signals",
    "href": "ss_12.html#even-and-odd-signals",
    "title": "Signal and Systems",
    "section": "Even and Odd Signals",
    "text": "Even and Odd Signals\nSignals can exhibit symmetry under time reversal.\n\n\n\nEven Signal: Identical to its time-reversed counterpart.\n\nContinuous Time: \\[\nx(-t)=x(t) \\tag{1.14}\n\\]\nDiscrete Time: \\[\nx[-n]=x[n] \\tag{1.15}\n\\]\n\n\n\n\n\nEven continuous-time signal"
  },
  {
    "objectID": "ss_12.html#even-and-odd-signals-1",
    "href": "ss_12.html#even-and-odd-signals-1",
    "title": "Signal and Systems",
    "section": "Even and Odd Signals",
    "text": "Even and Odd Signals\n\n\n\nOdd Signal: The negative of its time-reversed counterpart.\n\nContinuous Time: \\[\nx(-t)=-x(t) \\tag{1.16}\n\\]\nDiscrete Time: \\[\nx[-n]=-x[n] \\tag{1.17}\n\\]\nProperty: An odd signal must necessarily be \\(0\\) at \\(t=0\\) or \\(n=0\\) (since \\(x(0) = -x(0) \\implies 2x(0) = 0 \\implies x(0)=0\\)).\n\n\n\n\n\nOdd continuous-time signal\n\n\n\nVisualize even signals as being symmetrical around the vertical axis (like a mirror image). Cosine functions are classic examples of even signals. Visualize odd signals as being symmetrical with respect to the origin (rotate 180 degrees). Sine functions are classic examples of odd signals. Emphasize why \\(x(0)\\) must be zero for an odd signal. It’s a key point."
  },
  {
    "objectID": "ss_12.html#even-odd-decomposition",
    "href": "ss_12.html#even-odd-decomposition",
    "title": "Signal and Systems",
    "section": "Even-Odd Decomposition",
    "text": "Even-Odd Decomposition\nAny signal can be uniquely broken down into a sum of an even signal and an odd signal.\nFor a continuous-time signal \\(x(t)\\):\n\nEven Part: \\(\\mathcal{E} v\\{x(t)\\}\\) \\[\n\\mathcal{E} v\\{x(t)\\}=\\frac{1}{2}[x(t)+x(-t)] \\tag{1.18}\n\\]\nOdd Part: \\(\\mathcal{O} d\\{x(t)\\}\\) \\[\n\\mathcal{O} d\\{x(t)\\}=\\frac{1}{2}[x(t)-x(-t)] \\tag{1.19}\n\\]\nDecomposition: \\(x(t) = \\mathcal{E} v\\{x(t)\\} + \\mathcal{O} d\\{x(t)\\}\\)"
  },
  {
    "objectID": "ss_12.html#even-odd-decomposition-1",
    "href": "ss_12.html#even-odd-decomposition-1",
    "title": "Signal and Systems",
    "section": "Even-Odd Decomposition",
    "text": "Even-Odd Decomposition\nThese definitions hold analogously for discrete-time signals \\(x[n]\\).\n\n(Example of discrete-time decomposition)\n\nExplain why these definitions work: - To show \\(\\mathcal{E} v\\{x(t)\\}\\) is even: Replace \\(t\\) with \\(-t\\) in \\(\\mathcal{E} v\\{x(t)\\}\\). You get \\(\\frac{1}{2}[x(-t)+x(t)] = \\mathcal{E} v\\{x(t)\\}\\). - To show \\(\\mathcal{O} d\\{x(t)\\}\\) is odd: Replace \\(t\\) with \\(-t\\) in \\(\\mathcal{O} d\\{x(t)\\}\\). You get \\(\\frac{1}{2}[x(-t)-x(t)] = -\\frac{1}{2}[x(t)-x(-t)] = -\\mathcal{O} d\\{x(t)\\}\\). - To show \\(x(t)\\) is the sum: Add the two equations: \\(\\frac{1}{2}[x(t)+x(-t)] + \\frac{1}{2}[x(t)-x(-t)] = \\frac{1}{2}x(t) + \\frac{1}{2}x(-t) + \\frac{1}{2}x(t) - \\frac{1}{2}x(-t) = x(t)\\).\nThis decomposition is incredibly useful in signal processing, simplifying analysis of complex signals by breaking them into symmetric components. E.g., for Fourier Series, even functions only have cosine terms; odd functions only have sine terms."
  },
  {
    "objectID": "ss_12.html#interactive-demo-even-odd-parts",
    "href": "ss_12.html#interactive-demo-even-odd-parts",
    "title": "Signal and Systems",
    "section": "Interactive Demo: Even & Odd Parts",
    "text": "Interactive Demo: Even & Odd Parts\nVisualize the even and odd decomposition of a signal.\n\n\n\n\n\n\n\nExperiment with Signal Parameters: - Decay Rate: Changes how quickly the exponential component decays. - Offset Strength: Introduces a constant offset, which heavily influences the even part. - Observe that the Even Part is symmetrical about the y-axis, and the Odd Part is symmetrical about the origin (and is zero at \\(t=0\\)). - Mentally (or by adding up the plots), verify that Ev{x(t)} + Od{x(t)} equals the Original Signal x(t)."
  },
  {
    "objectID": "ss_12.html#summary",
    "href": "ss_12.html#summary",
    "title": "Signal and Systems",
    "section": "Summary",
    "text": "Summary\nFundamental transformations of the independent variable (time) for signals.\n\nTime Shift (\\(x(t \\pm t_0)\\) or \\(x[n \\pm n_0]\\)): Delays or advances a signal. Crucial for understanding propagation delays.\nTime Reversal (\\(x(-t)\\) or \\(x[-n]\\)): Reflects a signal about the origin. Important for symmetry concepts.\nTime Scaling (\\(x(\\alpha t)\\) or \\(x[\\alpha n]\\)): Compresses or stretches a signal. Linked to bandwidth in frequency domain.\nCombined Transformations (\\(x(\\alpha t + \\beta)\\)): Requires a systematic order (shift then scale/reverse)."
  },
  {
    "objectID": "ss_12.html#summary-1",
    "href": "ss_12.html#summary-1",
    "title": "Signal and Systems",
    "section": "Summary",
    "text": "Summary\n\nPeriodic Signals (\\(x(t) = x(t+T)\\) or \\(x[n] = x[n+N]\\)): Signals that repeat themselves, characterized by a fundamental period.\nEven and Odd Signals: Signals exhibiting specific symmetry under time reversal, and any signal can be uniquely decomposed into its even and odd parts.\n\n\nReinforce the “why” these transformations matter. They are not just mathematical curiosities. They represent real-world physical changes to signals as they are processed, transmitted, or interact with systems. They also form the basis for later analytical tools, such as the Convolution Integral/Sum and Fourier Transforms."
  },
  {
    "objectID": "rps.html#dosen-pengajar",
    "href": "rps.html#dosen-pengajar",
    "title": "Sinyal dan Sistem",
    "section": "Dosen Pengajar",
    "text": "Dosen Pengajar\n\nImron Rosyadi (kelas A)\nAgung Mubyarto (kelas B-C)\nM. Syaiful Aliim (kelas B-C)\nDinda Wahyu (kelas D)"
  },
  {
    "objectID": "rps.html#jadwal-kelas-google-meet",
    "href": "rps.html#jadwal-kelas-google-meet",
    "title": "Sinyal dan Sistem",
    "section": "Jadwal, Kelas, Google Meet",
    "text": "Jadwal, Kelas, Google Meet\n\nMK Sinyal dan Sistem - Kelas A\nLecturer: Imron Rosyadi\nSchedule: Monday, 09:45 – 12:25\nRoom: GEDUNG TEKNIK C 105\nGoogle Meet: meet.google.com/bat-crca-xua"
  },
  {
    "objectID": "rps.html#link-penting",
    "href": "rps.html#link-penting",
    "title": "Sinyal dan Sistem",
    "section": "Link Penting",
    "text": "Link Penting\n\nCourse Page: imron-course.vercel.app\nPresentation: imron-slide.vercel.app\nDiscussion: discord.gg/6nNTvfmz, expired at 29 Aug 2025\nAttendance: teraversa.unsoed.ac.id\nOnline Learning: eldiru.unsoed.ac.id"
  },
  {
    "objectID": "rps.html#metode-pembelajaran",
    "href": "rps.html#metode-pembelajaran",
    "title": "Sinyal dan Sistem",
    "section": "Metode Pembelajaran",
    "text": "Metode Pembelajaran\n\nEnglish-Indonesia: Salindia presentasi dalam bahasa Inggris, tetapi penyampaian menggunakan bahasa Indonesia.\nLuring-Daring: Pembelajaran dilakukan dengan tatap muka luring, tetapi diselingi secara daring. Cek Discord untuk informasi.\nVisualisasi-Interaktif: Pembelajaran akan menyediakan sumberdaya berupa visualisasi interaktif untuk memudahkan pemahaman. Cek berbagai link yang disediakan.\nCase-based Approach: Pembelajaran akan dikaitkan dengan topik bidang Electrical and Computer Engineering\nDiskusi-Informasi via Discord: Diskusikan materi, ajukan pertanyaan, langsung via Discord (channel #aljabar-linear-1).\nWeb-based Presentation: Salindia presentasi berbentuk laman web, tetapi anda bisa mengekspornya ke PDF.\nReview via Recording: Sesi pembelajaran akan (diusahakan untuk) direkam dan diunggah ke Youtube\nRileks: Pembelajaran dilakukan secara rileks."
  },
  {
    "objectID": "rps.html#referensi",
    "href": "rps.html#referensi",
    "title": "Sinyal dan Sistem",
    "section": "Referensi",
    "text": "Referensi\n\nReferensi Utama: Oppenheim, A. V., Willsky, A. S., & Hamid, S. (1996). Signals and Systems. Prentice Hall.\nReferensi Pendukung: Haykin, S., Van Veen, B. (1999). Signals and Systems. John Wiley & Sons."
  },
  {
    "objectID": "rps.html#capaian-pembelajaran-lulusan-cpl",
    "href": "rps.html#capaian-pembelajaran-lulusan-cpl",
    "title": "Sinyal dan Sistem",
    "section": "Capaian Pembelajaran Lulusan (CPL)",
    "text": "Capaian Pembelajaran Lulusan (CPL)\nCapaian Pembelajaran Lulusan (CPL) yang Dibebankan:\nMenguasai matematika, fisika, kimia dan statistik, teknologi informasi dan rekayasa sebagai landasan penerapan di bidang teknik elektro."
  },
  {
    "objectID": "rps.html#capaian-pembelajaran-mata-kuliah-cpmk-sinyal-dan-sistem",
    "href": "rps.html#capaian-pembelajaran-mata-kuliah-cpmk-sinyal-dan-sistem",
    "title": "Sinyal dan Sistem",
    "section": "Capaian Pembelajaran Mata Kuliah (CPMK) Sinyal dan Sistem",
    "text": "Capaian Pembelajaran Mata Kuliah (CPMK) Sinyal dan Sistem\n\nCPMK 1: Analisis Sinyal dan Sistem Linier Invarian Waktu (LTI) di Domain Waktu\nCPMK 2: Analisis Sinyal dan Sistem di Domain Frekuensi (Waktu-kontinu)\nCPMK 3: Analisis Sinyal dan Sistem di Domain Waktu-Diskret dan Transisinya"
  },
  {
    "objectID": "rps.html#cpmk-1-analisis-sinyal-dan-sistem-linier-invarian-waktu-lti-di-domain-waktu",
    "href": "rps.html#cpmk-1-analisis-sinyal-dan-sistem-linier-invarian-waktu-lti-di-domain-waktu",
    "title": "Sinyal dan Sistem",
    "section": "CPMK 1: Analisis Sinyal dan Sistem Linier Invarian Waktu (LTI) di Domain Waktu",
    "text": "CPMK 1: Analisis Sinyal dan Sistem Linier Invarian Waktu (LTI) di Domain Waktu\nDeskripsi:\nCPMK ini berfokus pada penguasaan konsep dasar sinyal dan sistem serta analisis fundamental sistem LTI di domain waktu.\nRumusan CPMK 1:\nMahasiswa mampu menganalisis sinyal dan sistem dasar serta mengkarakterisasi sistem Linier Invarian Waktu (LTI) baik dalam domain waktu-kontinu maupun waktu-diskret menggunakan operasi konvolusi.\nKeterkaitan dengan CPL:\nCPMK ini mendukung CPL dengan membangun pemahaman fundamental matematika rekayasa (konvolusi, persamaan diferensial/beda) untuk merepresentasikan dan menganalisis sinyal dan sistem fisis sebagai landasan rekayasa di bidang teknik elektro."
  },
  {
    "objectID": "rps.html#cpmk-1-analisis-sinyal-dan-sistem-linier-invarian-waktu-lti-di-domain-waktu-1",
    "href": "rps.html#cpmk-1-analisis-sinyal-dan-sistem-linier-invarian-waktu-lti-di-domain-waktu-1",
    "title": "Sinyal dan Sistem",
    "section": "CPMK 1: Analisis Sinyal dan Sistem Linier Invarian Waktu (LTI) di Domain Waktu",
    "text": "CPMK 1: Analisis Sinyal dan Sistem Linier Invarian Waktu (LTI) di Domain Waktu\nIndikator Kemampuan (Sub-CPMK):\n\nMampu mengklasifikasikan berbagai jenis sinyal (waktu-kontinu/diskret, periodik/aperiodik, energi/daya, deterministik/acak).\nMampu melakukan operasi dasar pada variabel independen sinyal (pergeseran waktu, pembalikan waktu, penskalaan waktu).\nMampu mengklasifikasikan properti sistem (linieritas, invariansi waktu, kausalitas, memori, dan kestabilan).\nMampu menghitung respons sistem LTI terhadap sinyal masukan menggunakan integral konvolusi untuk sistem waktu-kontinu.\nMampu menghitung respons sistem LTI terhadap sinyal masukan menggunakan jumlahan konvolusi untuk sistem waktu-diskret.\nMampu menghubungkan representasi sistem LTI dengan persamaan diferensial (waktu-kontinu) dan persamaan beda (waktu-diskret)."
  },
  {
    "objectID": "rps.html#cpmk-2-analisis-sinyal-dan-sistem-di-domain-frekuensi-waktu-kontinu",
    "href": "rps.html#cpmk-2-analisis-sinyal-dan-sistem-di-domain-frekuensi-waktu-kontinu",
    "title": "Sinyal dan Sistem",
    "section": "CPMK 2: Analisis Sinyal dan Sistem di Domain Frekuensi (Waktu-kontinu)",
    "text": "CPMK 2: Analisis Sinyal dan Sistem di Domain Frekuensi (Waktu-kontinu)\nDeskripsi:\nCPMK ini memfokuskan pada kemampuan analisis sinyal dan sistem LTI dari perspektif domain frekuensi untuk sinyal waktu-kontinu.\nRumusan CPMK 2:\nMahasiswa mampu menerapkan representasi sinyal dalam domain frekuensi menggunakan Deret Fourier dan Transformasi Fourier, serta menganalisis karakteristik sistem LTI menggunakan Transformasi Laplace untuk sinyal waktu-kontinu.\nKeterkaitan dengan CPL:\nCPMK ini memperkuat penguasaan matematika rekayasa (analisis kompleks, integral) untuk analisis frekuensi yang merupakan inti dari banyak aplikasi rekayasa di bidang teknik elektro (misalnya, telekomunikasi, sistem kontrol, dan elektronika)."
  },
  {
    "objectID": "rps.html#cpmk-2-analisis-sinyal-dan-sistem-di-domain-frekuensi-waktu-kontinu-1",
    "href": "rps.html#cpmk-2-analisis-sinyal-dan-sistem-di-domain-frekuensi-waktu-kontinu-1",
    "title": "Sinyal dan Sistem",
    "section": "CPMK 2: Analisis Sinyal dan Sistem di Domain Frekuensi (Waktu-kontinu)",
    "text": "CPMK 2: Analisis Sinyal dan Sistem di Domain Frekuensi (Waktu-kontinu)\nIndikator Kemampuan (Sub-CPMK):\n\nMampu merepresentasikan sinyal periodik waktu-kontinu menggunakan Deret Fourier.\nMampu menganalisis spektrum frekuensi sinyal aperiodik waktu-kontinu menggunakan Transformasi Fourier Waktu-kontinu (CTFT).\nMampu menggunakan properti-properti Transformasi Fourier untuk menyederhanakan analisis sinyal dan sistem.\nMampu menganalisis sistem LTI waktu-kontinu menggunakan respons frekuensi \\(H(j\\omega)\\).\nMampu menerapkan Transformasi Laplace untuk analisis sistem LTI, termasuk mencari solusi persamaan diferensial.\nMampu menganalisis kestabilan dan kausalitas sistem LTI berdasarkan Region of Convergence (ROC) dari Transformasi Laplace."
  },
  {
    "objectID": "rps.html#cpmk-3-analisis-sinyal-dan-sistem-di-domain-waktu-diskret-dan-transisinya",
    "href": "rps.html#cpmk-3-analisis-sinyal-dan-sistem-di-domain-waktu-diskret-dan-transisinya",
    "title": "Sinyal dan Sistem",
    "section": "CPMK 3: Analisis Sinyal dan Sistem di Domain Waktu-Diskret dan Transisinya",
    "text": "CPMK 3: Analisis Sinyal dan Sistem di Domain Waktu-Diskret dan Transisinya\nDeskripsi:\nCPMK ini menjembatani dunia analog dan digital melalui konsep sampling dan memfokuskan pada analisis domain frekuensi dan domain-z untuk sinyal dan sistem waktu-diskret.\nRumusan CPMK 3:\nMahasiswa mampu memahami konsep sampling sebagai jembatan antara sinyal waktu-kontinu dan waktu-diskret, serta menerapkan Transformasi Fourier Waktu-Diskret (DTFT) dan Transformasi-Z untuk menganalisis sinyal dan sistem waktu-diskret.\nKeterkaitan dengan CPL:\nCPMK ini mengintegrasikan pemahaman matematika dan teknologi informasi, yang menjadi landasan penting untuk rekayasa digital modern seperti Pemrosesan Sinyal Digital (DSP), sistem komunikasi digital, dan sistem kendali digital."
  },
  {
    "objectID": "rps.html#cpmk-3-analisis-sinyal-dan-sistem-di-domain-waktu-diskret-dan-transisinya-1",
    "href": "rps.html#cpmk-3-analisis-sinyal-dan-sistem-di-domain-waktu-diskret-dan-transisinya-1",
    "title": "Sinyal dan Sistem",
    "section": "CPMK 3: Analisis Sinyal dan Sistem di Domain Waktu-Diskret dan Transisinya",
    "text": "CPMK 3: Analisis Sinyal dan Sistem di Domain Waktu-Diskret dan Transisinya\nIndikator Kemampuan (Sub-CPMK):\n\nMampu menjelaskan Teorema Sampling Nyquist-Shannon dan implikasinya.\nMampu mengidentifikasi fenomena aliasing dan memahami proses rekonstruksi sinyal.\nMampu menganalisis spektrum frekuensi sinyal waktu-diskret menggunakan Transformasi Fourier Waktu-Diskret (DTFT).\nMampu menerapkan Transformasi-Z untuk menganalisis sistem LTI waktu-diskret.\nMampu menentukan fungsi transfer \\(H(z)\\) dari sistem yang direpresentasikan oleh persamaan beda.\nMampu menganalisis kestabilan dan kausalitas sistem LTI waktu-diskret berdasarkan Region of Convergence (ROC) pada bidang-z."
  },
  {
    "objectID": "rps.html#rencana-pertemuan",
    "href": "rps.html#rencana-pertemuan",
    "title": "Sinyal dan Sistem",
    "section": "Rencana Pertemuan",
    "text": "Rencana Pertemuan\nCPMK 1: Analisis Sinyal dan Sistem Linier Invarian Waktu (LTI) di Domain Waktu\n\nPekan 1. Signals and Systems\nPekan 2. Linear Time-Invariant Systems\nPekan 3. Linear Time-Invariant Systems\nPekan 4. Ujian Kompetensi CPMK 1"
  },
  {
    "objectID": "rps.html#rencana-pertemuan-1",
    "href": "rps.html#rencana-pertemuan-1",
    "title": "Sinyal dan Sistem",
    "section": "Rencana Pertemuan",
    "text": "Rencana Pertemuan\nCPMK 2: Analisis Sinyal dan Sistem di Domain Frekuensi (Waktu-kontinu)\n\nPekan 5. Fourier Series\nPekan 6. Fourier Series\nPekan 7. Continous-Time Fourier Transform\nPekan 8. Continous-Time Fourier Transform\nPekan 9. Laplace Transform\nPekan 10. Laplace Transform\nPekan 11. UTS / Ujian Kompetensi CPMK 2"
  },
  {
    "objectID": "rps.html#rencana-pertemuan-2",
    "href": "rps.html#rencana-pertemuan-2",
    "title": "Sinyal dan Sistem",
    "section": "Rencana Pertemuan",
    "text": "Rencana Pertemuan\nCPMK 3: Analisis Sinyal dan Sistem di Domain Waktu-Diskret dan Transisinya\n\nPekan 12. Sampling\nPekan 13. Discrete-Time Fourier Transform\nPekan 14. Discrete-Time Fourier Transform\nPekan 15. Z-Transform\nPekan 16. Z-Transform\nPekan 17. UAS / Ujian Kompetensi CPMK 3"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is Signals and Systems Course Notes."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Signals and Systems",
    "section": "",
    "text": "This is Signals and Systems Lecture Notes"
  },
  {
    "objectID": "index.html#week-1",
    "href": "index.html#week-1",
    "title": "Signals and Systems",
    "section": "Week 1",
    "text": "Week 1\n\nRPS\nSignals and Systems 1.1: CONTINUOUS-TIME AND DISCRETE-TIME SIGNALS\nSignals and Systems 1.2: TRANSFORMATIONS OF THE INDEPENDENT VARIABLE\nSignals and Systems 1.3: EXPONENTIAL AND SINUSOIDAL SIGNALS\nSignals and Systems 1.4: THE UNIT IMPULSE AND UNIT STEP FUNCTIONS\nSignals and Systems 1.5: CONTINUOUS-TIME AND DISCRETE-TIME SYSTEMS\nSignals and Systems 1.6: BASIC SYSTEM PROPERTIES"
  },
  {
    "objectID": "index.html#week-2",
    "href": "index.html#week-2",
    "title": "Signals and Systems",
    "section": "Week 2",
    "text": "Week 2\n\nSignals and Systems 2.1: DISCRETE-TIME LTI SYSTEMS: THE CONVOLUTION SUM\nSignals and Systems 2.2: CONTINUOUS-TIME LTI SYSTEMS: THE CONVOLUTION INTEGRAL\nSignals and Systems 2.3: PROPERTIES OF LINEAR TIME-INVARIANT SYSTEMS"
  },
  {
    "objectID": "index.html#week-3",
    "href": "index.html#week-3",
    "title": "Signals and Systems",
    "section": "Week 3",
    "text": "Week 3\n\nSignals and Systems 2.4: CAUSAL LTI SYSTEMS DESCRIBED BY DIFFERENTIAL AND DIFFERENCE EQUATIONS\nIntroduction to Fourier Analysis\nSignals and Systems 3.2: THE RESPONSE OF LTI SYSTEMS TO COMPLEX EXPONENTIALS"
  },
  {
    "objectID": "index.html#week-4",
    "href": "index.html#week-4",
    "title": "Signals and Systems",
    "section": "Week 4",
    "text": "Week 4\n\nIntroduction to Fourier Analysis\nBullying"
  },
  {
    "objectID": "index.html#week-5",
    "href": "index.html#week-5",
    "title": "Signals and Systems",
    "section": "Week 5",
    "text": "Week 5\n\nUjian CPMK\nSignals and Systems 3.2: THE RESPONSE OF LTI SYSTEMS TO COMPLEX EXPONENTIALS\nSignals and Systems 3.3: FOURIER SERIES REPRESENTATION OF CONTINUOUS-TIME PERIODIC SIGNALS"
  },
  {
    "objectID": "index.html#week-6",
    "href": "index.html#week-6",
    "title": "Signals and Systems",
    "section": "Week 6",
    "text": "Week 6\n\nSignals and Systems 3.4: CONVERGENCE OF THE FOURIER SERIES\nSignals and Systems 3.5: PROPERTIES OF CONTINUOUS-TIME FOURIER SERIES\nSignals and Systems 3.6: FOURIER SERIES REPRESENTATION OF DISCRETE-TIME PERIODIC SIGNALS\nSignals and Systems 3.7: PROPERTIES OF DISCRETE-TIME FOURIER SERIES\nCT Fourier Series and DT Fourier Series"
  },
  {
    "objectID": "index.html#week-7",
    "href": "index.html#week-7",
    "title": "Signals and Systems",
    "section": "Week 7",
    "text": "Week 7\n\nSignals and Systems 4.1: REPRESENTATION OF APERIODIC SIGNALS: THE CONTINUOUS-TIME FOURIER TRANSFORM\nSignals and Systems 4.2: THE FOURIER TRANSFORM FOR PERIODIC SIGNALS\nSignals and Systems 4.3: PROPERTIES OF THE CONTINUOUS-TIME FOURIER TRANSFORM"
  },
  {
    "objectID": "index.html#week-8",
    "href": "index.html#week-8",
    "title": "Signals and Systems",
    "section": "Week 8",
    "text": "Week 8\n\nSignals and Systems 4.4: THE CONVOLUTION PROPERTY\nSignals and Systems 4.5: THE MULTIPLICATION PROPERTY\nSignals and Systems 4.7: SYSTEMS CHARACTERIZED BY LINEAR CONSTANT-COEFFICIENT DIFFERENTIAL EQUATIONS"
  },
  {
    "objectID": "ss_11.html#what-is-a-signal",
    "href": "ss_11.html#what-is-a-signal",
    "title": "Signal and Systems",
    "section": "What is a Signal?",
    "text": "What is a Signal?\nSignals describe patterns of variation that convey information.\n\n\n\nElectrical Circuits: Voltage/current changes over time. \n\n\n\nAudio/Speech: Acoustic pressure fluctuations. \n\n\n\nSignals are essentially carriers of information. Think of them as dynamic entities where the information is encoded in how a physical quantity changes. We see examples all around us: the humble voltage in a circuit, how a car’s speed changes, the intricate pressure waves our vocal cords produce to form speech, or even the varying brightness levels that make up an image. Each of these variations holds specific information."
  },
  {
    "objectID": "ss_11.html#signal-examples",
    "href": "ss_11.html#signal-examples",
    "title": "Signal and Systems",
    "section": "Signal (Examples)",
    "text": "Signal (Examples)"
  },
  {
    "objectID": "ss_11.html#signal-examples-1",
    "href": "ss_11.html#signal-examples-1",
    "title": "Signal and Systems",
    "section": "Signal (Examples)",
    "text": "Signal (Examples)"
  },
  {
    "objectID": "ss_11.html#mathematical-representation",
    "href": "ss_11.html#mathematical-representation",
    "title": "Signal and Systems",
    "section": "Mathematical Representation",
    "text": "Mathematical Representation\nSignals are mathematically described as functions of one or more independent variables. Typically, we refer to the independent variable as “time.”\n\n\nContinuous-Time (CT) Signals: \\(x(t)\\)\n\nIndependent variable t is continuous.\nDefined for a continuum of values.\n\n\nDiscrete-Time (DT) Signals: \\(x[n]\\)\n\nIndependent variable n is discrete (integers).\nDefined only at discrete points."
  },
  {
    "objectID": "ss_11.html#mathematical-representation-1",
    "href": "ss_11.html#mathematical-representation-1",
    "title": "Signal and Systems",
    "section": "Mathematical Representation",
    "text": "Mathematical Representation\n\n\nContinuous-Time (CT) Signals: \\(x(t)\\)\n\nExamples:\n\nSpeech signal: \\(P(t)\\)\nWind profile vs. height: \\(W(h)\\) \n\n\n\nDiscrete-Time (DT) Signals: \\(x[n]\\)\n\nExamples:\n\nWeekly Dow-Jones Index: \\(D[k]\\)\nDemographic data \n\n\n\n\nMathematically, we represent signals as functions. For this course, we’ll primarily focus on signals with a single independent variable, which we most often call “time,” even if it represents something else like height or depth.\nWe distinguish between two major types: Continuous-Time (CT) signals, denoted \\(x(t)\\), are defined for every possible value of \\(t\\), much like how voltage in a circuit changes smoothly over time.\nIn contrast, Discrete-Time (DT) signals, denoted \\(x[n]\\), are only defined at specific, discrete points, usually integer values of \\(n\\). Think of a stock market index that’s only recorded at the end of each day or week – there’s no data point in between."
  },
  {
    "objectID": "ss_11.html#visualizing-ct-signals",
    "href": "ss_11.html#visualizing-ct-signals",
    "title": "Signal and Systems",
    "section": "Visualizing CT Signals",
    "text": "Visualizing CT Signals\nContinuous-Time Signal Example: \\(x(t) = \\cos(2\\pi t)\\)"
  },
  {
    "objectID": "ss_11.html#visualizing-dt-signals",
    "href": "ss_11.html#visualizing-dt-signals",
    "title": "Signal and Systems",
    "section": "Visualizing DT Signals",
    "text": "Visualizing DT Signals\nDiscrete-Time Signal Example: \\(x[n] = \\cos(\\frac{\\pi}{4}n)\\)\n\n\n\n\n\n\n\nThe visual representation is key to distinguishing these signal types. As you can see from the interactive plots: For a continuous-time signal, the graph is a smooth, unbroken curve, indicating that the signal has a value at every single point in continuous time. For a discrete-time signal, the graph consists of individual stems, or impulses, at integer points. There’s no signal defined in between these integer values. This emphasizes that discrete-time signals are sequences of values. Feel free to run and interact with these code blocks to see how they behave."
  },
  {
    "objectID": "ss_11.html#origins-of-discrete-time-signals",
    "href": "ss_11.html#origins-of-discrete-time-signals",
    "title": "Signal and Systems",
    "section": "Origins of Discrete-Time Signals",
    "text": "Origins of Discrete-Time Signals\nDiscrete-time signals can arise in two ways:\n\nInherently Discrete Phenomena:\n\nVariables are naturally discrete.\nExamples: Population counts, quarterly economic data, number of defects per batch.\n\nSampling of Continuous-Time Signals:\n\nConverting a continuous signal into a discrete sequence.\nProcess: Measuring the continuous signal’s value at regular intervals.\nImportance: Foundation of modern digital signal processing.\nApplications: Digital audio, image processing (pixels are samples of brightness), digital control systems (autopilots).\n\n\n\nIt’s important to understand that discrete-time signals aren’t always born discrete. Many are created by taking a continuous-time signal and sampling it. This process, often done by Analog-to-Digital Converters (ADCs), is fundamental to modern digital systems. Because digital processors thrive on discrete data, sampling allows us to bring real-world analog signals into the digital domain for processing, analysis, and storage. This is why when you zoom in on a digital image, you eventually see the individual pixels – each pixel is a sample of the original continuous image’s brightness."
  },
  {
    "objectID": "ss_11.html#signal-energy-and-power",
    "href": "ss_11.html#signal-energy-and-power",
    "title": "Signal and Systems",
    "section": "Signal Energy and Power",
    "text": "Signal Energy and Power\nMotivation derives from physical systems (e.g., electrical power \\(p(t) = v(t)i(t)\\) across a resistor).\nWe extend these concepts to any signal. For a general signal \\(x(t)\\) or \\(x[n]\\) , we use \\(|x|^2\\).\n\nTotal Energy over a finite interval:\n\nCT: \\(\\int_{t_1}^{t_2} |x(t)|^2 dt\\)\nDT: \\(\\sum_{n=n_1}^{n_2} |x[n]|^2\\)\n\nAverage Power over a finite interval:\n\nCT: \\(\\frac{1}{t_2-t_1} \\int_{t_1}^{t_2} |x(t)|^2 dt\\)\nDT: \\(\\frac{1}{n_2-n_1+1} \\sum_{n=n_1}^{n_2} |x[n]|^2\\)\n\n\n\nThe concepts of energy and power are borrowed from physics but are generalized in Signals and Systems. Even if a signal doesn’t directly represent physical energy (like velocity, or a sound wave frequency), we still use these terms to characterize its ‘strength’ or ‘intensity’ over time.\nFor signals that can take complex values, which we’ll encounter later, we use the magnitude squared, \\(|x|^2\\), analogous to how physical power is proportional to voltage or current squared. These definitions help us quantify how much ‘content’ a signal has within a specific duration."
  },
  {
    "objectID": "ss_11.html#total-energy-e_infty-for-ct-signals",
    "href": "ss_11.html#total-energy-e_infty-for-ct-signals",
    "title": "Signal and Systems",
    "section": "Total Energy (\\(E_{\\infty}\\)) for CT Signals",
    "text": "Total Energy (\\(E_{\\infty}\\)) for CT Signals\nThe total energy of a continuous-time signal \\(x(t)\\) over an infinite interval is:\n\\[\nE_{\\infty} \\triangleq \\lim _{T \\rightarrow \\infty} \\int_{-T}^{T}|x(t)|^{2} d t=\\int_{-\\infty}^{+\\infty}|x(t)|^{2} d t\n\\]\n\nIf \\(E_{\\infty} &lt; \\infty\\), the signal is a finite-energy signal.\nIf \\(E_{\\infty} = \\infty\\), the signal has infinite energy."
  },
  {
    "objectID": "ss_11.html#total-energy-e_infty-for-ct-signals-1",
    "href": "ss_11.html#total-energy-e_infty-for-ct-signals-1",
    "title": "Signal and Systems",
    "section": "Total Energy (\\(E_{\\infty}\\)) for CT Signals",
    "text": "Total Energy (\\(E_{\\infty}\\)) for CT Signals\nExample: Finite Energy Pulse Signal\nConsider a rectangular pulse \\(x(t) = 1\\) for \\(0 \\leq t \\leq 1\\) and \\(0\\) otherwise.\n\n\n\n\n\n\n\nWhen we analyze signals over their entire duration, from negative to positive infinity, we use the concept of total energy. This is defined by integrating the magnitude squared of the signal over all time. A signal with finite total energy is one where this integral converges to a finite value. Such signals are typically “transient” in nature, meaning they eventually die out and don’t persist indefinitely. The rectangular pulse shown is a classic example: it only exists for a brief period, and its energy is clearly finite."
  },
  {
    "objectID": "ss_11.html#total-energy-e_infty-for-dt-signals",
    "href": "ss_11.html#total-energy-e_infty-for-dt-signals",
    "title": "Signal and Systems",
    "section": "Total Energy (\\(E_{\\infty}\\)) for DT Signals",
    "text": "Total Energy (\\(E_{\\infty}\\)) for DT Signals\nThe total energy of a discrete-time signal \\(x[n]\\) over an infinite interval is:\n\\[\nE_{\\infty} \\triangleq \\lim _{N \\rightarrow \\infty} \\sum_{n=-N}^{+N}|x[n]|^{2}=\\sum_{n=-\\infty}^{+\\infty}|x[n]|^{2}\n\\]\n\nIf \\(E_{\\infty} &lt; \\infty\\), the signal is a finite-energy signal."
  },
  {
    "objectID": "ss_11.html#total-energy-e_infty-for-dt-signals-1",
    "href": "ss_11.html#total-energy-e_infty-for-dt-signals-1",
    "title": "Signal and Systems",
    "section": "Total Energy (\\(E_{\\infty}\\)) for DT Signals",
    "text": "Total Energy (\\(E_{\\infty}\\)) for DT Signals\nExample: Decaying Exponential\nConsider \\(x[n] = (0.5)^n u[n]\\), where \\(u[n]\\) is the unit step function (\\(u[n]=1\\) for \\(n \\ge 0\\), \\(0\\) for \\(n &lt; 0\\)).\n\n\n\n\n\n\n\nFor discrete-time signals, total energy is calculated by summing the squared magnitudes of all signal samples from negative to positive infinity. Like their continuous counterparts, discrete-time signals with finite total energy are those whose sum converges. The decaying exponential is a perfect illustration: as ‘n’ increases, the signal values become smaller and smaller, ensuring that the infinite sum of their squares remains finite. This is also a transient signal."
  },
  {
    "objectID": "ss_11.html#average-power-p_infty-for-ct-signals",
    "href": "ss_11.html#average-power-p_infty-for-ct-signals",
    "title": "Signal and Systems",
    "section": "Average Power (\\(P_{\\infty}\\)) for CT Signals",
    "text": "Average Power (\\(P_{\\infty}\\)) for CT Signals\nThe time-averaged power over an infinite interval for a continuous-time signal \\(x(t)\\) is:\n\\[\nP_{\\infty} \\triangleq \\lim _{T \\rightarrow \\infty} \\frac{1}{2 T} \\int_{-T}^{T}|x(t)|^{2} d t\n\\]\n\nIf \\(E_{\\infty} &lt; \\infty\\), then \\(P_{\\infty} = 0\\).\nIf \\(P_{\\infty} &gt; 0\\) and \\(P_{\\infty} &lt; \\infty\\), the signal is a finite-power signal (implying \\(E_{\\infty} = \\infty\\))."
  },
  {
    "objectID": "ss_11.html#average-power-p_infty-for-ct-signals-1",
    "href": "ss_11.html#average-power-p_infty-for-ct-signals-1",
    "title": "Signal and Systems",
    "section": "Average Power (\\(P_{\\infty}\\)) for CT Signals",
    "text": "Average Power (\\(P_{\\infty}\\)) for CT Signals\nExample: Sinusoidal Signal\nConsider \\(x(t) = A \\cos(\\omega t + \\phi)\\). Its average power is \\(P_{\\infty} = A^2/2\\). Let’s test with \\(x(t) = \\cos(2\\pi t)\\).\n\n\n\n\n\n\n\nAverage power describes how much energy, on average, is contained in a signal over an infinitely long period. It’s often more relevant for signals that persist indefinitely, such as periodic signals. If a signal has finite total energy, its average power must be zero, because you’re distributing that finite energy over an infinite duration.\nHowever, if a signal has a constant “strength” over time, it will have finite average power but infinite total energy. Sinusoidal signals, like the cosine wave demonstrated, are prime examples. Their amplitude oscillates but never dies down, so they continuously carry power, leading to infinite total energy over infinite time, but finite average power."
  },
  {
    "objectID": "ss_11.html#average-power-p_infty-for-dt-signals",
    "href": "ss_11.html#average-power-p_infty-for-dt-signals",
    "title": "Signal and Systems",
    "section": "Average Power (\\(P_{\\infty}\\)) for DT Signals",
    "text": "Average Power (\\(P_{\\infty}\\)) for DT Signals\nThe time-averaged power over an infinite interval for a discrete-time signal \\(x[n]\\) is:\n\\[\nP_{\\infty} \\triangleq \\lim _{N \\rightarrow \\infty} \\frac{1}{2 N+1} \\sum_{n=-N}^{+N}|x[n]|^{2}\n\\]\n\nIf \\(E_{\\infty} &lt; \\infty\\), then \\(P_{\\infty} = 0\\).\nIf \\(P_{\\infty} &gt; 0\\) and \\(P_{\\infty} &lt; \\infty\\), the signal is a finite-power signal (implying \\(E_{\\infty} = \\infty\\))."
  },
  {
    "objectID": "ss_11.html#average-power-p_infty-for-dt-signals-1",
    "href": "ss_11.html#average-power-p_infty-for-dt-signals-1",
    "title": "Signal and Systems",
    "section": "Average Power (\\(P_{\\infty}\\)) for DT Signals",
    "text": "Average Power (\\(P_{\\infty}\\)) for DT Signals\nExample: Constant Signal\nConsider \\(x[n] = 4\\).\n\n\n\n\n\n\n\nThe definition of average power for discrete-time signals is conceptually identical to continuous-time, using summation instead of integration. A crucial example is a constant signal, like \\(x[n]=4\\). While its total energy over infinite time is infinite (as you’re summing a non-zero value infinitely many times), its average power is simply the square of its magnitude – a finite, non-zero value. Similarly, discrete-time sinusoidal signals (like \\(x[n] = \\cos(\\omega n)\\)) also have infinite energy but finite average power."
  },
  {
    "objectID": "ss_11.html#signal-classification-based-on-energy-power",
    "href": "ss_11.html#signal-classification-based-on-energy-power",
    "title": "Signal and Systems",
    "section": "Signal Classification based on Energy & Power",
    "text": "Signal Classification based on Energy & Power\nSignals can be broadly classified into three categories:\n\n\n\n\n\ngraph LR\n    A[Signal x] --&gt; B{Calc. $$E_\\infty$$};\n    B --&gt; C{ $$E_\\infty \\lt \\infty$$ ?};\n    C -- Yes --&gt; D[Finite Energy Signal];\n    D --&gt; E[$$P_\\infty = 0$$];\n    C -- No --&gt; F{Calculate $$P_\\infty$$};\n    F --&gt; G{$$P_\\infty \\lt \\infty$$ ?};\n    G -- Yes --&gt; H[Finite Power Signal];\n    H --&gt; I[$$E_\\infty = \\infty$$];\n    G -- No --&gt; J[Neither Finite Energy nor Power];\n    J --&gt; K[$$E_\\infty = \\infty$$];\n    J --&gt; L[$$P_\\infty = \\infty$$];\n\n    style D fill:#ddf,stroke:#333,stroke-width:2px;\n    style H fill:#dfd,stroke:#333,stroke-width:2px;\n    style J fill:#fdd,stroke:#333,stroke-width:2px;\n\n\n\n\n\n\n\nFinite Energy Signals: \\(E_{\\infty} &lt; \\infty \\implies P_{\\infty} = 0\\). (e.g., pulses, decaying exponentials)\nFinite Power Signals: \\(P_{\\infty} &lt; \\infty\\) and \\(P_{\\infty} &gt; 0 \\implies E_{\\infty} = \\infty\\). (e.g., periodic signals, constant signals)\nNeither Finite Energy nor Finite Power: \\(E_{\\infty} = \\infty\\) AND \\(P_{\\infty} = \\infty\\). (e.g., \\(x(t)=t\\), \\(x[n]=n\\))\n\n\nThis diagram summarizes the classification of signals based on their energy and power properties. It’s a critical distinction in signal analysis. Finite energy signals are often those that are transient and die out, such as the output of an event. Finite power signals, on the other hand, are typically continuous, persistent signals like voltage from a power supply or a recurring musical note. It’s important to understand that a signal cannot simultaneously have finite non-zero energy and finite non-zero power from these definitions over an infinite interval. There are also signals that fall into neither category, growing without bound such that both energy and power are infinite."
  },
  {
    "objectID": "ss_11.html#key-takeaways",
    "href": "ss_11.html#key-takeaways",
    "title": "Signal and Systems",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nSignals convey information through patterns of variation.\nDistinguish between Continuous-Time (\\(x(t)\\)) and Discrete-Time (\\(x[n]\\)) signals.\n\nCT: Defined for a continuum of values.\nDT: Defined only at discrete points (often sampled CT signals).\n\nSignals are characterized by their Total Energy (\\(E_{\\infty}\\)) and Average Power (\\(P_{\\infty}\\)).\n\nUsed to classify signals into finite-energy, finite-power, or neither.\n\n\n\nTo wrap up this introductory session, remember these core concepts: signals are central to how we understand and interact with the world around us. Mastering the distinction between continuous and discrete time, and being able to classify signals based on their energy and power, will provide strong foundations for the more advanced topics we’ll cover, such as convolution, Fourier analysis, and system response. Next time, we’ll delve into basic operations we can perform on signals and introduce some fundamental signal types."
  },
  {
    "objectID": "ss_13.html#introduction-fundamental-building-blocks",
    "href": "ss_13.html#introduction-fundamental-building-blocks",
    "title": "Signal and Systems",
    "section": "Introduction: Fundamental Building Blocks",
    "text": "Introduction: Fundamental Building Blocks\nIn signals and systems, several basic types of signals frequently appear and serve as fundamental building blocks from which more complex signals can be constructed.\n\nImportance:\n\nRepresent a wide range of physical phenomena.\nAre crucial for analyzing the behavior of systems.\nForm the basis for powerful analytical tools (e.g., Fourier Analysis).\n\n\nWe will explore two key types:\n\nExponential Signals\nSinusoidal Signals\n\n\nThese signals are not just mathematical constructs; they have direct physical interpretations. For example, the natural response of an electrical circuit can often be described using exponentials or sinusoids. Understanding them is a prerequisite for understanding more advanced concepts like frequency response and filtering. They are to signals and systems what elementary particles are to physics – the smallest, irreducible components from which everything else is built."
  },
  {
    "objectID": "ss_13.html#continuous-time-real-exponential-signals",
    "href": "ss_13.html#continuous-time-real-exponential-signals",
    "title": "Signal and Systems",
    "section": "Continuous-Time Real Exponential Signals",
    "text": "Continuous-Time Real Exponential Signals\nThe continuous-time complex exponential signal is generally of the form \\(x(t)=C e^{a t}\\), where \\(C\\) and \\(a\\) are complex numbers.\nReal Exponential Case: \\(C, a\\) are real\n\n\n\n\n\\(a&gt;0\\)\n\n\n\n\n\\(a&lt;0\\)"
  },
  {
    "objectID": "ss_13.html#continuous-time-real-exponential-signals-1",
    "href": "ss_13.html#continuous-time-real-exponential-signals-1",
    "title": "Signal and Systems",
    "section": "Continuous-Time Real Exponential Signals",
    "text": "Continuous-Time Real Exponential Signals\n\nForm: \\(x(t)=C e^{a t}\\)\nBehavior types (Figure 1.19):\n\nIf \\(a &gt; 0\\): Growing exponential.\n\nUsed for chain reactions in atomic explosions, chemical reactions, population growth.\n\nIf \\(a &lt; 0\\): Decaying exponential.\n\nUsed for radioactive decay, responses of \\(RC\\) circuits, damped mechanical systems.\n\nIf \\(a = 0\\): Constant signal (\\(x(t)=C\\)).\n\n\n\nReal exponentials describe dynamic processes where a quantity either increases or decreases over time at a rate proportional to its current value. Think of compounding interest (\\(a&gt;0\\)) or the discharge of a capacitor (\\(a&lt;0\\)). These are fundamental to understanding the transient responses of many physical systems such as first-order RC or RL circuits."
  },
  {
    "objectID": "ss_13.html#demo-continuous-time-real-exponential",
    "href": "ss_13.html#demo-continuous-time-real-exponential",
    "title": "Signal and Systems",
    "section": "Demo: Continuous-Time Real Exponential",
    "text": "Demo: Continuous-Time Real Exponential\nExplore how changing parameters affect a real exponential signal.\n\n\n\n\n\n\n\nExperiment by changing values in the code: - a_val_default &gt; 0: Observe the exponential growth. Real-world example: Uncontrolled population growth, chain reactions. - a_val_default &lt; 0: Observe the exponential decay. Real-world example: Discharge of a capacitor in an RC circuit, radioactive decay. - a_val_default = 0: The signal becomes a constant. - C_val_default: Changes the initial amplitude and direction (if negative). Notice how quickly the signal can grow or decay with changes in a_val_default."
  },
  {
    "objectID": "ss_13.html#continuous-time-periodic-complex-exponential-sinusoidal-signals",
    "href": "ss_13.html#continuous-time-periodic-complex-exponential-sinusoidal-signals",
    "title": "Signal and Systems",
    "section": "Continuous-Time Periodic Complex Exponential & Sinusoidal Signals",
    "text": "Continuous-Time Periodic Complex Exponential & Sinusoidal Signals\nPurely Imaginary Exponential: \\(a = j \\omega_0\\)\n\nForm: \\(x(t) = e^{j \\omega_0 t}\\)\nThis signal represents a point rotating on the unit circle in the complex plane.\nPeriodicity: This signal is periodic.\n\nCondition: \\(e^{j \\omega_0 t} = e^{j \\omega_0 (t+T)} \\implies e^{j \\omega_0 T} = 1\\).\nRequires \\(\\omega_0 T\\) to be an integer multiple of \\(2\\pi\\).\nFundamental Period (\\(T_0\\)): Smallest positive \\(T\\) satisfying the condition. \\[\nT_0 = \\frac{2\\pi}{|\\omega_0|} \\quad (\\text{for } \\omega_0 \\ne 0) \\tag{1.24}\n\\]\n\\(\\omega_0\\) is the angular frequency (radians/second). \\(f_0 = \\omega_0 / (2\\pi)\\) is the ordinary frequency (Hertz, Hz)."
  },
  {
    "objectID": "ss_13.html#continuous-time-periodic-complex-exponential-sinusoidal-signals-1",
    "href": "ss_13.html#continuous-time-periodic-complex-exponential-sinusoidal-signals-1",
    "title": "Signal and Systems",
    "section": "Continuous-Time Periodic Complex Exponential & Sinusoidal Signals",
    "text": "Continuous-Time Periodic Complex Exponential & Sinusoidal Signals\nSinusoidal Signals\n\nForm: \\(x(t) = A \\cos(\\omega_0 t + \\phi)\\) (Figure 1.20)\n\n\\(A\\): Amplitude, \\(\\omega_0\\): Angular frequency, \\(\\phi\\): Phase (radians), representing a time shift.\n\nPeriodicity: Same fundamental period \\(T_0 = \\frac{2\\pi}{|\\omega_0|}\\) as the complex exponential.\nApplications: Ideal LC circuits, spring-mass systems, AC power signals.\n\n\n\nPeriodic complex exponentials are often called “rotating phasors” in the complex plane. Their real and imaginary parts are sinusoids. This connection is vital, as it allows us to analyze sinusoidal signals using the simpler algebra of complex exponentials. Stress the relationship between \\(\\omega_0\\) and \\(T_0\\): high frequency means short period, low frequency means long period. Sinusoids are the workhorses of EE, representing steady-state AC signals, waves, and oscillations in many physical systems where energy is conserved."
  },
  {
    "objectID": "ss_13.html#eulers-relation-and-signal-representation",
    "href": "ss_13.html#eulers-relation-and-signal-representation",
    "title": "Signal and Systems",
    "section": "Euler’s Relation and Signal Representation",
    "text": "Euler’s Relation and Signal Representation\nEuler’s Relation: Unites complex exponentials and sinusoids – a fundamental identity in ECE. \\[\ne^{j \\omega_0 t}=\\cos \\omega_0 t+j \\sin \\omega_0 t \\tag{1.26}\n\\]\nRepresenting Sinusoids with Exponentials: We can express a sinusoidal signal in terms of complex exponentials, making analysis easier: \\[\nA \\cos (\\omega_0 t+\\phi)=\\frac{A}{2} e^{j \\phi} e^{j \\omega_0 t}+\\frac{A}{2} e^{-j \\phi} e^{-j \\omega_0 t} \\tag{1.27}\n\\]"
  },
  {
    "objectID": "ss_13.html#eulers-relation-and-signal-representation-1",
    "href": "ss_13.html#eulers-relation-and-signal-representation-1",
    "title": "Signal and Systems",
    "section": "Euler’s Relation and Signal Representation",
    "text": "Euler’s Relation and Signal Representation\nAlternatively (using the real part of a single complex exponential): \\[\nA \\cos (\\omega_0 t+\\phi)=A \\mathcal{R} \\text{e}\\left\\{e^{j\\left(\\omega_0 t+\\phi\\right)}\\right\\} \\tag{1.28}\n\\] Similarly for sine (using the imaginary part): \\[\nA \\sin (\\omega_0 t+\\phi)=A \\mathcal{I} \\text{m}\\left\\{e^{j\\left(\\omega_0 t+\\phi\\right)}\\right\\} \\tag{1.29}\n\\]"
  },
  {
    "objectID": "ss_13.html#eulers-relation-and-signal-representation-2",
    "href": "ss_13.html#eulers-relation-and-signal-representation-2",
    "title": "Signal and Systems",
    "section": "Euler’s Relation and Signal Representation",
    "text": "Euler’s Relation and Signal Representation\nFrequency and Period Relationship (Inverse Proportionality): - Increasing \\(|\\omega_0|\\) means higher oscillation rate, smaller period \\(T_0\\). - Decreasing \\(|\\omega_0|\\) means lower oscillation rate, larger period \\(T_0\\).\n\n\n\nFigure 1.21: \\(\\omega_1 &gt; \\omega_2 &gt; \\omega_3 \\implies T_1 &lt; T_2 &lt; T_3\\).\n\nEuler’s relation is arguably the most powerful identity in ECE. It allows us to convert sinusoidal signals, which are common in real-world systems, into complex exponentials, which are mathematically much easier to manipulate (e.g., differentiation, integration, system analysis). This is a cornerstone of Fourier analysis. Explain how the magnitude and phase of the complex amplitudes in (1.27) relate to \\(A\\) and \\(\\phi\\): \\(\\frac{A}{2}e^{j\\phi}\\) is a complex number whose magnitude is \\(A/2\\) and phase is \\(\\phi\\)."
  },
  {
    "objectID": "ss_13.html#energy-and-power-of-periodic-signals",
    "href": "ss_13.html#energy-and-power-of-periodic-signals",
    "title": "Signal and Systems",
    "section": "Energy and Power of Periodic Signals",
    "text": "Energy and Power of Periodic Signals\n\nPeriodic complex exponentials and sinusoidal signals have infinite total energy but finite average power. This classifies them as power signals.\nExample: \\(x(t) = e^{j \\omega_0 t}\\)\n\nTotal energy integrated over all time is infinite, as the signal never dies out.\nHowever, the average power over one period is easily calculated: \\[\nE_{\\text{period}} = \\int_{0}^{T_0} |e^{j \\omega_0 t}|^2 dt = \\int_{0}^{T_0} 1 \\cdot dt = T_0 \\tag{1.30}\n\\]\nAverage power over one period: \\[\nP_{\\text{period}} = \\frac{1}{T_0} E_{\\text{period}} = 1 \\tag{1.31}\n\\]\nSince the signal repeats identically, the average power over all time is also: \\[\nP_x = \\lim_{T \\rightarrow \\infty} \\frac{1}{2T} \\int_{-T}^{T} |e^{j \\omega_0 t}|^2 dt = 1 \\tag{1.32}\n\\]"
  },
  {
    "objectID": "ss_13.html#energy-and-power-of-periodic-signals-1",
    "href": "ss_13.html#energy-and-power-of-periodic-signals-1",
    "title": "Signal and Systems",
    "section": "Energy and Power of Periodic Signals",
    "text": "Energy and Power of Periodic Signals\nHarmonically Related Complex Exponentials\n\nA set of periodic exponentials, all with a common period \\(T_0\\).\nTheir frequencies \\(\\omega\\) must be integer multiples of a fundamental frequency \\(\\omega_0 = 2\\pi/T_0\\). \\[\n\\phi_k(t) = e^{j k \\omega_0 t}, \\quad k=0, \\pm 1, \\pm 2, \\ldots \\tag{1.36}\n\\]\nEach \\(\\phi_k(t)\\) (for \\(k \\ne 0\\)) has a fundamental period \\(T_0/|k|\\). This means they complete \\(|k|\\) cycles within \\(T_0\\).\nThese are the “harmonics” used in music and form the basis for Fourier Series representation of periodic signals (Chapter 3).\n\n\nThe concept of infinite energy but finite average power is crucial. It differentiates “energy signals” (finite total energy) from “power signals” (infinite total energy but finite average power). Most continuous-time periodic signals are power signals. Emphasize that harmonically related exponentials are the basis for building any complex periodic signal into its fundamental frequency and its overtones, similar to how musical instruments produce and combine different frequencies to create timbre."
  },
  {
    "objectID": "ss_13.html#demo-continuous-time-sinusoidal-signal",
    "href": "ss_13.html#demo-continuous-time-sinusoidal-signal",
    "title": "Signal and Systems",
    "section": "Demo: Continuous-Time Sinusoidal Signal",
    "text": "Demo: Continuous-Time Sinusoidal Signal\nVisualize how amplitude, frequency, and phase affect a sinusoidal signal.\n\n\n\n\n\n\n\nExperiment by changing values in the code: - A_val_default: Changes the peak value of the oscillation. This is the maximum displacement from equilibrium. - omega0_val_default: Controls how rapidly the signal oscillates. Higher \\(\\omega_0\\) means higher frequency, shorter period. Try small values (e.g., 0.5) and large values (e.g., 4.0). Notice the number of cycles within the plotting range. - phi_val_default: Shifts the signal horizontally. Positive \\(\\phi\\) means a left shift (advance in time), negative \\(\\phi\\) means a right shift (delay in time). Observe how the cosine wave changes its starting point at \\(t=0\\). A phase of \\(\\pi/2\\) makes a cosine look like a negative sine."
  },
  {
    "objectID": "ss_13.html#example-1.5-sum-of-complex-exponentials",
    "href": "ss_13.html#example-1.5-sum-of-complex-exponentials",
    "title": "Signal and Systems",
    "section": "Example 1.5: Sum of Complex Exponentials",
    "text": "Example 1.5: Sum of Complex Exponentials\nIt’s useful to express the sum of two complex exponentials as a product of a single complex exponential and a sinusoid. This technique is often used in modulation.\nProblem: Plot the magnitude of the signal \\(x(t)=e^{j 2 t}+e^{j 3 t}\\).\nSolution Steps:\n\nFactor out average frequency: The average frequency of \\(2t\\) and \\(3t\\) is \\((2+3)/2 = 2.5t\\). \\[\nx(t)=e^{j 2.5 t}\\left(e^{-j 0.5 t}+e^{j 0.5 t}\\right) \\tag{1.39}\n\\]\nApply Euler’s relation: Recall \\(e^{jx} + e^{-jx} = 2 \\cos(x)\\). Here, \\(x=0.5t\\). \\[\nx(t)=2 e^{j 2.5 t} \\cos (0.5 t) \\tag{1.40}\n\\]"
  },
  {
    "objectID": "ss_13.html#example-1.5-sum-of-complex-exponentials-1",
    "href": "ss_13.html#example-1.5-sum-of-complex-exponentials-1",
    "title": "Signal and Systems",
    "section": "Example 1.5: Sum of Complex Exponentials",
    "text": "Example 1.5: Sum of Complex Exponentials\n\nFind magnitude: We use the property that \\(|z_1 z_2| = |z_1||z_2|\\). Since \\(e^{j\\theta}\\) represents a complex number on the unit circle, its magnitude is always unity (i.e., \\(|e^{j 2.5 t}| = 1\\)). \\[\n|x(t)|=2|\\cos (0.5 t)| \\tag{1.41}\n\\] This is a full-wave rectified sinusoid (Figure 1.22).\n\n\n\nThis example demonstrates a useful algebraic manipulation that simplifies analysis. The resulting expression clearly shows the envelope \\(2|\\cos(0.5t)|\\) over the underlying high-frequency oscillation (from \\(e^{j 2.5t}\\)). This concept is important in areas like amplitude modulation (AM) where a carrier signal is effectively multiplied by a message signal, producing sidebands around the carrier frequency. The ‘beating’ phenomenon in sound waves is also an example of this."
  },
  {
    "objectID": "ss_13.html#continuous-time-general-complex-exponential-signals",
    "href": "ss_13.html#continuous-time-general-complex-exponential-signals",
    "title": "Signal and Systems",
    "section": "Continuous-Time General Complex Exponential Signals",
    "text": "Continuous-Time General Complex Exponential Signals\nThe most general complex exponential combines both real exponential and periodic complex exponential characteristics.\n\nForm: \\(C e^{a t}\\) where \\(C = |C|e^{j\\theta}\\) (polar form amplitude) and \\(a = r + j\\omega_0\\) (rectangular form exponent). \\[\nC e^{a t}=|C| e^{r t} e^{j\\left(\\omega_0 t+\\theta\\right)} \\tag{1.42}\n\\]\nExpanded Form (real and imaginary parts are damped/growing sinusoids): \\[\nC e^{a t}=|C| e^{r t} \\cos \\left(\\omega_0 t+\\theta\\right)+j|C| e^{r t} \\sin \\left(\\omega_0 t+\\theta\\right) \\tag{1.43}\n\\]\n\n\\(r &gt; 0\\): Growing sinusoid (amplitude expands outward).\n\\(r &lt; 0\\): Decaying sinusoid (damped sinusoid, amplitude shrinks inward)."
  },
  {
    "objectID": "ss_13.html#continuous-time-general-complex-exponential-signals-1",
    "href": "ss_13.html#continuous-time-general-complex-exponential-signals-1",
    "title": "Signal and Systems",
    "section": "Continuous-Time General Complex Exponential Signals",
    "text": "Continuous-Time General Complex Exponential Signals\n\nEnvelope: The term \\(|C|e^{rt}\\) acts as an envelope, shaping the amplitude of the oscillations.\n\n\n\n\n\n\\(r&gt;0\\)\n\n\n\n\n\\(r&lt;0\\)\n\n\nApplications: - Damped sinusoids: Responses of RLC circuits, automotive suspension systems, and mechanical systems with both damping and restoring forces. These indicate energy dissipation with oscillations that decay in time. Modeling transient behavior in control systems.\n\nThis general form is extremely powerful for modeling real-world physical systems behavior, particularly the homogeneous solutions to linear differential equations with constant coefficients. r is the damping factor or growth rate, and \\(\\omega_0\\) is the oscillating frequency. For example, a car’s suspension system responding to a bump (\\(r&lt;0\\)) or the oscillations of a mass-spring-damper system. In electrical circuits, r relates to the resistance and \\(\\omega_0\\) to the inductance and capacitance, governing the natural modes of the system."
  },
  {
    "objectID": "ss_13.html#demo-continuous-time-dampedgrowing-sinusoid",
    "href": "ss_13.html#demo-continuous-time-dampedgrowing-sinusoid",
    "title": "Signal and Systems",
    "section": "Demo: Continuous-Time Damped/Growing Sinusoid",
    "text": "Demo: Continuous-Time Damped/Growing Sinusoid\nVisualize the real part of a general complex exponential \\(C e^{at}\\) as a damped or growing sinusoid.\n\n\n\n\n\n\n\nExperiment by changing values in the code: - r_val_default (decay/growth factor): - &lt; 0: The signal is a damped sinusoid (e.g., ringing in an RLC circuit after a disturbance). Oscillations decrease in amplitude. - = 0: The signal is a pure sinusoid (constant amplitude). - &gt; 0: The signal is a growing sinusoid. Oscillations increase in amplitude (e.g., unstable system). - C_magnitude_default (magnitude): Scales the overall initial amplitude. - omega0_val_default (angular frequency): Changes how fast the signal oscillates. - phi_val_deg_default (phase): Shifts the starting point of the oscillation. Observe how the black dashed lines (the envelope) define the boundaries of the sine wave’s peaks and troughs, visually demonstrating the \\(|C|e^{rt}\\) exponential behavior."
  },
  {
    "objectID": "ss_13.html#discrete-time-complex-exponential-sinusoidal-signals",
    "href": "ss_13.html#discrete-time-complex-exponential-sinusoidal-signals",
    "title": "Signal and Systems",
    "section": "Discrete-Time Complex Exponential & Sinusoidal Signals",
    "text": "Discrete-Time Complex Exponential & Sinusoidal Signals\nReal Exponential Signals\n\nForm: \\(x[n] = C \\alpha^n\\) (where \\(C\\) and \\(\\alpha\\) are real numbers). Sometimes expressed as \\(x[n] = C e^{\\beta n}\\), where \\(\\alpha = e^\\beta\\).\nUnlike continuous time, \\(n\\) is an integer here, so \\(\\alpha^n\\) can be interpreted directly for negative \\(\\alpha\\)."
  },
  {
    "objectID": "ss_13.html#discrete-time-complex-exponential-sinusoidal-signals-1",
    "href": "ss_13.html#discrete-time-complex-exponential-sinusoidal-signals-1",
    "title": "Signal and Systems",
    "section": "Discrete-Time Complex Exponential & Sinusoidal Signals",
    "text": "Discrete-Time Complex Exponential & Sinusoidal Signals\n\nBehavior types (Figure 1.24):\n\n\n\n\n\n\\(|\\alpha|&gt;1\\): Growing exponential (magnitudes increase).\n\n\n\n\\(0&lt;|\\alpha|&lt;1\\): Decaying exponential (magnitudes decrease).\n\n\n\n\n\\(-1 &lt; \\alpha &lt; 0\\): Decaying, but with alternating sign at each step.\n\n\n\n\\(\\alpha &lt; -1\\): Growing, with alternating sign."
  },
  {
    "objectID": "ss_13.html#discrete-time-complex-exponential-sinusoidal-signals-2",
    "href": "ss_13.html#discrete-time-complex-exponential-sinusoidal-signals-2",
    "title": "Signal and Systems",
    "section": "Discrete-Time Complex Exponential & Sinusoidal Signals",
    "text": "Discrete-Time Complex Exponential & Sinusoidal Signals\n\nSpecial Cases:\n\n\\(\\alpha=1 \\implies x[n]=C\\) (constant DC signal).\n\\(\\alpha=-1 \\implies x[n]=C(-1)^n\\) (alternates between \\(C\\) and \\(-C\\), a high-frequency square wave).\n\n\n\nThe discrete-time real exponentials are widely used in finance (compounding interest on a discrete basis) or population dynamics (growth/decay per generation). The alternating sign when \\(\\alpha\\) is negative intros oscillation at the highest possible frequency in discrete time (Nyquist frequency) for a real signal. This is due to the inherent digital nature where a signal can “flip” sign at each sample. E.g., a stock price observed daily."
  },
  {
    "objectID": "ss_13.html#demo-discrete-time-real-exponential",
    "href": "ss_13.html#demo-discrete-time-real-exponential",
    "title": "Signal and Systems",
    "section": "Demo: Discrete-Time Real Exponential",
    "text": "Demo: Discrete-Time Real Exponential\nExplore how changing C and alpha affect a discrete-time real exponential.\n\n\n\n\n\n\n\nExperiment by changing values in the code: - alpha_val_default_dt: - |alpha_val_default_dt| &gt; 1: The signal grows rapidly. - 0 &lt; |alpha_val_default_dt| &lt; 1: The signal decays. - alpha_val_default_dt &gt; 0: The sign of \\(x[n]\\) remains consistent with C_val_default_dt. This is like a smoothly changing quantity. - alpha_val_default_dt &lt; 0: The sign of \\(x[n]\\) alternates with each step in \\(n\\), creating an oscillatory appearance. This is a crucial distinction from continuous-time and represents high-frequency content. - C_val_default_dt: Scales the signal vertically. Check the behavior for special cases like \\(\\alpha = 1\\), \\(\\alpha = -1\\), and \\(\\alpha = 0.5\\), \\(\\alpha = -0.5\\)."
  },
  {
    "objectID": "ss_13.html#discrete-time-sinusoidal-signals",
    "href": "ss_13.html#discrete-time-sinusoidal-signals",
    "title": "Signal and Systems",
    "section": "Discrete-Time Sinusoidal Signals",
    "text": "Discrete-Time Sinusoidal Signals\n\nComplex Exponential Forms (where \\(\\alpha = e^{j\\omega_0}\\) or \\(\\beta = j\\omega_0\\)):\n\n\\(x[n] = e^{j \\omega_0 n}\\)\n\nSinusoidal Form:\n\n\\(x[n] = A \\cos(\\omega_0 n + \\phi)\\)\n\\(\\omega_0\\): discrete-time angular frequency (radians).\n\nEuler’s Relation (same arithmetic form as CT): \\[\ne^{j \\omega_0 n}=\\cos \\omega_0 n+j \\sin \\omega_0 n \\tag{1.48}\n\\] \\[\nA \\cos (\\omega_0 n+\\phi)=\\frac{A}{2} e^{j \\phi} e^{j \\omega_0 n}+\\frac{A}{2} e^{-j \\phi} e^{-j \\omega_0 n} \\tag{1.49}\n\\]"
  },
  {
    "objectID": "ss_13.html#discrete-time-sinusoidal-signals-1",
    "href": "ss_13.html#discrete-time-sinusoidal-signals-1",
    "title": "Signal and Systems",
    "section": "Discrete-Time Sinusoidal Signals",
    "text": "Discrete-Time Sinusoidal Signals\n\nLike continuous-time, these signals have infinite total energy but finite average power (e.g., \\(P_{\\text{avg}}=1\\) for \\(e^{j \\omega_0 n}\\)).\n\n\n\n\nFigure 1.25: Discrete-time sinusoidal signals.\n\nDiscrete-time sinusoids are prevalent in digital signal processing (DSP), such as digital audio synthesis, image processing algorithms, and digital communications systems. Their properties are slightly different from CT sinusoids, especially concerning periodicity and frequency uniqueness, which we will discuss next. Visually, they are simply samples taken from an underlying continuous-time sinusoid."
  },
  {
    "objectID": "ss_13.html#discrete-time-general-complex-exponential-signals",
    "href": "ss_13.html#discrete-time-general-complex-exponential-signals",
    "title": "Signal and Systems",
    "section": "Discrete-Time General Complex Exponential Signals",
    "text": "Discrete-Time General Complex Exponential Signals\nIf \\(C = |C|e^{j\\theta}\\) (amplitude) and \\(\\alpha = |\\alpha|e^{j\\omega_0}\\) (rate of change + frequency): \\[\nC \\alpha^n = |C| |\\alpha|^n \\cos (\\omega_0 n+\\theta)+j|C| |\\alpha|^n \\sin (\\omega_0 n+\\theta) \\tag{1.50}\n\\]\n\n\\(|\\alpha|=1\\): Purely sinusoidal samples (constant amplitude oscillations).\n\\(|\\alpha|&lt;1\\): Decaying sinusoidal samples (damped oscillations, e.g., digital filter responses).\n\\(|\\alpha|&gt;1\\): Growing sinusoidal samples (unstable system responses)."
  },
  {
    "objectID": "ss_13.html#discrete-time-general-complex-exponential-signals-1",
    "href": "ss_13.html#discrete-time-general-complex-exponential-signals-1",
    "title": "Signal and Systems",
    "section": "Discrete-Time General Complex Exponential Signals",
    "text": "Discrete-Time General Complex Exponential Signals\n\nFigure 1.26: (a) Growing discrete-time sinusoidal signals; (b) decaying discrete-time sinusoid.\n\nSimilar to continuous time, these complex exponentials serve as general solutions to discrete-time linear difference equations with constant coefficients, which are mathematical models for many discrete-time systems (e.g., digital filters). The \\(|\\alpha|\\) term determines the envelope’s growth or decay, and \\(\\omega_0\\) determines the oscillation frequency. Understanding these is key to stability analysis in digital filters and control systems."
  },
  {
    "objectID": "ss_13.html#periodicity-frequency-differences-discrete-time-vs.-continuous-time",
    "href": "ss_13.html#periodicity-frequency-differences-discrete-time-vs.-continuous-time",
    "title": "Signal and Systems",
    "section": "Periodicity & Frequency Differences (Discrete-Time vs. Continuous-Time)",
    "text": "Periodicity & Frequency Differences (Discrete-Time vs. Continuous-Time)\nA key distinction between discrete-time and continuous-time complex exponentials that impacts frequency analysis.\n\n\nContinuous-Time \\(e^{j \\omega_0 t}\\):\n\nDistinct Frequencies: All signals \\(e^{j \\omega_0 t}\\) are distinct for distinct values of \\(\\omega_0\\). An infinite number of unique frequencies.\n\n\nDiscrete-Time \\(e^{j \\omega_0 n}\\):\n\nNon-Distinct Frequencies (Aliasing): Frequencies separated by multiples of \\(2\\pi\\) are identical. \\[\ne^{j(\\omega_0+2\\pi)n} = e^{j\\omega_0 n} \\tag{1.51}\n\\]\n\nOnly need to consider \\(\\omega_0\\) in an interval of length \\(2\\pi\\) (e.g., \\([0, 2\\pi)\\) or \\((-\\pi, \\pi]\\)). All frequencies outside this interval are “aliases” of frequencies within it.\n\n\n\n\nThis slide is critical. The “folding” or “aliasing” of frequencies in discrete-time is a very important concept with practical implications, directly related to the Nyquist-Shannon sampling theorem. Explain that a high frequency like \\(\\omega_0 = 1.9\\pi\\) looks very similar to a low frequency like \\(\\omega_0 = -0.1\\pi\\) (which is effectively equivalent to \\(0.1\\pi\\) in terms of oscillation rate but reflected). The fastest oscillation is always at \\(\\omega_0 = \\pi\\). This difference arises because sampling can cause higher frequencies to appear as lower frequencies."
  },
  {
    "objectID": "ss_13.html#periodicity-frequency-differences-discrete-time-vs.-continuous-time-1",
    "href": "ss_13.html#periodicity-frequency-differences-discrete-time-vs.-continuous-time-1",
    "title": "Signal and Systems",
    "section": "Periodicity & Frequency Differences (Discrete-Time vs. Continuous-Time)",
    "text": "Periodicity & Frequency Differences (Discrete-Time vs. Continuous-Time)\nA key distinction between discrete-time and continuous-time complex exponentials that impacts frequency analysis.\n\n\nContinuous-Time \\(e^{j \\omega_0 t}\\):\n\nPeriodicity: Periodic for any value of \\(\\omega_0\\) (except \\(\\omega_0=0\\) where period is undefined but the signal is constant).\n\n\\(T_0 = 2\\pi/|\\omega_0|\\) (for \\(\\omega_0 \\ne 0\\)).\n\n\n\nDiscrete-Time \\(e^{j \\omega_0 n}\\):\n\nConditional Periodicity: Periodic only if \\(\\omega_0 / (2\\pi)\\) is a rational number.\n\n\\(\\omega_0 N = 2\\pi m \\implies \\frac{\\omega_0}{2\\pi} = \\frac{m}{N}\\) (for coprime integers \\(m, N&gt;0\\)).\nFundamental Period \\(N = m(2\\pi/\\omega_0)\\) (if \\(m,N\\) are coprime).\nIf \\(\\omega_0 / (2\\pi)\\) is irrational (e.g., \\(\\omega_0 = 1\\)), the signal is aperiodic.\n\\(T_0 = 2\\pi/|\\omega_0|\\) (for \\(\\omega_0 \\ne 0\\))."
  },
  {
    "objectID": "ss_13.html#periodicity-frequency-differences-discrete-time-vs.-continuous-time-2",
    "href": "ss_13.html#periodicity-frequency-differences-discrete-time-vs.-continuous-time-2",
    "title": "Signal and Systems",
    "section": "Periodicity & Frequency Differences (Discrete-Time vs. Continuous-Time)",
    "text": "Periodicity & Frequency Differences (Discrete-Time vs. Continuous-Time)\nA key distinction between discrete-time and continuous-time complex exponentials that impacts frequency analysis.\n\n\nContinuous-Time \\(e^{j \\omega_0 t}\\):\n\nFrequency Rate: Increasing \\(|\\omega_0|\\) always increases the rate of oscillation. No upper bound on frequency.\n\n\nDiscrete-Time \\(e^{j \\omega_0 n}\\):\n\nFrequency Rate: Rate of oscillation increases as \\(\\omega_0\\) goes from \\(0\\) to \\(\\pi\\), then decreases from \\(\\pi\\) to \\(2\\pi\\) (or \\(-\\pi\\)).\n\n\\(\\omega_0 = 0, 2\\pi, \\ldots\\) are “DC” or slowest.\n\\(\\omega_0 = \\pm \\pi, \\pm 3\\pi, \\ldots\\) are “fastest” (\\(e^{j \\pi n} = (-1)^n\\), alternating sample by sample)."
  },
  {
    "objectID": "ss_13.html#demo-discrete-time-frequency-behavior",
    "href": "ss_13.html#demo-discrete-time-frequency-behavior",
    "title": "Signal and Systems",
    "section": "Demo: Discrete-Time Frequency Behavior",
    "text": "Demo: Discrete-Time Frequency Behavior\nObserve unique frequency behavior in discrete-time sinusoids due to the sampling process.\n\n\n\n\n\n\n\nExperiment by changing omega0_val_default_dt_freq (angular frequency) in the code: - omega0_val_default_dt_freq near 0 (e.g., 0.1 or 0.2): Very slow oscillation, almost a constant (DC). - Increase omega0_val_default_dt_freq towards π (approximately 3.14): The oscillation rate increases. The samples become more spread out relative to the number of cycles within a range. - omega0_val_default_dt_freq = π: The signal becomes \\(cos(\\pi n) = (-1)^n\\), which is the fastest possible oscillation in discrete time (alternating between 1 and -1 at each sample). This is the Nyquist frequency. - Increase omega0_val_default_dt_freq from π towards 2π (approximately 6.28): The oscillation rate decreases again (it starts to “fold back”). For example, ω_0 = 1.9π will look identical to a signal with frequency -0.1π (which is effectively seen as 0.1π). - omega0_val_default_dt_freq = 2π (or 0): The signal becomes \\(cos(2\\pi n) = \\cos(0) = 1\\), which is a constant DC signal. This demonstrates how frequencies greater than \\(\\pi\\) are effectively “aliased” to lower frequencies within the fundamental range \\([0, 2\\pi)\\)."
  },
  {
    "objectID": "ss_13.html#example-1.6-fundamental-period-of-sum-of-dt-exponentials",
    "href": "ss_13.html#example-1.6-fundamental-period-of-sum-of-dt-exponentials",
    "title": "Signal and Systems",
    "section": "Example 1.6: Fundamental Period of Sum of DT Exponentials",
    "text": "Example 1.6: Fundamental Period of Sum of DT Exponentials\nProblem: Determine the fundamental period of the discrete-time signal \\(x[n]=e^{j(2 \\pi / 3) n}+e^{j(3 \\pi / 4) n}\\).\nStep-by-Step Solution:\n\nAnalyze the first term: \\(x_1[n] = e^{j(2 \\pi / 3) n}\\)\n\nFor \\(x_1[n]\\) to be periodic with period \\(N_1\\), we need \\(e^{j(2\\pi/3)(n+N_1)} = e^{j(2\\pi/3)n}\\).\nThis implies \\(e^{j(2\\pi/3)N_1} = 1\\).\nSo, \\((2\\pi/3)N_1\\) must be an integer multiple of \\(2\\pi\\).\n\\((2\\pi/3)N_1 = 2\\pi k_1 \\Rightarrow N_1/3 = k_1\\).\nThe smallest positive integer \\(N_1\\) occurs when \\(k_1=1\\), so \\(N_1=\\mathbf{3}\\)."
  },
  {
    "objectID": "ss_13.html#example-1.6-fundamental-period-of-sum-of-dt-exponentials-1",
    "href": "ss_13.html#example-1.6-fundamental-period-of-sum-of-dt-exponentials-1",
    "title": "Signal and Systems",
    "section": "Example 1.6: Fundamental Period of Sum of DT Exponentials",
    "text": "Example 1.6: Fundamental Period of Sum of DT Exponentials\n\nAnalyze the second term: \\(x_2[n] = e^{j(3 \\pi / 4) n}\\)\n\nSimilarly, we need \\(e^{j(3\\pi/4)N_2} = 1\\).\nSo, \\((3\\pi/4)N_2 = 2\\pi k_2 \\Rightarrow (3/4)N_2 = 2 k_2\\).\n\\(N_2 = (8/3) k_2\\). For \\(N_2\\) to be an integer, \\(k_2\\) must be a multiple of 3.\nThe smallest positive integer \\(N_2\\) occurs when \\(k_2=3\\), so \\(N_2 = (8/3) \\times 3 = \\mathbf{8}\\).\n\nFind the overall fundamental period: For the entire signal \\(x[n]\\) to repeat, both \\(x_1[n]\\) and \\(x_2[n]\\) must complete an integer number of their respective fundamental periods simultaneously.\n\nThis means the overall period of \\(x[n]\\) must be a common multiple of \\(N_1=3\\) and \\(N_2=8\\).\nThe fundamental period is the Least Common Multiple (LCM) of \\(N_1\\) and \\(N_2\\).\nLCM(3, 8) = \\(\\mathbf{24}\\).\n\n\nConclusion: The fundamental period of \\(x[n]\\) is \\(\\mathbf{24}\\).\n\nThis is a standard problem type when dealing with discrete-time periodic signals. Emphasize that for a sum of periodic discrete-time signals, the overall signal is periodic if and only if all individual signals are periodic. Its fundamental period is the LCM of their individual fundamental periods, assuming there isn’t a cancellation effect (e.g., if one signal is the negative of another after some shift). This understanding is crucial for designing periodic sequences in digital systems."
  },
  {
    "objectID": "ss_13.html#harmonically-related-discrete-time-complex-exponentials",
    "href": "ss_13.html#harmonically-related-discrete-time-complex-exponentials",
    "title": "Signal and Systems",
    "section": "Harmonically Related Discrete-Time Complex Exponentials",
    "text": "Harmonically Related Discrete-Time Complex Exponentials\n\nSet of periodic exponentials with a common period \\(N\\): \\[\n\\phi_k[n]=e^{j k(2 \\pi / N) n}, \\quad k=0, \\pm 1, \\pm 2, \\ldots \\tag{1.60}\n\\]\nCrucial Difference from CT (due to frequency aliasing): Unlike continuous time, these are not all distinct. \\[\n\\phi_{k+N}[n] = e^{j(k+N)(2\\pi/N)n} = e^{j k(2\\pi/N)n} e^{j 2\\pi n} = \\phi_k[n] \\tag{1.61}\n\\] Since \\(e^{j 2\\pi n} = (e^{j 2\\pi})^n = 1^n = 1\\).\nTherefore, there are only N distinct periodic exponentials in this set: \\[\n\\phi_0[n], \\phi_1[n], \\ldots, \\phi_{N-1}[n] \\tag{1.62}\n\\] Any other \\(\\phi_k[n]\\) is identical to one of these (e.g., \\(\\phi_N[n]=\\phi_0[n]\\), \\(\\phi_{-1}[n]=\\phi_{N-1}[n]\\)).\n\n\nThis property is fundamental to the Discrete Fourier Transform (DFT) and Discrete Fourier Series (DFS). It means that when analyzing discrete-time signals over a finite period \\(N\\), there are only a finite number (\\(N\\)) of unique frequencies or harmonics you can extract. This is a direct consequence of the sampling process inherent in discrete-time systems. It implies that information about frequencies beyond the Nyquist frequency is lost or “folded” down."
  },
  {
    "objectID": "ss_13.html#comparison-summary-ej-omega_0-t-vs.-ej-omega_0-n",
    "href": "ss_13.html#comparison-summary-ej-omega_0-t-vs.-ej-omega_0-n",
    "title": "Signal and Systems",
    "section": "Comparison Summary: \\(e^{j \\omega_0 t}\\) vs. \\(e^{j \\omega_0 n}\\)",
    "text": "Comparison Summary: \\(e^{j \\omega_0 t}\\) vs. \\(e^{j \\omega_0 n}\\)\n\n\n\n\n\n\n\n\nFeature\nContinuous-Time (\\(e^{j \\omega_0 t}\\))\nDiscrete-Time (\\(e^{j \\omega_0 n}\\))\n\n\n\n\nDistinctness of Freqs\nDistinct signals for distinct values of \\(\\omega_0\\). Infinite unique frequencies.\nIdentical signals for values of \\(\\omega_0\\) separated by multiples of \\(2\\pi\\). Only \\(2\\pi\\) range of unique frequencies.\n\n\nPeriodicity\nPeriodic for any choice of \\(\\omega_0\\) (except \\(\\omega_0=0\\)).\nPeriodic only if \\(\\omega_0 = 2\\pi m / N\\) for some positive integers \\(N\\) and \\(m\\).\n\n\nFundamental Frequency\n\\(\\omega_0\\) (for \\(\\omega_0 \\ne 0\\))\n\\(\\omega_0 / m\\) (for \\(\\omega_0 \\ne 0\\), assuming \\(m,N\\) are coprime).\n\n\nFundamental Period\n\\(\\omega_0=0\\): undefined\\(\\omega_0 \\ne 0: \\frac{2\\pi}{|\\omega_0|}\\)\n\\(\\omega_0=0\\): undefined\\(\\omega_0 \\ne 0: N = m\\left(\\frac{2\\pi}{\\omega_0}\\right)\\) (if \\(m,N\\) coprime)\n\n\nHighest Oscillation Rate\nAs \\(\\omega_0 \\rightarrow \\pm \\infty\\)\nAt \\(\\omega_0 = \\pi\\) (or odd multiples of \\(\\pi\\))\n\n\nLowest Oscillation Rate\nAt \\(\\omega_0 = 0\\)\nAt \\(\\omega_0 = 0\\) (or even multiples of \\(2\\pi\\))\n\n\n\n\nReview this table thoroughly. It synthesizes the most important differences and highlights why Discrete-Time Signal Processing requires careful consideration of frequency ranges and periodicity, topics that are less complex in continuous time. This table is essentially a cheat sheet for avoiding common misconceptions when transitioning from continuous to discrete domain, especially concerning frequency."
  },
  {
    "objectID": "ss_13.html#conclusion",
    "href": "ss_13.html#conclusion",
    "title": "Signal and Systems",
    "section": "Conclusion",
    "text": "Conclusion\nWe’ve introduced fundamental continuous-time and discrete-time signals:\n\nExponential Signals:\n\nReal exponentials: excellent models for growth or decay phenomena.\nComplex exponentials: fundamental “building blocks” related to rotation, basis for all sinusoids.\nGeneral complex exponentials: model damped or growing oscillatory responses, common in LTI systems.\n\nSinusoidal Signals:\n\nRepresent pure, stable oscillations, deeply connected to complex exponentials via Euler’s relation.\nUbiquitous in nature and engineering (AC circuits, mechanical vibrations, wave propagation)."
  },
  {
    "objectID": "ss_13.html#conclusion-1",
    "href": "ss_13.html#conclusion-1",
    "title": "Signal and Systems",
    "section": "Conclusion",
    "text": "Conclusion\nKey Takeaways on Differences:\n\nContinuous-time signals have distinct frequencies across the entire real number line.\nDiscrete-time signals have frequencies that repeat every \\(2\\pi\\) (aliasing).\nDiscrete-time signals are periodic only if their frequency is a rational multiple of \\(2\\pi\\).\n\nThese simple signals are profound building blocks for understanding linearity, time-invariance, and the powerful frequency domain analysis of signals and systems, which we will explore in subsequent chapters.\n\nReiterate the “building blocks” idea and how these signals will appear repeatedly throughout the course, especially when analyzing systems and using Fourier techniques. Emphasize the unique characteristics of discrete-time signals as they will be central in DSP courses. Understanding these basic signals at an intuitive level is paramount for mastering the rest of the Signals and Systems curriculum."
  },
  {
    "objectID": "ss_15.html#what-is-a-system",
    "href": "ss_15.html#what-is-a-system",
    "title": "Signal and Systems",
    "section": "What is a System?",
    "text": "What is a System?\nA system is a process by which input signals are transformed to produce output signals.\n\n\nConceptual Model:\n\n\n\n\n\ngraph LR\n    A[Input Signal] --&gt; B(System);\n    B --&gt; C[Output Signal];\n\n\n\n\n\n\nExamples:\n\nHi-Fi System: Raw audio \\(\\rightarrow\\) Amplified & Equalized sound\nRC Circuit: Input voltage \\(\\rightarrow\\) Capacitor voltage\nAutomobile: Force applied \\(\\rightarrow\\) Vehicle velocity\nImage Enhancement: Raw image \\(\\rightarrow\\) Improved contrast image\n\n\nKey Idea:\nSystems provide a mathematical framework to model how physical phenomena respond to external stimuli.\nThey allow us to predict, control, and design complex engineering applications.\n\n\nIn the broadest sense, a system is simply anything that takes an input and produces an output. Think of it as a black box where something goes in, is processed, and something else comes out. In electrical and computer engineering, these inputs and outputs are typically signals, and the “processing” is defined by mathematical relationships. We analyze these relationships to understand how a system behaves."
  },
  {
    "objectID": "ss_15.html#systems-examples",
    "href": "ss_15.html#systems-examples",
    "title": "Signal and Systems",
    "section": "Systems (Examples)",
    "text": "Systems (Examples)"
  },
  {
    "objectID": "ss_15.html#systems-examples-1",
    "href": "ss_15.html#systems-examples-1",
    "title": "Signal and Systems",
    "section": "Systems (Examples)",
    "text": "Systems (Examples)"
  },
  {
    "objectID": "ss_15.html#continuous-time-ct-vs.-discrete-time-dt-systems",
    "href": "ss_15.html#continuous-time-ct-vs.-discrete-time-dt-systems",
    "title": "Signal and Systems",
    "section": "Continuous-Time (CT) vs. Discrete-Time (DT) Systems",
    "text": "Continuous-Time (CT) vs. Discrete-Time (DT) Systems\nSystems are classified based on the nature of their input and output signals.\n\n\nContinuous-Time Systems\n\nInput \\(x(t)\\) and output \\(y(t)\\) are continuous functions of time.\nRepresented by: \\(x(t) \\rightarrow y(t)\\)\n\nReal-world examples:\n\nAnalog filters\nMechanical systems\nElectrical circuits (e.g., op-amp circuits)\n\n\nDiscrete-Time Systems\n\nInput \\(x[n]\\) and output \\(y[n]\\) are sequences (defined at discrete instants).\nRepresented by: \\(x[n] \\rightarrow y[n]\\)\n\nReal-world examples:\n\nDigital signal processors (DSPs)\nComputer algorithms\nFinancial modeling (e.g., monthly balances)\n\n\n\nThe fundamental distinction lies in how time is handled. Continuous-time systems operate on signals that are defined for all values of time, much like a continuous waveform. Discrete-time systems, on the other hand, operate on signals that are sampled or defined only at specific, discrete points in time. We will study both types in parallel throughout this course, but in Chapter 7, we’ll see how they are connected through the concept of sampling. Many real-world systems, especially in modern engineering, involve both, for example, processing an analog signal using a digital computer."
  },
  {
    "objectID": "ss_15.html#simple-examples-of-systems-continuous-time-12",
    "href": "ss_15.html#simple-examples-of-systems-continuous-time-12",
    "title": "Signal and Systems",
    "section": "Simple Examples of Systems: Continuous-Time (1/2)",
    "text": "Simple Examples of Systems: Continuous-Time (1/2)\nMany diverse physical systems can share the same mathematical description.\nExample 1.8: RC Circuit Voltage\nSystem: An RC circuit where \\(v_s(t)\\) is the input voltage and \\(v_c(t)\\) is the output voltage across the capacitor.\nInput-Output Relationship (Differential Equation): \\[\n\\frac{d v_{c}(t)}{d t}+\\frac{1}{R C} v_{c}(t)=\\frac{1}{R C} v_{s}(t) \\quad \\text{(Equation 1.82)}\n\\]\n\nLet’s look at some basic examples. Consider a simple RC circuit. The input is the source voltage vs(t) and the output is the capacitor voltage vc(t). Using Kirchhoff’s laws and the constitutive relations for resistors and capacitors, we can derive a first-order differential equation that describes how the output voltage vc(t) changes in response to the input voltage vs(t). This equation captures the dynamics of the circuit."
  },
  {
    "objectID": "ss_15.html#simple-examples-of-systems-continuous-time-22",
    "href": "ss_15.html#simple-examples-of-systems-continuous-time-22",
    "title": "Signal and Systems",
    "section": "Simple Examples of Systems: Continuous-Time (2/2)",
    "text": "Simple Examples of Systems: Continuous-Time (2/2)\nExample 1.9: Automobile Velocity\nSystem: An automobile where \\(f(t)\\) is the input force and \\(v(t)\\) is the output velocity. (Assumes mass \\(m\\) and friction \\(\\rho v\\)).\nInput-Output Relationship (Differential Equation): \\[\n\\frac{d v(t)}{d t}+\\frac{\\rho}{m} v(t)=\\frac{1}{m} f(t) \\quad \\text{(Equation 1.84)}\n\\]"
  },
  {
    "objectID": "ss_15.html#simple-examples-of-systems-continuous-time-22-1",
    "href": "ss_15.html#simple-examples-of-systems-continuous-time-22-1",
    "title": "Signal and Systems",
    "section": "Simple Examples of Systems: Continuous-Time (2/2)",
    "text": "Simple Examples of Systems: Continuous-Time (2/2)\nComparison: Both Example 1.8 (RC circuit) and 1.9 (automobile) are described by the same general first-order linear differential equation:\n\\[\n\\frac{d y(t)}{d t}+a y(t)=b x(t) \\quad \\text{(Equation 1.85)}\n\\]\n\n\\(y(t)\\): output signal, \\(x(t)\\): input signal.\n\\(a\\), \\(b\\): constants derived from system parameters.\n\n\nNow, let’s consider a completely different physical system: an automobile. If we model the applied force f(t) as the input and the vehicle’s velocity v(t) as the output, and account for mass and a linear approximation for frictional resistance, Newton’s second law leads us to another first-order differential equation.\nWhat’s striking is that this equation has the exact same mathematical form as the RC circuit equation. This demonstrates a powerful concept in systems analysis: seemingly different physical systems can be described by the same mathematical models. Developing tools to analyze a general class of systems, like first-order linear differential equations, allows us to apply those tools across a wide variety of engineering disciplines."
  },
  {
    "objectID": "ss_15.html#simple-examples-of-systems-discrete-time-12",
    "href": "ss_15.html#simple-examples-of-systems-discrete-time-12",
    "title": "Signal and Systems",
    "section": "Simple Examples of Systems: Discrete-Time (1/2)",
    "text": "Simple Examples of Systems: Discrete-Time (1/2)\nExample 1.10: Bank Account Balance\nSystem: A bank account where \\(x[n]\\) is the net deposit in month \\(n\\), and \\(y[n]\\) is the balance at the end of month \\(n\\). (Assumes 1% interest per month).\nInput-Output Relationship (Difference Equation): \\[\ny[n] = 1.01 y[n-1] + x[n] \\quad \\text{(Equation 1.86)}\n\\] \\[\ny[n] - 1.01 y[n-1] = x[n] \\quad \\text{(Equation 1.87)}\n\\]"
  },
  {
    "objectID": "ss_15.html#simple-examples-of-systems-discrete-time-12-1",
    "href": "ss_15.html#simple-examples-of-systems-discrete-time-12-1",
    "title": "Signal and Systems",
    "section": "Simple Examples of Systems: Discrete-Time (1/2)",
    "text": "Simple Examples of Systems: Discrete-Time (1/2)\nExample 1.10: Bank Account Balance\nInteractive Simulation: Assume initial balance \\(y[-1]=1000\\) and a monthly deposit \\(x[n]=100\\) for \\(n \\ge 0\\). The system calculates balance \\(y[n]\\) based on previous month’s balance and current deposit.\n\n\n\n\n\n\n\nDiscrete-time systems are modeled using difference equations. A classic example is tracking a bank account balance. Here, the current month’s balance, y[n], depends on the previous month’s balance, y[n-1], augmented by interest, and the current month’s net deposit, x[n]. This difference equation captures the temporal evolution of the balance. The interactive plot shows how the balance grows over time with regular deposits and compounding interest."
  },
  {
    "objectID": "ss_15.html#simple-examples-of-systems-discrete-time-22",
    "href": "ss_15.html#simple-examples-of-systems-discrete-time-22",
    "title": "Signal and Systems",
    "section": "Simple Examples of Systems: Discrete-Time (2/2)",
    "text": "Simple Examples of Systems: Discrete-Time (2/2)\nExample 1.11: Digital Simulation of Automobile Model\nSystem: A discrete-time approximation of the continuous-time automobile model from Example 1.9. (Approximates \\(\\frac{dv(t)}{dt}\\) with backward difference \\(\\frac{v(n\\Delta)-v((n-1)\\Delta)}{\\Delta}\\)).\nInput-Output Relationship (Difference Equation): \\[\nv[n]-\\frac{m}{(m+\\rho \\Delta)} v[n-1]=\\frac{\\Delta}{(m+\\rho \\Delta)} f[n] \\quad \\text{(Equation 1.88)}\n\\]\n\n\\(v[n]\\): sampled velocity, \\(f[n]\\): sampled force.\n\\(\\Delta\\): time step."
  },
  {
    "objectID": "ss_15.html#simple-examples-of-systems-discrete-time-22-1",
    "href": "ss_15.html#simple-examples-of-systems-discrete-time-22-1",
    "title": "Signal and Systems",
    "section": "Simple Examples of Systems: Discrete-Time (2/2)",
    "text": "Simple Examples of Systems: Discrete-Time (2/2)\nComparison: Both Example 1.10 (bank account) and 1.11 (digital simulation) are described by the same general first-order linear difference equation:\n\\[\ny[n]+a y[n-1]=b x[n] \\quad \\text{(Equation 1.89)}\n\\]\n\n\\(y[n]\\): output sequence, \\(x[n]\\): input sequence.\n\\(a\\), \\(b\\): constants derived from system parameters or approximation schemes.\n\n\nJust as with continuous-time systems, discrete-time systems from different applications can share the same mathematical form. Here, we see a digital simulation of the automobile model, where the continuous-time derivative is approximated by a discrete-time difference. This gives us a first-order linear difference equation. Comparing it to the bank account example, they both fit the same general form. This highlights the power of abstraction in signals and systems: by understanding general forms of equations, we can analyze countless real-world systems, whether they are inherently discrete or are continuous systems approximated for digital processing."
  },
  {
    "objectID": "ss_15.html#interconnections-of-systems",
    "href": "ss_15.html#interconnections-of-systems",
    "title": "Signal and Systems",
    "section": "1.5.2 Interconnections of Systems",
    "text": "1.5.2 Interconnections of Systems\nComplex systems are often built by interconnecting simpler subsystems.\n\nBenefit: Analyze complex systems by understanding their components and their connections.\nApplication: Design and synthesize new systems from basic building blocks."
  },
  {
    "objectID": "ss_15.html#interconnections-of-systems-1",
    "href": "ss_15.html#interconnections-of-systems-1",
    "title": "Signal and Systems",
    "section": "1.5.2 Interconnections of Systems",
    "text": "1.5.2 Interconnections of Systems\n\n\nReal-World Application: Audio System\n\n\n\n\n\ngraph TD\n    A[Radio Receiver] --&gt; B(Amplifier);\n    B --&gt; C[Speakers];\n\n\n\n\n\n\nAn audio system cascaded for playback.\n\nReal-World Application: Digitally Controlled Aircraft\n\n\n\n\n\ngraph TD\n    Pilot[Pilot Commands] --&gt; Autopilot(Digital Autopilot);\n    Sensors --&gt; Autopilot;\n    Autopilot --&gt; Actuators(Aircraft Actuators);\n    Actuators --&gt; Aircraft(Aircraft);\n    Aircraft --&gt; Sensors[\"Sensors (Velocity)\"];\n    Aircraft --&gt; PilotsDisplay(Pilot Display);\n    Autopilot --Desired--&gt; Aircraft;\n    Aircraft --Actual--&gt; Autopilot;\n\n    style Autopilot fill:#f9f\n    style Aircraft fill:#cf9\n    style Sensors fill:#9fc\n    style Actuators fill:#c9f\n\n\n\n\n\n\nA feedback system structure.\n\n\nMost real-world engineering systems aren’t just one simple block. They are complex structures made by connecting several simpler components. Understanding how systems are interconnected is crucial for both analysis and design. By breaking down a large system into its component parts, we can apply our knowledge of the simpler subsystems and understand the overall behavior. This modular approach is fundamental in system engineering."
  },
  {
    "objectID": "ss_15.html#basic-interconnections-series-cascade",
    "href": "ss_15.html#basic-interconnections-series-cascade",
    "title": "Signal and Systems",
    "section": "Basic Interconnections: Series (Cascade)",
    "text": "Basic Interconnections: Series (Cascade)\nIn a series or cascade interconnection, the output of one system becomes the input to the next.\n\n\n\n\n\ngraph LR\n    input(Input) --&gt; Sys1(System 1);\n    Sys1 --&gt; Sys2(System 2);\n    Sys2 --&gt; output(Output);\n\n    subgraph Overall System\n        Sys1\n        Sys2\n    end\n\n\n\n\n\n\n\nDescription: Input signal is processed by System 1, then its output is processed by System 2.\nExample: A radio receiver (System 1) connected to an amplifier (System 2).\nNotation: If System 1 output = \\(y_1\\) and System 2 output = \\(y_2\\), then \\(y_1 = \\text{Sys1}(\\text{input})\\) and \\(y_2 = \\text{Sys2}(y_1)\\).\n\n\nThe series or cascade interconnection is the most straightforward. Imagine a chain: the output of the first link feeds into the second, and so on. In block diagrams, this is represented by arrows connecting blocks sequentially. An example is a signal flowing from your phone to an amplifier, then to speakers. Each component performs a specific function in sequence."
  },
  {
    "objectID": "ss_15.html#basic-interconnections-parallel",
    "href": "ss_15.html#basic-interconnections-parallel",
    "title": "Signal and Systems",
    "section": "Basic Interconnections: Parallel",
    "text": "Basic Interconnections: Parallel\nIn a parallel interconnection, the same input signal is applied to multiple systems, and their outputs are combined (typically summed).\n\n\n\n\n\ngraph LR\n    input(Input) --&gt; Sys1(System 1);\n    input --&gt; Sys2(System 2);\n    Sys1 --&gt; Sum;\n    Sys2 --&gt; Sum;\n    Sum(($\\oplus$)) --&gt; output(Output);\n\n    subgraph Overall System\n        Sys1\n        Sys2\n        Sum\n    end\n\n\n\n\n\n\n\nDescription: Input simultaneously feeds System 1 and System 2. Their individual outputs are added together to form the overall output.\nExample: Multiple microphones (inputs) feeding into a mixing console (summing outputs) then to a single amplifier.\n\n\nIn a parallel interconnection, the input signal is duplicated and sent to two or more systems simultaneously. Their individual outputs are then combined, usually by addition, to form the final output. Think of it like a sound system with multiple microphones all feeding into a single mixer. Each microphone is a “system” producing an output, and the mixer sums these outputs."
  },
  {
    "objectID": "ss_15.html#basic-interconnections-feedback",
    "href": "ss_15.html#basic-interconnections-feedback",
    "title": "Signal and Systems",
    "section": "Basic Interconnections: Feedback",
    "text": "Basic Interconnections: Feedback\nIn a feedback interconnection, the output of a system (or subsystem) is fed back and used to influence its own input.\n\n\n\n\n\ngraph LR\n    input(External Input) --&gt; Summer;\n    System2_out(Output of S2) --&gt; Summer;\n    Summer(($\\oplus$)) --&gt; System1(System 1);\n    System1 --&gt; System2(System 2);\n    System2 --&gt; output(Output);\n    System2_out -- Feedback --&gt; Summer;\n\n    subgraph Overall System\n        Summer\n        System1\n        System2\n    end\n\n\n\n\n\n\n\nDescription: The output of System 2 is added (or subtracted) from the external input, which then becomes the effective input to System 1.\nExamples:\n\nCruise Control: Senses vehicle speed, adjusts engine to maintain desired speed.\nThermostat: Senses room temperature, turns heating/cooling on/off to reach set temperature.\nOperational Amplifiers: Many op-amp circuits use feedback for stability and gain control.\n\n\n\nFeedback is arguably the most powerful and common type of system interconnection. In a feedback system, part of the output is “fed back” to the input, influencing the system’s future behavior. This creates a closed loop. A common household example is a thermostat. It senses the room temperature (output), compares it to the desired temperature (input), and then adjusts the heating or cooling (actuator) to correct any difference. Feedback systems are essential for control, stability, and achieving precise performance in dynamic environments."
  },
  {
    "objectID": "ss_15.html#key-takeaways",
    "href": "ss_15.html#key-takeaways",
    "title": "Signal and Systems",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nSystem Definition\n\nTransforms input signals into output signals.\nCan be continuous-time (\\(x(t) \\rightarrow y(t)\\)) or discrete-time (\\(x[n] \\rightarrow y[n]\\)).\n\nMathematical Models\n\nContinuous-time systems often described by differential equations.\nDiscrete-time systems often described by difference equations.\nMany diverse physical systems share the same mathematical forms (e.g., first-order linear differential/difference equations)."
  },
  {
    "objectID": "ss_15.html#key-takeaways-1",
    "href": "ss_15.html#key-takeaways-1",
    "title": "Signal and Systems",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nSystem Interconnections\n\nComplex systems can be understood and built from simpler subsystems.\nSeries (Cascade): Output of one system feeds the input of another.\nParallel: Same input to multiple systems; outputs are combined.\nFeedback: Output of a system/subsystem is routed back to influence its input, crucial for control and stability.\n\nUnderstanding these concepts is foundational for further study in Signals and Systems.\n\nTo wrap up our discussion on systems: Remember that a system is a fundamental concept where an input signal is transformed into an output signal. We differentiate between continuous-time systems, described by differential equations, and discrete-time systems, described by difference equations. A crucial takeaway is that the mathematical description of a system is often consistent across very different physical phenomena, providing a powerful framework for analysis. Finally, understanding how systems are interconnected—through series, parallel, or feedback arrangements—is key to analyzing complex real-world engineering systems and designing sophisticated solutions. These foundational concepts will be built upon throughout the course as we delve deeper into system properties and analysis techniques."
  },
  {
    "objectID": "ss_21.html#discrete-time-lti-systems-the-convolution-sum",
    "href": "ss_21.html#discrete-time-lti-systems-the-convolution-sum",
    "title": "Signal and Systems",
    "section": "DISCRETE-TIME LTI SYSTEMS: THE CONVOLUTION SUM",
    "text": "DISCRETE-TIME LTI SYSTEMS: THE CONVOLUTION SUM\nECE Undergraduate Course\nImron Rosyadi\n\nWelcome everyone. Today, we’re diving into one of the most fundamental concepts in Signals and Systems: The Convolution Sum. This operation is the key to understanding how Linear Time-Invariant, or LTI, systems work. By the end of this lecture, you’ll be able to represent any discrete-time signal in a new way and use that representation to find the output of any LTI system for any given input."
  },
  {
    "objectID": "ss_21.html#moment-of-silence",
    "href": "ss_21.html#moment-of-silence",
    "title": "Signal and Systems",
    "section": "Moment of Silence",
    "text": "Moment of Silence"
  },
  {
    "objectID": "ss_21.html#what-is-a-system",
    "href": "ss_21.html#what-is-a-system",
    "title": "Signal and Systems",
    "section": "What is a System?",
    "text": "What is a System?\nA system is a process by which input signals are transformed to produce output signals.\n\n\n\n\n\ngraph LR\n    A[Input Signal] --&gt; B(System);\n    B --&gt; C[Output Signal];"
  },
  {
    "objectID": "ss_21.html#signal-decomposition-with-impulses",
    "href": "ss_21.html#signal-decomposition-with-impulses",
    "title": "Signal and Systems",
    "section": "Signal Decomposition with Impulses",
    "text": "Signal Decomposition with Impulses\nAny discrete-time signal \\(x[n]\\) can be represented as a sum of scaled and shifted unit impulses.\nThink of it as breaking down a signal into its most basic building blocks.\nThe “Sifting Property”\nThis decomposition is also known as the sifting property of the unit impulse:\n\\[\nx[n] = \\sum_{k=-\\infty}^{\\infty} x[k] \\delta[n-k]\n\\]\nFor any given \\(n\\), the summation “sifts” through all values of \\(x[k]\\) and picks out only the one where \\(k=n\\)."
  },
  {
    "objectID": "ss_21.html#signal-decomposition-with-impulses-1",
    "href": "ss_21.html#signal-decomposition-with-impulses-1",
    "title": "Signal and Systems",
    "section": "Signal Decomposition with Impulses",
    "text": "Signal Decomposition with Impulses\n\n\n\n\n\ngraph TD\n    subgraph Decomposing x[n]\n        XN(\"x[n]\") --&gt; IMP1(\"x[-1]δ[n+1]\")\n        XN --&gt; IMP0(\"x[0]δ[n]\")\n        XN --&gt; IMP2(\"x[1]δ[n-1]\")\n        XN --&gt; ETC(...)\n    end\n    subgraph Reconstructing x[n]\n        IMP1 --&gt; SUM(\"(&Sigma;)\")\n        IMP0 --&gt; SUM\n        IMP2 --&gt; SUM\n        ETC --&gt; SUM\n        SUM --&gt; XN_OUT(\"x[n]\")\n    end\n\n\n\n\n\n\n\nThe core idea here is incredibly powerful. We take a complex signal, \\(x[n]\\), and express it as a linear combination of the simplest possible signal: the unit impulse, \\(\\delta[n]\\). Each term in the sum, \\(x[k]\\delta[n-k]\\), isolates a single point from the original signal. The weight of each impulse is simply the value of the signal at that point in time. This representation is the foundation for everything that follows."
  },
  {
    "objectID": "ss_21.html#visualizing-decomposition",
    "href": "ss_21.html#visualizing-decomposition",
    "title": "Signal and Systems",
    "section": "Visualizing Decomposition",
    "text": "Visualizing Decomposition\nLet’s see this in action. The signal \\(x[n]\\) is built by summing its individual impulse components.\n\n\n\n\n\n\n\nHere you can see the process graphically, just like in Figure 2.1 from the textbook. The top-left plot shows our original signal, \\(x[n]\\). The next four plots each show a single, scaled impulse. For example, the plot titled \\(x[0]\\delta[n-0]\\) is a signal that’s zero everywhere except at \\(n=0\\), where its value is \\(x[0]=2.0\\). When we add all these simple impulse components together, as shown in the bottom-right plot, we perfectly reconstruct the original signal."
  },
  {
    "objectID": "ss_21.html#lti-systems-the-convolution-sum",
    "href": "ss_21.html#lti-systems-the-convolution-sum",
    "title": "Signal and Systems",
    "section": "LTI Systems & The Convolution Sum",
    "text": "LTI Systems & The Convolution Sum\nHow does an LTI system respond to an input \\(x[n]\\)?\n\nLinearity: The response to a sum of inputs is the sum of the individual responses.\n\nInput: \\(x[n] = \\sum_k x[k] \\delta[n-k]\\)\nOutput: \\(y[n] = \\sum_k x[k] \\cdot \\{\\text{Response to } \\delta[n-k]\\}\\)\n\nTime-Invariance: A shift in the input causes the same shift in the output.\n\nResponse to \\(\\delta[n]\\) is the impulse response, \\(h[n]\\).\nResponse to \\(\\delta[n-k]\\) is just a shifted impulse response, \\(h[n-k]\\).\n\n\nCombining these gives the Convolution Sum:\n\\[\ny[n] = \\sum_{k=-\\infty}^{\\infty} x[k] h[n-k]\n\\]\nWe denote this operation with an asterisk: \\(y[n] = x[n] * h[n]\\).\n\nThis is the central result. Because the system is linear, we can find the output by summing the responses to all the simple impulse components of the input. And because the system is time-invariant, the response to a shifted impulse \\(\\delta[n-k]\\) is just a shifted version of the response to a non-shifted impulse \\(\\delta[n]\\). We call the response to \\(\\delta[n]\\) the “impulse response” and denote it \\(h[n]\\). By putting these two properties together, we arrive at the convolution sum. It tells us that the output \\(y[n]\\) is a weighted sum of shifted versions of the impulse response. The weights are the values of the input signal, \\(x[k]\\)."
  },
  {
    "objectID": "ss_21.html#the-flip-and-slide-method",
    "href": "ss_21.html#the-flip-and-slide-method",
    "title": "Signal and Systems",
    "section": "The “Flip-and-Slide” Method",
    "text": "The “Flip-and-Slide” Method\nThe convolution sum \\(y[n]=\\sum_{k=-\\infty}^{\\infty} x[k] h[n-k]\\) can be computed graphically for each output sample n.\nProcedure for a fixed n:\n\nPlot signals vs. k: Plot the input \\(x[k]\\) and the impulse response \\(h[k]\\).\nFlip: Time-reverse \\(h[k]\\) to get \\(h[-k]\\).\nSlide: Shift \\(h[-k]\\) by \\(n\\) to get \\(h[n-k]\\).\n\nShift right for \\(n &gt; 0\\); left for \\(n &lt; 0\\).\n\nMultiply: Point-wise multiply the sequences \\(x[k]\\) and \\(h[n-k]\\).\nSum: Sum all the values of the product sequence. The result is \\(y[n]\\).\n\nRepeat for all values of n to find the entire output signal \\(y[n]\\).\n\nWhile the formula looks abstract, there’s a very mechanical, graphical way to compute it, which we call the “flip-and-slide” method. The key is to think of n as a fixed value for now. We are trying to compute a single output point, y[n]. The expression involves a sum over the dummy variable k. So, we plot both signals, x and h, as functions of k. The tricky part is the term \\(h[n-k]\\). As a function of k, it’s a flipped and shifted version of the original impulse response. Once you have \\(x[k]\\) and \\(h[n-k]\\) plotted, you just multiply them point by point and add up all the results. That sum gives you the single value \\(y[n]\\). Then you change n and do it all over again."
  },
  {
    "objectID": "ss_21.html#interactive-demo-flip-and-slide",
    "href": "ss_21.html#interactive-demo-flip-and-slide",
    "title": "Signal and Systems",
    "section": "Interactive Demo: Flip-and-Slide",
    "text": "Interactive Demo: Flip-and-Slide\n\n\\(x[n] = 0.5\\delta[n] + 2\\delta[n-1]\\)\n\\(h[n] = u[n] - u[n-3]\\)\n\nUse the slider to change the value of n and observe the convolution process.\n\nviewof n = Inputs.range([-4, 12], {value: 2, step: 1, label: \"Time shift (n)\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive demo brings the flip-and-slide method to life. The top plot shows the stationary input signal \\(x[k]\\) in blue and the flipped and shifted impulse response, \\(h[n-k]\\), in red. As you move the slider for n, you can see the red signal slide. The middle plot shows the point-wise product. The bottom plot shows the final output signal \\(y[n]\\). The cyan triangle shows the value of \\(y[n]\\) for the current n, which is calculated by summing the green stems in the middle plot."
  },
  {
    "objectID": "ss_21.html#example-accumulator-system",
    "href": "ss_21.html#example-accumulator-system",
    "title": "Signal and Systems",
    "section": "Example: Accumulator System",
    "text": "Example: Accumulator System\nLet’s convolve an exponential signal with a unit step. This models a system called an accumulator.\nProblem\n\nInput: \\(x[n] = \\alpha^n u[n]\\), for \\(0 &lt; \\alpha &lt; 1\\).\nImpulse Response: \\(h[n] = u[n]\\).\n\nAnalysis\n\nFor \\(n &lt; 0\\), there’s no overlap between \\(x[k]\\) and \\(h[n-k]\\). So, \\(y[n] = 0\\).\nFor \\(n \\ge 0\\), the overlap is for \\(0 \\le k \\le n\\). \\[\ny[n] = \\sum_{k=0}^{n} \\alpha^k = \\frac{1 - \\alpha^{n+1}}{1 - \\alpha}\n\\] Result: \\(y[n] = \\left(\\frac{1 - \\alpha^{n+1}}{1 - \\alpha}\\right) u[n]\\)."
  },
  {
    "objectID": "ss_21.html#example-accumulator-system-1",
    "href": "ss_21.html#example-accumulator-system-1",
    "title": "Signal and Systems",
    "section": "Example: Accumulator System",
    "text": "Example: Accumulator System\n\n\n\n\n\n\n\nThis is a classic example. The impulse response, \\(h[n]=u[n]\\), defines an accumulator. We feed in a decaying exponential signal. As seen in the derivation, the output is zero for \\(n&lt;0\\) because there is no overlap. For \\(n \\ge 0\\), the sum is a finite geometric series. The plot on the right shows the output starting at \\(y[0]=1\\) and rising to a final value of \\(1/(1-\\alpha)\\)."
  },
  {
    "objectID": "ss_21.html#example-convolving-two-pulses",
    "href": "ss_21.html#example-convolving-two-pulses",
    "title": "Signal and Systems",
    "section": "Example: Convolving Two Pulses",
    "text": "Example: Convolving Two Pulses\nLet’s convolve two finite-length pulses. The output shape and length depend on the inputs.\n\n\\(x[n] = 1\\) for \\(0 \\le n \\le 4\\).\n\\(h[n] = \\alpha^n\\) for \\(0 \\le n \\le 6\\) (with \\(\\alpha &gt; 1\\)).\n\nThe convolution \\(y[n]\\) is non-zero for \\(0 \\le n \\le 10\\), with a trapezoidal shape due to changing overlap.\n\n\n\n\n\n\n\nHere we convolve two pulses of finite length. The key to solving this analytically is to break it down into five regions for n based on the overlap: no overlap, partial increasing overlap, full overlap, partial decreasing overlap, and no overlap again. The result has a trapezoidal shape and its length is \\(N_x + N_h - 1 = 5 + 7 - 1 = 11\\) samples. This length property is general for the convolution of two finite-length sequences."
  },
  {
    "objectID": "ss_21.html#application-digital-audio-reverb",
    "href": "ss_21.html#application-digital-audio-reverb",
    "title": "Signal and Systems",
    "section": "Application: Digital Audio Reverb",
    "text": "Application: Digital Audio Reverb\nConvolution is used in audio engineering to create effects like reverberation (reverb).\n\nInput Signal \\(x[n]\\): A “dry” audio signal (e.g., a single clap).\nImpulse Response \\(h[n]\\): The “room response.” This is what you would record if you made a perfect impulse (like a starter pistol shot) in a concert hall. It captures all the echoes.\nOutput Signal \\(y[n]\\): The “wet” audio signal, with reverb. \\[y[n] = x[n] * h[n]\\]\n\nBy convolving any dry sound with the impulse response of a space, we can make it sound like it was recorded there!"
  },
  {
    "objectID": "ss_21.html#application-digital-audio-reverb-1",
    "href": "ss_21.html#application-digital-audio-reverb-1",
    "title": "Signal and Systems",
    "section": "Application: Digital Audio Reverb",
    "text": "Application: Digital Audio Reverb\n\n\n\n\n\ngraph TD\n    A[\"Dry Audio&lt;br&gt;x[n]\"] --&gt; C{\"Convolution&lt;br&gt;y[n] = x[n]*h[n]\"};\n    B[\"Room Impulse Response&lt;br&gt;h[n]\"] --&gt; C;\n    C --&gt; D[\"Audio with Reverb&lt;br&gt;y[n]\"];\n\n\n\n\n\n\n\nLet’s talk about a fun, real-world application: creating artificial reverb for music and movies. Imagine you record a singer in a perfectly “dead” room with no echoes. This is your dry input signal, \\(x[n]\\). Now, you go to a large cathedral and pop a balloon. The sound you record—a series of echoes that die out—is the impulse response, \\(h[n]\\), of the cathedral. If you convolve the singer’s dry vocal track with the cathedral’s impulse response, the output will sound exactly as if the singer was performing there! This is the principle behind most digital reverb plugins."
  },
  {
    "objectID": "ss_21.html#summary",
    "href": "ss_21.html#summary",
    "title": "Signal and Systems",
    "section": "Summary",
    "text": "Summary\n\nSignal Decomposition: Any discrete signal \\(x[n]\\) can be written as a sum of scaled, shifted impulses: \\(x[n] = \\sum_k x[k]\\delta[n-k]\\).\nLTI System Response: The output \\(y[n]\\) of an LTI system is the input \\(x[n]\\) convolved with the system’s impulse response \\(h[n]\\).\nThe Convolution Sum: This fundamental operation is defined as: \\[ y[n] = x[n] * h[n] = \\sum_{k=-\\infty}^{\\infty} x[k]h[n-k] \\]\nCalculation: We can compute this using the graphical “flip-and-slide” method.\nKey Insight: The impulse response \\(h[n]\\) is a complete characterization of an LTI system. If you know \\(h[n]\\), you know how the system will react to any input.\n\n\nLet’s recap. We started with the idea that any signal can be broken down into impulses. This allowed us to derive the convolution sum, which is the mathematical tool for finding the output of any LTI system. We learned the practical “flip-and-slide” method for computing it. And most importantly, we established that the impulse response, \\(h[n]\\), is the ultimate fingerprint of an LTI system. It tells you everything you need to know about its behavior."
  },
  {
    "objectID": "ss_23.html#properties-of-linear-time-invariant-systems",
    "href": "ss_23.html#properties-of-linear-time-invariant-systems",
    "title": "Signal and Systems",
    "section": "PROPERTIES OF LINEAR TIME-INVARIANT SYSTEMS",
    "text": "PROPERTIES OF LINEAR TIME-INVARIANT SYSTEMS\nECE Undergraduate Course\nImron Rosyadi\n\nIn our last sessions, we derived the convolution sum and integral. These are powerful tools because they show that an LTI system is completely defined by its impulse response. Today, we’ll explore the profound implications of this fact by examining the key properties of LTI systems, such as commutativity, associativity, stability, and causality, all through the lens of the impulse response."
  },
  {
    "objectID": "ss_23.html#the-power-of-the-impulse-response",
    "href": "ss_23.html#the-power-of-the-impulse-response",
    "title": "Signal and Systems",
    "section": "The Power of the Impulse Response",
    "text": "The Power of the Impulse Response\nThe convolution representation shows that an LTI system is completely characterized by its impulse response, \\(h[n]\\) or \\(h(t)\\).\n\nDiscrete-Time: \\(y[n] = x[n] * h[n] = \\sum_{k=-\\infty}^{\\infty} x[k]h[n-k]\\)\nContinuous-Time: \\(y(t) = x(t) * h(t) = \\int_{-\\infty}^{\\infty} x(\\tau)h(t-\\tau)d\\tau\\)\n\nThis is a unique feature of LTI systems. For non-linear systems, the impulse response is not a complete characterization.\nFor example, an LTI system with \\(h[n] = \\delta[n] + \\delta[n-1]\\) is uniquely defined as \\(y[n] = x[n] + x[n-1]\\).\nBut non-linear systems like \\(y[n] = (x[n]+x[n-1])^2\\) or \\(y[n]=\\max(x[n], x[n-1])\\) have the same impulse response, yet behave differently for other inputs.\n\nIt’s crucial to grasp this distinction. For an LTI system, if I give you its impulse response, I’ve told you everything. You can predict its output for any input. This is not true for a non-linear system. Multiple different non-linear systems can share the same impulse response. This is why the properties we’re about to discuss are so special to the LTI world."
  },
  {
    "objectID": "ss_23.html#the-commutative-property",
    "href": "ss_23.html#the-commutative-property",
    "title": "Signal and Systems",
    "section": "The Commutative Property",
    "text": "The Commutative Property\nConvolution is a commutative operation. The order doesn’t matter.\n\\[\nx[n] * h[n] = h[n] * x[n]\n\\] \\[\nx(t) * h(t) = h(t) * x(t)\n\\]\nThis means we can swap the roles of the input and the impulse response."
  },
  {
    "objectID": "ss_23.html#the-commutative-property-1",
    "href": "ss_23.html#the-commutative-property-1",
    "title": "Signal and Systems",
    "section": "The Commutative Property",
    "text": "The Commutative Property\n\n\n\n\n\ngraph TD\n    subgraph \"Standard View\"\n        A[Input: x] --&gt; S1[System: h]\n        S1 --&gt; O1[Output: y]\n    end\n\n\n\n\n\n\n\n\n\n\n\ngraph TD\n    subgraph \"Equivalent View\"\n        B[Input: h] --&gt; S2[System: x]\n        S2 --&gt; O2[Output: y]\n    end\n\n\n\n\n\n\nWhy is this useful? Sometimes flipping and sliding one signal is much easier than the other. Commutativity lets us choose the easier path.\n\nThe commutative property is more than just a mathematical curiosity. It has a practical benefit. When you’re faced with a convolution problem, you have a choice: you can either flip and slide ‘h’ or you can flip and slide ‘x’. Often, one of the signals is much simpler (like a rectangle or an impulse), making the “flip and slide” operation on that signal significantly easier. Commutativity gives you the freedom to choose the simpler calculation."
  },
  {
    "objectID": "ss_23.html#the-distributive-property",
    "href": "ss_23.html#the-distributive-property",
    "title": "Signal and Systems",
    "section": "The Distributive Property",
    "text": "The Distributive Property\nConvolution distributes over addition.\n\\[\nx(t) * [h_1(t) + h_2(t)] = x(t) * h_1(t) + x(t) * h_2(t)\n\\]\nThis means a parallel combination of LTI systems is equivalent to a single system whose impulse response is the sum of the individual impulse responses."
  },
  {
    "objectID": "ss_23.html#the-distributive-property-1",
    "href": "ss_23.html#the-distributive-property-1",
    "title": "Signal and Systems",
    "section": "The Distributive Property",
    "text": "The Distributive Property\n\n\n\n\n\ngraph TD\n    subgraph \"Parallel Systems\"\n        X(x) --&gt; H1(h1);\n        X --&gt; H2(h2);\n        H1 --&gt; A(\"(+)\");\n        H2 --&gt; A;\n        A --&gt; Y(y);\n    end\n\n    subgraph \"is equivalent to\"\n       X2(x) --&gt; H_EQ(h1 + h2);\n       H_EQ --&gt; Y2(y);\n    end\n\n\n\n\n\n\nThis property can simplify complex convolutions by breaking them into simpler parts.\n\nThe distributive property provides the rule for handling parallel systems. If you have an input that feeds into two different LTI systems and their outputs are added together, you can replace that entire parallel structure with a single LTI system. The impulse response of this new equivalent system is simply the sum of the two original impulse responses. We can also use this property in reverse to break down a complicated convolution problem into several simpler ones."
  },
  {
    "objectID": "ss_23.html#distributive-property-in-action",
    "href": "ss_23.html#distributive-property-in-action",
    "title": "Signal and Systems",
    "section": "Distributive Property in Action",
    "text": "Distributive Property in Action\nLet’s convolve \\(x[n] = (\\frac{1}{2})^n u[n] + 2^n u[-n]\\) with the accumulator \\(h[n]=u[n]\\).\nWe can break \\(x[n]\\) into two parts: a right-sided part \\(x_1[n]\\) and a left-sided part \\(x_2[n]\\).\n\\(y[n] = (x_1[n] + x_2[n]) * h[n] = (x_1[n]*h[n]) + (x_2[n]*h[n])\\)\n\n\n\n\n\n\n\nHere’s a concrete example of using the distributive property. The input signal \\(x[n]\\) is “two-sided,” which makes direct convolution tedious. By splitting \\(x[n]\\) into a right-sided exponential \\(x_1\\) and a left-sided exponential \\(x_2\\), we create two simpler problems. We already solved these two separate convolutions in previous examples! The final result is just the sum of those two simpler results, as shown in the green plot on the right."
  },
  {
    "objectID": "ss_23.html#the-associative-property",
    "href": "ss_23.html#the-associative-property",
    "title": "Signal and Systems",
    "section": "The Associative Property",
    "text": "The Associative Property\nConvolution is also an associative operation.\n\\[\n[x(t) * h_1(t)] * h_2(t) = x(t) * [h_1(t) * h_2(t)]\n\\]\nThis means for a cascade (series) of LTI systems, the overall impulse response is the convolution of the individual impulse responses.\n\n\n\n\n\ngraph LR\n    subgraph \"Cascade of Systems\"\n        X(x) --&gt; H1(h1) --&gt; W(w) --&gt; H2(h2) --&gt; Y(y);\n    end\n\n    subgraph \"is equivalent to\"\n        X2(x) --&gt; H_EQ(h1 * h2) --&gt; Y2(y);\n    end\n\n\n\n\n\n\n\nAssociativity gives us the rule for systems in series. When the output of one LTI system becomes the input to another, this is a cascade. The entire cascade can be replaced by a single equivalent LTI system. The impulse response of this new system is found by convolving the impulse responses of the individual systems in the cascade."
  },
  {
    "objectID": "ss_23.html#order-doesnt-matter-for-lti-systems",
    "href": "ss_23.html#order-doesnt-matter-for-lti-systems",
    "title": "Signal and Systems",
    "section": "Order Doesn’t Matter… for LTI systems!",
    "text": "Order Doesn’t Matter… for LTI systems!\nCombining the associative and commutative properties leads to a powerful conclusion:\nThe order of LTI systems in a cascade can be interchanged without changing the overall system response.\n\\(h_1 * h_2 = h_2 * h_1\\)\nThis is a very special characteristic of LTI systems. It does NOT hold for non-linear systems.\n\n\nLTI Systems\n\n\n\n\n\ngraph TD\n    X1(x) --&gt; H1(h1) --&gt; H2(h2) --&gt; Y1(y)\n    Y1 -- yields same y --&gt; Y2\n    X2(x) --&gt; H2b(h2) --&gt; H1b(h1) --&gt; Y2(y)\n\n\n\n\n\n\n\nNon-Linear Counterexample\n\nLet System 1 be squaring\nLet System 2 be multiply by 2\n\n\\(x \\rightarrow [\\text{Square}] \\rightarrow \\times 2 \\rightarrow y = 2x^2\\)\n\\(x \\rightarrow \\times 2 \\rightarrow [\\text{Square}] \\rightarrow y = (2x)^2 = 4x^2\\)\nThe results are different! (\\(2x^2 \\ne 4x^2\\))\n\n\nThis is one of the most important results. Because convolution is both associative and commutative, we can swap the order of LTI systems in a series connection and the final output remains exactly the same. The counterexample on the right shows how quickly this breaks down for non-linear systems. Simply changing the order of a squaring operation and a gain of 2 completely changes the final result. This flexibility is a privilege we only enjoy with LTI systems."
  },
  {
    "objectID": "ss_23.html#system-properties-via-impulse-response",
    "href": "ss_23.html#system-properties-via-impulse-response",
    "title": "Signal and Systems",
    "section": "System Properties via Impulse Response",
    "text": "System Properties via Impulse Response\nWe can determine key system properties directly from \\(h\\).\n\n\n\n\n\n\n\nProperty\nCondition on Impulse Response \\(h\\)\n\n\n\n\nMemoryless\n\\(h[n]=K\\delta[n]\\) or \\(h(t)=K\\delta(t)\\). Non-zero only at the origin.\n\n\nCausal\n\\(h[n]=0\\) for \\(n&lt;0\\) or \\(h(t)=0\\) for \\(t&lt;0\\). Response can’t precede the impulse.\n\n\nStable (BIBO)\nImpulse response must be absolutely summable/integrable. \\(\\sum \\|h[k]\\| &lt; \\infty\\) or \\(\\int \\|h(\\tau)\\|d\\tau &lt; \\infty\\).\n\n\nInvertible\nAn inverse system \\(h_{inv}\\) exists such that \\(h * h_{inv} = \\delta\\).\n\n\n\n\nThis summary table is your cheat sheet. It connects the high-level system properties we’ve discussed—memory, causality, stability, and invertibility—to concrete, testable conditions on the impulse response. By simply inspecting \\(h\\), we can immediately determine these fundamental characteristics of an LTI system."
  },
  {
    "objectID": "ss_23.html#invertibility-of-lti-systems",
    "href": "ss_23.html#invertibility-of-lti-systems",
    "title": "Signal and Systems",
    "section": "Invertibility of LTI Systems",
    "text": "Invertibility of LTI Systems\nAn LTI system is invertible if we can find an inverse system, \\(h_{inv}\\), that perfectly undoes its effect.\nIn a cascade, they form an identity system.\n\n\n\n\n\ngraph LR\n    X(\"x(t)\") --&gt; H(\"h(t)\") --&gt; W(\"w(t)\") --&gt; H_INV(\"h_inv(t)\") --&gt; Y(\"y(t) = x(t)\");\n\n    subgraph \"is equivalent to\"\n        X2(\"x(t)\") --&gt; ID(\"δ(t)\") --&gt; Y2(\"y(t)=x(t)\");\n    end\n\n\n\n\n\n\nThe condition for invertibility is:\n\\[\nh(t) * h_{inv}(t) = \\delta(t) \\quad \\text{or} \\quad h[n] * h_{inv}[n] = \\delta[n]\n\\]\n\nThe concept of an inverse is intuitive: it’s a system that gets you back your original input. For LTI systems, this concept has a precise mathematical form. The inverse system, when convolved with the original system, must produce a unit impulse. The unit impulse acts as the “identity element” for convolution, just like the number 1 is the identity for multiplication."
  },
  {
    "objectID": "ss_23.html#example-inverting-the-accumulator",
    "href": "ss_23.html#example-inverting-the-accumulator",
    "title": "Signal and Systems",
    "section": "Example: Inverting the Accumulator",
    "text": "Example: Inverting the Accumulator\n\nSystem: The discrete-time accumulator. Its impulse response is the unit step, \\(h[n] = u[n]\\).\n\n\\(y[n] = \\sum_{k=-\\infty}^{n} x[k]\\)\n\nInverse System: The first-difference system.\n\n\\(w[n] = y[n] - y[n-1]\\)\nIts impulse response is \\(h_{inv}[n] = \\delta[n] - \\delta[n-1]\\).\n\n\nLet’s verify: \\(h[n] * h_{inv}[n] = u[n] * (\\delta[n] - \\delta[n-1]) = u[n] - u[n-1] = \\delta[n]\\) ✔️\n\n\n\n\n\n\n\nThe accumulator and the first-difference system are a perfect pair of inverse systems. The accumulator sums up values over time. The first-difference looks at the change from one sample to the next. It makes intuitive sense that differencing undoes summation. The plot confirms this mathematically: convolving the impulse response of the accumulator (a step function) with the impulse response of the first-difference system gives us a single unit impulse."
  },
  {
    "objectID": "ss_23.html#causality-and-stability",
    "href": "ss_23.html#causality-and-stability",
    "title": "Signal and Systems",
    "section": "Causality and Stability",
    "text": "Causality and Stability\n\nCausality: An LTI system is causal if its output at time n only depends on inputs up to time n (present and past).\n\nCondition: \\(h[n] = 0\\) for \\(n&lt;0\\) or \\(h(t)=0\\) for \\(t&lt;0\\).\nIntuition: The system can’t react to an impulse before it happens.\n\nStability (BIBO): A system is stable if every bounded input produces a bounded output.\n\nCondition: The impulse response must be absolutely summable/integrable.\n\\(\\sum_{k=-\\infty}^{\\infty} |h[k]| &lt; \\infty\\) or \\(\\int_{-\\infty}^{\\infty} |h(\\tau)| d\\tau &lt; \\infty\\)\nIntuition: The system’s “memory” or “echoes” must eventually die out.\n\n\nExample: The accumulator, \\(h[n]=u[n]\\), is causal but unstable because \\(\\sum_{n=0}^\\infty |u[n]| = \\infty\\).\n\nCausality is a simple check: is the impulse response zero for all negative time? If yes, it’s causal. Stability is a bit more involved. The condition means that the total “energy” of the impulse response, ignoring sign, must be finite. If the impulse response doesn’t decay to zero fast enough, like the unit step, a bounded input (like a constant value of 1) can cause the output to grow infinitely."
  },
  {
    "objectID": "ss_23.html#the-unit-step-response",
    "href": "ss_23.html#the-unit-step-response",
    "title": "Signal and Systems",
    "section": "The Unit Step Response",
    "text": "The Unit Step Response\nBesides the impulse response \\(h(t)\\), the unit step response \\(s(t)\\) is also used to characterize an LTI system. It’s the output when the input is a unit step, \\(u(t)\\).\nThe two are directly related:\n\n\nDiscrete-Time\nThe step response is the running sum of the impulse response. \\[s[n] = \\sum_{k=-\\infty}^{n} h[k]\\] The impulse response is the first difference of the step response. \\[h[n] = s[n] - s[n-1]\\]\n\nContinuous-Time\nThe step response is the running integral of the impulse response. \\[s(t) = \\int_{-\\infty}^{t} h(\\tau)d\\tau\\] The impulse response is the derivative of the step response. \\[h(t) = \\frac{ds(t)}{dt}\\]\n\nKnowing either \\(h(t)\\) or \\(s(t)\\) allows you to fully describe the LTI system.\n\nThe impulse response is the fundamental theoretical tool, but in a real-world lab, creating a perfect impulse is impossible. Creating a good step input (like flipping a switch) is much easier. The step response is therefore a common practical way to measure and characterize a system. These equations show that the impulse response and step response are two sides of the same coin. They contain the same information about the system, just presented in different ways."
  },
  {
    "objectID": "ss_32.html#introduction-why-basic-signals",
    "href": "ss_32.html#introduction-why-basic-signals",
    "title": "Signals and Systems",
    "section": "1. Introduction: Why Basic Signals?",
    "text": "1. Introduction: Why Basic Signals?\nIn Signals and Systems, we seek “basic signals” with two crucial properties:\n\nBroad Applicability: They can construct a wide range of useful signals.\nSimple Response: The system’s response to them is simple enough for convenient analysis.\n\nComplex exponentials (\\(e^{st}\\) and \\(z^n\\)) fit these criteria perfectly.\n\nTo effectively analyze and design systems, we often look for fundamental building blocks. Imagine trying to understand complex machinery without knowing about simple gears or levers. In signals, we want signals that, when fed into an LTI system, behave predictably and simply. This predictability allows us to analyze the system’s overall behavior for any input that can be decomposed into these basic signals. The complex exponential is precisely this kind of fundamental building block for LTI systems. It simplifies the inherently complex operation of convolution into something much more manageable."
  },
  {
    "objectID": "ss_32.html#eigentheory-for-lti-systems",
    "href": "ss_32.html#eigentheory-for-lti-systems",
    "title": "Signals and Systems",
    "section": "2. Eigentheory for LTI Systems",
    "text": "2. Eigentheory for LTI Systems\nA signal for which the system output is a (possibly complex) constant times the input is called an eigenfunction of the system.\nFor a system \\(\\mathcal{H}\\), if: \\[\n\\mathcal{H}\\{x(t)\\} = y(t) = \\lambda x(t)\n\\]\nthen \\(x(t)\\) is an eigenfunction of the system, and \\(\\lambda\\) is the corresponding eigenvalue.\n\n\nContinuous-Time\nInput: \\(x(t) = e^{st}\\)\nOutput: \\(y(t) = H(s) e^{st}\\)\nHere, \\(e^{st}\\) is the eigenfunction, and \\(H(s)\\) is the corresponding eigenvalue.\n\nDiscrete-Time\nInput: \\(x[n] = z^n\\)\nOutput: \\(y[n] = H(z) z^n\\)\nHere, \\(z^n\\) is the eigenfunction, and \\(H(z)\\) is the corresponding eigenvalue.\n\n\nThe term “eigen” comes from German, meaning “own” or “characteristic”. So, an eigenfunction is a characteristic function of the system. It means that when you apply this specific type of signal to an LTI system, its fundamental form doesn’t change; only its amplitude and phase might be modified. This property is incredibly powerful for simplifying system analysis, as we only need to understand how the system scales these specific basic signals, rather than performing complex convolutions for every possible input."
  },
  {
    "objectID": "ss_32.html#derivation-continuous-time-case",
    "href": "ss_32.html#derivation-continuous-time-case",
    "title": "Signals and Systems",
    "section": "3. Derivation: Continuous-Time Case",
    "text": "3. Derivation: Continuous-Time Case\nLet an LTI system have impulse response \\(h(t)\\).\nInput: \\(x(t) = e^{st}\\).\nThe output \\(y(t)\\) is given by the convolution integral: \\[\ny(t) = \\int_{-\\infty}^{+\\infty} h(\\tau) x(t-\\tau) d \\tau\n\\]\nSubstitute \\(x(t-\\tau) = e^{s(t-\\tau)}\\): \\[\ny(t) = \\int_{-\\infty}^{+\\infty} h(\\tau) e^{s(t-\\tau)} d \\tau\n\\] \\[\ny(t) = \\int_{-\\infty}^{+\\infty} h(\\tau) e^{st} e^{-s\\tau} d \\tau\n\\]"
  },
  {
    "objectID": "ss_32.html#derivation-continuous-time-case-cont.",
    "href": "ss_32.html#derivation-continuous-time-case-cont.",
    "title": "Signals and Systems",
    "section": "3. Derivation: Continuous-Time Case (cont.)",
    "text": "3. Derivation: Continuous-Time Case (cont.)\nSince \\(e^{st}\\) is independent of \\(\\tau\\), we can pull it out of the integral: \\[\ny(t) = e^{st} \\int_{-\\infty}^{+\\infty} h(\\tau) e^{-s\\tau} d \\tau\n\\]\nThus, the output is: \\[\ny(t) = H(s) e^{st}\n\\] where the eigenvalue \\(H(s)\\) is: \\[\nH(s) = \\int_{-\\infty}^{+\\infty} h(\\tau) e^{-s\\tau} d \\tau\n\\]\n\nThis derivation explicitly illustrates how the convolution integral simplifies when the input is a complex exponential. The integral term, \\(\\int_{-\\infty}^{+\\infty} h(\\tau) e^{-s\\tau} d \\tau\\), is a fundamental transformation of the system’s impulse response. For particular values of ‘s’, this expression corresponds to the Laplace Transform of \\(h(t)\\), or more specifically, the Fourier Transform if \\(s=j\\omega\\). This \\(H(s)\\) completely characterizes how the system behaves for each specific complex exponential frequency, providing a direct link between the system’s time-domain properties and its response in the complex frequency domain."
  },
  {
    "objectID": "ss_32.html#derivation-discrete-time-case",
    "href": "ss_32.html#derivation-discrete-time-case",
    "title": "Signals and Systems",
    "section": "4. Derivation: Discrete-Time Case",
    "text": "4. Derivation: Discrete-Time Case\nSimilarly, for a discrete-time LTI system with impulse response \\(h[n]\\).\nInput: \\(x[n] = z^n\\).\nThe output \\(y[n]\\) is given by the convolution sum: \\[\ny[n] = \\sum_{k=-\\infty}^{+\\infty} h[k] x[n-k]\n\\]\nSubstitute \\(x[n-k] = z^{n-k}\\): \\[\ny[n] = \\sum_{k=-\\infty}^{+\\infty} h[k] z^{n-k}\n\\] \\[\ny[n] = \\sum_{k=-\\infty}^{+\\infty} h[k] z^n z^{-k}\n\\]"
  },
  {
    "objectID": "ss_32.html#derivation-discrete-time-case-cont.",
    "href": "ss_32.html#derivation-discrete-time-case-cont.",
    "title": "Signals and Systems",
    "section": "4. Derivation: Discrete-Time Case (cont.)",
    "text": "4. Derivation: Discrete-Time Case (cont.)\nSince \\(z^n\\) is independent of \\(k\\), we can pull it out of the sum: \\[\ny[n] = z^n \\sum_{k=-\\infty}^{+\\infty} h[k] z^{-k}\n\\]\nThus, the output is: \\[\ny[n] = H(z) z^n\n\\] where the eigenvalue \\(H(z)\\) is: \\[\nH(z) = \\sum_{k=-\\infty}^{+\\infty} h[k] z^{-k}\n\\]\n\nThe discrete-time derivation strongly parallels the continuous-time one, demonstrating the consistent mathematical framework for LTI systems in both domains. The sum, \\(\\sum_{k=-\\infty}^{+\\infty} h[k] z^{-k}\\), represents the Z-Transform of \\(h[n]\\). This again highlights how the eigenvalue function, \\(H(z)\\), is a transform of the system’s impulse response, providing a direct link between the system’s internal characteristics (its impulse response) and its external behavior when excited by discrete-time complex exponentials. This parallel makes the concepts transferable and reinforces the universality of eigenfunctions for LTI analysis."
  },
  {
    "objectID": "ss_32.html#power-of-superposition",
    "href": "ss_32.html#power-of-superposition",
    "title": "Signals and Systems",
    "section": "5. Power of Superposition",
    "text": "5. Power of Superposition\nThe eigenfunction property, combined with the linearity of LTI systems (superposition), significantly simplifies analysis.\nConsider an input composed of a sum of complex exponentials: \\[\nx(t) = a_1 e^{s_1 t} + a_2 e^{s_2 t} + a_3 e^{s_3 t}\n\\]\nBy superposition, the response to the sum is the sum of individual responses:\n\n\\(a_1 e^{s_1 t} \\longrightarrow a_1 H(s_1) e^{s_1 t}\\)\n\\(a_2 e^{s_2 t} \\longrightarrow a_2 H(s_2) e^{s_2 t}\\)\n\\(a_3 e^{s_3 t} \\longrightarrow a_3 H(s_3) e^{s_3 t}\\)\n\nSumming these up, the total output \\(y(t)\\) is: \\[\ny(t) = a_1 H(s_1) e^{s_1 t} + a_2 H(s_2) e^{s_2 t} + a_3 H(s_3) e^{s_3 t}\n\\]\n\nThis is the core insight that underpins Fourier analysis. If we can represent any complex-looking signal as a sum (or integral) of these simple complex exponentials, then finding the system’s output just means multiplying each component’s amplitude coefficient by the corresponding eigenvalue. We completely avoid the computationally intensive process of performing a full convolution for the entire input signal. This drastically simplifies the problem of finding system responses and provides a powerful alternative to time-domain analysis."
  },
  {
    "objectID": "ss_32.html#general-form-continuous-time",
    "href": "ss_32.html#general-form-continuous-time",
    "title": "Signals and Systems",
    "section": "6. General Form: Continuous-Time",
    "text": "6. General Form: Continuous-Time\nIf the input to a continuous-time LTI system is a linear combination of complex exponentials:\n\n\nInput Signal \\[\nx(t) = \\sum_{k} a_k e^{s_k t}\n\\]\n\nOutput Signal \\[\ny(t) = \\sum_{k} a_k H(s_k) e^{s_k t}\n\\]\n\nThe output is simply a linear combination of the same complex exponentials, but each scaled by its corresponding eigenvalue \\(H(s_k)\\).\n\nThis equation is truly fundamental in Signals and Systems. It means that the LTI system preserves the frequency components of the input complex exponentials. It doesn’t generate new frequencies; it only changes their amplitudes and phases. This property is why LTI systems are often called “linear filters.” They selectively pass or attenuate different frequency components based on the value of \\(H(s_k)\\) at that specific complex frequency \\(s_k\\)."
  },
  {
    "objectID": "ss_32.html#general-form-discrete-time",
    "href": "ss_32.html#general-form-discrete-time",
    "title": "Signals and Systems",
    "section": "7. General Form: Discrete-Time",
    "text": "7. General Form: Discrete-Time\nSimilarly, for a discrete-time LTI system:\n\n\nInput Signal \\[\nx[n] = \\sum_{k} a_k z_k^n\n\\]\n\nOutput Signal \\[\ny[n] = \\sum_{k} a_k H(z_k) z_k^n\n\\]\n\nThe output is also a linear combination of the same complex exponential sequences, each scaled by its corresponding eigenvalue \\(H(z_k)\\).\n\nThe strong parallelism between continuous-time and discrete-time systems in this context is both elegant and important. It demonstrates that the underlying principles of how LTI systems interact with complex exponentials are consistent across both domains, whether the signals are continuous or sampled. This makes the concepts widely applicable and reinforces the universality of eigenfunctions for LTI analysis, simplifying the transition between analog and digital signal processing."
  },
  {
    "objectID": "ss_32.html#restriction-for-fourier-analysis",
    "href": "ss_32.html#restriction-for-fourier-analysis",
    "title": "Signals and Systems",
    "section": "8. Restriction for Fourier Analysis",
    "text": "8. Restriction for Fourier Analysis\nWhile \\(s\\) and \\(z\\) can be arbitrary complex numbers, Fourier analysis focuses on specific values:\n\n\nContinuous-Time We restrict \\(s\\) to purely imaginary values: \\[\ns = j\\omega\n\\] This focuses on complex exponentials of the form \\(e^{j\\omega t}\\).\n\nDiscrete-Time We restrict \\(z\\) to values of unit magnitude: \\[\nz = e^{j\\omega}\n\\] This focuses on complex exponentials of the form \\(e^{j\\omega n}\\).\n\n\nThe restriction to \\(s=j\\omega\\) and \\(z=e^{j\\omega}\\) is profoundly important. These specific forms represent sinusoidal signals, which are ubiquitous in electrical and computer engineering (e.g., AC circuits, oscillations, modulation). \\(e^{j\\omega t}\\) can be decomposed into \\(\\cos(\\omega t)\\) and \\(\\sin(\\omega t)\\) using Euler’s formula. By focusing on these values, we are effectively analyzing the system’s frequency response. The values of \\(H(j\\omega)\\) and \\(H(e^{j\\omega})\\) directly tell us how the system passes or attenuates different frequencies, analogous to how an audio equalizer or a radio filter works."
  },
  {
    "objectID": "ss_32.html#example-3.1-time-shift-system",
    "href": "ss_32.html#example-3.1-time-shift-system",
    "title": "Signals and Systems",
    "section": "9. Example 3.1: Time Shift System",
    "text": "9. Example 3.1: Time Shift System\nConsider an LTI system where the output \\(y(t)\\) is a time-shifted version of the input \\(x(t)\\): \\[\ny(t)=x(t-3).\n\\] This system represents a pure time delay of 3 units.\nPart 1: Response to a single complex exponential input\nInput: \\(x(t) = e^{j2t}\\).\nTo find the output, we directly substitute \\(x(t)\\) into the system equation: \\[\ny(t) = e^{j2(t-3)} = e^{-j6} e^{j2t}.\n\\]\nComparing \\(y(t) = e^{-j6} e^{j2t}\\) with the eigenfunction form \\(y(t) = H(s) e^{st}\\), we can identify: The eigenvalue \\(H(j2) = e^{-j6}\\).\n\nThis simple example clearly illustrates the eigenfunction property in action. The output is still a complex exponential of the exact same frequency (\\(2\\) rad/sec), but its amplitude has been modified by the complex constant \\(e^{-j6}\\). This complex constant is crucial: its magnitude (which is 1 here, meaning no gain or attenuation) and its phase (\\(-6\\) radians, indicating a lag) reveal how the system affects this specific frequency component. It’s a phase shift corresponding to the time delay at that particular frequency."
  },
  {
    "objectID": "ss_32.html#example-3.1-impulse-response-verification",
    "href": "ss_32.html#example-3.1-impulse-response-verification",
    "title": "Signals and Systems",
    "section": "10. Example 3.1: Impulse Response Verification",
    "text": "10. Example 3.1: Impulse Response Verification\nLet’s verify the eigenvalue \\(H(s)\\) by first determining the system’s impulse response \\(h(t)\\).\nFor a system defined by \\(y(t)=x(t-3)\\), the impulse response is a delayed impulse function: \\(h(t) = \\delta(t-3)\\).\nNow, we use the formula for \\(H(s)\\) derived earlier: \\[\nH(s) = \\int_{-\\infty}^{+\\infty} h(\\tau) e^{-s\\tau} d \\tau\n\\] Substitute \\(h(\\tau) = \\delta(\\tau-3)\\): \\[\nH(s) = \\int_{-\\infty}^{+\\infty} \\delta(\\tau-3) e^{-s\\tau} d \\tau\n\\]"
  },
  {
    "objectID": "ss_32.html#example-3.1-impulse-response-verification-cont.",
    "href": "ss_32.html#example-3.1-impulse-response-verification-cont.",
    "title": "Signals and Systems",
    "section": "10. Example 3.1: Impulse Response Verification (cont.)",
    "text": "10. Example 3.1: Impulse Response Verification (cont.)\nDue to the sifting property of the Dirac delta function, this integral evaluates to: \\[\nH(s) = e^{-s(3)} = e^{-3s}\n\\]\nFinally, we evaluate \\(H(s)\\) at \\(s=j2\\) (the frequency of our input \\(x(t)=e^{j2t}\\)): \\[\nH(j2) = e^{-j3(2)} = e^{-j6}\n\\] This result precisely matches the eigenvalue we found by direct substitution on the previous slide!\n\nThis verification step is important because it shows the consistency of the eigenfunction framework. Whether we calculate the eigenvalue by observing the output directly or by transforming the system’s impulse response, we get the same result. The use of the Dirac delta function here significantly simplifies the integral, demonstrating its powerful sifting property. This consistency reinforces the fundamental relationship between a system’s time-domain characteristic (its impulse response) and its frequency-domain characteristic (its eigenvalue function)."
  },
  {
    "objectID": "ss_32.html#example-3.1-superposition-with-cosines",
    "href": "ss_32.html#example-3.1-superposition-with-cosines",
    "title": "Signals and Systems",
    "section": "11. Example 3.1: Superposition with Cosines",
    "text": "11. Example 3.1: Superposition with Cosines\nConsider a more complex input signal composed of a sum of two cosine waves: \\[\nx(t) = \\cos(4t) + \\cos(7t)\n\\]\nDirect approach: From \\(y(t)=x(t-3)\\), the output is simply: \\[\ny(t) = \\cos(4(t-3)) + \\cos(7(t-3))\n\\]\nUsing Eigenfunctions & Superposition: First, we express \\(x(t)\\) using Euler’s formula to decompose it into complex exponentials: \\[\nx(t) = \\frac{1}{2}e^{j4t} + \\frac{1}{2}e^{-j4t} + \\frac{1}{2}e^{j7t} + \\frac{1}{2}e^{-j7t}\n\\]\nWe already found the system’s eigenvalue \\(H(j\\omega)\\) for a time delay of 3: \\(H(j\\omega) = e^{-j3\\omega}\\).\n\nThis example transitions from a single exponential to a sum of real sinusoids. This is where the true power of the eigenfunction approach, combined with linearity, becomes apparent. By using Euler’s formula, we transform the real-valued input into a sum of complex exponentials. Each of these complex exponential components is an eigenfunction of the LTI system, allowing us to find their individual responses simply by scaling. This decomposition is the critical first step in applying Fourier analysis to real-world signals."
  },
  {
    "objectID": "ss_32.html#example-3.1-superposition-cont.",
    "href": "ss_32.html#example-3.1-superposition-cont.",
    "title": "Signals and Systems",
    "section": "12. Example 3.1: Superposition (Cont.)",
    "text": "12. Example 3.1: Superposition (Cont.)\nNow, we apply the eigenfunction property to each complex exponential component of \\(x(t)\\):\n\nFor \\(\\frac{1}{2}e^{j4t}\\): The eigenvalue is \\(H(j4) = e^{-j3(4)} = e^{-j12}\\).\nOutput component: \\(\\frac{1}{2} H(j4) e^{j4t} = \\frac{1}{2} e^{-j12} e^{j4t} = \\frac{1}{2} e^{j(4t-12)} = \\frac{1}{2} e^{j4(t-3)}\\)\nFor \\(\\frac{1}{2}e^{-j4t}\\): The eigenvalue is \\(H(-j4) = e^{-j3(-4)} = e^{j12}\\).\nOutput component: \\(\\frac{1}{2} H(-j4) e^{-j4t} = \\frac{1}{2} e^{j12} e^{-j4t} = \\frac{1}{2} e^{-j(4t-12)} = \\frac{1}{2} e^{-j4(t-3)}\\)\nFor \\(\\frac{1}{2}e^{j7t}\\): The eigenvalue is \\(H(j7) = e^{-j3(7)} = e^{-j21}\\).\nOutput component: \\(\\frac{1}{2} H(j7) e^{j7t} = \\frac{1}{2} e^{-j21} e^{j7t} = \\frac{1}{2} e^{j(7t-21)} = \\frac{1}{2} e^{j7(t-3)}\\)\nFor \\(\\frac{1}{2}e^{-j7t}\\): The eigenvalue is \\(H(-j7) = e^{-j3(-7)} = e^{j21}\\).\nOutput component: \\(\\frac{1}{2} H(-j7) e^{-j7t} = \\frac{1}{2} e^{j21} e^{-j7t} = \\frac{1}{2} e^{-j(7t-21)} = \\frac{1}{2} e^{-j7(t-3)}\\)"
  },
  {
    "objectID": "ss_32.html#example-3.1-superposition-cont.-1",
    "href": "ss_32.html#example-3.1-superposition-cont.-1",
    "title": "Signals and Systems",
    "section": "12. Example 3.1: Superposition (Cont.)",
    "text": "12. Example 3.1: Superposition (Cont.)\nSumming these individual outputs, we get the total output \\(y(t)\\): \\[\ny(t) = \\frac{1}{2} e^{j4(t-3)} + \\frac{1}{2} e^{-j4(t-3)} + \\frac{1}{2} e^{j7(t-3)} + \\frac{1}{2} e^{-j7(t-3)}\n\\] Applying Euler’s formula in reverse (recall \\(\\cos(\\theta) = \\frac{e^{j\\theta} + e^{-j\\theta}}{2}\\)): \\[\ny(t) = \\cos(4(t-3)) + \\cos(7(t-3))\n\\] This result precisely matches the direct calculation, demonstrating the power of the eigenfunction approach!\n\nThis detailed breakdown confirms that the eigenfunction approach, coupled with the principle of superposition, correctly and systematically determines the system’s output for complex inputs. While direct substitution was trivial for this particular simple time-delay system, the eigenfunction method is universally applicable and becomes absolutely indispensable for more complex LTI systems where direct convolution or time-domain manipulation is cumbersome or intractable. This systematically transforms complex time-domain problems into simpler frequency-domain multiplications, paving the way for advanced frequency-domain analysis and filter design."
  },
  {
    "objectID": "ss_32.html#conclusion-next-steps",
    "href": "ss_32.html#conclusion-next-steps",
    "title": "Signals and Systems",
    "section": "14. Conclusion & Next Steps",
    "text": "14. Conclusion & Next Steps\nKey Takeaways:\n\nComplex exponentials (\\(e^{st}\\), \\(z^n\\)) are the eigenfunctions of LTI systems.\nLTI systems respond to these signals by simply scaling them by an eigenvalue (\\(H(s)\\), \\(H(z)\\)).\nThis property, combined with superposition, simplifies the analysis of complex signals through LTI systems from convolution to simple multiplication.\n\nWhat’s Next?\nThis lays the essential foundation for Fourier Analysis:\n\nFourier Series: A powerful tool for representing periodic signals as sums of complex exponentials.\nFourier Transform: Extending this representation to aperiodic signals, enabling universal frequency-domain analysis.\n\n\nUnderstanding eigenfunctions is the critical bridge that connects time-domain convolution with the powerful frequency-domain analysis techniques. It’s the “why” behind transforms like the Fourier, Laplace, and Z-transforms. These transforms don’t just calculate; they inherently leverage this eigenfunction property to transform the complex operation of convolution into simpler multiplication operations in the frequency domain. This shift makes system analysis and design immensely more intuitive and manageable, forming the backbone for nearly all aspects of electrical and computer engineering, from communication systems and audio processing to control theory and image processing. This is a concept that truly permeates the entire field."
  },
  {
    "objectID": "ss_34.html#convergence-of-the-fourier-series",
    "href": "ss_34.html#convergence-of-the-fourier-series",
    "title": "Signals and Systems",
    "section": "3.4 Convergence of the Fourier Series",
    "text": "3.4 Convergence of the Fourier Series\n\nWelcome to this lecture on the convergence of the Fourier Series. Today, we’ll explore the conditions under which a periodic signal can be accurately represented by its Fourier series, and delve into some interesting phenomena that arise when approximating discontinuous signals. This is a critical topic for understanding the practical applications and limitations of Fourier analysis in ECE."
  },
  {
    "objectID": "ss_34.html#approximating-signals-euler-lagrange-and-fourier",
    "href": "ss_34.html#approximating-signals-euler-lagrange-and-fourier",
    "title": "Signals and Systems",
    "section": "Approximating Signals: Euler, Lagrange, and Fourier",
    "text": "Approximating Signals: Euler, Lagrange, and Fourier\n\nEarly mathematicians like Euler and Lagrange struggled with Fourier series for discontinuous signals.\n\nFourier, however, maintained that even discontinuous signals like square waves could be represented.\n\nWe approximate a periodic signal \\(x(t)\\) using a finite sum of harmonically related complex exponentials: \\[\nx_{N}(t)=\\sum_{k=-N}^{N} a_{k} e^{j k \\omega_{0} t}\n\\]\nThe approximation error is defined as: \\[\ne_{N}(t)=x(t)-x_{N}(t)\n\\]\n\n\nHistorically, the idea of representing a discontinuous function as a sum of continuous sinusoids was quite controversial. Euler and Lagrange, pillars of classical analysis, found this problematic. Fourier, with his more applied perspective, pushed the boundaries, asserting the validity of such representations for a much wider class of signals. This finite sum, \\(x_N(t)\\), is our practical tool for approximation, and understanding its behavior as N grows is key to convergence."
  },
  {
    "objectID": "ss_34.html#minimizing-approximation-error",
    "href": "ss_34.html#minimizing-approximation-error",
    "title": "Signals and Systems",
    "section": "Minimizing Approximation Error",
    "text": "Minimizing Approximation Error\n\nTo quantify the approximation’s quality, we use the energy in the error over one period: \\[\nE_{N}=\\int_{T}\\left|e_{N}(t)\\right|^{2} d t\n\\]\nKey Result: The coefficients \\(a_k\\) that minimize this energy \\(E_N\\) are precisely the Fourier series coefficients: \\[\na_{k}=\\frac{1}{T} \\int_{T} x(t) e^{-j k \\omega_{0} t} d t\n\\]\n\n\n\n\n\n\n\nTip\n\n\nThis means that truncating the Fourier series provides the “best” approximation in a least-squares sense for a finite number of terms.\n\n\n\n\nMinimizing the energy of the error is a common and powerful criterion in signal processing, often leading to optimal solutions. The fact that the Fourier series coefficients naturally emerge as the minimizers here is a fundamental and elegant result, strengthening the importance of the Fourier series. As we increase N, adding more terms, the energy in the error, E_N, will decrease. For a convergent series, E_N will approach zero as N approaches infinity."
  },
  {
    "objectID": "ss_34.html#when-does-a-fourier-series-converge",
    "href": "ss_34.html#when-does-a-fourier-series-converge",
    "title": "Signals and Systems",
    "section": "When Does a Fourier Series Converge?",
    "text": "When Does a Fourier Series Converge?\n\nNot all periodic signals have a valid Fourier series representation.\n\nThe integral for \\(a_k\\) might diverge, or the infinite series for \\(x(t)\\) might not converge to \\(x(t)\\).\n\nFortunately, most practical ECE signals do have Fourier series representations.\nWe’ll discuss two main classes of conditions guaranteeing convergence:\n\nFinite Energy over a Period (Square Integrability)\nDirichlet Conditions (Point-wise Convergence)\n\n\n\nWhile Fourier’s claim that any periodic signal could be represented was a bit ambitious, his intuition was largely correct for engineering applications. Pathological signals that don’t converge are rarely encountered in practice. Understanding these conditions helps us appreciate the robustness of Fourier analysis and its applicability to a vast range of real-world signals, including those with discontinuities."
  },
  {
    "objectID": "ss_34.html#condition-1-finite-energy-square-integrability",
    "href": "ss_34.html#condition-1-finite-energy-square-integrability",
    "title": "Signals and Systems",
    "section": "Condition 1: Finite Energy (Square Integrability)",
    "text": "Condition 1: Finite Energy (Square Integrability)\n\nA periodic signal \\(x(t)\\) has a Fourier series representation if it has finite energy over a single period: \\[ \\int_{T}|x(t)|^{2} d t&lt;\\infty \\]\nGuarantees:\n\nCoefficients \\(a_k\\) are finite.\nThe energy in the approximation error \\(E_N\\) converges to 0 as \\(N \\rightarrow \\infty\\). \\[ \\int_{T}|e(t)|^{2} d t=0 \\quad \\text{where} \\quad e(t)=x(t)-\\sum_{k=-\\infty}^{+\\infty} a_{k} e^{j k \\omega_{0} t} \\]\n\n\n\n\n\n\n\n\nImportant\n\n\nThis means \\(x(t)\\) and its infinite Fourier series representation are indistinguishable from an energy perspective, even if they differ at isolated points. Physical systems respond to signal energy.\n\n\n\n\nThis condition is perhaps the most fundamental for engineers because it directly relates to the physical reality of signals. If the difference between a signal and its Fourier series has zero energy, then any practical system responding to energy will treat them as identical. This is a very strong form of convergence for engineering purposes. Most signals we encounter, like square waves, triangle waves, and even pulse trains, satisfy this condition."
  },
  {
    "objectID": "ss_34.html#condition-2-dirichlet-conditions",
    "href": "ss_34.html#condition-2-dirichlet-conditions",
    "title": "Signals and Systems",
    "section": "Condition 2: Dirichlet Conditions",
    "text": "Condition 2: Dirichlet Conditions\n\nA set of conditions, developed by P. L. Dirichlet, that guarantee point-wise convergence.\nThis means \\(x(t)\\) equals its Fourier series representation except at isolated discontinuities.\nAt discontinuities, the series converges to the average of the values on either side.\n\n\nWhile finite energy convergence is great for overall system response, sometimes we need to know that the series converges to the exact value of the signal at every point. This is where the Dirichlet conditions come in. They are more restrictive but provide a stronger guarantee about the specific values of the signal. Let’s look at each of these conditions."
  },
  {
    "objectID": "ss_34.html#dirichlet-condition-1-absolute-integrability",
    "href": "ss_34.html#dirichlet-condition-1-absolute-integrability",
    "title": "Signals and Systems",
    "section": "Dirichlet Condition 1: Absolute Integrability",
    "text": "Dirichlet Condition 1: Absolute Integrability\n\nOver any period, \\(x(t)\\) must be absolutely integrable: \\[ \\int_{T}|x(t)| d t&lt;\\infty \\]\nEnsures: Each coefficient \\(a_k\\) will be finite. \\[ \\left|a_{k}\\right| \\leq \\frac{1}{T} \\int_{T}|x(t)| d t \\]\nViolation Example: \\(x(t) = 1/t\\) for \\(0 &lt; t \\leq 1\\), periodic with \\(T=1\\).\n\nThe integral \\(\\int_0^1 (1/t) dt\\) diverges (see Figure 3.8a in textbook).\n\n\n\nThis condition prevents signals from having “infinite area” over a period, which would make the Fourier coefficients infinite. The example \\(x(t)=1/t\\) blows up at \\(t=0\\), making its absolute integral infinite. Such signals are generally not physically realizable or encountered in typical ECE contexts."
  },
  {
    "objectID": "ss_34.html#dirichlet-condition-2-bounded-variation",
    "href": "ss_34.html#dirichlet-condition-2-bounded-variation",
    "title": "Signals and Systems",
    "section": "Dirichlet Condition 2: Bounded Variation",
    "text": "Dirichlet Condition 2: Bounded Variation\n\nIn any finite interval of time, \\(x(t)\\) must have a finite number of maxima and minima during any single period.\nViolation Example: \\(x(t) = \\sin(2\\pi/t)\\) for \\(0 &lt; t \\leq 1\\), periodic with \\(T=1\\).\n\nThis function has an infinite number of oscillations (maxima and minima) as \\(t \\rightarrow 0\\) (see Figure 3.8b in textbook).\n\n\n\nThis condition essentially means the signal can’t oscillate infinitely fast or have an infinite number of “wiggles” within a finite time. The example given, \\(\\sin(2\\pi/t)\\), illustrates this perfectly. As t approaches zero, the argument of the sine function goes to infinity, causing infinitely many oscillations. Again, this is a highly pathological case not typically seen in practical signals."
  },
  {
    "objectID": "ss_34.html#dirichlet-condition-3-finite-discontinuities",
    "href": "ss_34.html#dirichlet-condition-3-finite-discontinuities",
    "title": "Signals and Systems",
    "section": "Dirichlet Condition 3: Finite Discontinuities",
    "text": "Dirichlet Condition 3: Finite Discontinuities\n\nIn any finite interval of time, there are only a finite number of discontinuities.\nFurthermore, each of these discontinuities must be finite.\nViolation Example: A signal with an infinite number of increasingly smaller sections, each introducing a discontinuity (see Figure 3.8c in textbook).\n\n\n\n\n\n\n\nNote\n\n\nMost physically generated or processed signals in ECE satisfy all three Dirichlet conditions.\n\n\n\n\nThis condition is about the “smoothness” of the signal in terms of its jumps. A signal cannot have infinitely many sudden jumps within a finite period. The example describes a signal that gets progressively more complex with an infinite number of steps, violating this condition. In engineering, we generally deal with signals that have a finite number of step changes or impulses within any given time frame."
  },
  {
    "objectID": "ss_34.html#practical-implications-of-dirichlet-conditions",
    "href": "ss_34.html#practical-implications-of-dirichlet-conditions",
    "title": "Signals and Systems",
    "section": "Practical Implications of Dirichlet Conditions",
    "text": "Practical Implications of Dirichlet Conditions\n\n\nFor Signals Satisfying Dirichlet Conditions:\n\nFourier series converges and equals \\(x(t)\\) everywhere except at isolated discontinuities.\nAt discontinuities, the series converges to the average value of the signal on either side.\n\n\nWhy it matters for ECE:\n\nSignals differing only at isolated points have identical integrals.\nThey behave identically under convolution.\nTherefore, they are considered equivalent for LTI system analysis.\n\n\n\nThis is a crucial takeaway for our field. Even with discontinuities, the Fourier series representation is incredibly robust. The differences are so localized that they don’t affect integral properties, which are fundamental to how LTI systems process signals (e.g., convolution involves integration). From a system’s perspective, these signals are effectively the same."
  },
  {
    "objectID": "ss_34.html#the-gibbs-phenomenon-an-introduction",
    "href": "ss_34.html#the-gibbs-phenomenon-an-introduction",
    "title": "Signals and Systems",
    "section": "The Gibbs Phenomenon: An Introduction",
    "text": "The Gibbs Phenomenon: An Introduction\n\nDiscovered by Michelson (1898) and explained by Gibbs (1899).\nIt describes the peculiar behavior of the Fourier series approximation near discontinuities.\nObservation: When approximating a discontinuous signal (like a square wave) with a finite Fourier series, ripples and overshoot occur at the discontinuities.\n\n\nMichelson, a physicist, built a harmonic analyzer and was puzzled by the results when feeding it a square wave. He thought his machine was broken! He contacted Josiah Gibbs, a prominent mathematical physicist, who then provided the full explanation. This phenomenon highlights a key characteristic of approximating discontinuous functions with smooth sinusoids."
  },
  {
    "objectID": "ss_34.html#visualizing-the-gibbs-phenomenon",
    "href": "ss_34.html#visualizing-the-gibbs-phenomenon",
    "title": "Signals and Systems",
    "section": "Visualizing the Gibbs Phenomenon",
    "text": "Visualizing the Gibbs Phenomenon\n\nConsider a symmetric square wave.\nWe’ll observe its finite Fourier series approximation, \\(x_N(t)\\), as \\(N\\) increases.\nNotice the overshoot and ripples near the edges of the square wave.\n\n\nviewof N = Inputs.range([1, 100], {step: 2, value: 5, label: \"Number of terms (N)\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere we have an interactive plot. You can adjust the number of terms, N, using the slider. As you increase N, you’ll see the approximation get closer to the square wave. However, pay close attention to the regions around the discontinuities, where the signal jumps. You’ll notice persistent ripples and an overshoot, regardless of how large N becomes. This is the essence of the Gibbs phenomenon. This visualization directly relates to Figure 3.9 in your textbook."
  },
  {
    "objectID": "ss_34.html#understanding-the-gibbs-phenomenon",
    "href": "ss_34.html#understanding-the-gibbs-phenomenon",
    "title": "Signals and Systems",
    "section": "Understanding the Gibbs Phenomenon",
    "text": "Understanding the Gibbs Phenomenon\n\nOvershoot: For a discontinuity of unity height, the peak amplitude of the ripple is approximately 1.09 (an overshoot of ~9%).\n\nThis overshoot does not decrease with increasing \\(N\\).\n\nRipples: As \\(N\\) increases, the ripples become compressed closer to the discontinuity.\n\nThe width of the ripples decreases, but their peak amplitude remains constant.\n\nConvergence at a point: For any fixed \\(t_1\\), \\(x_N(t_1)\\) will converge to \\(x(t_1)\\) as \\(N \\rightarrow \\infty\\) (or to the average at a discontinuity).\n\nHowever, the closer \\(t_1\\) is to a discontinuity, the larger \\(N\\) must be for the error to be acceptably small.\n\n\n\nIt’s crucial to distinguish between point-wise convergence and the behavior of the peak overshoot. While the Fourier series does converge to the correct value at any given point (including the midpoint of a discontinuity), the maximum error, the overshoot, persists in the vicinity of the discontinuity. This means that even with millions of terms, you’ll still see that 9% overshoot, just squeezed into a smaller and smaller region around the jump. This is why it’s a “phenomenon.”"
  },
  {
    "objectID": "ss_34.html#practical-significance-of-gibbs-phenomenon",
    "href": "ss_34.html#practical-significance-of-gibbs-phenomenon",
    "title": "Signals and Systems",
    "section": "Practical Significance of Gibbs Phenomenon",
    "text": "Practical Significance of Gibbs Phenomenon\n\nIn real-world applications, if a truncated Fourier series \\(x_N(t)\\) is used to approximate a discontinuous signal:\n\nExpect high-frequency ripples and overshoot near discontinuities.\n\nEngineering Consideration:\n\nChoose a sufficiently large \\(N\\) so that the total energy in these ripples becomes insignificant.\nWhile the peak overshoot remains, the energy contained within the shrinking ripple region diminishes.\n\n\n\n\n\n\n\n\nCaution\n\n\nFor applications sensitive to peak values (e.g., driving an amplifier to saturation), the Gibbs phenomenon can be a critical design consideration.\n\n\n\n\nThe Gibbs phenomenon is not a failure of Fourier series, but rather a characteristic of approximating discontinuous signals with continuous functions. It means that if your system is sensitive to instantaneous peak values, you need to be aware that a Fourier series approximation will likely exceed the original signal’s amplitude at the edges. However, for most applications that depend on average power or integrated energy, the effect becomes negligible with enough terms, as the ripples become very narrow."
  },
  {
    "objectID": "ss_34.html#conclusion-convergence-and-practicality",
    "href": "ss_34.html#conclusion-convergence-and-practicality",
    "title": "Signals and Systems",
    "section": "Conclusion: Convergence and Practicality",
    "text": "Conclusion: Convergence and Practicality\n\nMost practical signals in ECE have Fourier series representations.\n\nGuaranteed by finite energy or Dirichlet conditions.\n\nFourier series provides the best least-squares approximation for a finite number of terms.\nThe Gibbs Phenomenon highlights a limitation for discontinuous signals:\n\nPersistent overshoot and ripples near discontinuities, even as \\(N \\rightarrow \\infty\\).\nRipples compress, but peak amplitude remains constant (for a given discontinuity height).\n\nDespite Gibbs, Fourier series remains an indispensable tool for analyzing and synthesizing signals in ECE.\n\n\nTo wrap up, the Fourier series is an incredibly powerful tool in signals and systems. It allows us to decompose complex periodic signals into simpler, harmonically related components. While convergence is generally guaranteed for signals we encounter in practice, understanding the nuances like the Gibbs phenomenon is key to applying these tools effectively and interpreting their results correctly in engineering contexts. This knowledge helps us design better filters, understand system responses, and accurately model signal behavior."
  },
  {
    "objectID": "ss_36.html#introduction-discrete-time-periodic-signals",
    "href": "ss_36.html#introduction-discrete-time-periodic-signals",
    "title": "Signals and Systems",
    "section": "Introduction: Discrete-Time Periodic Signals",
    "text": "Introduction: Discrete-Time Periodic Signals\nDiscrete-time (DT) signals \\(x[n]\\) are periodic with period \\(N\\) if \\(x[n] = x[n+N]\\).\nThe fundamental period \\(N\\) is the smallest positive integer for which this holds.\nThe fundamental frequency is \\(\\omega_0 = 2\\pi/N\\).\n\n\n\n\n\n\nNote\n\n\nKey Difference from Continuous-Time (CT): The Fourier series representation for discrete-time signals is a finite series, unlike the infinite series required for continuous-time signals. This simplifies convergence issues significantly.\n\n\n\n\nToday, we’re diving into the discrete-time Fourier series. Recall from Chapter 1 that a discrete-time signal is periodic if it repeats itself after a certain number of samples, denoted by \\(N\\). The fundamental frequency \\(\\omega_0\\) is directly related to this period \\(N\\). A crucial point to remember from the outset is that the DTFS is a finite sum, which has profound implications, especially regarding convergence, as we’ll see later."
  },
  {
    "objectID": "ss_36.html#harmonically-related-complex-exponentials",
    "href": "ss_36.html#harmonically-related-complex-exponentials",
    "title": "Signals and Systems",
    "section": "Harmonically Related Complex Exponentials",
    "text": "Harmonically Related Complex Exponentials\nThe set of all discrete-time complex exponential signals periodic with period \\(N\\) is given by:\n\\[ \\phi_k[n] = e^{j k \\omega_0 n} = e^{j k(2\\pi/N)n}, \\quad k = 0, \\pm 1, \\pm 2, \\ldots \\]\nThere are only \\(N\\) distinct signals in this set. This is because discrete-time complex exponentials differing by a multiple of \\(2\\pi\\) in frequency are identical.\n\\[ \\phi_k[n] = \\phi_{k+rN}[n] \\]\nThis means \\(e^{j k(2\\pi/N)n}\\) and \\(e^{j (k+N)(2\\pi/N)n}\\) are the same sequence.\n\nJust like in continuous time, we build our periodic signals from harmonically related complex exponentials. However, discrete-time is special. While \\(k\\) can theoretically go from negative to positive infinity, only \\(N\\) of these complex exponentials are unique. After \\(N\\) terms, the sequence of complex exponentials repeats itself. This is a direct consequence of the periodic nature of discrete-time exponentials. Let’s see an example."
  },
  {
    "objectID": "ss_36.html#demonstrating-distinct-exponentials",
    "href": "ss_36.html#demonstrating-distinct-exponentials",
    "title": "Signals and Systems",
    "section": "Demonstrating Distinct Exponentials",
    "text": "Demonstrating Distinct Exponentials\nLet’s visualize how \\(\\phi_k[n]\\) repeats for \\(k\\) and \\(k+N\\). Consider \\(N=5\\). We expect \\(\\phi_0[n] = \\phi_5[n]\\), \\(\\phi_1[n] = \\phi_6[n]\\), etc."
  },
  {
    "objectID": "ss_36.html#demonstrating-distinct-exponentials-cont.",
    "href": "ss_36.html#demonstrating-distinct-exponentials-cont.",
    "title": "Signals and Systems",
    "section": "Demonstrating Distinct Exponentials (cont.)",
    "text": "Demonstrating Distinct Exponentials (cont.)\nThe plots show the real and imaginary parts of \\(\\phi_k[n]\\) for \\(k=1\\) and \\(k=1+N\\) (with \\(N=5\\)). Observe that the two plots are identical, confirming that \\(\\phi_k[n] = \\phi_{k+N}[n]\\).\n\n\n\n\n\n\nTip\n\n\nInteractive Element: Feel free to change \\(N_{val}\\) and \\(k\\) (by modifying the get_phi_k calls) in the Python code to explore this property further! For example, try \\(k=0\\) and \\(k=N_{val}\\).\n\n\n\n\nHere, we use Python to plot two complex exponentials. The top plot shows \\(\\phi_1[n]\\) for \\(N=5\\). The bottom plot shows \\(\\phi_{1+5}[n]\\), which is \\(\\phi_6[n]\\). As you can clearly see, both plots produce the exact same sequence of real and imaginary values. This visually demonstrates the periodicity of the discrete-time complex exponentials in the index \\(k\\). Try changing the value of N_val or the k value in the get_phi_k calls in the code block to see how this relationship holds. For instance, set k_val=0 and observe k_val=N_val for any N_val."
  },
  {
    "objectID": "ss_36.html#discrete-time-fourier-series-dtfs-synthesis",
    "href": "ss_36.html#discrete-time-fourier-series-dtfs-synthesis",
    "title": "Signals and Systems",
    "section": "Discrete-Time Fourier Series (DTFS) Synthesis",
    "text": "Discrete-Time Fourier Series (DTFS) Synthesis\nA periodic discrete-time signal \\(x[n]\\) can be represented as a linear combination of these harmonically related complex exponentials.\n\\[ x[n] = \\sum_{k=\\langle N\\rangle} a_k \\phi_k[n] = \\sum_{k=\\langle N\\rangle} a_k e^{j k \\omega_0 n} = \\sum_{k=\\langle N\\rangle} a_k e^{j k(2\\pi/N)n} \\]\nThe notation \\(\\sum_{k=\\langle N\\rangle}\\) indicates summation over any set of \\(N\\) successive integers for \\(k\\). Common choices include \\(k=0, 1, \\ldots, N-1\\) or \\(k=- (N-1)/2, \\ldots, (N-1)/2\\) (for odd \\(N\\)).\nThe coefficients \\(a_k\\) are called the Fourier series coefficients.\n\nNow that we understand the building blocks, we can construct any periodic discrete-time signal using a sum of these exponentials. This equation is the synthesis equation. Notice the summation is over \\(N\\) terms. This is because, as we just saw, there are only \\(N\\) distinct complex exponentials. The specific range for \\(k\\) doesn’t change the set of exponentials used, only which \\(N\\) coefficients we’re solving for."
  },
  {
    "objectID": "ss_36.html#determination-of-dtfs-coefficients-12",
    "href": "ss_36.html#determination-of-dtfs-coefficients-12",
    "title": "Signals and Systems",
    "section": "Determination of DTFS Coefficients (1/2)",
    "text": "Determination of DTFS Coefficients (1/2)\nGiven \\(x[n]\\) periodic with fundamental period \\(N\\), we want to find \\(a_k\\).\nWe can solve a system of \\(N\\) linear equations, but a more direct method exists.\nThe key identity (similar to orthogonality in CT):\n\\[ \\sum_{n=\\langle N\\rangle} e^{j k(2\\pi/N)n} = \\begin{cases} N, & k = 0, \\pm N, \\pm 2N, \\ldots \\\\ 0, & \\text{otherwise} \\end{cases} \\]\nThis identity is crucial for isolating each \\(a_k\\).\n\nHow do we find these \\(a_k\\) coefficients? Similar to continuous time, we use an orthogonality property. This specific identity states that if you sum a complex exponential over one period, the result is zero unless the exponential is a constant (i.e., its frequency is a multiple of \\(2\\pi\\)). When it’s a constant, the sum is simply \\(N\\) times the constant. Let’s verify this interactively."
  },
  {
    "objectID": "ss_36.html#interactive-identity-verification",
    "href": "ss_36.html#interactive-identity-verification",
    "title": "Signals and Systems",
    "section": "Interactive Identity Verification",
    "text": "Interactive Identity Verification\nLet’s verify the summation identity: \\(\\sum_{n=\\langle N\\rangle} e^{j k(2\\pi/N)n}\\).\n\n\n\n\n\n\nTip\n\n\nExperiment:\nChange N_period and k_val in the code above and run it. Observe the sum. What happens when k_val is a multiple of N_period? What happens otherwise?\n\n\n\n\n\n\n\n\n\n\nThis interactive code block allows you to directly test the identity. When k_val is 0, the exponential is \\(e^0 = 1\\), and summing \\(N\\) ones gives \\(N\\). When k_val is a multiple of \\(N\\), for example \\(k=N\\), the exponential is \\(e^{jN(2\\pi/N)n} = e^{j2\\pi n} = 1\\) for all integer \\(n\\), so the sum is again \\(N\\). For any other \\(k\\), the terms \\(e^{j k(2\\pi/N)n}\\) will trace a full circle (or multiple full circles) in the complex plane, summing to zero over one period. This is a fundamental property."
  },
  {
    "objectID": "ss_36.html#determination-of-dtfs-coefficients-22",
    "href": "ss_36.html#determination-of-dtfs-coefficients-22",
    "title": "Signals and Systems",
    "section": "Determination of DTFS Coefficients (2/2)",
    "text": "Determination of DTFS Coefficients (2/2)\nTo derive \\(a_k\\):\n\nMultiply the synthesis equation by \\(e^{-j r(2\\pi/N)n}\\).\nSum over one period \\(n=\\langle N\\rangle\\).\n\n\\[ \\sum_{n=\\langle N\\rangle} x[n] e^{-j r(2\\pi/N)n} = \\sum_{n=\\langle N\\rangle} \\sum_{k=\\langle N\\rangle} a_k e^{j(k-r)(2\\pi/N)n} \\]\nInterchanging summation order and applying the identity:\n\\[ \\sum_{n=\\langle N\\rangle} x[n] e^{-j r(2\\pi/N)n} = \\sum_{k=\\langle N\\rangle} a_k \\left( \\sum_{n=\\langle N\\rangle} e^{j(k-r)(2\\pi/N)n} \\right) \\]\nThe inner sum is \\(N\\) if \\(k-r\\) is a multiple of \\(N\\) (i.e., \\(k=r\\) within the \\(\\langle N \\rangle\\) range), and \\(0\\) otherwise. This simplifies to \\(N a_r\\), leading to:\n\\[ a_k = \\frac{1}{N} \\sum_{n=\\langle N\\rangle} x[n] e^{-j k(2\\pi/N)n} \\]\nThis is the Discrete-Time Fourier Series Analysis Equation.\n\nWith that identity established, the derivation of the analysis equation is straightforward. We multiply the synthesis equation by a specific complex exponential, \\(e^{-j r(2\\pi/N)n}\\), and sum over one period. This process effectively “filters out” all terms except for the one corresponding to \\(a_r\\). The result is a direct formula to compute each \\(a_k\\) from the signal \\(x[n]\\)."
  },
  {
    "objectID": "ss_36.html#dtfs-synthesis-and-analysis-pair",
    "href": "ss_36.html#dtfs-synthesis-and-analysis-pair",
    "title": "Signals and Systems",
    "section": "DTFS Synthesis and Analysis Pair",
    "text": "DTFS Synthesis and Analysis Pair\nThese two equations form the Discrete-Time Fourier Series pair:\n\n\nSynthesis Equation:\n(How to build \\(x[n]\\) from coefficients \\(a_k\\))\n\\[ x[n] = \\sum_{k=\\langle N\\rangle} a_k e^{j k \\omega_0 n} = \\sum_{k=\\langle N\\rangle} a_k e^{j k(2\\pi/N)n} \\]\n\nAnalysis Equation:\n(How to find coefficients \\(a_k\\) from \\(x[n]\\))\n\\[ a_k = \\frac{1}{N} \\sum_{n=\\langle N\\rangle} x[n] e^{-j k \\omega_0 n} = \\frac{1}{N} \\sum_{n=\\langle N\\rangle} x[n] e^{-j k(2\\pi/N)n} \\]\n\n\n\n\n\n\n\nImportant\n\n\nThe Fourier series coefficients \\(a_k\\) are periodic with period \\(N\\): \\(a_k = a_{k+N}\\). This is a direct consequence of the periodicity of \\(\\phi_k[n]\\).\n\n\n\n\nHere we have the complete Discrete-Time Fourier Series pair. The synthesis equation tells us how to construct the signal from its frequency components, and the analysis equation tells us how to decompose the signal into those components. Crucially, remember the coefficients \\(a_k\\) themselves are periodic with period \\(N\\). This means if you calculate \\(a_0, a_1, \\ldots, a_{N-1}\\), you automatically know \\(a_N, a_{N+1}\\), etc., because they simply repeat."
  },
  {
    "objectID": "ss_36.html#example-3.10-discrete-time-sine-wave",
    "href": "ss_36.html#example-3.10-discrete-time-sine-wave",
    "title": "Signals and Systems",
    "section": "Example 3.10: Discrete-Time Sine Wave",
    "text": "Example 3.10: Discrete-Time Sine Wave\nConsider the signal \\(x[n] = \\sin(\\omega_0 n)\\). This signal is periodic only if \\(2\\pi/\\omega_0\\) is an integer (or ratio of integers). If \\(\\omega_0 = 2\\pi/N\\), then \\(x[n]\\) is periodic with fundamental period \\(N\\).\nWe can expand \\(x[n]\\) using Euler’s formula:\n\\[ x[n] = \\frac{1}{2j} e^{j(2\\pi/N)n} - \\frac{1}{2j} e^{-j(2\\pi/N)n} \\]\nBy comparing this with the synthesis equation \\(x[n] = \\sum_{k=\\langle N\\rangle} a_k e^{j k(2\\pi/N)n}\\), we can identify the coefficients:\n\\[ a_1 = \\frac{1}{2j}, \\quad a_{-1} = -\\frac{1}{2j} \\]\nAll other \\(a_k\\) (for \\(k\\) within one period) are zero.\n\nLet’s apply our new tools to an example. A discrete-time sine wave is only periodic under specific conditions related to its frequency. If \\(\\omega_0\\) is a multiple of \\(2\\pi/N\\), it’s periodic. By expressing the sine wave as a sum of two complex exponentials, we can directly compare it to the Fourier series synthesis equation and pick out the coefficients by inspection. This is a very common technique when the signal is already in the form of complex exponentials."
  },
  {
    "objectID": "ss_36.html#example-3.10-sine-wave-dtfs-visualization",
    "href": "ss_36.html#example-3.10-sine-wave-dtfs-visualization",
    "title": "Signals and Systems",
    "section": "Example 3.10: Sine Wave DTFS Visualization",
    "text": "Example 3.10: Sine Wave DTFS Visualization\nIf \\(\\omega_0 = (2\\pi M)/N\\) (where \\(M, N\\) are coprime), \\(x[n]\\) has fundamental period \\(N\\).\nThen, \\(a_M = \\frac{1}{2j}\\), \\(a_{-M} = -\\frac{1}{2j}\\), and other \\(a_k=0\\) within one period.\n\nviewof N_val = Inputs.range([5, 20], {\n  label: \"N:\",\n  step: 1,\n  value: 5\n})\n\nviewof M_val = Inputs.range([1, 4], {\n  label: \"M:\",\n  step: 1,\n  value: 1\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive plot visualizes the Fourier series coefficients for a discrete-time sine wave. You can adjust \\(N\\) (the period) and \\(M\\) (the multiplier for the fundamental frequency) using the sliders. Observe how only two coefficients are non-zero within any given period of \\(N\\). When \\(M=1\\), you see coefficients at \\(k=1\\) and \\(k=N-1\\) (which is equivalent to \\(k=-1\\)). When \\(M\\) changes, the locations of these non-zero coefficients shift to \\(k=M\\) and \\(k=N-M\\). Notice the magnitudes are always 0.5, and the phases are \\(\\pm 90^\\circ\\) (or \\(\\pm \\pi/2\\) radians), which corresponds to \\(\\pm j/2\\). This clearly illustrates the spectral content of a discrete-time sine wave."
  },
  {
    "objectID": "ss_36.html#example-3.11-more-complex-signal",
    "href": "ss_36.html#example-3.11-more-complex-signal",
    "title": "Signals and Systems",
    "section": "Example 3.11: More Complex Signal",
    "text": "Example 3.11: More Complex Signal\nConsider \\(x[n]=1+\\sin\\left(\\frac{2\\pi}{N}\\right)n+3\\cos\\left(\\frac{2\\pi}{N}\\right)n+\\cos\\left(\\frac{4\\pi}{N}n+\\frac{\\pi}{2}\\right)\\).\nThis signal is periodic with period \\(N\\).\nWe expand each term into complex exponentials:\n\\(1 \\implies a_0 = 1\\)\n\\(\\sin\\left(\\frac{2\\pi}{N}\\right)n \\implies a_1 = \\frac{1}{2j}, a_{-1} = -\\frac{1}{2j}\\)\n\\(3\\cos\\left(\\frac{2\\pi}{N}\\right)n \\implies a_1 = \\frac{3}{2}, a_{-1} = \\frac{3}{2}\\)\n\\(\\cos\\left(\\frac{4\\pi}{N}n+\\frac{\\pi}{2}\\right) \\implies \\frac{1}{2}e^{j\\pi/2}e^{j2(2\\pi/N)n} + \\frac{1}{2}e^{-j\\pi/2}e^{-j2(2\\pi/N)n}\\)\n\\(\\implies a_2 = \\frac{1}{2}e^{j\\pi/2} = \\frac{1}{2}j, a_{-2} = \\frac{1}{2}e^{-j\\pi/2} = -\\frac{1}{2}j\\)"
  },
  {
    "objectID": "ss_36.html#example-3.11-more-complex-signal-1",
    "href": "ss_36.html#example-3.11-more-complex-signal-1",
    "title": "Signals and Systems",
    "section": "Example 3.11: More Complex Signal",
    "text": "Example 3.11: More Complex Signal\nCombining terms for each \\(k\\):\n\\(a_0 = 1\\)\n\\(a_1 = \\frac{1}{2j} + \\frac{3}{2} = \\frac{3}{2} - \\frac{1}{2}j\\)\n\\(a_{-1} = -\\frac{1}{2j} + \\frac{3}{2} = \\frac{3}{2} + \\frac{1}{2}j\\)\n\\(a_2 = \\frac{1}{2}j\\)\n\\(a_{-2} = -\\frac{1}{2}j\\)\nAll other \\(a_k=0\\) within the period.\n\n\n\n\n\n\nTip\n\n\nProperty for Real Signals: For a real signal \\(x[n]\\), the Fourier coefficients exhibit conjugate symmetry: \\(a_{-k} = a_k^*\\). Verify this for the calculated coefficients!\n\n\n\n\nThis example shows how to find Fourier coefficients for a more complex signal by breaking it down into its constituent complex exponentials. Each sinusoidal component contributes two complex exponential terms, and the constant term contributes to \\(a_0\\). We then sum the coefficients for each harmonic index \\(k\\). Notice that \\(a_{-1}\\) is the complex conjugate of \\(a_1\\), and \\(a_{-2}\\) is the complex conjugate of \\(a_2\\). This is a general property for real-valued signals, known as conjugate symmetry."
  },
  {
    "objectID": "ss_36.html#example-3.11-coefficients-visualization",
    "href": "ss_36.html#example-3.11-coefficients-visualization",
    "title": "Signals and Systems",
    "section": "Example 3.11: Coefficients Visualization",
    "text": "Example 3.11: Coefficients Visualization\nVisualizing the real, imaginary, magnitude, and phase of the coefficients for \\(N=10\\).\n\n\n\n\n\n\n\nHere we see a visual representation of the coefficients calculated in the previous slide. The top-left plot shows the real parts, and the bottom-left shows the imaginary parts. Notice the symmetry: Real parts are even (\\(Re\\{a_k\\} = Re\\{a_{-k}\\}\\)), and imaginary parts are odd (\\(Im\\{a_k\\} = -Im\\{a_{-k}\\}\\)). On the right, we have the magnitude and phase. Magnitudes are even (\\(|a_k| = |a_{-k}|\\)), and phases are odd (\\(Arg\\{a_k\\} = -Arg\\{a_{-k}\\}\\)). This visual confirms the conjugate symmetry property for real signals: \\(a_{-k} = a_k^*\\)."
  },
  {
    "objectID": "ss_36.html#example-3.12-discrete-time-periodic-square-wave",
    "href": "ss_36.html#example-3.12-discrete-time-periodic-square-wave",
    "title": "Signals and Systems",
    "section": "Example 3.12: Discrete-Time Periodic Square Wave",
    "text": "Example 3.12: Discrete-Time Periodic Square Wave\nConsider a discrete-time periodic square wave \\(x[n]\\) with period \\(N\\).\nIt is defined as \\(x[n]=1\\) for \\(-N_1 \\leq n \\leq N_1\\), and \\(x[n]=0\\) otherwise within one period.\nThe analysis equation is: \\[ a_k = \\frac{1}{N} \\sum_{n=\\langle N\\rangle} x[n] e^{-j k(2\\pi/N)n} \\]\nChoosing the summation range from \\(-N_1\\) to \\(N_1\\):\n\\[ a_k = \\frac{1}{N} \\sum_{n=-N_1}^{N_1} e^{-j k(2\\pi/N)n} \\]\nThis sum is a geometric series.\n\nLet’s analyze a common signal: the discrete-time periodic square wave. This signal is 1 for a certain duration around \\(n=0\\) and 0 otherwise within its period. To find its Fourier coefficients, we plug its definition into the analysis equation. The summation simplifies because \\(x[n]\\) is only non-zero for a specific range. The sum then becomes a geometric series, which we can solve using a known formula."
  },
  {
    "objectID": "ss_36.html#example-3.12-square-wave-coefficients",
    "href": "ss_36.html#example-3.12-square-wave-coefficients",
    "title": "Signals and Systems",
    "section": "Example 3.12: Square Wave Coefficients",
    "text": "Example 3.12: Square Wave Coefficients\nApplying the geometric series sum formula and simplifying, we get:\n\\[ a_k = \\frac{1}{N} \\frac{\\sin\\left[2\\pi k(N_1+1/2)/N\\right]}{\\sin(\\pi k/N)}, \\quad k \\neq 0, \\pm N, \\pm 2N, \\ldots \\]\nAnd for \\(k=0, \\pm N, \\pm 2N, \\ldots\\):\n\\[ a_k = \\frac{2N_1+1}{N} \\]\nThis formula resembles the continuous-time sinc function, but uses \\(\\sin(\\cdot)/\\sin(\\cdot)\\) due to discrete nature.\n\nAfter some algebraic manipulation, which involves using the geometric series sum formula and Euler’s identity, we arrive at this closed-form expression for the Fourier coefficients. It looks similar to the sinc function we encountered in continuous-time, but it’s a ratio of sines. This is typical for discrete-time Fourier transforms and series, as the discrete nature often leads to these periodic sinc-like functions. Note the special case for \\(k=0\\) (and its multiples), where the denominator would be zero if not handled separately. At \\(k=0\\), \\(a_0\\) represents the average value of the signal over one period, which is simply the number of non-zero samples (\\(2N_1+1\\)) divided by the total period (\\(N\\))."
  },
  {
    "objectID": "ss_36.html#example-3.12-coefficients-visualization",
    "href": "ss_36.html#example-3.12-coefficients-visualization",
    "title": "Signals and Systems",
    "section": "Example 3.12: Coefficients Visualization",
    "text": "Example 3.12: Coefficients Visualization\nLet’s visualize the coefficients for \\(2N_1+1=5\\) and varying \\(N\\).\n\nviewof N_val_sq = Inputs.range([10, 40], {\n  label: \"N (Period):\",\n  step: 5,\n  value: 10\n})\nviewof N1_val_sq = Inputs.range([1, 5], {\n  label: \"N1 (Pulse Half-Width):\",\n  step: 1,\n  value: 2\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive plot lets you see how the Fourier coefficients of a discrete-time square wave change with the period \\(N\\) and the pulse width \\(N_1\\). The shape of the coefficients resembles a sampled sinc function. As \\(N\\) increases (making the signal sparser in time), the coefficients become more closely spaced in frequency. The main lobe width is inversely proportional to the pulse width. Experiment with the sliders to see these effects. For instance, notice how the \\(a_k\\) values are scaled by \\(1/N\\)."
  },
  {
    "objectID": "ss_36.html#convergence-of-discrete-time-fourier-series",
    "href": "ss_36.html#convergence-of-discrete-time-fourier-series",
    "title": "Signals and Systems",
    "section": "Convergence of Discrete-Time Fourier Series",
    "text": "Convergence of Discrete-Time Fourier Series\nUnlike continuous-time Fourier series, discrete-time Fourier series have no convergence issues.\nThe DTFS is a finite sum of \\(N\\) terms.\nA discrete-time periodic sequence \\(x[n]\\) is completely specified by its \\(N\\) values over one period.\nThe DTFS analysis equation transforms these \\(N\\) values into \\(N\\) Fourier coefficients.\nThe DTFS synthesis equation perfectly reconstructs the original \\(N\\) values from these \\(N\\) coefficients.\n\n\n\n\n\n\nImportant\n\n\nThere is no Gibbs phenomenon in discrete-time Fourier series. The partial sum (if it includes all \\(N\\) distinct terms) will exactly equal \\(x[n]\\).\n\n\n\n\nThis is a critical distinction between continuous-time and discrete-time Fourier series. Because a discrete-time signal is defined by a finite number of samples within a period, its Fourier series also has a finite number of terms. This means that if you sum up all \\(N\\) terms of the DTFS, you will exactly reconstruct the original signal. There are no approximation errors, no ripples at discontinuities, and therefore, no Gibbs phenomenon. This makes DTFS mathematically much simpler in terms of convergence."
  },
  {
    "objectID": "ss_36.html#dtfs-reconstruction-no-gibbs-phenomenon",
    "href": "ss_36.html#dtfs-reconstruction-no-gibbs-phenomenon",
    "title": "Signals and Systems",
    "section": "DTFS Reconstruction: No Gibbs Phenomenon",
    "text": "DTFS Reconstruction: No Gibbs Phenomenon\nLet’s reconstruct the square wave using partial sums.\n\\(\\hat{x}[n] = \\sum_{k=-M}^{M} a_k e^{j k(2\\pi/N)n}\\) (for odd \\(N\\), \\(M=(N-1)/2\\) includes all terms).\n\nviewof N_rec = Inputs.range([5, 15], {\n  label: \"N (Period, odd):\",\n  step: 2,\n  value: 9\n})\n\nviewof N1_rec = Inputs.range([1, 3], {\n  label: \"N1 (Pulse Half-Width):\",\n  step: 1,\n  value: 2\n}) // Note: 2N1+1 should be &lt;= N\n\nviewof M_rec = Inputs.range([0, 4], {\n  label: \"M (Summation Terms):\",\n  step: 1,\n  value: 0\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive demonstration highlights the key difference in convergence. We plot the original square wave and its reconstruction \\(\\hat{x}[n]\\) using a partial sum of \\(2M+1\\) terms. Adjust the M slider. Notice that as M increases, the reconstructed signal gets closer to the original. When M reaches its maximum value, which corresponds to including all \\(N\\) unique terms (e.g., \\(M=(N-1)/2\\) for odd \\(N\\)), the reconstructed signal perfectly matches the original signal. There are no overshoots or undershoots, no Gibbs phenomenon, because the series is finite and exact. This is a powerful advantage of the discrete-time Fourier series."
  },
  {
    "objectID": "ss_36.html#summary-and-key-takeaways",
    "href": "ss_36.html#summary-and-key-takeaways",
    "title": "Signals and Systems",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\nDiscrete-Time Fourier Series (DTFS):\n\nRepresents periodic discrete-time signals \\(x[n]\\) with period \\(N\\).\nUses a finite sum of \\(N\\) harmonically related complex exponentials.\n\nKey Equations:\n\nSynthesis: \\(x[n] = \\sum_{k=\\langle N\\rangle} a_k e^{j k(2\\pi/N)n}\\)\nAnalysis: \\(a_k = \\frac{1}{N} \\sum_{n=\\langle N\\rangle} x[n] e^{-j k(2\\pi/N)n}\\)\n\nCrucial Differences from Continuous-Time (CTFS):\n\nOnly \\(N\\) distinct complex exponentials \\(\\phi_k[n]\\).\nFourier coefficients \\(a_k\\) are periodic with period \\(N\\) (\\(a_k = a_{k+N}\\)).\nNo convergence issues or Gibbs phenomenon. The finite sum perfectly reconstructs \\(x[n]\\).\n\n\nTo summarize, the Discrete-Time Fourier Series is a powerful tool for analyzing periodic discrete-time signals. Its finite nature is its most distinguishing feature, leading to exact reconstruction and avoiding the convergence complexities of its continuous-time counterpart. Understanding these differences is crucial for working with sampled signals and discrete systems. Thank you."
  },
  {
    "objectID": "ss_3x.html#fourier-analysis-deconstructing-signals",
    "href": "ss_3x.html#fourier-analysis-deconstructing-signals",
    "title": "Signals and Systems",
    "section": "Fourier Analysis: Deconstructing Signals",
    "text": "Fourier Analysis: Deconstructing Signals\nImron Rosyadi"
  },
  {
    "objectID": "ss_3x.html#the-power-of-complex-exponentials",
    "href": "ss_3x.html#the-power-of-complex-exponentials",
    "title": "Signals and Systems",
    "section": "1. The Power of Complex Exponentials",
    "text": "1. The Power of Complex Exponentials\nRecall from our previous discussion that complex exponentials are eigenfunctions of LTI systems.\n\n\nContinuous-Time \\(x(t) = e^{st} \\quad \\xrightarrow{\\text{LTI System}} \\quad y(t) = H(s) e^{st}\\)\n\nDiscrete-Time \\(x[n] = z^n \\quad \\xrightarrow{\\text{LTI System}} \\quad y[n] = H(z) z^n\\)\n\nThis means:\n\nLTI systems only scale complex exponentials, they don’t change their fundamental form.\nThe system’s behavior is fully characterized by \\(H(s)\\) or \\(H(z)\\) at specific values of \\(s\\) or \\(z\\).\n\nThe BIG Question: If we can represent any signal as a sum (or integral) of these simple complex exponentials, then analyzing complex LTI systems becomes a simple matter of multiplication.\nThis is exactly what Fourier Analysis enables!\n\nStart by reminding students of the crucial eigenfunction property. This sets the stage for why Fourier analysis is so powerful. It’s not just a mathematical curiosity; it’s a direct consequence of how LTI systems behave. The fundamental insight is that if we can decompose any signal into these elementary complex exponential building blocks, then the analysis of complex signals through LTI systems transforms from difficult convolution operations in the time domain to simple algebraic multiplications in the frequency domain. This is the cornerstone of frequency-domain analysis in ECE."
  },
  {
    "objectID": "ss_3x.html#a-glimpse-into-history-joseph-fourier",
    "href": "ss_3x.html#a-glimpse-into-history-joseph-fourier",
    "title": "Signals and Systems",
    "section": "2. A Glimpse into History: Joseph Fourier",
    "text": "2. A Glimpse into History: Joseph Fourier\nJoseph Fourier (1768-1830)\n\n\n\n\n\n\n\nMathematica, quae ad calorem deduxi, sunt mihi graviora. (The mathematics that I have derived concerning heat, are to me of greater importance.) — Joseph Fourier\n\n\nFrench mathematician and physicist.\nBest known for initiating the investigation of Fourier series and their applications to problems of heat transfer and vibrations.\nOriginal Motivation: Solving the one-dimensional heat equation (1807). He proposed that any continuous function could be represented as a sum of sines and cosines.\nControversial Idea: His assertion that any function could be represented as such was highly controversial among his peers (Lagrange, Laplace), as the concept of “function” was very narrow at the time.\n\nImpact: Revolutionized mathematics and physics, enabling frequency-domain analysis.\n\n\nIt’s important to provide historical context. Understanding that Fourier’s ideas were revolutionary and even controversial helps students appreciate the depth and impact of his work. His motivation came from a practical physical problem (heat conduction), illustrating how theoretical breakthroughs often arise from engineering challenges. Emphasize that the general idea of decomposing a function into a sum of simple terms was not new, but Fourier’s specific claim about trigonometric functions for any function was radical at the time."
  },
  {
    "objectID": "ss_3x.html#fourier-series-representing-periodic-signals",
    "href": "ss_3x.html#fourier-series-representing-periodic-signals",
    "title": "Signals and Systems",
    "section": "3. Fourier Series: Representing Periodic Signals",
    "text": "3. Fourier Series: Representing Periodic Signals\nThe Fourier Series allows us to represent a periodic signal \\(x(t)\\) with period \\(T_0\\) (and fundamental frequency \\(\\omega_0 = 2\\pi/T_0\\)) as a weighted sum of harmonically related complex exponentials.\n3.1 Complex Exponential Fourier Series\n\\[\nx(t) = \\sum_{k=-\\infty}^{\\infty} c_k e^{jk\\omega_0 t}\n\\] where the coefficients \\(c_k\\) are given by: \\[\nc_k = \\frac{1}{T_0} \\int_{T_0} x(t) e^{-jk\\omega_0 t} dt\n\\] (The integral is over any single period \\(T_0\\))."
  },
  {
    "objectID": "ss_3x.html#fourier-series-representing-periodic-signals-1",
    "href": "ss_3x.html#fourier-series-representing-periodic-signals-1",
    "title": "Signals and Systems",
    "section": "3. Fourier Series: Representing Periodic Signals",
    "text": "3. Fourier Series: Representing Periodic Signals\n3.2 Trigonometric Fourier Series (Alternative Form)\n\\[\nx(t) = a_0 + \\sum_{k=1}^{\\infty} (a_k \\cos(k\\omega_0 t) + b_k \\sin(k\\omega_0 t))\n\\] where \\(a_0 = c_0 = \\frac{1}{T_0} \\int_{T_0} x(t) dt\\) and \\(c_k = \\frac{1}{2}(a_k - jb_k)\\). (For real signals: \\(a_k = 2 \\text{Re}\\{c_k\\}\\), \\(b_k = -2 \\text{Im}\\{c_k\\}\\)).\nKey Idea: Any periodic signal can be decomposed into a sum of a DC component, a fundamental frequency component, and components at integer multiples (harmonics) of the fundamental frequency.\n\nIntroduce both the complex exponential and trigonometric forms. Emphasize that the complex exponential form is often more mathematically convenient, especially when dealing with LTI systems due to the eigenfunction property. Explain the concept of “harmonics” – components at integer multiples of the fundamental frequency. The coefficients \\(c_k\\), \\(a_k\\), \\(b_k\\) represent the “strength” or “amplitude” of each frequency component present in the signal. The integral formula for \\(c_k\\) is essentially a correlation, telling us how much of \\(e^{jk\\omega_0t}\\) is “in” \\(x(t)\\)."
  },
  {
    "objectID": "ss_3x.html#interactive-demo-fourier-series-synthesis-of-a-square-wave",
    "href": "ss_3x.html#interactive-demo-fourier-series-synthesis-of-a-square-wave",
    "title": "Signals and Systems",
    "section": "4. Interactive Demo: Fourier Series Synthesis of a Square Wave",
    "text": "4. Interactive Demo: Fourier Series Synthesis of a Square Wave\nLet’s synthesize a square wave by summing its Fourier Series components. A square wave is rich in odd harmonics.\n\\[\nx(t)_{\\text{square}} = \\frac{4}{\\pi} \\sum_{k \\text{ odd}, k \\ge 1} \\frac{1}{k} \\sin(k\\omega_0 t)\n\\]\nAdjust the number of Harmonics to see how well the approximation matches the ideal square wave.\n\n\n                            \n                                            \n\n\n\nThis interactive demo is crucial for building intuition. Students can visually see how adding more harmonics progressively refines the approximation of the square wave. Point out the Gibbs phenomenon (overshoots at discontinuities) as an interesting artifact. Explain that signals with sharp transitions or discontinuities require many harmonics to be accurately represented, while smooth signals might require fewer. This provides a direct link between a signal’s time-domain characteristics and its frequency-domain representation."
  },
  {
    "objectID": "ss_3x.html#fourier-transform-for-aperiodic-signals",
    "href": "ss_3x.html#fourier-transform-for-aperiodic-signals",
    "title": "Signals and Systems",
    "section": "5. Fourier Transform: For Aperiodic Signals",
    "text": "5. Fourier Transform: For Aperiodic Signals\nFourier Series applies only to periodic signals. What about non-periodic, transient signals? The Fourier Transform (FT) extends this concept to aperiodic signals.\n5.1 From Fourier Series to Fourier Transform (Conceptual)\nImagine a periodic signal whose period \\(T_0\\) approaches infinity (\\(T_0 \\to \\infty\\)). As \\(T_0 \\to \\infty\\):\n\nThe fundamental frequency \\(\\omega_0 = 2\\pi/T_0 \\to 0\\).\nThe discrete sum over harmonics becomes a continuous integral over frequency.\nThe Fourier coefficients \\(c_k\\) become a continuous function of frequency, \\(X(j\\omega)\\)."
  },
  {
    "objectID": "ss_3x.html#fourier-transform-for-aperiodic-signals-1",
    "href": "ss_3x.html#fourier-transform-for-aperiodic-signals-1",
    "title": "Signals and Systems",
    "section": "5. Fourier Transform: For Aperiodic Signals",
    "text": "5. Fourier Transform: For Aperiodic Signals\n5.2 The Fourier Transform Pair\nForward Fourier Transform: Transforms a time-domain signal \\(x(t)\\) into its frequency-domain representation \\(X(j\\omega)\\). \\[\nX(j\\omega) = \\int_{-\\infty}^{\\infty} x(t) e^{-j\\omega t} dt\n\\]\nInverse Fourier Transform: Transforms a frequency-domain spectrum \\(X(j\\omega)\\) back into its time-domain signal \\(x(t)\\). \\[\nx(t) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} X(j\\omega) e^{j\\omega t} d\\omega\n\\]\nInterpretation: \\(X(j\\omega)\\) is the spectrum of the signal \\(x(t)\\), showing how much of each frequency \\(\\omega\\) is present in the signal.\n\nTransition from FS to FT by explaining the limiting process where the period goes to infinity. This helps students grasp the conceptual continuity. Emphasize the interpretation of \\(X(j\\omega)\\) as the “frequency spectrum” – it’s crucial for understanding applications. \\(X(j\\omega)\\) is a complex-valued function and can be visualized by its magnitude \\(|X(j\\omega)|\\) (how much of that frequency is present) and its phase \\(\\angle X(j\\omega)\\) (relative phase of that frequency component)."
  },
  {
    "objectID": "ss_3x.html#key-properties-of-the-fourier-transform-in-lti-systems",
    "href": "ss_3x.html#key-properties-of-the-fourier-transform-in-lti-systems",
    "title": "Signals and Systems",
    "section": "6. Key Properties of the Fourier Transform in LTI Systems",
    "text": "6. Key Properties of the Fourier Transform in LTI Systems\nThe FT reveals powerful properties crucial for ECE:\n\nLinearity: If \\(\\mathcal{F}\\{x_1(t)\\} = X_1(j\\omega)\\) and \\(\\mathcal{F}\\{x_2(t)\\} = X_2(j\\omega)\\), then \\(\\mathcal{F}\\{ax_1(t) + bx_2(t)\\} = aX_1(j\\omega) + bX_2(j\\omega)\\)\nTime Shift: If \\(\\mathcal{F}\\{x(t)\\} = X(j\\omega)\\), then \\(\\mathcal{F}\\{x(t-t_0)\\} = e^{-j\\omega t_0} X(j\\omega)\\) (A time shift in the time domain corresponds to a phase shift in the frequency domain).\nConvolution Property (THE MOST IMPORTANT FOR LTI): If \\(y(t) = x(t) * h(t)\\) (convolution in time domain), then \\(\\mathcal{F}\\{y(t)\\} = Y(j\\omega) = X(j\\omega) H(j\\omega)\\) (Convolution in time domain becomes multiplication in frequency domain!)\nHere, \\(H(j\\omega) = \\mathcal{F}\\{h(t)\\}\\) is the frequency response of the LTI system."
  },
  {
    "objectID": "ss_3x.html#key-properties-of-the-fourier-transform-in-lti-systems-1",
    "href": "ss_3x.html#key-properties-of-the-fourier-transform-in-lti-systems-1",
    "title": "Signals and Systems",
    "section": "6. Key Properties of the Fourier Transform in LTI Systems",
    "text": "6. Key Properties of the Fourier Transform in LTI Systems\n\n\n\n\n\ngraph TD\n    A[Time Domain] -- \"Convolution: x(t) * h(t)\" --&gt; B[\"Time Domain Result: y(t)\"]\n\n    C[Frequency Domain] -- \"FT: X(j&omega;), H(j&omega;)\" --&gt; D[\"Frequency Domain Result: Y(j&omega;)\"]\n\n    A -- FT --&gt; C\n    B -- FT --&gt; D\n  \n    C -- Multiplication: X(j&omega;) * H(j&omega;) --&gt; D\n\n    style A fill:#a2c4c9,stroke:#333,stroke-width:2px;\n    style B fill:#a2c4c9,stroke:#333,stroke-width:2px;\n    style C fill:#fce5cd,stroke:#333,stroke-width:2px;\n    style D fill:#fce5cd,stroke:#333,stroke-width:2px;\n\n    linkStyle 0 stroke:#666,stroke-width:2px;\n    linkStyle 1 stroke:#333,stroke-width:3px,color:red;\n    linkStyle 2 stroke:#666,stroke-width:2px;\n    linkStyle 3 stroke:#666,stroke-width:2px;\n    linkStyle 4 stroke:#333,stroke-width:3px,color:blue;\n\n\n\n\n\n\n\nHighlight these properties, especially the convolution property. This is the “AHA!” moment for LTI systems. Explain that calculating convolution in the time domain (an integral or sum) can be computationally intensive and conceptually opaque. Transforming to the frequency domain allows us to simply multiply the input spectrum by the system’s frequency response, and then inverse transform the result. This transforms a complex operation into two transforms and a simple multiplication, providing immense analytical and computational benefits."
  },
  {
    "objectID": "ss_3x.html#interactive-demo-filtering-in-the-frequency-domain",
    "href": "ss_3x.html#interactive-demo-filtering-in-the-frequency-domain",
    "title": "Signals and Systems",
    "section": "7. Interactive Demo: Filtering in the Frequency Domain",
    "text": "7. Interactive Demo: Filtering in the Frequency Domain\nLet’s apply a simple Low-Pass Filter (LPF) to a signal composed of two sine waves with different frequencies.\nObserve:\n\nThe input signal’s time-domain plot and its frequency spectrum.\nThe filter’s frequency response (magnitude).\nThe output signal’s time-domain plot and its frequency spectrum.\n\nAdjust the Cutoff Frequency of the filter. See how the filter attenuates higher frequencies while passing lower frequencies.\n\n\n                            \n                                            \n\n\n\nThis is another critical interactive demo. It visually demonstrates the action of a filter directly in both the time and frequency domains. As students move the cutoff frequency, they should observe: 1. The input signal contains both low (0.5 Hz) and high (3.0 Hz) frequency components. 2. The filter’s frequency response is a “gate” that passes frequencies below the cutoff and blocks those above. 3. The output signal’s spectrum clearly shows which frequencies were passed and which were attenuated. 4. The output signal in the time domain visibly changes, retaining the lower frequency components and smoothing out the higher ones. This directly illustrates the meaning of \\(Y(j\\omega) = X(j\\omega)H(j\\omega)\\) in a dynamic way."
  },
  {
    "objectID": "ss_3x.html#ece-applications-of-fourier-analysis",
    "href": "ss_3x.html#ece-applications-of-fourier-analysis",
    "title": "Signals and Systems",
    "section": "8. ECE Applications of Fourier Analysis",
    "text": "8. ECE Applications of Fourier Analysis\nFourier Analysis is fundamental to almost every area of Electrical and Computer Engineering.\n1. Communication Systems\n\nModulation/Demodulation: Shifting a signal’s spectrum to a different carrier frequency for transmission (radio, WiFi).\nMultiplexing: Combining multiple signals into one channel by allocating different frequency bands.\nSpectrum Analysis: Understanding bandwidth requirements, interference.\n\n2. Audio & Speech Processing\n\nEqualizers: Boosting or cutting specific frequency ranges.\nNoise Reduction: Filtering out unwanted frequency components.\nCompression (e.g., MP3): Discarding inaudible frequency components.\n\n3. Image & Video Processing\n\nFiltering: Sharpening (high-pass), blurring (low-pass), edge detection.\nCompression (e.g., JPEG): Representing images efficiently in the frequency domain."
  },
  {
    "objectID": "ss_3x.html#ece-applications-of-fourier-analysis-1",
    "href": "ss_3x.html#ece-applications-of-fourier-analysis-1",
    "title": "Signals and Systems",
    "section": "8. ECE Applications of Fourier Analysis",
    "text": "8. ECE Applications of Fourier Analysis\n4. Circuit Analysis\n\nAC Steady-State Analysis: Transforming differential equations into algebraic equations in the frequency domain (phasors).\nFilter Design: Designing circuits that pass or block specific frequencies (e.g., Butterworth, Chebyshev filters).\n\n5. Control Systems\n\nFrequency Response Analysis: Assessing system stability and performance by analyzing how different input frequencies are processed.\nSystem Identification: Determining a system’s characteristics by observing its response to various frequencies.\n\n6. Digital Signal Processing (DSP)\n\nDiscrete Fourier Transform (DFT) / Fast Fourier Transform (FFT): Efficient algorithms for computing the FT on discrete (sampled) data, enabling real-time applications.\n\n\nDedicate a slide to the sheer breadth of applications. This motivates students and shows them how theoretical concepts translate directly into real-world technologies they encounter daily. Emphasize that Fourier analysis is not just a mathematical tool, but a core component of how these systems are designed, analyzed, and implemented. Mention the FFT as the computational backbone that made many of these applications practical."
  },
  {
    "objectID": "ss_3x.html#conclusion-the-ubiquity-of-fourier",
    "href": "ss_3x.html#conclusion-the-ubiquity-of-fourier",
    "title": "Signals and Systems",
    "section": "9. Conclusion: The Ubiquity of Fourier",
    "text": "9. Conclusion: The Ubiquity of Fourier\n\nDecomposition: Fourier analysis provides a powerful framework to decompose complex signals into simpler, understandable frequency components (complex exponentials).\nSimplification: It transforms complex time-domain operations (like convolution) into simpler frequency-domain operations (like multiplication) for LTI systems.\nInsight: It offers deep insights into phenomena related to frequency, resonance, filtering, and system behavior.\n\nIn essence, Fourier Analysis is the lens through which engineers view and manipulate signals and systems in the frequency domain.\nIt is a cornerstone of modern electrical and computer engineering, underpinning technologies from your smartphone to medical imaging, and from radar to robotics."
  },
  {
    "objectID": "ss_3x.html#conclusion-the-ubiquity-of-fourier-1",
    "href": "ss_3x.html#conclusion-the-ubiquity-of-fourier-1",
    "title": "Signals and Systems",
    "section": "9. Conclusion: The Ubiquity of Fourier",
    "text": "9. Conclusion: The Ubiquity of Fourier\n\n\n\n\n\ngraph TD\n    A[Complex Time-Domain Signal] --&gt; B{Fourier Transform}\n    B --&gt; C[Simple Frequency Components]\n    C --&gt; D[\"LTI System Analysis: Multiplication by H(j&omega;)\"]\n    D --&gt; E[Output Frequency Components]\n    E --&gt; F{Inverse Fourier Transform}\n    F --&gt; G[System Output in Time-Domain]\n\n    style A fill:#DDA0DD,stroke:#333,stroke-width:2px;\n    style G fill:#DDA0DD,stroke:#333,stroke-width:2px;\n    style C fill:#ADD8E6,stroke:#333,stroke-width:2px;\n    style E fill:#ADD8E6,stroke:#333,stroke-width:2px;\n    style B fill:#8FBC8F,stroke:#333,stroke-width:2px;\n    style F fill:#8FBC8F,stroke:#333,stroke-width:2px;\n    style D fill:#F4A460,stroke:#333,stroke-width:2px;\n\n\n\n\n\n\n\nSummarize the key takeaways and reinforce why Fourier analysis is indispensable. Reiterate that it’s a fundamental paradigm shift in how we approach signals and systems. The final Mermaid diagram provides a visual conceptual flow from time to frequency and back, showing where the ‘magic’ of simplification happens. End on an inspiring note connecting the theory back to everyday technology."
  },
  {
    "objectID": "ss_41.html#representing-aperiodic-signals-the-continuous-time-fourier-transform",
    "href": "ss_41.html#representing-aperiodic-signals-the-continuous-time-fourier-transform",
    "title": "Continuous-Time Signals and Systems",
    "section": "Representing Aperiodic Signals: The Continuous-Time Fourier Transform",
    "text": "Representing Aperiodic Signals: The Continuous-Time Fourier Transform\nObjective:\nTo extend the concept of Fourier series, which is for periodic signals, to represent aperiodic signals in the frequency domain. This is crucial for analyzing a wide range of real-world signals."
  },
  {
    "objectID": "ss_41.html#development-of-the-fourier-transform-representation-of-an-aperiodic-signal",
    "href": "ss_41.html#development-of-the-fourier-transform-representation-of-an-aperiodic-signal",
    "title": "Continuous-Time Signals and Systems",
    "section": "4.1.1 Development of the Fourier Transform Representation of an Aperiodic Signal",
    "text": "4.1.1 Development of the Fourier Transform Representation of an Aperiodic Signal\nWe begin by considering a continuous-time periodic square wave. Its definition over one period is:\n\\[\nx(t)= \\begin{cases}1, & |t|&lt;T_{1} \\\\ 0, & T_{1}&lt;|t|&lt;T / 2\\end{cases}\n\\]\nThis signal periodically repeats with period \\(T\\).\nThe Fourier series coefficients \\(a_{k}\\) for this square wave are given by:\n\\[\na_{k}=\\frac{2 \\sin \\left(k \\omega_{0} T_{1}\\right)}{k \\omega_{0} T} \\quad \\text{where } \\omega_{0}=\\frac{2 \\pi}{T}\n\\]\n\nRecall that the Fourier series allows us to represent any periodic signal as a sum of complex exponentials. Each exponential has a specific frequency, which is a multiple of the fundamental frequency, and a corresponding complex amplitude, the Fourier series coefficient. For a square wave, these coefficients describe the amplitude and phase of each harmonic component."
  },
  {
    "objectID": "ss_41.html#a_k-as-samples-of-an-envelope-function",
    "href": "ss_41.html#a_k-as-samples-of-an-envelope-function",
    "title": "Continuous-Time Signals and Systems",
    "section": "\\(a_k\\) as Samples of an Envelope Function",
    "text": "\\(a_k\\) as Samples of an Envelope Function\nThe Fourier series coefficients \\(a_k\\) can be viewed as samples of a continuous envelope function.\nSpecifically, \\(T a_{k}\\) samples the function:\n\\[\nT a_{k}=\\left.\\frac{2 \\sin \\omega T_{1}}{\\omega}\\right|_{\\omega=k \\omega_{0}}\n\\]\nAs \\(T\\) increases (or \\(\\omega_0\\) decreases), the samples become more closely spaced.\nThis continuous function, \\(\\frac{2 \\sin \\omega T_{1}}{\\omega}\\), represents the envelope of \\(T a_k\\) and is independent of \\(T\\).\nLet’s visualize this as \\(T\\) changes.\nObserve how the discrete spectral lines become denser as the period \\(T\\) increases.\n\n\n\n\n\n\nTip\n\n\nThis concept is key to bridging the gap between Fourier series (discrete spectrum) and Fourier transform (continuous spectrum).\n\n\n\n\nThe core idea here is to observe what happens to the Fourier series coefficients as the period of the signal becomes very large. As T increases, the fundamental frequency omega_0 decreases, meaning the harmonic frequencies k*omega_0 get closer together. The coefficients themselves, when multiplied by T, start to look like samples of an underlying continuous function. This continuous function is what we will define as the Fourier Transform."
  },
  {
    "objectID": "ss_41.html#interactive-demo-fourier-series-coefficients-as-samples",
    "href": "ss_41.html#interactive-demo-fourier-series-coefficients-as-samples",
    "title": "Continuous-Time Signals and Systems",
    "section": "Interactive Demo: Fourier Series Coefficients as Samples",
    "text": "Interactive Demo: Fourier Series Coefficients as Samples\nVisualize the discrete Fourier series coefficients approaching a continuous envelope as the period \\(T\\) increases.\n\nviewof T_val = Inputs.range([1, 20], {label: \"Period (T)\", step: 1, value: 4})\nviewof T1_val = Inputs.range([0.1, 2], {label: \"Pulse Width (T1)\", step: 0.1, value: 0.5})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, students can manipulate T and T1 to see the effect. As T increases, omega0 decreases, and the red bars (representing T*a_k) get closer together, visually approximating the continuous blue curve (the envelope). This demonstrates how a discrete spectrum transitions to a continuous one. Observe that the shape of the envelope remains the same, only the sampling density changes."
  },
  {
    "objectID": "ss_41.html#aperiodic-signal-as-a-limiting-case",
    "href": "ss_41.html#aperiodic-signal-as-a-limiting-case",
    "title": "Continuous-Time Signals and Systems",
    "section": "Aperiodic Signal as a Limiting Case",
    "text": "Aperiodic Signal as a Limiting Case\nWe can think of an aperiodic signal \\(x(t)\\) as the limit of a periodic signal \\(\\tilde{x}(t)\\) as its period \\(T\\) approaches infinity.\n\n\nOriginal Aperiodic Signal \\(x(t)\\)\n\nFinite duration.\n\\(x(t) = 0\\) for \\(|t| &gt; T_1\\).\n\n\nConstructed Periodic Signal \\(\\tilde{x}(t)\\)\n\n\\(\\tilde{x}(t)\\) is formed by repeating \\(x(t)\\) with period \\(T\\).\nAs \\(T \\to \\infty\\), \\(\\tilde{x}(t)\\) becomes identical to \\(x(t)\\) over any finite interval.\n\n\n\n\n\n\n\ngraph LR\n  A[\"Aperiodic Signal $$x(t)$$\"] --&gt; B{\"Construct Periodic $$\\tilde{x}(t)$$\"};\n  B --&gt; C{\"Increase Period $$T$$\"};\n  C --&gt; D{\"As $$T \\to \\infty$$\"};\n  D --&gt; E[\"$$\\tilde{x}(t) \\to x(t)$$\"];\n  E --&gt; F[\"F. Series becomes F. Transform of $$x(t)$$\"];\n\n\n\n\n\n\n\nThis conceptual step is fundamental. We are essentially taking a finite segment of an aperiodic signal, making it periodic, and then letting the period grow infinitely large. As the period grows, the “copies” of the original signal in the periodic version move infinitely far apart, so for any finite observation window, the periodic signal looks exactly like the original aperiodic signal."
  },
  {
    "objectID": "ss_41.html#deriving-the-fourier-transform-coefficients",
    "href": "ss_41.html#deriving-the-fourier-transform-coefficients",
    "title": "Continuous-Time Signals and Systems",
    "section": "Deriving the Fourier Transform: Coefficients",
    "text": "Deriving the Fourier Transform: Coefficients\nLet’s examine the Fourier series representation of \\(\\tilde{x}(t)\\):\n\\[\n\\tilde{x}(t)=\\sum_{k=-\\infty}^{+\\infty} a_{k} e^{j k \\omega_{0} t}\n\\]\nThe Fourier coefficients \\(a_k\\) are:\n\\[\na_{k}=\\frac{1}{T} \\int_{-T / 2}^{T / 2} \\tilde{x}(t) e^{-j k \\omega_{0} t} d t\n\\]\nSince \\(\\tilde{x}(t)=x(t)\\) for \\(|t|&lt;T/2\\) and \\(x(t)=0\\) outside this interval (assuming \\(T_1 &lt; T/2\\)), we can rewrite \\(a_k\\) as:\n\\[\na_{k}=\\frac{1}{T} \\int_{-T / 2}^{T / 2} x(t) e^{-j k \\omega_{0} t} d t=\\frac{1}{T} \\int_{-\\infty}^{+\\infty} x(t) e^{-j k \\omega_{0} t} d t\n\\]\n\nWe start with the familiar Fourier series equations. The key simplification happens when we realize that the periodic signal \\(\\tilde{x}(t)\\) is equal to the aperiodic signal \\(x(t)\\) within one period, and \\(x(t)\\) is zero outside a certain range. This allows us to change the integration limits from one period to infinity, effectively focusing on the aperiodic signal itself."
  },
  {
    "objectID": "ss_41.html#defining-the-fourier-transform-xjomega",
    "href": "ss_41.html#defining-the-fourier-transform-xjomega",
    "title": "Continuous-Time Signals and Systems",
    "section": "Defining the Fourier Transform \\(X(j\\omega)\\)",
    "text": "Defining the Fourier Transform \\(X(j\\omega)\\)\nFrom the previous slide, we have:\n\\[\na_k = \\frac{1}{T} \\int_{-\\infty}^{+\\infty} x(t) e^{-j k \\omega_0 t} dt\n\\]\nWe define the Fourier Transform \\(X(j\\omega)\\) as the integral:\n\\[\nX(j \\omega)=\\int_{-\\infty}^{+\\infty} x(t) e^{-j \\omega t} d t \\quad \\text{(Analysis Equation)}\n\\]\nUsing this definition, the Fourier coefficients \\(a_k\\) can be expressed as samples of \\(X(j\\omega)\\):\n\\[\na_{k}=\\frac{1}{T} X\\left(j k \\omega_{0}\\right)\n\\]\n\nThis is the formal definition of the Continuous-Time Fourier Transform (CTFT). It takes a time-domain signal \\(x(t)\\) and transforms it into a frequency-domain representation \\(X(j\\omega)\\). Notice the similarity to the Fourier series coefficient formula, but now it’s an integral over continuous frequency \\(\\omega\\)."
  },
  {
    "objectID": "ss_41.html#deriving-the-fourier-transform-synthesis-equation",
    "href": "ss_41.html#deriving-the-fourier-transform-synthesis-equation",
    "title": "Continuous-Time Signals and Systems",
    "section": "Deriving the Fourier Transform: Synthesis Equation",
    "text": "Deriving the Fourier Transform: Synthesis Equation\nSubstitute \\(a_k = \\frac{1}{T} X(j k \\omega_0)\\) back into the Fourier series of \\(\\tilde{x}(t)\\):\n\\[\n\\tilde{x}(t)=\\sum_{k=-\\infty}^{+\\infty} \\frac{1}{T} X\\left(j k \\omega_{0}\\right) e^{j k \\omega_{0} t}\n\\]\nSince \\(\\omega_0 = 2\\pi/T\\), we can write \\(\\frac{1}{T} = \\frac{\\omega_0}{2\\pi}\\). So,\n\\[\n\\tilde{x}(t)=\\frac{1}{2 \\pi} \\sum_{k=-\\infty}^{+\\infty} X\\left(j k \\omega_{0}\\right) e^{j k \\omega_{0} t} \\omega_{0}\n\\]\n\nWe’re taking the expression for the Fourier series of the periodic signal \\(\\tilde{x}(t)\\) and substituting our newly defined \\(X(j\\omega)\\) into it. This sets the stage for the crucial limiting step."
  },
  {
    "objectID": "ss_41.html#the-limit-from-sum-to-integral",
    "href": "ss_41.html#the-limit-from-sum-to-integral",
    "title": "Continuous-Time Signals and Systems",
    "section": "The Limit: From Sum to Integral",
    "text": "The Limit: From Sum to Integral\nAs \\(T \\to \\infty\\), we have:\n\n\\(\\tilde{x}(t) \\to x(t)\\)\n\\(\\omega_0 \\to 0\\)\nThe summation becomes an integral.\n\nThe summation: \\[\n\\tilde{x}(t)=\\frac{1}{2 \\pi} \\sum_{k=-\\infty}^{+\\infty} X\\left(j k \\omega_{0}\\right) e^{j k \\omega_{0} t} \\omega_{0}\n\\] Can be graphically interpreted as a Riemann sum. Each term is the area of a rectangle of height \\(X(j k \\omega_0)e^{j k \\omega_0 t}\\) and width \\(\\omega_0\\)."
  },
  {
    "objectID": "ss_41.html#the-limit-from-sum-to-integral-cont.",
    "href": "ss_41.html#the-limit-from-sum-to-integral-cont.",
    "title": "Continuous-Time Signals and Systems",
    "section": "The Limit: From Sum to Integral (cont.)",
    "text": "The Limit: From Sum to Integral (cont.)\nThe summation resembles a Riemann sum in integral calculus:\n\nA Riemann sum approximates an integral by summing rectangles under a curve.\nEach term in the Fourier series can be seen as a rectangle:\n\nHeight: \\(X(jk\\omega_0) e^{jk\\omega_0 t}\\) — the value of the function (complex amplitude) at frequency \\(k\\omega_0\\)\nWidth: \\(\\omega_0\\) — the spacing between frequency samples\n\n\nSo the entire sum approximates the inverse Fourier transform integral:\n\\[\nx(t) = \\frac{1}{2\\pi} \\int_{-\\infty}^{+\\infty} X(j\\omega) e^{j\\omega t} d\\omega\n\\]\nThe discrete version (your summation) is a Riemann sum approximation of this integral, where the continuous spectrum \\(X(j\\omega)\\) is sampled at intervals of \\(\\omega_0\\).\n\n\n\n\n\ngraph LR\n    A[\"Summation $$\\sum_{k=-\\infty}^{+\\infty} f(k \\omega_0) \\omega_0$$\"] --&gt; B{\"As $$\\omega_0 \\to 0$$\"};\n    B --&gt; C[\"Integral $$\\int_{-\\infty}^{+\\infty} f(\\omega) d\\omega$$\"];\n\n\n\n\n\n\nAs \\(\\omega_0 \\to 0\\), the sum converges to an integral.\n\nThis is the heart of the derivation. The discrete sum, with infinitesimally small frequency spacing \\(\\omega_0\\), transforms into a continuous integral. This is analogous to how a Riemann sum approximates and then becomes a definite integral in calculus."
  },
  {
    "objectID": "ss_41.html#the-continuous-time-fourier-transform-pair",
    "href": "ss_41.html#the-continuous-time-fourier-transform-pair",
    "title": "Continuous-Time Signals and Systems",
    "section": "The Continuous-Time Fourier Transform Pair",
    "text": "The Continuous-Time Fourier Transform Pair\nTaking the limit, we arrive at the Fourier Transform Pair:\nSynthesis Equation (Inverse Fourier Transform)\nReconstructs the time-domain signal \\(x(t)\\) from its frequency-domain representation \\(X(j\\omega)\\):\n\\[\nx(t)=\\frac{1}{2 \\pi} \\int_{-\\infty}^{+\\infty} X(j \\omega) e^{j \\omega t} d \\omega\n\\]\nAnalysis Equation (Forward Fourier Transform)\nComputes the frequency-domain representation \\(X(j\\omega)\\) from the time-domain signal \\(x(t)\\):\n\\[\nX(j \\omega)=\\int_{-\\infty}^{+\\infty} x(t) e^{-j \\omega t} d t\n\\]\n\n\n\n\n\n\nImportant\n\n\nThe function \\(X(j\\omega)\\) is often referred to as the spectrum of \\(x(t)\\). It describes the signal’s content at different frequencies.\n\n\n\n\nThese two equations are the cornerstone of continuous-time Fourier analysis. The analysis equation tells us “what frequencies are present and how much of each,” while the synthesis equation tells us “how to put those frequencies back together to get the original signal.” Emphasize that \\(X(j\\omega)\\) is a continuous function of frequency \\(\\omega\\), unlike the discrete \\(a_k\\) coefficients for periodic signals."
  },
  {
    "objectID": "ss_41.html#fourier-series-coefficients-from-fourier-transform",
    "href": "ss_41.html#fourier-series-coefficients-from-fourier-transform",
    "title": "Continuous-Time Signals and Systems",
    "section": "Fourier Series Coefficients from Fourier Transform",
    "text": "Fourier Series Coefficients from Fourier Transform\nWe established a direct relationship between the Fourier series coefficients \\(a_k\\) of a periodic signal \\(\\tilde{x}(t)\\) and the Fourier transform \\(X(j\\omega)\\) of one period of that signal.\nIf \\(\\tilde{x}(t)\\) is periodic with period \\(T\\) and Fourier coefficients \\(a_k\\), and \\(x(t)\\) is a finite-duration signal equal to \\(\\tilde{x}(t)\\) over one period (and zero otherwise), then:\n\\[\na_{k}=\\left.\\frac{1}{T} X(j \\omega)\\right|_{\\omega=k \\omega_{0}}\n\\]\nwhere \\(X(j\\omega)\\) is the Fourier transform of \\(x(t)\\).\n\n\n\n\n\n\nTip\n\n\nThis means the discrete spectrum of a periodic signal is proportional to samples of the continuous spectrum of a single period of that signal.\n\n\n\n\nThis equation is very useful in practice. If you know how to find the Fourier transform of a single pulse, you can immediately find the Fourier series coefficients for a periodic repetition of that pulse by simply sampling the Fourier transform at the harmonic frequencies and scaling by \\(1/T\\)."
  },
  {
    "objectID": "ss_41.html#convergence-of-fourier-transforms",
    "href": "ss_41.html#convergence-of-fourier-transforms",
    "title": "Continuous-Time Signals and Systems",
    "section": "Convergence of Fourier Transforms",
    "text": "Convergence of Fourier Transforms\nFor the Fourier Transform to be a valid representation, certain conditions must be met.\nFinite Energy Condition\nIf \\(x(t)\\) has finite energy (is square integrable):\n\\[\n\\int_{-\\infty}^{+\\infty}|x(t)|^{2} d t&lt;\\infty\n\\]\nThen \\(X(j\\omega)\\) is finite, and the energy in the error between \\(x(t)\\) and its Fourier representation \\(\\hat{x}(t)\\) is zero:\n\\[\n\\int_{-\\infty}^{+\\infty}|e(t)|^{2} d t=0\n\\]\n\n\n\n\n\n\nNote\n\n\nThis means that even if \\(x(t)\\) and \\(\\hat{x}(t)\\) differ at isolated points, their difference contains no energy.\n\n\n\n\nThese conditions are analogous to those for Fourier series. The finite energy condition is a very broad and useful criterion for convergence in an engineering context. It implies that the signal doesn’t “blow up” to infinity and has a well-behaved frequency spectrum."
  },
  {
    "objectID": "ss_41.html#dirichlet-conditions-for-pointwise-convergence",
    "href": "ss_41.html#dirichlet-conditions-for-pointwise-convergence",
    "title": "Continuous-Time Signals and Systems",
    "section": "Dirichlet Conditions for Pointwise Convergence",
    "text": "Dirichlet Conditions for Pointwise Convergence\nAn alternative set of conditions, known as the Dirichlet conditions, ensures that \\(\\hat{x}(t)\\) equals \\(x(t)\\) for all \\(t\\), except at discontinuities where it equals the average of the values on either side.\n\nAbsolutely Integrable: \\[\n\\int_{-\\infty}^{+\\infty}|x(t)| d t&lt;\\infty\n\\]\nFinite Maxima and Minima: \\(x(t)\\) must have a finite number of maxima and minima within any finite interval.\nFinite Discontinuities: \\(x(t)\\) must have a finite number of discontinuities within any finite interval, and each must be finite.\n\n\n\n\n\n\n\nTip\n\n\nMost physically realizable signals satisfy these conditions. However, periodic signals are not absolutely integrable over an infinite interval, requiring a different approach (e.g., using impulse functions in the transform, which we’ll see later).\n\n\n\n\nThe Dirichlet conditions are stricter and guarantee pointwise convergence. They are easier to check for many common signals. It’s important to note that periodic signals, while having Fourier series, do not satisfy the absolute integrability condition over an infinite range, which is why their Fourier transforms require special handling (impulse functions)."
  },
  {
    "objectID": "ss_41.html#examples-of-continuous-time-fourier-transforms",
    "href": "ss_41.html#examples-of-continuous-time-fourier-transforms",
    "title": "Continuous-Time Signals and Systems",
    "section": "Examples of Continuous-Time Fourier Transforms",
    "text": "Examples of Continuous-Time Fourier Transforms\nLet’s explore some common signals and their Fourier Transforms. These examples build intuition about how signals in the time domain relate to their frequency domain representations.\n\n\n\n\n\n\nNote\n\n\nPay attention to the shape of the time-domain signal and the corresponding shape of its spectrum. Look for symmetries and characteristics in both domains."
  },
  {
    "objectID": "ss_41.html#example-4.1-exponential-decay-xt-e-atut-a0",
    "href": "ss_41.html#example-4.1-exponential-decay-xt-e-atut-a0",
    "title": "Continuous-Time Signals and Systems",
    "section": "Example 4.1: Exponential Decay \\(x(t) = e^{-at}u(t)\\), \\(a>0\\)",
    "text": "Example 4.1: Exponential Decay \\(x(t) = e^{-at}u(t)\\), \\(a&gt;0\\)\nSignal Definition\n\\[\nx(t)=e^{-a t} u(t) \\quad a&gt;0\n\\]\nFourier Transform Derivation\n\\[\nX(j \\omega)=\\int_{0}^{\\infty} e^{-a t} e^{-j \\omega t} d t = \\frac{1}{a+j \\omega}\n\\]\nMagnitude and Phase\n\\[\n|X(j \\omega)|=\\frac{1}{\\sqrt{a^{2}+\\omega^{2}}}, \\quad \\angle X(j \\omega)=-\\tan ^{-1}\\left(\\frac{\\omega}{a}\\right)\n\\]\n\nThis is a fundamental example. The signal starts at 1 and decays exponentially. It’s causal because of the unit step function \\(u(t)\\). The Fourier transform is complex-valued, so we typically look at its magnitude and phase."
  },
  {
    "objectID": "ss_41.html#interactive-demo-fourier-transform-of-e-atut",
    "href": "ss_41.html#interactive-demo-fourier-transform-of-e-atut",
    "title": "Continuous-Time Signals and Systems",
    "section": "Interactive Demo: Fourier Transform of \\(e^{-at}u(t)\\)",
    "text": "Interactive Demo: Fourier Transform of \\(e^{-at}u(t)\\)\nExplore how the parameter \\(a\\) affects both the time-domain signal and its frequency spectrum.\n\nviewof a_val_exp = Inputs.range([0.1, 5], {label: \"Decay Rate (a)\", step: 0.1, value: 1})\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs a increases, the time-domain signal decays faster (becomes narrower). In the frequency domain, the magnitude spectrum |X(jω)| becomes wider, indicating a broader distribution of frequencies. A faster decay in time means more high-frequency components are needed to represent the sharp transition."
  },
  {
    "objectID": "ss_41.html#example-4.2-double-sided-exponential-xt-e-at-a0",
    "href": "ss_41.html#example-4.2-double-sided-exponential-xt-e-at-a0",
    "title": "Continuous-Time Signals and Systems",
    "section": "Example 4.2: Double-Sided Exponential \\(x(t) = e^{-a|t|}\\), \\(a>0\\)",
    "text": "Example 4.2: Double-Sided Exponential \\(x(t) = e^{-a|t|}\\), \\(a&gt;0\\)\nSignal Definition\n\\[\nx(t)=e^{-a|t|}, \\quad a&gt;0\n\\]\nThis signal is symmetric around \\(t=0\\).\nFourier Transform Derivation\n\\[\n\\begin{aligned}\nX(j \\omega) & =\\int_{-\\infty}^{+\\infty} e^{-a|t|} e^{-j \\omega t} d t \\\\\n& = \\int_{-\\infty}^{0} e^{a t} e^{-j \\omega t} d t + \\int_{0}^{\\infty} e^{-a t} e^{-j \\omega t} d t \\\\\n& = \\frac{1}{a-j \\omega} + \\frac{1}{a+j \\omega} \\\\\n& = \\frac{2a}{a^{2}+\\omega^{2}}\n\\end{aligned}\n\\]\n\nThis signal is an even function of time. A key property of Fourier transforms is that if \\(x(t)\\) is real and even, then \\(X(j\\omega)\\) is also real and even. Notice that the result here is purely real, unlike the previous example."
  },
  {
    "objectID": "ss_41.html#interactive-demo-fourier-transform-of-e-at",
    "href": "ss_41.html#interactive-demo-fourier-transform-of-e-at",
    "title": "Continuous-Time Signals and Systems",
    "section": "Interactive Demo: Fourier Transform of \\(e^{-a|t|}\\)",
    "text": "Interactive Demo: Fourier Transform of \\(e^{-a|t|}\\)\nObserve the symmetry in both time and frequency domains for this real and even signal.\n\nviewof a_val_abs = Inputs.range([0.1, 5], {label: \"Decay Rate (a)\", step: 0.1, value: 1})\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimilar to the previous example, as a increases, the time-domain signal becomes narrower (more concentrated around \\(t=0\\)). Consequently, its frequency spectrum X(jω) becomes wider, indicating a broader range of frequencies. This inverse relationship between time-domain spread and frequency-domain spread is a fundamental concept."
  },
  {
    "objectID": "ss_41.html#example-4.3-unit-impulse-xt-deltat",
    "href": "ss_41.html#example-4.3-unit-impulse-xt-deltat",
    "title": "Continuous-Time Signals and Systems",
    "section": "Example 4.3: Unit Impulse \\(x(t) = \\delta(t)\\)",
    "text": "Example 4.3: Unit Impulse \\(x(t) = \\delta(t)\\)\nSignal Definition\n\\[\nx(t)=\\delta(t)\n\\]\nThe unit impulse is a signal that is zero everywhere except at \\(t=0\\), where its integral is one.\nFourier Transform Derivation\nUsing the sifting property of the impulse function:\n\\[\nX(j \\omega)=\\int_{-\\infty}^{+\\infty} \\delta(t) e^{-j \\omega t} d t = e^{-j \\omega (0)} = 1\n\\]\n\n\n\n\n\n\nNote\n\n\nThe Fourier transform of a unit impulse is \\(1\\). This means the unit impulse contains equal contributions from all frequencies. It has a “flat” or “constant” spectrum.\n\n\n\n\nThis is a very important result. An impulse in time is perfectly localized in time, and to achieve this perfect localization, it must contain all possible frequencies with equal amplitude. This tells us that very short, sharp signals require a very wide range of frequencies to represent them."
  },
  {
    "objectID": "ss_41.html#example-4.4-rectangular-pulse",
    "href": "ss_41.html#example-4.4-rectangular-pulse",
    "title": "Continuous-Time Signals and Systems",
    "section": "Example 4.4: Rectangular Pulse",
    "text": "Example 4.4: Rectangular Pulse\nSignal Definition\n\\[\nx(t)= \\begin{cases}1, & |t|&lt;T_{1} \\\\ 0, & |t|&gt;T_{1}\\end{cases}\n\\]\nThis is a pulse of width \\(2T_1\\) and height 1, centered at \\(t=0\\).\nFourier Transform Derivation\n\\[\nX(j \\omega)=\\int_{-T_{1}}^{T_{1}} e^{-j \\omega t} d t = 2 \\frac{\\sin \\omega T_{1}}{\\omega}\n\\]\n\nThis is another classic example. The Fourier transform of a rectangular pulse is a sinc function (or a scaled version of it). This function has a main lobe and then decaying side lobes. The width of the main lobe is inversely proportional to the width of the pulse in the time domain."
  },
  {
    "objectID": "ss_41.html#interactive-demo-fourier-transform-of-a-rectangular-pulse",
    "href": "ss_41.html#interactive-demo-fourier-transform-of-a-rectangular-pulse",
    "title": "Continuous-Time Signals and Systems",
    "section": "Interactive Demo: Fourier Transform of a Rectangular Pulse",
    "text": "Interactive Demo: Fourier Transform of a Rectangular Pulse\nAdjust the pulse width \\(T_1\\) and observe its effect on the frequency spectrum.\n\nviewof T1_val_rect = Inputs.range([0.1, 5], {label: \"Pulse Half-Width (T1)\", step: 0.1, value: 1})\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice the inverse relationship: a wider pulse in time (T1 increases) results in a narrower main lobe in the frequency spectrum. This means that a longer pulse has a more concentrated frequency content. This is a crucial concept for understanding bandwidth and signal duration. Also, observe the Gibbs phenomenon if you try to reconstruct this signal from a finite number of frequency components."
  },
  {
    "objectID": "ss_41.html#example-4.5-ideal-low-pass-filter-sinc-function-in-time",
    "href": "ss_41.html#example-4.5-ideal-low-pass-filter-sinc-function-in-time",
    "title": "Continuous-Time Signals and Systems",
    "section": "Example 4.5: Ideal Low-Pass Filter (Sinc Function in Time)",
    "text": "Example 4.5: Ideal Low-Pass Filter (Sinc Function in Time)\nTransform Definition\n\\[\nX(j \\omega)= \\begin{cases}1, & |\\omega|&lt;W \\\\ 0, & |\\omega|&gt;W\\end{cases}\n\\]\nThis is a rectangular pulse in the frequency domain. It represents an ideal low-pass filter, passing all frequencies below \\(W\\) and blocking those above.\nInverse Fourier Transform Derivation\n\\[\nx(t)=\\frac{1}{2 \\pi} \\int_{-W}^{W} e^{j \\omega t} d \\omega=\\frac{\\sin W t}{\\pi t}\n\\]\n\nThis is the dual of the previous example! Here, the frequency spectrum is a rectangle. When we take the inverse Fourier transform, the time-domain signal is a sinc function. This signal is non-causal (extends to negative infinity) and oscillates, which are characteristics of an ideal filter’s impulse response."
  },
  {
    "objectID": "ss_41.html#interactive-demo-inverse-ft-of-a-frequency-pulse",
    "href": "ss_41.html#interactive-demo-inverse-ft-of-a-frequency-pulse",
    "title": "Continuous-Time Signals and Systems",
    "section": "Interactive Demo: Inverse FT of a Frequency Pulse",
    "text": "Interactive Demo: Inverse FT of a Frequency Pulse\nAdjust the bandwidth \\(W\\) of the frequency-domain pulse and see its effect on the time-domain signal.\n\nviewof W_val_sinc = Inputs.range([0.1, 10], {label: \"Bandwidth (W)\", step: 0.1, value: 3})\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, a wider frequency bandwidth W results in a narrower main lobe in the time-domain signal. This again illustrates the inverse relationship. A signal with a broader frequency content can be more concentrated in time. This is fundamental to understanding concepts like time-bandwidth product."
  },
  {
    "objectID": "ss_41.html#the-sinc-function",
    "href": "ss_41.html#the-sinc-function",
    "title": "Continuous-Time Signals and Systems",
    "section": "The Sinc Function",
    "text": "The Sinc Function\nFunctions of the form \\(\\frac{\\sin(\\theta)}{\\theta}\\) appear frequently in Fourier analysis. A commonly used precise form for the sinc function is:\n\\[\n\\operatorname{sinc}(\\theta)=\\frac{\\sin \\pi \\theta}{\\pi \\theta}\n\\]\nExamples Using Sinc\n\nFourier Transform of Rectangular Pulse: \\[ \\frac{2 \\sin \\omega T_{1}}{\\omega} = 2 T_{1} \\operatorname{sinc}\\left(\\frac{\\omega T_{1}}{\\pi}\\right) \\]\nInverse Fourier Transform of Rectangular Frequency Pulse: \\[ \\frac{\\sin W t}{\\pi t} = \\frac{W}{\\pi} \\operatorname{sinc}\\left(\\frac{W t}{\\pi}\\right) \\]"
  },
  {
    "objectID": "ss_41.html#the-sinc-function-1",
    "href": "ss_41.html#the-sinc-function-1",
    "title": "Continuous-Time Signals and Systems",
    "section": "The Sinc Function",
    "text": "The Sinc Function\nPlotting \\(\\operatorname{sinc}(\\theta)\\)\nLet’s see how the \\(\\operatorname{sinc}\\) function behaves.\n\n\n\n\n\n\n\nThe sinc function is normalized such that sinc(0) = 1 and its zero crossings occur at integer values (e.g., \\(\\theta = \\pm 1, \\pm 2, \\dots\\)). It’s important to be aware of this specific definition as sometimes sin(x)/x is also informally called sinc. The properties of the sinc function (main lobe, side lobes, decay) are critical for understanding signal reconstruction and filter design."
  },
  {
    "objectID": "ss_41.html#time-frequency-duality-and-bandwidth",
    "href": "ss_41.html#time-frequency-duality-and-bandwidth",
    "title": "Continuous-Time Signals and Systems",
    "section": "Time-Frequency Duality and Bandwidth",
    "text": "Time-Frequency Duality and Bandwidth\nA fundamental concept in Fourier analysis is the inverse relationship between the spread of a signal in the time domain and its spread in the frequency domain.\n\n\nWide in Time, Narrow in Frequency\n\nA signal that is spread out (long duration) in the time domain tends to have a narrow (concentrated) spectrum in the frequency domain.\nExample: A long rectangular pulse in time has a narrow \\(\\operatorname{sinc}\\) function in frequency.\n\n\nNarrow in Time, Wide in Frequency\n\nA signal that is concentrated (short duration) in the time domain tends to have a wide (broad) spectrum in the frequency domain.\nExample: A short rectangular pulse in time has a wide \\(\\operatorname{sinc}\\) function in frequency.\n\n\n\n\n\n\n\n\nImportant\n\n\nThis time-frequency duality implies that you cannot simultaneously localize a signal arbitrarily well in both time and frequency. This is often referred to as the uncertainty principle in signal processing.\n\n\n\n\nThis duality is a cornerstone of signal processing. It explains why a very short burst of sound (like a click) contains a wide range of frequencies, and conversely, why a pure tone (a single frequency) must extend infinitely in time. It has implications for filter design, communication system bandwidth, and quantum mechanics (Heisenberg’s Uncertainty Principle)."
  },
  {
    "objectID": "ss_41.html#interactive-demo-time-frequency-duality",
    "href": "ss_41.html#interactive-demo-time-frequency-duality",
    "title": "Continuous-Time Signals and Systems",
    "section": "Interactive Demo: Time-Frequency Duality",
    "text": "Interactive Demo: Time-Frequency Duality\nObserve the inverse relationship between signal duration and spectral width.\n\nviewof param_val = Inputs.range([0.1, 5], {label: \"Pulse Parameter (T1 or W)\", step: 0.1, value: 1})\nviewof type_val = Inputs.select([\"Time Pulse (Rect)\", \"Freq Pulse (Sinc)\"], {label: \"Signal Type\", value: \"Time Pulse (Rect)\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse the Signal Type dropdown to switch between a time-domain rectangular pulse and a frequency-domain rectangular pulse. Then, adjust Pulse Parameter (T1 or W). When T1 increases for the “Time Pulse”, the pulse gets wider, and its sinc spectrum gets narrower. When W increases for the “Freq Pulse”, the frequency pulse gets wider, and its sinc time function gets narrower. This visually reinforces the time-frequency duality."
  },
  {
    "objectID": "ss_41.html#summary",
    "href": "ss_41.html#summary",
    "title": "Continuous-Time Signals and Systems",
    "section": "Summary",
    "text": "Summary\nKey Takeaways\n\nFrom Series to Transform: The Fourier Transform represents aperiodic signals as the limit of Fourier series as the period approaches infinity.\nFourier Transform Pair: \\[ X(j \\omega)=\\int_{-\\infty}^{+\\infty} x(t) e^{-j \\omega t} d t \\] \\[ x(t)=\\frac{1}{2 \\pi} \\int_{-\\infty}^{+\\infty} X(j \\omega) e^{j \\omega t} d \\omega \\]\nConvergence: Conditions like finite energy or Dirichlet conditions ensure a valid Fourier Transform representation.\nExamples: We explored the transforms of exponential decay, double-sided exponential, unit impulse, rectangular pulse, and the sinc function.\nTime-Frequency Duality: A fundamental principle showing an inverse relationship between a signal’s spread in time and its spread in frequency.\n\n\n\n\n\n\n\nNote\n\n\nThe Fourier Transform is a powerful tool for analyzing signals and systems, providing insights into their frequency content and behavior.\n\n\n\n\nThis summary covers the main points discussed. Encourage students to remember the core definitions, the examples as building blocks, and especially the concept of time-frequency duality, which will be revisited many times in the course."
  },
  {
    "objectID": "ss_43.html#properties-of-the-continuous-time-fourier-transform",
    "href": "ss_43.html#properties-of-the-continuous-time-fourier-transform",
    "title": "Continuous-Time Fourier Transform",
    "section": "Properties of the Continuous-Time Fourier Transform",
    "text": "Properties of the Continuous-Time Fourier Transform"
  },
  {
    "objectID": "ss_43.html#why-study-ft-properties",
    "href": "ss_43.html#why-study-ft-properties",
    "title": "Continuous-Time Fourier Transform",
    "section": "Why Study FT Properties?",
    "text": "Why Study FT Properties?\nUnderstanding Fourier Transform properties is crucial for several reasons. They provide significant insight into the transform and the relationship between time-domain and frequency-domain signal descriptions. Often useful in reducing the complexity of evaluating Fourier transforms or inverse transforms. Many properties translate directly to Fourier Series properties, highlighting fundamental connections.\nShorthand Notation\nA signal \\(x(t)\\) and its Fourier transform \\(X(j \\omega)\\) are related by:\n\\[\nx(t)=\\frac{1}{2 \\pi} \\int_{-\\infty}^{+\\infty} X(j \\omega) e^{j \\omega t} d \\omega\n\\]\n\\[\nX(j \\omega)=\\int_{-\\infty}^{+\\infty} x(t) e^{-j \\omega t} d t\n\\]\nWe denote this relationship as a Fourier transform pair: \\(x(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X(j \\omega)\\).\n\n\n\n\n\n\nNote\n\n\nFor example: $ e^{-a t} u(t) $.\n\n\n\n\nWelcome to this session on the properties of the Continuous-Time Fourier Transform. Understanding these properties is crucial for several reasons. Firstly, they provide a deep insight into how signals behave in both the time and frequency domains. Secondly, they can significantly simplify the process of finding Fourier transforms or inverse transforms, often allowing us to avoid complex integral calculations. Finally, many of these properties have direct parallels with Fourier Series properties we encountered earlier, highlighting the fundamental connections in signal analysis. We’ll also establish a common shorthand notation to make our discussions more concise, using the double-headed arrow to indicate a Fourier transform pair."
  },
  {
    "objectID": "ss_43.html#linearity-property",
    "href": "ss_43.html#linearity-property",
    "title": "Continuous-Time Fourier Transform",
    "section": "Linearity Property",
    "text": "Linearity Property\nIf \\(x(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X(j \\omega)\\) and \\(y(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} Y(j \\omega)\\), then:\n\\[\na x(t)+b y(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} a X(j \\omega)+b Y(j \\omega)\n\\]\nThe proof follows directly by applying the analysis equation to \\(a x(t)+b y(t)\\). This property extends easily to a linear combination of an arbitrary number of signals.\n\n\n\n\n\n\nTip\n\n\nThis property is fundamental! It means we can break down complex signals into simpler components, find the transform of each, and then linearly combine them. This greatly simplifies analysis!\n\n\n\n\nThe linearity property is perhaps one of the most straightforward yet powerful properties. It states that the Fourier transform of a linear combination of signals is simply the same linear combination of their individual Fourier transforms. This is extremely useful because it means we can often decompose a complicated signal into a sum of simpler signals, find the transform of each simple signal, and then just add them up. This principle is widely used in signal processing, for instance, when analyzing circuits with multiple inputs."
  },
  {
    "objectID": "ss_43.html#time-shifting-property",
    "href": "ss_43.html#time-shifting-property",
    "title": "Continuous-Time Fourier Transform",
    "section": "Time Shifting Property",
    "text": "Time Shifting Property\nIf \\(x(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X(j \\omega)\\), then:\n\\[\nx\\left(t-t_{0}\\right) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} e^{-j \\omega t_{0}} X(j \\omega)\n\\]\nEffect of Time Shift\nA time shift in the time domain introduces a phase shift in the frequency domain. The magnitude of the Fourier transform remains unaltered: \\(|X(j \\omega)|\\). The phase changes linearly with frequency: \\(\\angle X(j \\omega) - \\omega t_0\\).\n\nThe time-shifting property tells us what happens to the Fourier transform when a signal is merely delayed or advanced in time. The key takeaway here is that a time shift only affects the phase of the Fourier transform, not its magnitude. The phase shift is linear with frequency, which is a very important characteristic. This property is crucial in understanding how delays in communication systems or physical phenomena affect the frequency content of a signal."
  },
  {
    "objectID": "ss_43.html#example-4.9-time-shifting-application",
    "href": "ss_43.html#example-4.9-time-shifting-application",
    "title": "Continuous-Time Fourier Transform",
    "section": "Example 4.9: Time Shifting Application",
    "text": "Example 4.9: Time Shifting Application\nDecomposing a signal \\(x(t)\\) into simpler pulses using linearity and time-shifting.\nOriginal Signal \\(x(t)\\) (Conceptual)\n\n\n\n\n\ngraph LR\n    A[\"x(t)\"] --&gt; B{Decomposition}\n    B --&gt; C[\"0.5 * x1(t-2.5)\"]\n    B --&gt; D[\"x2(t-2.5)\"]\n    C & D --&gt; E[\"X(jω)\"]\n\n\n\n\n\n\nThis diagram illustrates how a complex signal x(t) can be broken down into simpler, shifted components x1 and x2.\nComponent Signals\n\n\\(x_1(t)\\) is a rectangular pulse of width 1, centered at 0.\n\\(x_2(t)\\) is a rectangular pulse of width 3, centered at 0."
  },
  {
    "objectID": "ss_43.html#example-4.9-time-shifting-application-cont.",
    "href": "ss_43.html#example-4.9-time-shifting-application-cont.",
    "title": "Continuous-Time Fourier Transform",
    "section": "Example 4.9: Time Shifting Application (cont.)",
    "text": "Example 4.9: Time Shifting Application (cont.)\nKnown Transforms of Components\n\n\\(X_1(j \\omega) = \\frac{2 \\sin(\\omega/2)}{\\omega}\\)\n\\(X_2(j \\omega) = \\frac{2 \\sin(3\\omega/2)}{\\omega}\\)\n\nApplying Linearity & Time-Shift\nGiven \\(x(t) = \\frac{1}{2} x_1(t-2.5) + x_2(t-2.5)\\), then:\n\\(X(j \\omega) = \\mathcal{F}\\left\\{ \\frac{1}{2} x_1(t-2.5) \\right\\} + \\mathcal{F}\\left\\{ x_2(t-2.5) \\right\\}\\)\n\\(X(j \\omega) = \\frac{1}{2} e^{-j 2.5\\omega} X_1(j \\omega) + e^{-j 2.5\\omega} X_2(j \\omega)\\)\n\\(X(j \\omega) = e^{-j 5\\omega / 2} \\left\\{ \\frac{1}{2} \\frac{2 \\sin(\\omega/2)}{\\omega} + \\frac{2 \\sin(3\\omega/2)}{\\omega} \\right\\}\\)\n\\(X(j \\omega) = e^{-j 5\\omega / 2} \\left\\{ \\frac{\\sin(\\omega/2) + 2 \\sin(3\\omega/2)}{\\omega} \\right\\}\\)\n\n\n\n\n\n\nNote\n\n\nNotice how a complex signal’s transform can be found by breaking it into pieces whose transforms are known, then applying the properties.\n\n\n\n\nLet’s look at Example 4.9, which beautifully illustrates the combined power of linearity and time-shifting. The original signal x(t) is a composite shape. Instead of directly integrating to find its Fourier transform, we can decompose it into two simpler rectangular pulses, x1(t) and x2(t). We know the transforms of basic rectangular pulses. Then, by applying the time-shifting property to account for their shifted positions and the linearity property for their weighted sum, we can easily find the transform of the overall signal. This approach is significantly simpler than direct integration for such a signal."
  },
  {
    "objectID": "ss_43.html#conjugation-and-conjugate-symmetry",
    "href": "ss_43.html#conjugation-and-conjugate-symmetry",
    "title": "Continuous-Time Fourier Transform",
    "section": "Conjugation and Conjugate Symmetry",
    "text": "Conjugation and Conjugate Symmetry\nConjugation Property\nIf \\(x(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X(j \\omega)\\), then:\n\\[\nx^{*}(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X^{*}(-j \\omega)\n\\]\nConjugate Symmetry for Real Signals\nIf $ x(t) $ is a real-valued signal, then its Fourier Transform $ X(j ) $ exhibits conjugate symmetry:\n\\[\nX(-j \\omega) = X^{*}(j \\omega) \\quad [x(t) \\text{ real}]\n\\]\nImplications for Real Signals\n\nThe real part of \\(X(j \\omega)\\), \\(\\operatorname{Re}\\{X(j \\omega)\\}\\), is an even function of \\(\\omega\\).\nThe imaginary part of \\(X(j \\omega)\\), \\(\\operatorname{Im}\\{X(j \\omega)\\}\\), is an odd function of \\(\\omega\\).\nThe magnitude, \\(|X(j \\omega)|\\), is an even function of \\(\\omega\\).\nThe phase, \\(\\Varangle X(j \\omega)\\), is an odd function of \\(\\omega\\).\n\n\nThe conjugation property deals with the relationship between the Fourier transform of a signal and its complex conjugate. This leads directly to the concept of conjugate symmetry, which is particularly important for real-valued signals. For real signals, the Fourier transform exhibits a specific symmetry: its real part is even, its imaginary part is odd, its magnitude is even, and its phase is odd. This means we only need to compute the transform for positive frequencies, as the negative frequency components are determined by symmetry. This halves the computational effort and provides a deeper understanding of the frequency content."
  },
  {
    "objectID": "ss_43.html#example-4.10-using-symmetry",
    "href": "ss_43.html#example-4.10-using-symmetry",
    "title": "Continuous-Time Fourier Transform",
    "section": "Example 4.10: Using Symmetry",
    "text": "Example 4.10: Using Symmetry\nFind the Fourier transform of \\(x(t) = e^{-a|t|}\\), where \\(a&gt;0\\).\n\nWe know the transform pair: \\(e^{-at}u(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\frac{1}{a+j \\omega}\\).\nObserve that \\(x(t) = e^{-a|t|} = e^{-at}u(t) + e^{at}u(-t)\\). This can be written as \\(x(t) = 2 \\mathcal{E}v\\{e^{-at}u(t)\\}\\), where \\(\\mathcal{E}v\\{\\cdot\\}\\) denotes the even part. (Since \\(e^{-at}u(t)\\) is real, \\(\\mathcal{E}v\\{e^{-at}u(t)\\} = \\frac{e^{-at}u(t) + (e^{-at}u(t))^*|_{t \\to -t}}{2} = \\frac{e^{-at}u(t) + e^{at}u(-t)}{2}\\)).\nSince \\(e^{-at}u(t)\\) is real-valued, the Fourier transform of its even part is the real part of its Fourier transform: \\(\\mathcal{E}v\\{e^{-at}u(t)\\} \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\operatorname{Re}\\left\\{\\frac{1}{a+j \\omega}\\right\\}\\).\nTherefore, applying linearity: \\(X(j \\omega) = 2 \\operatorname{Re}\\left\\{\\frac{1}{a+j \\omega}\\right\\} = 2 \\operatorname{Re}\\left\\{\\frac{a-j \\omega}{a^2+\\omega^2}\\right\\} = \\frac{2a}{a^2+\\omega^2}\\).\n\n\n\n\n\n\n\nTip\n\n\nThis method leverages the symmetry properties to avoid direct integration, simplifying the calculation.\n\n\n\n\nExample 4.10 demonstrates how these symmetry properties can be used to simplify calculations. For the signal e^(-a|t|), we can express it as twice the even part of e^(-at)u(t). Since e^(-at)u(t) is a real signal, we know that the Fourier transform of its even part corresponds to the real part of its Fourier transform. By applying this, we can quickly derive the Fourier transform of e^(-a|t|) without performing a new integration. This is a common strategy in signal analysis."
  },
  {
    "objectID": "ss_43.html#differentiation-in-time-domain",
    "href": "ss_43.html#differentiation-in-time-domain",
    "title": "Continuous-Time Fourier Transform",
    "section": "Differentiation in Time Domain",
    "text": "Differentiation in Time Domain\nIf \\(x(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X(j \\omega)\\), then:\n\\[\n\\frac{d x(t)}{d t} \\stackrel{\\mathcal{F}}{\\longleftrightarrow} j \\omega X(j \\omega)\n\\]\nSignificance\nThis is a particularly important property because it replaces the operation of differentiation in the time domain with simple multiplication by \\(j \\omega\\) in the frequency domain.\nThis transformation is extremely useful for analyzing LTI systems described by differential equations, converting them into algebraic equations.\n\nThe differentiation property is incredibly powerful, especially in the context of LTI systems. It states that taking the derivative of a signal in the time domain is equivalent to multiplying its Fourier transform by jω in the frequency domain. This transforms differential equations into algebraic equations in the frequency domain, greatly simplifying their solution. This is a cornerstone for analyzing filters, circuits, and control systems."
  },
  {
    "objectID": "ss_43.html#integration-in-time-domain",
    "href": "ss_43.html#integration-in-time-domain",
    "title": "Continuous-Time Fourier Transform",
    "section": "Integration in Time Domain",
    "text": "Integration in Time Domain\nIf \\(x(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X(j \\omega)\\), then:\n\\[\n\\int_{-\\infty}^{t} x(\\tau) d \\tau \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\frac{1}{j \\omega} X(j \\omega) + \\pi X(0) \\delta(\\omega)\n\\]\nNote on the Impulse Term\nThe impulse term \\(\\pi X(0) \\delta(\\omega)\\) on the right-hand side accounts for any DC (average) value that can result from integration.\nIf \\(X(0) = 0\\), meaning there is no DC component in \\(x(t)\\), then this term vanishes.\n\nConversely, the integration property tells us that integration in the time domain corresponds to division by jω in the frequency domain. However, there’s an important additional term: πX(0)δ(ω). This impulse term at ω=0 accounts for any DC offset or average value that can result from integrating a signal. For instance, integrating a non-zero constant will result in a ramp, which has a DC component. This term ensures the property holds for all signals, including those with DC components."
  },
  {
    "objectID": "ss_43.html#example-4.11-transform-of-unit-step",
    "href": "ss_43.html#example-4.11-transform-of-unit-step",
    "title": "Continuous-Time Fourier Transform",
    "section": "Example 4.11: Transform of Unit Step",
    "text": "Example 4.11: Transform of Unit Step\nLet’s determine the Fourier transform \\(X(j \\omega)\\) of the unit step \\(x(t)=u(t)\\).\n\nWe know that \\(\\frac{d u(t)}{d t} = \\delta(t)\\).\nThe Fourier transform of the impulse is \\(\\mathcal{F}\\{\\delta(t)\\} = 1\\).\nFrom the integration property, if \\(g(t) = \\delta(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} G(j \\omega) = 1\\), and \\(u(t) = \\int_{-\\infty}^{t} g(\\tau) d \\tau\\), then: \\(X(j \\omega) = \\frac{G(j \\omega)}{j \\omega} + \\pi G(0) \\delta(\\omega)\\)\nSubstitute \\(G(j \\omega)=1\\) and \\(G(0)=1\\): \\[ \\mathcal{F}\\{u(t)\\} = \\frac{1}{j \\omega} + \\pi \\delta(\\omega) \\]\n\n\n\n\n\n\n\nNote\n\n\nThis is a very important transform pair to remember in Signals and Systems!\n\n\n\n\nLet’s use these properties to find the Fourier transform of the unit step function, u(t). We know that the derivative of u(t) is the impulse function δ(t), and its Fourier transform is simply 1. Applying the integration property, we can find the transform of u(t). The result, 1/(jω) + πδ(ω), is a fundamental transform pair. The πδ(ω) term accounts for the DC value of the unit step, which is 0.5 (or rather, the integral of δ(t) up to t will have a DC component)."
  },
  {
    "objectID": "ss_43.html#interactive-differentiation-demo",
    "href": "ss_43.html#interactive-differentiation-demo",
    "title": "Continuous-Time Fourier Transform",
    "section": "Interactive Differentiation Demo",
    "text": "Interactive Differentiation Demo\nThis interactive plot demonstrates a rectangular pulse \\(x(t)\\) and its derivative \\(dx/dt\\).\nObserve how abrupt changes in \\(x(t)\\) (discontinuities) result in impulses in its derivative.\nAdjust the amplitude and width of the pulse to see the effect on the derivative’s impulses.\n\nviewof amplitude = Inputs.range([0.1, 2], {label: \"Amplitude\", step: 0.1, value: 1});\nviewof width = Inputs.range([0.5, 4], {label: \"Width\", step: 0.1, value: 2});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere’s an interactive demo to visualize the differentiation property. We’re looking at a rectangular pulse x(t). As you adjust its amplitude and width, observe its derivative dx/dt. You’ll see that the derivative consists of impulses at the points where the signal x(t) changes abruptly. Specifically, a positive impulse where the signal goes up, and a negative impulse where it goes down. This visually reinforces how sharp changes in the time domain translate to impulses in the derivative, and how these impulses would then correspond to specific frequency components in the Fourier transform."
  },
  {
    "objectID": "ss_43.html#time-and-frequency-scaling",
    "href": "ss_43.html#time-and-frequency-scaling",
    "title": "Continuous-Time Fourier Transform",
    "section": "Time and Frequency Scaling",
    "text": "Time and Frequency Scaling\nIf \\(x(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X(j \\omega)\\), then:\n\\[\nx(a t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\frac{1}{|a|} X\\left(\\frac{j \\omega}{a}\\right)\n\\]\nWhere \\(a\\) is a non-zero real number.\nInterpretation\n\nTime Compression ($ |a|&gt;1 $): The signal becomes narrower in time. Its spectrum expands in frequency (frequencies are scaled up), and its amplitude scales by \\(1/|a|\\).\nTime Expansion ($ |a|&lt;1 $): The signal becomes wider in time. Its spectrum contracts in frequency (frequencies are scaled down), and its amplitude scales by \\(1/|a|\\).\n\n\n\n\n\n\n\nImportant\n\n\nThis property highlights the inverse relationship between time and frequency domains: compression in one domain means expansion in the other.\n\n\n\n\nThe time and frequency scaling property is another crucial concept that underscores the inverse relationship between the time and frequency domains. If we scale a signal in time by a factor a, its Fourier transform scales in frequency by 1/a and its amplitude by 1/|a|. This means that if a signal is compressed in time (e.g., played faster), its frequency content spreads out, becoming higher pitched. Conversely, if a signal is expanded in time (e.g., played slower), its frequency content contracts, becoming lower pitched. This principle is evident in everyday life, such as changing the playback speed of audio recordings, and is a cornerstone of topics like the uncertainty principle in physics."
  },
  {
    "objectID": "ss_43.html#scaling-in-practice-audio-playback",
    "href": "ss_43.html#scaling-in-practice-audio-playback",
    "title": "Continuous-Time Fourier Transform",
    "section": "Scaling in Practice: Audio Playback",
    "text": "Scaling in Practice: Audio Playback\nA common illustration of the time and frequency scaling property is the effect on frequency content that results when an audiotape is recorded at one speed and played back at a different speed.\n\nFaster Playback (\\(a &gt; 1\\)): This corresponds to time compression. The audio sounds higher pitched because its frequency spectrum expands.\nSlower Playback (\\(0 &lt; a &lt; 1\\)): This corresponds to time expansion. The audio sounds lower pitched because its frequency spectrum contracts.\n\nExample: If a recording of a small bell ringing is played back at a reduced speed, it will sound like the chiming of a larger and deeper sounding bell.\nAlso, a special case: \\(x(-t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X(-j \\omega)\\), meaning reversing a signal in time also reverses its Fourier transform.\n\nA great real-world analogy for time and frequency scaling is audio playback. Imagine you record a sound, say a small bell ringing. If you play that recording back at a faster speed, you’re compressing the signal in time. According to the scaling property, its frequency content will expand, making the bell sound higher pitched. Conversely, if you play it back slower, you’re expanding it in time, and its frequency content will contract, making it sound lower pitched, perhaps like a much larger, deeper bell. This simple example vividly illustrates the fundamental inverse relationship between time and frequency. Also, a special case is time reversal, where a=-1, which simply reflects the frequency spectrum."
  },
  {
    "objectID": "ss_43.html#duality-property",
    "href": "ss_43.html#duality-property",
    "title": "Continuous-Time Fourier Transform",
    "section": "Duality Property",
    "text": "Duality Property\nThe Fourier analysis and synthesis equations exhibit a remarkable symmetry:\n\\[\nX(j \\omega)=\\int_{-\\infty}^{+\\infty} x(t) e^{-j \\omega t} d t\n\\]\n\\[\nx(t)=\\frac{1}{2 \\pi} \\int_{-\\infty}^{+\\infty} X(j \\omega) e^{j \\omega t} d \\omega\n\\]\nPrinciple of Duality\nIf \\(x(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X(j \\omega)\\), then:\n\\[\nX(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} 2 \\pi x(- \\omega)\n\\]\nThis property allows us to derive new Fourier transform pairs from existing ones by interchanging the time and frequency variables (with some scaling and reflection).\n\nThe duality property arises from the inherent symmetry between the Fourier analysis and synthesis equations. Essentially, if you have a Fourier transform pair, you can often find a dual pair by swapping the roles of time and frequency. This means if a function x(t) has a transform X(jω), then a function X(t) (where X is the same functional form as the original frequency domain function) will have a transform related to the original time-domain function x(t). This is a powerful tool for discovering new Fourier transform pairs without having to perform complex integrations."
  },
  {
    "objectID": "ss_43.html#example-4.13-applying-duality",
    "href": "ss_43.html#example-4.13-applying-duality",
    "title": "Continuous-Time Fourier Transform",
    "section": "Example 4.13: Applying Duality",
    "text": "Example 4.13: Applying Duality\nLet’s find the Fourier transform \\(G(j \\omega)\\) of the signal \\(g(t) = \\frac{2}{1+t^2}\\).\n\nRecall a known transform pair from Example 4.2: \\(e^{-|t|} \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\frac{2}{1+\\omega^2}\\).\nLet \\(x(t) = e^{-|t|}\\) and \\(X(j \\omega) = \\frac{2}{1+\\omega^2}\\).\nNotice that our target signal \\(g(t) = \\frac{2}{1+t^2}\\) has the same functional form as \\(X(j \\omega)\\). So, we can set \\(g(t) = X(t)\\).\nApplying the duality property: if \\(X(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} 2 \\pi x(-\\omega)\\).\nSubstitute \\(x(-\\omega) = e^{-|-\\omega|} = e^{-|\\omega|}\\): \\[ \\mathcal{F}\\left\\{\\frac{2}{1+t^2}\\right\\} = 2 \\pi e^{-|\\omega|} \\]\n\n\n\n\n\n\n\nTip\n\n\nDuality often turns a known frequency-domain shape into a new time-domain signal, and vice versa.\n\n\n\n\nLet’s see duality in action with Example 4.13. We want to find the Fourier transform of g(t) = 2 / (1 + t^2). We recall a known transform pair: e^(-|t|) transforms to 2 / (1 + ω^2). Notice that g(t) has the same functional form as the frequency domain expression from our known pair. By applying the duality property, we simply substitute t for ω in the frequency domain function, and ω for t in the time domain function, and multiply by 2π (and account for the time reversal). This quickly gives us the transform 2πe^(-|ω|), a result that would be much harder to obtain via direct integration."
  },
  {
    "objectID": "ss_43.html#other-dual-properties",
    "href": "ss_43.html#other-dual-properties",
    "title": "Continuous-Time Fourier Transform",
    "section": "Other Dual Properties",
    "text": "Other Dual Properties\nDuality extends to other properties as well, revealing symmetric relationships.\nMultiplication by \\(jt\\) (Dual of Differentiation)\nIf \\(x(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X(j \\omega)\\), then:\n\\[\n-j t x(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\frac{d X(j \\omega)}{d \\omega}\n\\]\nMultiplication by \\(e^{j \\omega_0 t}\\) (Frequency Shifting)\nIf \\(x(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X(j \\omega)\\), then:\n\\[\ne^{j \\omega_{0} t} x(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X\\left(j\\left(\\omega-\\omega_{0}\\right)\\right)\n\\]\n\nMultiplying a signal by a complex exponential in the time domain results in a shift of its spectrum in the frequency domain. This is a core principle in modulation.\n\nIntegration in Frequency (Dual of Multiplication by \\(1/(jt)\\))\nIf \\(x(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X(j \\omega)\\), then:\n\\[\n-\\frac{1}{j t} x(t)+\\pi x(0) \\delta(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} \\int_{-\\infty}^{\\omega} X(\\eta) d \\eta\n\\]\n\nDuality isn’t just about swapping t and ω; it also extends to the properties themselves. For example, just as differentiation in time corresponds to multiplication by jω in frequency, multiplication by -jt in time corresponds to differentiation in frequency. Similarly, time shifting has a dual in frequency shifting: multiplying a signal by a complex exponential e^(jω₀t) in the time domain shifts its entire spectrum by ω₀ in the frequency domain. This is incredibly important for modulation techniques in communication systems."
  },
  {
    "objectID": "ss_43.html#parsevals-relation",
    "href": "ss_43.html#parsevals-relation",
    "title": "Continuous-Time Fourier Transform",
    "section": "Parseval’s Relation",
    "text": "Parseval’s Relation\nEnergy Conservation\nParseval’s relation states that the total energy of a signal can be computed equivalently in either the time domain or the frequency domain. If \\(x(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} X(j \\omega)\\), then:\n\\[\n\\int_{-\\infty}^{+\\infty}|x(t)|^{2} d t = \\frac{1}{2 \\pi} \\int_{-\\infty}^{+\\infty}|X(j \\omega)|^{2} d \\omega\n\\]\nEnergy Density Spectrum\nThe term \\(|X(j \\omega)|^2\\) is often referred to as the energy-density spectrum of the signal \\(x(t)\\). It describes how the signal’s energy is distributed across different frequencies.\nAnalogy\nThis relation is the direct counterpart of Parseval’s relation for periodic signals, which relates the average power of a periodic signal to the sum of the average powers of its harmonic components.\n\nParseval’s relation is a powerful statement about energy conservation. It tells us that the total energy of a signal, calculated by integrating its squared magnitude over all time, is equal to the total energy calculated by integrating its squared magnitude (scaled by 1/(2π)) over all frequencies. The term |X(jω)|^2 is thus often called the energy-density spectrum, as it tells us how the signal’s energy is distributed across different frequencies. This property is fundamental in understanding power and energy distribution in signals, and it has a direct parallel in the average power calculations for periodic signals using Fourier series."
  },
  {
    "objectID": "ss_43.html#example-4.14-applying-parsevals-relation",
    "href": "ss_43.html#example-4.14-applying-parsevals-relation",
    "title": "Continuous-Time Fourier Transform",
    "section": "Example 4.14: Applying Parseval’s Relation",
    "text": "Example 4.14: Applying Parseval’s Relation\nFor a given Fourier transform \\(X(j \\omega)\\), we want to calculate: 1. Total Energy: \\(E = \\int_{-\\infty}^{\\infty}|x(t)|^{2} d t\\) 2. Derivative at \\(t=0\\): \\(D = \\left.\\frac{d}{d t} x(t)\\right|_{t=0}\\)\nLet’s consider a simple case: \\(X(j \\omega) = \\begin{cases} 1, & |\\omega| &lt; W \\\\ 0, & |\\omega| &gt; W \\end{cases}\\) (a rectangular pulse in frequency).\nCalculating Total Energy (\\(E\\)) using Parseval’s Relation\n\\(E = \\frac{1}{2 \\pi} \\int_{-\\infty}^{\\infty}|X(j \\omega)|^{2} d \\omega\\) For our \\(X(j \\omega)\\): \\(E = \\frac{1}{2 \\pi} \\int_{-W}^{W} |1|^2 d \\omega = \\frac{1}{2 \\pi} [\\omega]_{-W}^{W} = \\frac{1}{2 \\pi} (W - (-W)) = \\frac{2W}{2 \\pi} = \\frac{W}{\\pi}\\)."
  },
  {
    "objectID": "ss_43.html#example-4.14-applying-parsevals-relation-cont.",
    "href": "ss_43.html#example-4.14-applying-parsevals-relation-cont.",
    "title": "Continuous-Time Fourier Transform",
    "section": "Example 4.14: Applying Parseval’s Relation (cont.)",
    "text": "Example 4.14: Applying Parseval’s Relation (cont.)\nCalculating Derivative at \\(t=0\\) (\\(D\\)) using Differentiation Property\n\nWe know \\(\\frac{d}{d t} x(t) \\stackrel{\\mathcal{F}}{\\longleftrightarrow} j \\omega X(j \\omega)\\). Let \\(G(j \\omega) = j \\omega X(j \\omega)\\).\nTo find \\(g(0)\\) (which is \\(D\\)), we use the inverse Fourier transform at \\(t=0\\): \\(D = g(0) = \\frac{1}{2 \\pi} \\int_{-\\infty}^{\\infty} G(j \\omega) e^{j \\omega (0)} d \\omega = \\frac{1}{2 \\pi} \\int_{-\\infty}^{\\infty} G(j \\omega) d \\omega\\).\nFor our \\(X(j \\omega)\\): \\(G(j \\omega) = j \\omega \\cdot 1 = j \\omega\\) for \\(|\\omega| &lt; W\\), and \\(0\\) otherwise. \\(D = \\frac{1}{2 \\pi} \\int_{-W}^{W} j \\omega d \\omega = \\frac{j}{2 \\pi} \\left[ \\frac{\\omega^2}{2} \\right]_{-W}^{W} = \\frac{j}{2 \\pi} \\left( \\frac{W^2}{2} - \\frac{(-W)^2}{2} \\right) = \\frac{j}{2 \\pi} (0) = 0\\).\n\n\nExample 4.14 shows how we can use Fourier transform properties to extract information about a signal in the time domain directly from its Fourier transform, without needing to compute the inverse transform. We can find the total energy using Parseval’s relation by integrating the squared magnitude of X(jω). Similarly, we can find the value of the signal’s derivative at t=0 by first transforming the derivative (which involves multiplying X(jω) by jω), and then integrating this new transform over all frequencies. This demonstrates the power of working in the frequency domain for certain calculations."
  },
  {
    "objectID": "ss_43.html#summary-and-next-steps",
    "href": "ss_43.html#summary-and-next-steps",
    "title": "Continuous-Time Fourier Transform",
    "section": "Summary and Next Steps",
    "text": "Summary and Next Steps\nWe have explored several fundamental properties of the Continuous-Time Fourier Transform.\nKey Properties Covered\n\nLinearity: Simplifies analysis of composite signals.\nTime Shifting: Introduces linear phase shift in frequency.\nConjugation & Conjugate Symmetry: Important for real-valued signals.\nDifferentiation & Integration: Converts differential/integral equations to algebraic ones.\nTime & Frequency Scaling: Highlights the inverse relationship between time and frequency domains.\nDuality: Allows deriving new transform pairs and properties.\nParseval’s Relation: Relates signal energy in time and frequency domains.\n\nThese properties are essential tools for:\n\nUnderstanding the intricate relationships between time and frequency domains.\nSimplifying Fourier transform calculations.\nAnalyzing Linear Time-Invariant (LTI) systems effectively."
  },
  {
    "objectID": "ss_45.html#the-multiplication-property",
    "href": "ss_45.html#the-multiplication-property",
    "title": "Signals and Systems",
    "section": "The Multiplication Property",
    "text": "The Multiplication Property\n4.5 The Multiplication Property\nThe multiplication property is the dual of the convolution property. It states that multiplication in the time domain corresponds to convolution in the frequency domain.\n\\[\n\\begin{equation*}\nr(t)=s(t) p(t) \\longleftrightarrow R(j \\omega)=\\frac{1}{2 \\pi} \\int_{-\\infty}^{+\\infty} S(j \\theta) P(j(\\omega-\\theta)) d \\theta \\tag{4.70}\n\\end{equation*}\n\\]\nThis property is crucial for understanding amplitude modulation, a core concept in communication systems.\n\n\n\n\n\n\nNote\n\n\nDuality Principle:\nMany Fourier Transform properties have a dual. If an operation in one domain corresponds to another operation in the dual domain, then the inverse also holds (with scaling factors).\n\n\n\n\nJust as convolution in time became multiplication in frequency, multiplication in time becomes convolution in frequency. This duality is a powerful concept in Fourier analysis.\nEquation 4.70 shows this formally. The integral is a convolution integral, but it’s performed in the frequency domain. The \\(1/(2\\pi)\\) factor is important and arises from the definition of the Fourier Transform and its inverse.\nThis property is sometimes called the “modulation property” because multiplication by a carrier signal (often a sinusoid) is the basis of amplitude modulation. This is how we shift signals to different frequency bands for transmission."
  },
  {
    "objectID": "ss_45.html#example-4.21-amplitude-modulation",
    "href": "ss_45.html#example-4.21-amplitude-modulation",
    "title": "Signals and Systems",
    "section": "Example 4.21: Amplitude Modulation",
    "text": "Example 4.21: Amplitude Modulation\nLet \\(s(t)\\) be a signal with spectrum \\(S(j \\omega)\\). Consider multiplying it by a sinusoidal carrier \\(p(t)=\\cos \\omega_{0} t\\).\nThe Fourier Transform of \\(p(t)\\) is \\(P(j \\omega)=\\pi \\delta\\left(\\omega-\\omega_{0}\\right)+\\pi \\delta\\left(\\omega+\\omega_{0}\\right)\\).\nApplying the multiplication property, the spectrum \\(R(j \\omega)\\) of \\(r(t)=s(t) p(t)\\) is:\n\\[\n\\begin{align*}\nR(j \\omega) & =\\frac{1}{2 \\pi} \\int_{-\\infty}^{+\\infty} S(j \\theta) P(j(\\omega-\\theta)) d \\theta \\\\\n& =\\frac{1}{2} S\\left(j\\left(\\omega-\\omega_{0}\\right)\\right)+\\frac{1}{2} S\\left(j\\left(\\omega+\\omega_{0}\\right)\\right) \\tag{4.71}\n\\end{align*}\n\\]\nThe original signal’s information is preserved but shifted to higher frequencies.\nThis is the basis of sinusoidal amplitude modulation.\n\nThis example is fundamental to understanding how communication systems work.\nWe take a baseband signal \\(s(t)\\), which typically occupies low frequencies (e.g., speech or audio).\nWe multiply it by a high-frequency cosine wave, the carrier \\(p(t)=\\cos(\\omega_0 t)\\).\nIn the frequency domain, this multiplication translates to convolving \\(S(j\\omega)\\) with the two impulses of \\(P(j\\omega)\\).\nThe result is that the spectrum \\(S(j\\omega)\\) is split into two copies, each scaled by 1/2, and shifted to be centered around \\(+\\omega_0\\) and \\(-\\omega_0\\).\nThis process allows us to transmit signals over radio waves or other high-frequency channels. We assume here that \\(\\omega_0\\) is large enough so that the shifted copies of \\(S(j\\omega)\\) do not overlap. If they overlap, distortion occurs."
  },
  {
    "objectID": "ss_45.html#interactive-amplitude-modulation",
    "href": "ss_45.html#interactive-amplitude-modulation",
    "title": "Signals and Systems",
    "section": "Interactive Amplitude Modulation",
    "text": "Interactive Amplitude Modulation\nAdjust the carrier frequency \\(\\omega_0\\) and the bandwidth of the message signal \\(\\omega_1\\).\nObserve how the message spectrum \\(S(j\\omega)\\) is shifted and replicated.\n\nviewof omega0_mod = Inputs.range([5, 20], {step: 1, value: 10, label: \"Carrier Frequency (ω0)\"})\nviewof omega1_mod = Inputs.range([1, 4], {step: 0.5, value: 2, label: \"Message Bandwidth (ω1)\"})"
  },
  {
    "objectID": "ss_45.html#example-4.22-demodulation",
    "href": "ss_45.html#example-4.22-demodulation",
    "title": "Signals and Systems",
    "section": "Example 4.22: Demodulation",
    "text": "Example 4.22: Demodulation\nTo recover the original signal \\(s(t)\\) from the modulated signal \\(r(t)\\), we can use demodulation.\nMultiply \\(r(t)\\) by the same carrier \\(p(t)=\\cos \\omega_{0} t\\) again: \\(g(t)=r(t) p(t)\\).\n\\(G(j \\omega)\\) will contain a scaled version of \\(S(j \\omega)\\) at baseband, plus components shifted to \\(\\pm 2 \\omega_{0}\\).\nSpecifically, \\(G(j\\omega) = \\frac{1}{2} S(j\\omega) + \\frac{1}{4}S(j(\\omega-2\\omega_0)) + \\frac{1}{4}S(j(\\omega+2\\omega_0))\\).\nA frequency-selective lowpass filter can then extract the original signal.\n\n\n\n\n\n\nTip\n\n\nThis process of modulation and demodulation is fundamental to wireless communication, allowing multiple signals to share the same medium by occupying different frequency bands.\n\n\n\n\nAfter modulating \\(s(t)\\) to \\(r(t)\\), the signal is at a higher frequency. To listen to it (e.g., on a radio), we need to bring it back to baseband. This is demodulation.\nMultiplying \\(r(t)\\) by \\(\\cos(\\omega_0 t)\\) again effectively shifts the spectrum \\(R(j\\omega)\\) both to the left and right by \\(\\omega_0\\).\nThe component of \\(R(j\\omega)\\) centered at \\(\\omega_0\\) shifts to \\(0\\) and \\(2\\omega_0\\).\nThe component of \\(R(j\\omega)\\) centered at \\(-\\omega_0\\) shifts to \\(-2\\omega_0\\) and \\(0\\).\nThe terms shifting to \\(0\\) add up to form \\(\\frac{1}{2}S(j\\omega)\\) at baseband. The terms shifting to \\(\\pm 2\\omega_0\\) are unwanted high-frequency components.\nA lowpass filter is then used to remove these high-frequency components, leaving only the desired \\(s(t)\\). This is how a radio receiver works."
  },
  {
    "objectID": "ss_45.html#interactive-demodulation-and-filtering",
    "href": "ss_45.html#interactive-demodulation-and-filtering",
    "title": "Signals and Systems",
    "section": "Interactive Demodulation and Filtering",
    "text": "Interactive Demodulation and Filtering\nContinue from the previous modulation example. We now multiply \\(r(t)\\) by \\(p(t)\\) again to get \\(g(t)\\), and then apply a lowpass filter.\n\nviewof omega0_demod = Inputs.range([5, 20], {step: 1, value: 10, label: \"Carrier Frequency (ω0)\"})\nviewof omega1_demod = Inputs.range([1, 4], {step: 0.5, value: 2, label: \"Message Bandwidth (ω1)\"})\nviewof lp_cutoff = Inputs.range([1, 10], {step: 0.5, value: 3, label: \"Lowpass Filter Cutoff\"})"
  },
  {
    "objectID": "ss_45.html#example-4.23-fourier-transform-of-a-product",
    "href": "ss_45.html#example-4.23-fourier-transform-of-a-product",
    "title": "Signals and Systems",
    "section": "Example 4.23: Fourier Transform of a Product",
    "text": "Example 4.23: Fourier Transform of a Product\nDetermine the Fourier Transform of \\(x(t)=\\frac{\\sin (t) \\sin (t / 2)}{\\pi t^{2}}\\).\nRecognize \\(x(t)\\) as a product of two sinc functions:\n\\(x(t)=\\pi\\left(\\frac{\\sin (t)}{\\pi t}\\right)\\left(\\frac{\\sin (t / 2)}{\\pi t}\\right)\\)\nRecall \\(\\mathcal{F}\\left\\{\\frac{\\sin W t}{\\pi t}\\right\\} = \\text{rect}(\\omega/2W)\\).\nSo, \\(\\mathcal{F}\\left\\{\\frac{\\sin (t)}{\\pi t}\\right\\}\\) is a rect pulse of width 2 (from -1 to 1).\nAnd \\(\\mathcal{F}\\left\\{\\frac{\\sin (t / 2)}{\\pi t}\\right\\}\\) is a rect pulse of width 1 (from -0.5 to 0.5).\nApplying the multiplication property:\n\\(X(j \\omega)=\\frac{1}{2 \\pi} \\mathcal{F}\\left\\{\\frac{\\sin (t)}{\\pi t}\\right\\} * \\mathcal{F}\\left\\{\\frac{\\sin (t / 2)}{\\pi t}\\right\\}\\)\nThe convolution of two rectangular pulses results in a triangular pulse.\n\nThis example demonstrates a practical application of the multiplication property for finding the Fourier Transform of a somewhat complex signal.\nInstead of directly integrating the Fourier Transform definition for \\(x(t)\\), we leverage known Fourier Transform pairs and properties.\nThe key insight is to recognize the sinc functions. The Fourier Transform of \\(\\frac{\\sin(Wt)}{\\pi t}\\) is a rectangular pulse from \\(-W\\) to \\(W\\) with amplitude 1.\nFor \\(\\frac{\\sin(t)}{\\pi t}\\), \\(W=1\\), so its FT is a rect from -1 to 1.\nFor \\(\\frac{\\sin(t/2)}{\\pi t}\\), the argument is \\(t/2\\), so \\(W=1/2\\). Its FT is a rect from -0.5 to 0.5.\nConvolving these two rectangular pulses in the frequency domain gives a triangular pulse shape. The \\(1/(2\\pi)\\) factor comes from the multiplication property."
  },
  {
    "objectID": "ss_45.html#interactive-convolution-of-rectangular-pulses",
    "href": "ss_45.html#interactive-convolution-of-rectangular-pulses",
    "title": "Signals and Systems",
    "section": "Interactive Convolution of Rectangular Pulses",
    "text": "Interactive Convolution of Rectangular Pulses\nVisualize the convolution of two rectangular pulses in the frequency domain."
  },
  {
    "objectID": "ss_45.html#frequency-selective-filtering-with-variable-center-frequency",
    "href": "ss_45.html#frequency-selective-filtering-with-variable-center-frequency",
    "title": "Signals and Systems",
    "section": "4.5.1 Frequency-Selective Filtering with Variable Center Frequency",
    "text": "4.5.1 Frequency-Selective Filtering with Variable Center Frequency\nThe multiplication property enables the creation of tunable frequency-selective filters.\nInstead of physically changing filter components, we shift the signal’s spectrum.\nConsider the system below for implementing a bandpass filter:\n\n\n\n\n\ngraph LR\n    A[\"x(t)\"] --&gt; B{\"x(t) * e^(jωct)\"}\n    B --&gt; C[\"y(t)\"]\n    C --&gt; D(\"Lowpass Filter, H_LP(jω)\")\n    D --&gt; E[\"w(t)\"]\n    E --&gt; F{\"w(t) * e^(-jωct)\"}\n    F --&gt; G[\"f(t)\"]\n\n\n\n\n\n\n\nTraditional analog filters have fixed characteristics determined by their components (resistors, capacitors, inductors). To change the center frequency of a bandpass filter, you’d need to change multiple components simultaneously, which is difficult. This section introduces a clever technique using modulation to achieve a tunable filter without physically altering the filter itself. The core idea is to shift the desired frequency band down to baseband, filter it with a fixed lowpass filter, and then shift it back to the desired center frequency. The Mermaid diagram illustrates the block diagram for this system."
  },
  {
    "objectID": "ss_45.html#spectral-transformation-in-tunable-filtering",
    "href": "ss_45.html#spectral-transformation-in-tunable-filtering",
    "title": "Signals and Systems",
    "section": "Spectral Transformation in Tunable Filtering",
    "text": "Spectral Transformation in Tunable Filtering\nLet’s trace the spectrum of the signal through the system:\n\nInput \\(x(t)\\): Has spectrum \\(X(j\\omega)\\).\nMultiply by \\(e^{j \\omega_{c} t}\\): \\(y(t) = x(t)e^{j \\omega_{c} t}\\).\nUsing the frequency-shifting property, \\(Y(j \\omega) = X(j(\\omega-\\omega_{c}))\\).\nThe spectrum of \\(x(t)\\) is shifted to the right by \\(\\omega_c\\).\nLowpass Filter: \\(w(t)\\) has spectrum \\(W(j\\omega) = Y(j\\omega)H_{LP}(j\\omega)\\).\nThis selects the portion of \\(Y(j\\omega)\\) that falls within the lowpass filter’s bandwidth.\nMultiply by \\(e^{-j \\omega_{c} t}\\): \\(f(t) = w(t)e^{-j \\omega_{c} t}\\).\nThe spectrum \\(W(j\\omega)\\) is shifted to the left by \\(\\omega_c\\).\n\n\n\n\n\n\n\nImportant\n\n\nBy varying \\(\\omega_c\\), the center frequency of the effective bandpass filter changes, without altering the lowpass filter.\n\n\n\n\nThis slide walks through the spectral operations at each stage of the tunable filter system.\nThe first multiplication by \\(e^{j\\omega_c t}\\) is key. It takes the baseband signal \\(X(j\\omega)\\) and shifts its entire spectrum up by \\(\\omega_c\\). This means the frequencies around \\(0\\) in \\(X(j\\omega)\\) are now around \\(\\omega_c\\) in \\(Y(j\\omega)\\).\nThe fixed lowpass filter then acts on this shifted spectrum. It effectively “picks out” a band of frequencies from the shifted signal that corresponds to the original signal’s passband, but now centered around \\(\\omega_c\\).\nThe final multiplication by \\(e^{-j\\omega_c t}\\) shifts this selected band back down. The part that was around \\(\\omega_c\\) and passed by the lowpass filter is now shifted back to baseband, and the part that was around \\(0\\) in \\(W(j\\omega)\\) is now around \\(-\\omega_c\\).\nThe net effect is that the system acts as a bandpass filter whose center frequency is determined by \\(\\omega_c\\)."
  },
  {
    "objectID": "ss_45.html#interactive-tunable-bandpass-filter",
    "href": "ss_45.html#interactive-tunable-bandpass-filter",
    "title": "Signals and Systems",
    "section": "Interactive Tunable Bandpass Filter",
    "text": "Interactive Tunable Bandpass Filter\nAdjust the carrier frequency \\(\\omega_c\\) and the lowpass filter cutoff \\(\\omega_0\\).\nObserve how the spectrum evolves through the system.\n\nviewof omega_c_tunable = Inputs.range([5, 20], {step: 1, value: 10, label: \"Carrier Frequency (ωc)\"})\nviewof lp_cutoff_tunable = Inputs.range([1, 4], {step: 0.5, value: 2, label: \"Lowpass Filter Cutoff (ω0)\"})\nviewof message_bw_tunable = Inputs.range([0.5, 2], {step: 0.1, value: 1, label: \"Message Bandwidth (Wm)\"})"
  },
  {
    "objectID": "ss_45.html#equivalent-bandpass-filter-and-real-signals",
    "href": "ss_45.html#equivalent-bandpass-filter-and-real-signals",
    "title": "Signals and Systems",
    "section": "Equivalent Bandpass Filter and Real Signals",
    "text": "Equivalent Bandpass Filter and Real Signals\nThe overall system effectively creates a bandpass filter.\n\n\n\nFor complex signals, the system of Figure 4.26 is equivalent to an ideal bandpass filter with center frequency \\(-\\omega_c\\) and bandwidth \\(2\\omega_0\\).\nBy varying \\(\\omega_c\\), we tune the center frequency of this bandpass filter.\n\n\nIf \\(x(t)\\) is a real signal, the intermediate signals \\(y(t), w(t), f(t)\\) are generally complex.\nIf we take only the real part of \\(f(t)\\), the resulting spectrum will be symmetric, passing bands of frequencies centered around \\(\\omega_c\\) and \\(-\\omega_c\\).\n\n\nThe first point on this slide summarizes the complex exponential modulation case. The system effectively creates a “negative” center frequency bandpass filter when using \\(e^{j\\omega_c t}\\) and \\(e^{-j\\omega_c t}\\).\nThe second column addresses the more common scenario where the input signal \\(x(t)\\) is real. If \\(x(t)\\) is real, then its spectrum \\(X(j\\omega)\\) is conjugate symmetric. When you multiply by \\(e^{j\\omega_c t}\\), you get a single-sided spectrum. However, if you use a real carrier like \\(\\cos(\\omega_c t)\\) for both modulation and demodulation (as in Example 4.22), then the spectrum of the output will be symmetric (it will have both positive and negative frequency components). This effectively means the filter passes both positive and negative frequency bands. This is a common setup in practical communication systems."
  },
  {
    "objectID": "ss_45.html#tables-of-fourier-properties-and-of-basic-fourier-transform-pairs",
    "href": "ss_45.html#tables-of-fourier-properties-and-of-basic-fourier-transform-pairs",
    "title": "Signals and Systems",
    "section": "4.6 Tables of Fourier Properties and of Basic Fourier Transform Pairs",
    "text": "4.6 Tables of Fourier Properties and of Basic Fourier Transform Pairs\nThe Fourier Transform properties and common transform pairs are invaluable tools for signal and system analysis.\n\n\nKey Properties (Table 4.1):\n\nLinearity\nTime Shift\nFrequency Shift\nConjugation\nTime Reversal\nDifferentiation/Integration\nScaling\nConvolution in Time &lt;-&gt; Multiplication in Frequency\nMultiplication in Time &lt;-&gt; Convolution in Frequency\nDuality\nParseval’s Relation\n\n\nBasic Transform Pairs (Table 4.2):\n\nImpulses (\\(\\delta(t)\\), \\(\\delta(\\omega)\\))\nExponentials (\\(e^{j\\omega_0 t}\\), \\(e^{-at}u(t)\\))\nSinusoids (\\(\\cos(\\omega_0 t)\\), \\(\\sin(\\omega_0 t)\\))\nRectangular pulses \\(\\leftrightarrow\\) Sinc functions\nStep function \\(u(t)\\)\nAnd others derived from properties.\n\n\n\nThis slide serves as a summary and a pointer to the reference tables. It’s crucial for students to become familiar with these properties and pairs, as they are the workhorses of Fourier analysis. Encourage students to not just memorize them, but to understand the intuition behind them and how they relate to each other (e.g., duality). The convolution and multiplication properties are arguably the most important for understanding LTI systems and communication principles."
  },
  {
    "objectID": "ss_45.html#table-4.1-properties-of-the-fourier-transform",
    "href": "ss_45.html#table-4.1-properties-of-the-fourier-transform",
    "title": "Signals and Systems",
    "section": "TABLE 4.1 PROPERTIES OF THE FOURIER TRANSFORM",
    "text": "TABLE 4.1 PROPERTIES OF THE FOURIER TRANSFORM"
  },
  {
    "objectID": "ss_45.html#table-4.2-basic-fourier-transform-pairs",
    "href": "ss_45.html#table-4.2-basic-fourier-transform-pairs",
    "title": "Signals and Systems",
    "section": "TABLE 4.2 BASIC FOURIER TRANSFORM PAIRS",
    "text": "TABLE 4.2 BASIC FOURIER TRANSFORM PAIRS\n\n\n\n\n\n\n\n\nSignal\nFourier transform\nFourier series coefficients  (if periodic)\n\n\n\n\n\\(\\sum_{k=-\\infty}^{+\\infty} a_{k} e^{j k \\omega_{0} t}\\)\n\\(2 \\pi \\sum_{k=-\\infty}^{+\\infty} a_{k} \\delta\\left(\\omega-k \\omega_{0}\\right)\\)\n\\(a_{k}\\)\n\n\n\\(e^{j \\omega_{0} t}\\)\n\\(2 \\pi \\delta\\left(\\omega-\\omega_{0}\\right)\\)\n\\(a_{1}=1\\)  \\(a_{k}=0, \\quad\\) otherwise\n\n\n\\(\\cos \\omega_{0} t\\)\n\\(\\pi\\left[\\delta\\left(\\omega-\\omega_{0}\\right)+\\delta\\left(\\omega+\\omega_{0}\\right)\\right]\\)\n\\(a_{1}=a_{-1}=\\frac{1}{2}\\)  \\(a_{k}=0, \\quad\\) otherwise\n\n\n\\(\\sin \\omega_{0} t\\)\n\\(\\frac{\\pi}{j}\\left[\\delta\\left(\\omega-\\omega_{0}\\right)-\\delta\\left(\\omega+\\omega_{0}\\right)\\right]\\)\n\\(a_{1}=-a_{-1}=\\frac{1}{2 j}\\)  \\(a_{k}=0, \\quad\\) otherwise\n\n\n\\(x(t)=1\\)\n\\(2 \\pi \\delta(\\omega)\\)\n\\(a_{0}=1, \\quad a_{k}=0, k \\neq 0\\)  (this is the Fourier series representation for  any choice of \\(T&gt;0\\)\n\n\nPeriodic square wave  \\(x(t)= \\begin{cases}1, & \\|t\\|&lt;T_{1} \\\\ 0, & T_{1}&lt;\\|t\\| \\leq \\frac{T}{2}\\end{cases}\\)  and  \\(x(t+T)=x(t)\\)\n\\(\\sum_{k=-\\infty}^{+\\infty} \\frac{2 \\sin k \\omega_{0} T_{1}}{k} \\delta\\left(\\omega-k \\omega_{0}\\right)\\)\n\\(\\frac{\\omega_{0} T_{1}}{\\pi} \\operatorname{sinc}\\left(\\frac{k \\omega_{0} T_{1}}{\\pi}\\right)=\\frac{\\sin k \\omega_{0} T_{1}}{k \\pi}\\)\n\n\n\\(\\sum_{n=-\\infty}^{+\\infty} \\delta(t-n T)\\)\n\\(\\frac{2 \\pi}{T} \\sum_{k=-\\infty}^{+\\infty} \\delta\\left(\\omega-\\frac{2 \\pi k}{T}\\right)\\)\n\\(a_{k}=\\frac{1}{T}\\) for all \\(k\\)\n\n\n\\(x(t) \\begin{cases}1, & \\|t\\|&lt;T_{1} \\\\ 0, & \\|t\\|&gt;T_{1}\\end{cases}\\)\n\\(\\frac{2 \\sin \\omega T_{1}}{\\omega}\\)\n-\n\n\n\\(\\frac{\\sin W t}{\\pi t}\\)\n\\(X(j \\omega)= \\begin{cases}1, & \\|\\omega\\|&lt;W \\\\ 0, & \\|\\omega\\|&gt;W\\end{cases}\\)\n-\n\n\n\\(\\delta(t)\\)\n1\n-\n\n\n\\(u(t)\\)\n\\(\\frac{1}{j \\omega}+\\pi \\delta(\\omega)\\)\n-\n\n\n\\(\\delta\\left(t-t_{0}\\right)\\)\n\\(e^{-j \\omega t_{0}}\\)\n-\n\n\n\\(e^{-a t} u(t), \\operatorname{Re}\\{a\\}&gt;0\\)\n\\(\\frac{1}{a+j \\omega}\\)\n-\n\n\n\\(t e^{-a t} u(t), \\operatorname{Re}\\{a\\}&gt;0\\)\n\\(\\frac{1}{(a+j \\omega)^{2}}\\)\n-\n\n\n\\(\\frac{t^{n-1}}{(n-1) !} e^{-a t} u(t)\\)  \\(\\operatorname{Re}\\{a\\}&gt;0\\)\n\\(\\frac{1}{(a+j \\omega)^{n}}\\)\n-"
  },
  {
    "objectID": "ss_prompt/signal_systems_prompt.html",
    "href": "ss_prompt/signal_systems_prompt.html",
    "title": "Signal and Systems",
    "section": "",
    "text": "Task: Create an interactive Quarto-based presentation using reveal.js for an undergraduate Signals and Systems course in the Electrical and Computer Engineering (ECE) program. Follow the slide structure, formatting rules, and course-specific requirements below."
  },
  {
    "objectID": "ss_prompt/signal_systems_prompt.html#slide-structure",
    "href": "ss_prompt/signal_systems_prompt.html#slide-structure",
    "title": "Signal and Systems",
    "section": "1. Slide Structure",
    "text": "1. Slide Structure\n\nSlide Separator: ---\nTitle Slide: Begin with a first-level heading (#) and/or second-level heading (##).\nStandard Slides: Start with a second-level heading (##) followed by content."
  },
  {
    "objectID": "ss_prompt/signal_systems_prompt.html#content-formatting",
    "href": "ss_prompt/signal_systems_prompt.html#content-formatting",
    "title": "Signal and Systems",
    "section": "2. Content & Formatting",
    "text": "2. Content & Formatting\n\nDiagrams: use Mermaid.js as following\n\n\\`\\`\\`\\{mermaid\\}\nmermaid code here\n… \n\\`\\`\\`\\\n\nCode Blocks: use Python and Pyodide as following\n\n\\`\\`\\`\\{pyodide\\}\n#| max-lines: 10\npython code here\nshow figure in figsize=(7, 4)\n… \n\\`\\`\\`\\\n\nMath (LaTeX):\n\nInline: $E=mc^2$\nBlock:\n$$\nE=mc^2\n$$\n\nHTML Entities: Use &entity_name; (e.g., &ne; for ≠).\nWeb Content: Embed via &lt;iframe&gt;."
  },
  {
    "objectID": "ss_prompt/signal_systems_prompt.html#multi-column-layout",
    "href": "ss_prompt/signal_systems_prompt.html#multi-column-layout",
    "title": "Signal and Systems",
    "section": "3. Multi-Column Layout",
    "text": "3. Multi-Column Layout\n:::: {.columns}\n::: {.column width=\"40%\"}\n**Left Column**  \n- Item L1  \n- Item L2  \n:::\n::: {.column width=\"60%\"}\n**Right Column**  \n- Item R1  \n- Item R2  \n- Item R3  \n:::\n::::"
  },
  {
    "objectID": "ss_prompt/signal_systems_prompt.html#speaker-notes",
    "href": "ss_prompt/signal_systems_prompt.html#speaker-notes",
    "title": "Signal and Systems",
    "section": "4. Speaker Notes",
    "text": "4. Speaker Notes\nSlide content  \n\n- Point 1  \n- Point 2  \n\n::: {.notes}\nSpeaker notes here.\n:::"
  },
  {
    "objectID": "ss_prompt/signal_systems_prompt.html#course-specific-requirements",
    "href": "ss_prompt/signal_systems_prompt.html#course-specific-requirements",
    "title": "Signal and Systems",
    "section": "5. Course-Specific Requirements",
    "text": "5. Course-Specific Requirements\n\nContent Source: Use provided text and linked images.\nAcademic Context: Ensure explanations, examples, and terminology match Electrical and Computer Engineering standards.\nInteractivity:\n\nInclude Python-based interactive elements (e.g., signal plots, spectrum analysis, convolution demos).\nEmbed charts, plots, and simulations directly in slides.\n\nEnhancement: Add real-world engineering applications, analogies, and problem-solving examples.\nClarity & Engagement: Maintain a clear, logical structure and engaging visuals.\nConciseness: Keep slide text concise; expand explanations in speaker notes."
  },
  {
    "objectID": "ss_prompt/signal_systems_prompt.html#presentation-yaml",
    "href": "ss_prompt/signal_systems_prompt.html#presentation-yaml",
    "title": "Signal and Systems",
    "section": "6. Presentation YAML",
    "text": "6. Presentation YAML\nUse this YAML as the document header. Replace {Lecture Title} with the specific lecture subtitle.\n---\ntitle: \"Signal and Systems\"\nsubtitle: \"{Lecture Title}\"\nauthor: \"Imron Rosyadi\"\nauthor: \"Imron Rosyadi\"\nformat:\n  live-revealjs:\n    logo: \"qrjs_assets/unsoed_logo.png\"\n    footer: \"[irosyadi-2025](https://imron-slide.vercel.app)\"\n    slide-number: true\n    chalkboard: true\n    scrollable: true\n    controls: true\n    progress: true\n    preview-links: false\n    transition: fade\n    incremental: false\n    smaller: false\n    theme: [default, qrjs_assets/ir_style.scss]\nfilters:\n  - pyodide\n---\n\nGoal: Deliver an interactive, educational slide deck that combines theoretical depth with practical demonstrations, making Signals and Systems concepts clear, engaging, and relevant to undergraduate ECE students."
  },
  {
    "objectID": "ss_91.html#what-is-the-laplace-transform",
    "href": "ss_91.html#what-is-the-laplace-transform",
    "title": "Signals and Systems",
    "section": "What is the Laplace Transform?",
    "text": "What is the Laplace Transform?\nThe Laplace Transform extends the Fourier Transform to analyze a broader class of signals and systems, particularly those that are unstable or non-causal.\nIt transforms a time-domain signal \\(x(t)\\) into a complex frequency-domain function \\(X(s)\\), where \\(s\\) is a complex variable.\nLTI System Response to Complex Exponentials\nFor an LTI system with impulse response \\(h(t)\\), the response to \\(e^{st}\\) is:\n\\[\ny(t) = H(s) e^{st} \\quad \\text{(9.1)}\n\\]\nwhere \\(H(s)\\) is the Laplace Transform of \\(h(t)\\):\n\\[\nH(s) = \\int_{-\\infty}^{\\infty} h(t) e^{-st} dt \\quad \\text{(9.2)}\n\\]"
  },
  {
    "objectID": "ss_91.html#laplace-transform-and-the-fourier-transform",
    "href": "ss_91.html#laplace-transform-and-the-fourier-transform",
    "title": "Signals and Systems",
    "section": "Laplace Transform and the Fourier Transform",
    "text": "Laplace Transform and the Fourier Transform\nThe complex variable \\(s\\) can be written as \\(s = \\sigma + j\\omega\\), where \\(\\sigma\\) is the real part and \\(\\omega\\) is the imaginary part.\nSubstituting \\(s = \\sigma + j\\omega\\) into the Laplace Transform definition:\n\\[\nX(\\sigma+j\\omega) = \\int_{-\\infty}^{+\\infty} x(t) e^{-(\\sigma+j\\omega)t} dt \\quad \\text{(9.7)}\n\\]\nThis can be rewritten as:\n\\[\nX(\\sigma+j\\omega) = \\int_{-\\infty}^{+\\infty} [x(t) e^{-\\sigma t}] e^{-j\\omega t} dt \\quad \\text{(9.8)}\n\\]\n\n\n\n\n\n\nNote\n\n\nThe Laplace Transform \\(X(s)\\) is essentially the Fourier Transform of \\(x(t)e^{-\\sigma t}\\)! When \\(s=j\\omega\\) (i.e., \\(\\sigma=0\\)), the Laplace Transform becomes the Fourier Transform: \\(X(j\\omega) = \\int_{-\\infty}^{+\\infty} x(t) e^{-j\\omega t} dt = \\mathfrak{F}\\{x(t)\\}\\)"
  },
  {
    "objectID": "ss_91.html#region-of-convergence-roc",
    "href": "ss_91.html#region-of-convergence-roc",
    "title": "Signals and Systems",
    "section": "Region of Convergence (ROC)",
    "text": "Region of Convergence (ROC)\nThe Laplace Transform integral (Eq 9.3) does not always converge for all values of \\(s\\).\nThe Region of Convergence (ROC) is the set of all \\(s\\) values for which the integral converges.\nWhy is ROC Important?\n\nThe algebraic expression for \\(X(s)\\) alone is not sufficient to uniquely define the time-domain signal \\(x(t)\\).\nDifferent time-domain signals can have the same algebraic \\(X(s)\\) but vastly different ROCs.\nThe ROC determines properties of the signal, such as causality and stability.\n\nThe s-Plane\nThe complex variable \\(s = \\sigma + j\\omega\\) is visualized on the s-plane, with \\(\\operatorname{Re}\\{s\\}\\) (the \\(\\sigma\\)-axis) as the horizontal axis and \\(\\operatorname{Im}\\{s\\}\\) (the \\(j\\omega\\)-axis) as the vertical axis.\n\n\n\n\n\n\nImportant\n\n\nThe ROC is always a strip or a half-plane in the s-plane.\n\n\n\n\nThe Region of Convergence is perhaps the most critical concept when dealing with the Laplace Transform. Without specifying the ROC, the algebraic expression of \\(X(s)\\) is ambiguous. I’ll emphasize that the ROC is not just a mathematical detail; it provides vital information about the nature of the time-domain signal. We’ll see in upcoming examples how two very different signals can share the same algebraic Laplace Transform expression, but their ROCs will be distinct, allowing us to differentiate them. The s-plane provides a visual map for understanding where the transform converges."
  },
  {
    "objectID": "ss_91.html#example-9.1-causal-exponential",
    "href": "ss_91.html#example-9.1-causal-exponential",
    "title": "Signals and Systems",
    "section": "Example 9.1: Causal Exponential",
    "text": "Example 9.1: Causal Exponential\nConsider the signal \\(x(t)=e^{-at}u(t)\\).\nThe Laplace Transform is:\n\\[\nX(s) = \\int_{0}^{\\infty} e^{-at} e^{-st} dt = \\int_{0}^{\\infty} e^{-(s+a)t} dt \\quad \\text{(9.10)}\n\\]\nFor convergence, we require \\(\\operatorname{Re}\\{s+a\\} &gt; 0\\), or \\(\\operatorname{Re}\\{s\\} &gt; -a\\).\nThus,\n\\[\nX(s) = \\frac{1}{s+a}, \\quad \\operatorname{Re}\\{s\\} &gt; -a \\quad \\text{(9.13)}\n\\]\n\n\n\n\n\n\nTip\n\n\nFor a causal signal \\(x(t)u(t)\\), the ROC is always a right-half plane."
  },
  {
    "objectID": "ss_91.html#example-9.2-anti-causal-exponential",
    "href": "ss_91.html#example-9.2-anti-causal-exponential",
    "title": "Signals and Systems",
    "section": "Example 9.2: Anti-Causal Exponential",
    "text": "Example 9.2: Anti-Causal Exponential\nConsider the signal \\(x(t)=-e^{-at}u(-t)\\).\nThe Laplace Transform is:\n\\[\nX(s) = -\\int_{-\\infty}^{0} e^{-at} e^{-st} dt = -\\int_{-\\infty}^{0} e^{-(s+a)t} dt \\quad \\text{(9.17)}\n\\]\nFor convergence, we require \\(\\operatorname{Re}\\{s+a\\} &lt; 0\\), or \\(\\operatorname{Re}\\{s\\} &lt; -a\\).\nThus,\n\\[\nX(s) = \\frac{1}{s+a}, \\quad \\operatorname{Re}\\{s\\} &lt; -a \\quad \\text{(9.19)}\n\\]\n\n\n\n\n\n\nTip\n\n\nFor an anti-causal signal \\(x(t)u(-t)\\), the ROC is always a left-half plane."
  },
  {
    "objectID": "ss_91.html#poles-zeros-and-the-s-plane",
    "href": "ss_91.html#poles-zeros-and-the-s-plane",
    "title": "Signals and Systems",
    "section": "Poles, Zeros, and the s-Plane",
    "text": "Poles, Zeros, and the s-Plane\nFor many practical signals, especially those from LTI systems described by differential equations, the Laplace Transform \\(X(s)\\) is a rational function:\n\\[\nX(s) = \\frac{N(s)}{D(s)} \\quad \\text{(9.31)}\n\\]\nwhere \\(N(s)\\) and \\(D(s)\\) are polynomials in \\(s\\).\nZeros\nThe roots of the numerator polynomial \\(N(s)\\) are called the zeros of \\(X(s)\\). At these values of \\(s\\), \\(X(s)=0\\).\nPoles\nThe roots of the denominator polynomial \\(D(s)\\) are called the poles of \\(X(s)\\). At these values of \\(s\\), \\(X(s)\\) becomes infinite."
  },
  {
    "objectID": "ss_91.html#example-9.3-sum-of-two-causal-exponentials",
    "href": "ss_91.html#example-9.3-sum-of-two-causal-exponentials",
    "title": "Signals and Systems",
    "section": "Example 9.3: Sum of Two Causal Exponentials",
    "text": "Example 9.3: Sum of Two Causal Exponentials\nConsider \\(x(t)=3e^{-2t}u(t) - 2e^{-t}u(t)\\).\nUsing linearity and results from Example 9.1:\n\\[\n\\mathcal{L}\\{3e^{-2t}u(t)\\} = \\frac{3}{s+2}, \\quad \\operatorname{Re}\\{s\\} &gt; -2\n\\]\n\\[\n\\mathcal{L}\\{-2e^{-t}u(t)\\} = \\frac{-2}{s+1}, \\quad \\operatorname{Re}\\{s\\} &gt; -1\n\\]\nFor \\(X(s)\\) to converge, both individual transforms must converge. The intersection of their ROCs is \\(\\operatorname{Re}\\{s\\} &gt; -1\\).\nCombining terms:\n\\[\nX(s) = \\frac{3}{s+2} - \\frac{2}{s+1} = \\frac{3(s+1) - 2(s+2)}{(s+2)(s+1)} = \\frac{s-1}{s^2+3s+2}\n\\]"
  },
  {
    "objectID": "ss_91.html#example-9.4-complex-exponentials",
    "href": "ss_91.html#example-9.4-complex-exponentials",
    "title": "Signals and Systems",
    "section": "Example 9.4: Complex Exponentials",
    "text": "Example 9.4: Complex Exponentials\nConsider \\(x(t)=e^{-2t}u(t) + e^{-t}(\\cos 3t)u(t)\\).\nUsing Euler’s relation, \\(\\cos 3t = \\frac{1}{2}(e^{j3t} + e^{-j3t})\\), so:\n\\[\nx(t) = \\left[e^{-2t} + \\frac{1}{2}e^{-(1-j3)t} + \\frac{1}{2}e^{-(1+j3)t}\\right]u(t)\n\\]\nThe individual Laplace Transforms are:\n\n\\(\\mathcal{L}\\{e^{-2t}u(t)\\} = \\frac{1}{s+2}, \\quad \\operatorname{Re}\\{s\\} &gt; -2\\)\n\\(\\mathcal{L}\\{\\frac{1}{2}e^{-(1-j3)t}u(t)\\} = \\frac{1/2}{s+(1-j3)}, \\quad \\operatorname{Re}\\{s\\} &gt; -1\\)\n\\(\\mathcal{L}\\{\\frac{1}{2}e^{-(1+j3)t}u(t)\\} = \\frac{1/2}{s+(1+j3)}, \\quad \\operatorname{Re}\\{s\\} &gt; -1\\)\n\nThe common ROC is \\(\\operatorname{Re}\\{s\\} &gt; -1\\)."
  },
  {
    "objectID": "ss_91.html#example-9.5-unit-impulse-and-exponentials",
    "href": "ss_91.html#example-9.5-unit-impulse-and-exponentials",
    "title": "Signals and Systems",
    "section": "Example 9.5: Unit Impulse and Exponentials",
    "text": "Example 9.5: Unit Impulse and Exponentials\nConsider \\(x(t)=\\delta(t) - \\frac{4}{3}e^{-t}u(t) + \\frac{1}{3}e^{2t}u(t)\\).\n\n\\(\\mathcal{L}\\{\\delta(t)\\} = 1\\). The ROC is the entire s-plane.\n\\(\\mathcal{L}\\{-\\frac{4}{3}e^{-t}u(t)\\} = \\frac{-4/3}{s+1}, \\quad \\operatorname{Re}\\{s\\} &gt; -1\\).\n\\(\\mathcal{L}\\{\\frac{1}{3}e^{2t}u(t)\\} = \\frac{1/3}{s-2}, \\quad \\operatorname{Re}\\{s\\} &gt; 2\\).\n\nThe overall ROC is the intersection of all three, which is \\(\\operatorname{Re}\\{s\\} &gt; 2\\).\nCombining terms:\n\\[\nX(s) = 1 - \\frac{4/3}{s+1} + \\frac{1/3}{s-2} = \\frac{(s+1)(s-2) - \\frac{4}{3}(s-2) + \\frac{1}{3}(s+1)}{(s+1)(s-2)}\n\\]\n\\[\nX(s) = \\frac{s^2-s-2 - \\frac{4}{3}s + \\frac{8}{3} + \\frac{1}{3}s + \\frac{1}{3}}{(s+1)(s-2)} = \\frac{s^2 - \\frac{6}{3}s + \\frac{3}{3}}{(s+1)(s-2)} = \\frac{s^2-2s+1}{(s+1)(s-2)}\n\\]\n\\[\nX(s) = \\frac{(s-1)^2}{(s+1)(s-2)}, \\quad \\operatorname{Re}\\{s\\} &gt; 2 \\quad \\text{(9.35)}\n\\]"
  },
  {
    "objectID": "ss_91.html#application-system-analysis-in-ece",
    "href": "ss_91.html#application-system-analysis-in-ece",
    "title": "Signals and Systems",
    "section": "Application: System Analysis in ECE",
    "text": "Application: System Analysis in ECE\nThe Laplace Transform is a cornerstone in ECE for analyzing Linear Time-Invariant (LTI) systems.\nWhy use Laplace Transform?\n\nSimplifies Differential Equations: Transforms complex differential equations into algebraic equations, which are much easier to solve.\nFrequency Domain Analysis: Provides insights into system behavior across a range of complex frequencies, not just real frequencies.\nStability Analysis: Pole locations directly indicate system stability.\nCircuit Analysis: Ideal for analyzing RLC circuits, filters, and control systems."
  },
  {
    "objectID": "ss_91.html#key-takeaways",
    "href": "ss_91.html#key-takeaways",
    "title": "Signals and Systems",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nGeneralization: The Laplace Transform is a generalization of the Fourier Transform, allowing analysis of a wider class of signals.\nComplex Frequency \\(s\\): Uses a complex variable \\(s = \\sigma + j\\omega\\), where \\(\\sigma\\) influences convergence.\nRegion of Convergence (ROC): Crucial for uniquely identifying a signal and understanding its properties (causality, stability). The ROC is always bounded by poles.\nPoles & Zeros: These are the roots of the denominator and numerator polynomials of \\(X(s)\\), respectively, and define its algebraic form.\n\n\n\n\n\n\n\nNote\n\n\nPractical Impact: The Laplace Transform simplifies the analysis of LTI systems by converting differential equations into algebraic ones, making it indispensable for circuit analysis, control systems, and signal processing.\n\n\n\n\nTo wrap up, remember these core ideas: the Laplace Transform is a powerful extension, the complex ‘s’ variable and the ROC are fundamental to its meaning, and poles and zeros are its fingerprint. This tool is not just theoretical; it’s a practical workhorse in ECE. Continue exploring its properties and applications to truly master it."
  },
  {
    "objectID": "ss_91.html#what-is-the-laplace-transform-cont.",
    "href": "ss_91.html#what-is-the-laplace-transform-cont.",
    "title": "Signals and Systems",
    "section": "What is the Laplace Transform? (cont.)",
    "text": "What is the Laplace Transform? (cont.)\nGeneral Definition\nThe Laplace Transform of a general signal \\(x(t)\\) is defined as:\n\\[\nX(s) \\triangleq \\int_{-\\infty}^{+\\infty} x(t) e^{-st} dt \\quad \\text{(9.3)}\n\\]\nWe denote this relationship as \\(x(t) \\stackrel{\\mathfrak{L}}{\\longleftrightarrow} X(s)\\).\n\nThe Laplace Transform is a cornerstone of signals and systems analysis. Unlike the Fourier Transform, which is limited to absolutely integrable signals, the Laplace Transform uses a complex exponential \\(e^{-st}\\) as its basis function, where \\(s\\) is a complex variable \\(\\sigma + j\\omega\\). This generalization allows us to analyze a much wider range of signals and systems, including unstable ones or those that grow exponentially. The transform converts differential equations into algebraic equations, simplifying analysis significantly."
  },
  {
    "objectID": "ss_91.html#laplace-transform-and-the-fourier-transform-1",
    "href": "ss_91.html#laplace-transform-and-the-fourier-transform-1",
    "title": "Signals and Systems",
    "section": "Laplace Transform and the Fourier Transform",
    "text": "Laplace Transform and the Fourier Transform\nConceptual Relationship\n\n\n\n\n\nflowchart LR\n    A[\"x(t)\"] --&gt; B{\"Multiply by &lt;br&gt; Real Exponential\"}\n    B --&gt; C[\"$$x(t)e^{-\\\\sigma t}$$\"]\n    C --&gt; D[\"Apply &lt;br&gt; Fourier Transform\"]\n    D --&gt; E[\"$$X(s) = X(\\\\sigma + j\\\\omega)$$\"]\n\n\n\n\n\n\n\nThis slide highlights the crucial connection between the Laplace and Fourier Transforms. By expressing \\(s\\) as \\(\\sigma + j\\omega\\), we see that the Laplace Transform is fundamentally the Fourier Transform of the original signal \\(x(t)\\) multiplied by a real exponential \\(e^{-\\sigma t}\\). The term \\(e^{-\\sigma t}\\) acts as a weighting function, which can either dampen or amplify the signal, making it absolutely integrable and thus allowing its Fourier Transform to exist, even if \\(x(t)\\) itself does not have a Fourier Transform. When \\(\\sigma=0\\), the real exponential term becomes \\(e^0 = 1\\), and the Laplace Transform perfectly reduces to the Fourier Transform. This means the Fourier Transform is a special case of the Laplace Transform, evaluated on the imaginary axis of the s-plane."
  },
  {
    "objectID": "ss_91.html#example-9.1-causal-exponential-cont.",
    "href": "ss_91.html#example-9.1-causal-exponential-cont.",
    "title": "Signals and Systems",
    "section": "Example 9.1: Causal Exponential (cont.)",
    "text": "Example 9.1: Causal Exponential (cont.)\nInteractive Demonstration: Convergence of \\(x(t)e^{-\\sigma t}\\)\nAdjust ‘a’ and ‘sigma’ (\\(\\operatorname{Re}\\{s\\}\\)) to observe how \\(x(t)e^{-\\sigma t}\\) changes and whether it converges.\n\nviewof a_val = Inputs.range([-2, 2], {value: 1, step: 0.1, label: \"Parameter 'a'\"});\nviewof sigma_val = Inputs.range([-3, 3], {value: 0.5, step: 0.1, label: \"sigma (Re{s})\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we take a common signal, the causal exponential \\(e^{-at}u(t)\\). We derive its Laplace Transform and identify its ROC. The interactive plot demonstrates the term \\(x(t)e^{-\\sigma t}\\). Notice that for the integral to converge, this modified signal must decay to zero as \\(t\\) approaches infinity. By adjusting ‘a’ and ‘sigma’, you can visually observe when this decay condition is met, directly correlating to the \\(\\operatorname{Re}\\{s\\} &gt; -a\\) condition for convergence. If \\(a+\\sigma\\) is positive, the exponential decays, and the integral converges. If \\(a+\\sigma\\) is negative, it grows, and the integral diverges."
  },
  {
    "objectID": "ss_91.html#example-9.2-anti-causal-exponential-cont.",
    "href": "ss_91.html#example-9.2-anti-causal-exponential-cont.",
    "title": "Signals and Systems",
    "section": "Example 9.2: Anti-Causal Exponential (cont.)",
    "text": "Example 9.2: Anti-Causal Exponential (cont.)\nComparing ROCs\n\n\nExample 9.1: \\(e^{-at}u(t)\\) \\(X(s) = \\frac{1}{s+a}\\), \\(\\operatorname{Re}\\{s\\} &gt; -a\\)\nThe ROC is to the right of the pole.\n\nExample 9.2: \\(-e^{-at}u(-t)\\) \\(X(s) = \\frac{1}{s+a}\\), \\(\\operatorname{Re}\\{s\\} &lt; -a\\)\nThe ROC is to the left of the pole.\n\n\nThis example is crucial for understanding the importance of the ROC. Here, we encounter an anti-causal exponential, \\(-e^{-at}u(-t)\\). Notice that its algebraic Laplace Transform form, \\(1/(s+a)\\), is identical to that of the causal exponential from Example 9.1. However, the condition for convergence is entirely different: \\(\\operatorname{Re}\\{s\\} &lt; -a\\).\nThe side-by-side comparison clearly shows that the same algebraic expression can correspond to two distinct time-domain signals, differentiated solely by their ROCs. This emphasizes that to fully specify a signal’s Laplace Transform, you must provide both the algebraic form and its corresponding ROC."
  },
  {
    "objectID": "ss_91.html#poles-zeros-and-the-s-plane-cont.",
    "href": "ss_91.html#poles-zeros-and-the-s-plane-cont.",
    "title": "Signals and Systems",
    "section": "Poles, Zeros, and the s-Plane (cont.)",
    "text": "Poles, Zeros, and the s-Plane (cont.)\nPole-Zero Plot\n\nA pole-zero plot graphically represents the locations of poles and zeros in the s-plane.\nPoles are typically marked with an ‘x’.\nZeros are typically marked with an ‘o’.\nThe pole-zero plot, combined with the ROC, fully characterizes a rational Laplace Transform.\n\n\n\n\n\n\n\nImportant\n\n\nThe ROC is always bounded by poles and never contains any poles.\n\n\n\n\nRational Laplace Transforms are very common in ECE, particularly when dealing with systems described by linear constant-coefficient differential equations. The concepts of poles and zeros provide a powerful way to characterize these transforms. Poles are the values of ‘s’ where the transform “blows up” or goes to infinity, while zeros are where it becomes zero. Plotting these critical points on the s-plane, along with the ROC, gives us a complete visual signature of the system or signal in the Laplace domain. This pole-zero representation is fundamental for understanding system stability, frequency response, and transient behavior."
  },
  {
    "objectID": "ss_91.html#example-9.3-sum-of-two-causal-exponentials-cont.",
    "href": "ss_91.html#example-9.3-sum-of-two-causal-exponentials-cont.",
    "title": "Signals and Systems",
    "section": "Example 9.3: Sum of Two Causal Exponentials (cont.)",
    "text": "Example 9.3: Sum of Two Causal Exponentials (cont.)\nThus,\n\\[\nX(s) = \\frac{s-1}{(s+2)(s+1)}, \\quad \\operatorname{Re}\\{s\\} &gt; -1 \\quad \\text{(9.23)}\n\\]\nPole-Zero Plot\n\nPoles: \\(s = -1, s = -2\\) (roots of \\(D(s)\\))\nZeros: \\(s = 1\\) (root of \\(N(s)\\))\n\n\n\n\n\n\ngraph TD\n    S_Plane_Ex3(\"s-Plane for Example 9.3\")\n    subgraph Locations\n        Pole_1(\"X: s = -1\")\n        Pole_2(\"X: s = -2\")\n        Zero_1(\"O: s = 1\")\n    end\n    S_Plane_Ex3 --&gt; Pole_1\n    S_Plane_Ex3 --&gt; Pole_2\n    S_Plane_Ex3 --&gt; Zero_1\n    ROC_Ex3(\"ROC: Re{s} &gt; -1 (Right of the rightmost pole)\")\n    S_Plane_Ex3 -- is bounded by --&gt; ROC_Ex3\n\n\n\n\n\n\n\nIn this example, we combine two causal exponentials. Due to linearity, the Laplace Transform of the sum is the sum of the individual Laplace Transforms. The crucial point here is how the ROC is determined: it must be the intersection of the ROCs of all individual terms. Since both terms are causal exponentials, their ROCs are right-half planes. The overall ROC is the right-half plane to the right of the rightmost pole, which is at \\(s=-1\\). We identify the poles at \\(s=-1\\) and \\(s=-2\\), and a zero at \\(s=1\\)."
  },
  {
    "objectID": "ss_91.html#example-9.4-complex-exponentials-cont.",
    "href": "ss_91.html#example-9.4-complex-exponentials-cont.",
    "title": "Signals and Systems",
    "section": "Example 9.4: Complex Exponentials (cont.)",
    "text": "Example 9.4: Complex Exponentials (cont.)\nCombining these over a common denominator yields:\n\\[\nX(s)=\\frac{2s^2+5s+12}{(s^2+2s+10)(s+2)}, \\quad \\operatorname{Re}\\{s\\}&gt;-1 \\quad \\text{(9.30)}\n\\]\nPole-Zero Plot\n\nPoles: \\(s = -2\\), and \\(s = -1 \\pm j3\\) (roots of \\(s^2+2s+10=0\\))\nZeros: Roots of \\(2s^2+5s+12=0\\) (complex conjugate pair)\n\n\n\n\n\n\ngraph TD\n    S_Plane_Ex4(\"s-Plane for Example 9.4\")\n    subgraph Locations\n        Pole_1(\"X: s = -2\")\n        Pole_2(\"X: s = -1 + j3\")\n        Pole_3(\"X: s = -1 - j3\")\n        Zero_1(\"O: s = complex (conjugate pair)\")\n    end\n    S_Plane_Ex4 --&gt; Pole_1\n    S_Plane_Ex4 --&gt; Pole_2\n    S_Plane_Ex4 --&gt; Pole_3\n    S_Plane_Ex4 --&gt; Zero_1\n    ROC_Ex4(\"ROC: Re{s} &gt; -1 (Right of the rightmost poles)\")\n    S_Plane_Ex4 -- is bounded by --&gt; ROC_Ex4\n\n\n\n\n\n\n\nThis example introduces complex exponentials, which are crucial for representing sinusoidal signals. By using Euler’s formula, we decompose the cosine into two complex exponentials. Each exponential has its own Laplace Transform and ROC. Again, the overall ROC is the intersection of these individual ROCs, leading to \\(\\operatorname{Re}\\{s\\} &gt; -1\\). This results in complex conjugate poles and zeros on the s-plane, which always appear in pairs for real-valued signals. The location of these complex poles and zeros provides insight into the oscillatory behavior of the signal."
  },
  {
    "objectID": "ss_91.html#example-9.5-unit-impulse-and-exponentials-cont.",
    "href": "ss_91.html#example-9.5-unit-impulse-and-exponentials-cont.",
    "title": "Signals and Systems",
    "section": "Example 9.5: Unit Impulse and Exponentials (cont.)",
    "text": "Example 9.5: Unit Impulse and Exponentials (cont.)\nPole-Zero Plot and Fourier Transform\n\nPoles: \\(s = -1, s = 2\\)\nZeros: \\(s = 1\\) (a second-order zero, as it’s repeated)\n\n\n\n\n\n\ngraph TD\n    S_Plane_Ex5(\"s-Plane for Example 9.5\")\n    subgraph Locations\n        Pole_1(\"X: s = -1\")\n        Pole_2(\"X: s = 2\")\n        Zero_1(\"O: s = 1 (2nd order)\")\n    end\n    S_Plane_Ex5 --&gt; Pole_1\n    S_Plane_Ex5 --&gt; Pole_2\n    S_Plane_Ex5 --&gt; Zero_1\n    ROC_Ex5(\"ROC: Re{s} &gt; 2 (Right of the rightmost pole)\")\n    S_Plane_Ex5 -- is bounded by --&gt; ROC_Ex5\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nSince the ROC does not include the \\(j\\omega\\)-axis (\\(\\operatorname{Re}\\{s\\}=0\\)), the Fourier Transform for this signal does not converge. This is due to the growing exponential term \\(e^{2t}u(t)\\).\n\n\n\n\nThis final example demonstrates a signal that includes a unit impulse and a growing exponential. The unit impulse has an ROC that covers the entire s-plane, meaning its Laplace Transform always converges. However, the growing exponential \\(e^{2t}u(t)\\) requires \\(\\operatorname{Re}\\{s\\} &gt; 2\\) for its transform to converge. The overall ROC is thus dictated by the most restrictive condition, \\(\\operatorname{Re}\\{s\\} &gt; 2\\).\nNotice the second-order zero at \\(s=1\\), meaning \\((s-1)\\) appears twice in the numerator. Critically, because the ROC does not include the \\(j\\omega\\)-axis (the imaginary axis where \\(\\operatorname{Re}\\{s\\}=0\\)), this signal does not possess a Fourier Transform. This highlights a key advantage of the Laplace Transform: its ability to analyze signals for which the Fourier Transform does not exist, thanks to its broader convergence properties."
  },
  {
    "objectID": "ss_91.html#application-system-analysis-in-ece-1",
    "href": "ss_91.html#application-system-analysis-in-ece-1",
    "title": "Signals and Systems",
    "section": "Application: System Analysis in ECE",
    "text": "Application: System Analysis in ECE\nLTI System Analysis Workflow\n\n\n\n\n\nflowchart LR\n    A[\"Input Signal x(t)\"] --&gt; B{\"LTI System &lt;br&gt; h(t)\"}\n    B --&gt; C[\"Output Signal y(t)\"]\n\n    A_LT[\"X(s)\"] --&gt; B_LT{\"System Function &lt;br&gt; H(s)\"}\n    B_LT --&gt; C_LT[\"Y(s)\"]\n\n    subgraph \"Time Domain\"\n        A\n        B\n        C\n    end\n\n    subgraph \"Laplace Domain\"\n        A_LT\n        B_LT\n        C_LT\n    end\n\n    A -- \"Laplace Transform\" --&gt; A_LT\n    C_LT -- \"Inverse Laplace Transform\" --&gt; C\n    B_LT -- \"$$H(s)=Y(s)/X(s)$$\" --&gt; B\n\n\n\n\n\n\n\n\nThis slide summarizes the practical utility of the Laplace Transform in ECE. Its ability to convert differential equations into algebraic ones is a game-changer for analyzing complex systems like filters, control systems, and communication channels. The system function \\(H(s)\\), which is the Laplace Transform of the impulse response \\(h(t)\\), becomes the algebraic representation of the system. By solving for \\(Y(s) = H(s)X(s)\\) in the Laplace domain and then performing an inverse Laplace Transform, we can find the time-domain output \\(y(t)\\). This systematic approach makes analyzing LTI systems much more tractable and intuitive than working directly with convolution in the time domain or differential equations. Poles and zeros of \\(H(s)\\) directly tell us about the system’s stability and frequency response characteristics."
  },
  {
    "objectID": "ss_92.html#the-region-of-convergence-roc-for-laplace-transforms",
    "href": "ss_92.html#the-region-of-convergence-roc-for-laplace-transforms",
    "title": "Signals and Systems",
    "section": "The Region of Convergence (ROC) for Laplace Transforms",
    "text": "The Region of Convergence (ROC) for Laplace Transforms\nUnlocking Signal Properties from the s-Plane\nIn the previous section, we established that the Region of Convergence (ROC) is vital for a complete Laplace Transform specification. The ROC helps distinguish between signals that might share the same algebraic \\(X(s)\\) expression.\nThis section explores specific properties of the ROC, which allow us to:\n\nImplicitly determine the ROC from \\(X(s)\\) and signal characteristics.\nReconstruct the time-domain signal \\(x(t)\\) from \\(X(s)\\) and its ROC.\nUnderstand the time-domain features of \\(x(t)\\) (e.g., causality, stability) directly from the ROC.\n\n\nWe’ve seen that two different signals can have the same algebraic form for their Laplace Transform. The ROC is the key differentiator. Today, we’ll dive into eight fundamental properties of the ROC. These properties are not just theoretical; they are practical tools that bridge the gap between the complex s-plane and the time-domain characteristics of our signals and systems. Understanding these will allow you to infer a lot about a signal just by looking at its Laplace Transform and its ROC."
  },
  {
    "objectID": "ss_92.html#property-1-roc-as-strips-parallel-to-the-jomega-axis",
    "href": "ss_92.html#property-1-roc-as-strips-parallel-to-the-jomega-axis",
    "title": "Signals and Systems",
    "section": "Property 1: ROC as Strips Parallel to the \\(j\\omega\\)-axis",
    "text": "Property 1: ROC as Strips Parallel to the \\(j\\omega\\)-axis\nProperty: The ROC of \\(X(s)\\) consists of strips parallel to the \\(j\\omega\\)-axis in the \\(s\\)-plane.\nExplanation: The ROC consists of values of \\(s = \\sigma + j\\omega\\) for which the Fourier Transform of \\(x(t)e^{-\\sigma t}\\) converges. This means \\(x(t)e^{-\\sigma t}\\) must be absolutely integrable:\n\\[\n\\int_{-\\infty}^{+\\infty}|x(t)| e^{-\\sigma t} dt &lt; \\infty \\quad \\text{(9.36)}\n\\]\nThis condition depends only on \\(\\sigma\\) (the real part of \\(s\\)), not on \\(j\\omega\\). Therefore, if a vertical line \\(\\operatorname{Re}\\{s\\} = \\sigma_0\\) is in the ROC, then the entire strip containing that line, for all imaginary values, is also in the ROC."
  },
  {
    "objectID": "ss_92.html#property-2-roc-does-not-contain-any-poles",
    "href": "ss_92.html#property-2-roc-does-not-contain-any-poles",
    "title": "Signals and Systems",
    "section": "Property 2: ROC Does Not Contain Any Poles",
    "text": "Property 2: ROC Does Not Contain Any Poles\nProperty: For rational Laplace transforms, the ROC does not contain any poles.\nExplanation: At a pole, the Laplace Transform \\(X(s)\\) is infinite. By definition, for \\(s\\) to be in the ROC, the integral \\(\\int_{-\\infty}^{+\\infty} x(t) e^{-st} dt\\) must converge to a finite value. Since \\(X(s)\\) is infinite at a pole, the integral clearly does not converge at a pole. Therefore, poles always lie on the boundaries of the ROC, never within it."
  },
  {
    "objectID": "ss_92.html#property-3-finite-duration-signals",
    "href": "ss_92.html#property-3-finite-duration-signals",
    "title": "Signals and Systems",
    "section": "Property 3: Finite-Duration Signals",
    "text": "Property 3: Finite-Duration Signals\nProperty: If \\(x(t)\\) is of finite duration and is absolutely integrable, then the ROC is the entire \\(s\\)-plane.\nExplanation: A finite-duration signal is zero outside a finite interval, say \\(T_1 &lt; t &lt; T_2\\). For \\(s=\\sigma+j\\omega\\), we need to check the convergence of \\(\\int_{T_1}^{T_2}|x(t)| e^{-\\sigma t} dt\\).\n\nSince \\(x(t)\\) is absolutely integrable over \\([T_1, T_2]\\), \\(\\int_{T_1}^{T_2}|x(t)| dt &lt; \\infty\\).\nFor any \\(\\sigma\\), \\(e^{-\\sigma t}\\) is bounded over the finite interval \\([T_1, T_2]\\).\n\nIf \\(\\sigma &gt; 0\\), \\(e^{-\\sigma t}\\) is bounded by \\(e^{-\\sigma T_1}\\).\nIf \\(\\sigma &lt; 0\\), \\(e^{-\\sigma t}\\) is bounded by \\(e^{-\\sigma T_2}\\).\n\nThus, \\(|x(t)|e^{-\\sigma t}\\) remains absolutely integrable for all \\(\\sigma\\).\n\n\n\n\n\n\n\nTip\n\n\nFinite-duration signals are “well-behaved” enough that the exponential weighting \\(e^{-\\sigma t}\\) never causes divergence within their non-zero interval.\n\n\n\n\nThis property is a bit of a special case but important. If a signal only exists for a finite amount of time, its Laplace Transform will always converge, no matter what value \\(s\\) takes. This is because the exponential weighting function \\(e^{-\\sigma t}\\) can’t grow unboundedly over a finite interval. Think of a short pulse or a single sine wave cycle. Its transform will converge everywhere."
  },
  {
    "objectID": "ss_92.html#example-9.6-finite-duration-signal",
    "href": "ss_92.html#example-9.6-finite-duration-signal",
    "title": "Signals and Systems",
    "section": "Example 9.6: Finite Duration Signal",
    "text": "Example 9.6: Finite Duration Signal\nLet \\(x(t) = e^{-at}\\) for \\(0 &lt; t &lt; T\\), and \\(0\\) otherwise.\nThe Laplace Transform is:\n\\[\nX(s) = \\int_{0}^{T} e^{-at} e^{-st} dt = \\int_{0}^{T} e^{-(s+a)t} dt = \\frac{1}{s+a}\\left[1-e^{-(s+a)T}\\right] \\quad \\text{(9.42)}\n\\]\nApparent Pole vs. Actual Behavior\nIf we look at the expression, it seems like there is a pole at \\(s=-a\\). However, for a finite-duration signal, the ROC should be the entire \\(s\\)-plane (Property 3). This implies no poles.\nLet’s check the behavior at \\(s=-a\\) using L’Hôpital’s Rule:\n\\[\n\\lim _{s \\rightarrow-a} X(s) = \\lim _{s \\rightarrow-a}\\left[\\frac{\\frac{d}{d s}\\left(1-e^{-(s+a) T)}\\right.}{\\frac{d}{d s}(s+a)}\\right] = \\lim _{s \\rightarrow-a} T e^{-a T} e^{-s T}\n\\]\nSo, \\(X(-a) = T e^{-aT} e^{-(-a)T} = T e^{-aT} e^{aT} = T\\).\nSince \\(X(-a)\\) is a finite value (\\(T\\)), there is no pole at \\(s=-a\\). The apparent pole is canceled by a zero at the same location."
  },
  {
    "objectID": "ss_92.html#property-4-right-sided-signals",
    "href": "ss_92.html#property-4-right-sided-signals",
    "title": "Signals and Systems",
    "section": "Property 4: Right-Sided Signals",
    "text": "Property 4: Right-Sided Signals\nProperty: If \\(x(t)\\) is right-sided, and if the line \\(\\operatorname{Re}\\{s\\}=\\sigma_0\\) is in the ROC, then all values of \\(s\\) for which \\(\\operatorname{Re}\\{s\\}&gt;\\sigma_0\\) will also be in the ROC.\nDefinition: A right-sided signal is zero prior to some finite time \\(T_1\\) (i.e., \\(x(t)=0\\) for \\(t &lt; T_1\\)).\nExplanation: If \\(x(t)e^{-\\sigma_0 t}\\) is absolutely integrable, then for any \\(\\sigma_1 &gt; \\sigma_0\\):\n\\[\n\\int_{T_1}^{\\infty}|x(t)| e^{-\\sigma_1 t} dt = \\int_{T_1}^{\\infty}|x(t)| e^{-\\sigma_0 t} e^{-(\\sigma_1-\\sigma_0)t} dt\n\\]\nSince \\(\\sigma_1 &gt; \\sigma_0\\), the term \\(e^{-(\\sigma_1-\\sigma_0)t}\\) is a decaying exponential for \\(t&gt;T_1\\). This means \\(e^{-(\\sigma_1-\\sigma_0)t}\\) decays faster than \\(e^{-(\\sigma_0-\\sigma_0)t} = 1\\) as \\(t \\to \\infty\\). Therefore, if the integral converges for \\(\\sigma_0\\), it will also converge for any \\(\\sigma_1 &gt; \\sigma_0\\).\n\n\n\n\n\n\nTip\n\n\nFor a right-sided signal, the ROC is always a right-half plane, extending to the right from some vertical line. If \\(X(s)\\) is rational, the ROC is to the right of the rightmost pole."
  },
  {
    "objectID": "ss_92.html#property-5-left-sided-signals",
    "href": "ss_92.html#property-5-left-sided-signals",
    "title": "Signals and Systems",
    "section": "Property 5: Left-Sided Signals",
    "text": "Property 5: Left-Sided Signals\nProperty: If \\(x(t)\\) is left-sided, and if the line \\(\\operatorname{Re}\\{s\\}=\\sigma_0\\) is in the ROC, then all values of \\(s\\) for which \\(\\operatorname{Re}\\{s\\}&lt;\\sigma_0\\) will also be in the ROC.\nDefinition: A left-sided signal is zero after some finite time \\(T_2\\) (i.e., \\(x(t)=0\\) for \\(t &gt; T_2\\)).\nExplanation: The argument is analogous to Property 4. If \\(x(t)e^{-\\sigma_0 t}\\) is absolutely integrable, then for any \\(\\sigma_1 &lt; \\sigma_0\\):\n\\[\n\\int_{-\\infty}^{T_2}|x(t)| e^{-\\sigma_1 t} dt = \\int_{-\\infty}^{T_2}|x(t)| e^{-\\sigma_0 t} e^{-(\\sigma_1-\\sigma_0)t} dt\n\\]\nSince \\(\\sigma_1 &lt; \\sigma_0\\), the term \\(e^{-(\\sigma_1-\\sigma_0)t}\\) is a decaying exponential for \\(t&lt;T_2\\) (as \\(t \\to -\\infty\\)). This means \\(e^{-(\\sigma_1-\\sigma_0)t}\\) decays faster than \\(1\\) as \\(t \\to -\\infty\\). Therefore, if the integral converges for \\(\\sigma_0\\), it will also converge for any \\(\\sigma_1 &lt; \\sigma_0\\).\n\n\n\n\n\n\nTip\n\n\nFor a left-sided signal, the ROC is always a left-half plane, extending to the left from some vertical line. If \\(X(s)\\) is rational, the ROC is to the left of the leftmost pole."
  },
  {
    "objectID": "ss_92.html#property-6-two-sided-signals",
    "href": "ss_92.html#property-6-two-sided-signals",
    "title": "Signals and Systems",
    "section": "Property 6: Two-Sided Signals",
    "text": "Property 6: Two-Sided Signals\nProperty: If \\(x(t)\\) is two-sided, and if the line \\(\\operatorname{Re}\\{s\\}=\\sigma_0\\) is in the ROC, then the ROC will consist of a strip in the \\(s\\)-plane that includes the line \\(\\operatorname{Re}\\{s\\}=\\sigma_0\\).\nDefinition: A two-sided signal is of infinite extent for both \\(t&gt;0\\) and \\(t&lt;0\\).\nExplanation: A two-sided signal \\(x(t)\\) can be decomposed into a right-sided part \\(x_R(t)\\) and a left-sided part \\(x_L(t)\\): \\(x(t) = x_R(t) + x_L(t)\\), where \\(x_R(t)\\) is \\(x(t)u(t-T_0)\\) and \\(x_L(t)\\) is \\(x(t)u(T_0-t)\\).\n\nThe ROC for \\(x_R(t)\\) is a right-half plane: \\(\\operatorname{Re}\\{s\\} &gt; \\sigma_R\\).\nThe ROC for \\(x_L(t)\\) is a left-half plane: \\(\\operatorname{Re}\\{s\\} &lt; \\sigma_L\\).\n\nThe ROC for \\(x(t)\\) is the intersection of these two ROCs. This intersection forms a vertical strip: \\(\\sigma_R &lt; \\operatorname{Re}\\{s\\} &lt; \\sigma_L\\).\n\n\n\n\n\n\nNote\n\n\nFor a two-sided signal, a ROC exists only if \\(\\sigma_R &lt; \\sigma_L\\). If \\(\\sigma_R \\ge \\sigma_L\\), there is no overlap, and thus no Laplace Transform for \\(x(t)\\)."
  },
  {
    "objectID": "ss_92.html#example-9.7-two-sided-exponential",
    "href": "ss_92.html#example-9.7-two-sided-exponential",
    "title": "Signals and Systems",
    "section": "Example 9.7: Two-Sided Exponential",
    "text": "Example 9.7: Two-Sided Exponential\nLet \\(x(t) = e^{-b|t|}\\). We can write this as \\(x(t) = e^{-bt}u(t) + e^{bt}u(-t)\\).\n\nRight-sided part: \\(x_R(t) = e^{-bt}u(t)\\) \\(\\mathcal{L}\\{x_R(t)\\} = \\frac{1}{s+b}\\), with ROC: \\(\\operatorname{Re}\\{s\\} &gt; -b\\).\nLeft-sided part: \\(x_L(t) = e^{bt}u(-t)\\) \\(\\mathcal{L}\\{x_L(t)\\} = \\frac{-1}{s-b}\\), with ROC: \\(\\operatorname{Re}\\{s\\} &lt; b\\).\n\nFor \\(X(s)\\) to exist, the ROCs must overlap: \\(\\operatorname{Re}\\{s\\} &gt; -b\\) AND \\(\\operatorname{Re}\\{s\\} &lt; b\\). This defines a strip: \\(-b &lt; \\operatorname{Re}\\{s\\} &lt; b\\).\nThis overlap exists only if \\(b &gt; 0\\). If \\(b \\le 0\\), there is no common ROC, and \\(x(t)\\) has no Laplace Transform.\nFor \\(b &gt; 0\\), the combined Laplace Transform is:\n\\[\nX(s) = \\frac{1}{s+b} - \\frac{1}{s-b} = \\frac{(s-b) - (s+b)}{(s+b)(s-b)} = \\frac{-2b}{s^2-b^2}\n\\]\n\\[\nX(s) = \\frac{-2b}{s^2-b^2}, \\quad -b &lt; \\operatorname{Re}\\{s\\} &lt; b \\quad \\text{(9.51)}\n\\]"
  },
  {
    "objectID": "ss_92.html#property-7-rational-xs-roc-bounded-by-poles",
    "href": "ss_92.html#property-7-rational-xs-roc-bounded-by-poles",
    "title": "Signals and Systems",
    "section": "Property 7: Rational \\(X(s)\\) ROC Bounded by Poles",
    "text": "Property 7: Rational \\(X(s)\\) ROC Bounded by Poles\nProperty: If the Laplace transform \\(X(s)\\) of \\(x(t)\\) is rational, then its ROC is bounded by poles or extends to infinity. In addition, no poles of \\(X(s)\\) are contained in the ROC.\nExplanation: This property combines insights from previous ones:\n\nPoles as Boundaries: As established by Property 2, poles define the edges of the ROC.\nStructure of Rational Transforms: Rational Laplace Transforms arise from signals composed of sums of exponentials. Each exponential has an ROC that is a half-plane.\nIntersection of Half-Planes: The overall ROC for a sum of exponentials is the intersection of these half-planes, resulting in a strip or a single half-plane bounded by the poles.\n\n\n\n\n\n\n\nImportant\n\n\nThis property reinforces that poles are critical in defining the exact shape and location of the ROC for rational transforms.\n\n\n\n\nProperty 7 essentially formalizes what we’ve been observing: for rational Laplace Transforms, poles are the geographic markers of the ROC. They define its boundaries. This is a direct consequence of rational transforms arising from sums of exponentials, each of which has a half-plane ROC. The combined ROC is simply the intersection of these half-planes."
  },
  {
    "objectID": "ss_92.html#property-8-roc-for-rational-xs-and-sidedness",
    "href": "ss_92.html#property-8-roc-for-rational-xs-and-sidedness",
    "title": "Signals and Systems",
    "section": "Property 8: ROC for Rational \\(X(s)\\) and Sidedness",
    "text": "Property 8: ROC for Rational \\(X(s)\\) and Sidedness\nProperty: If the Laplace transform \\(X(s)\\) of \\(x(t)\\) is rational, then:\n\nIf \\(x(t)\\) is right-sided, the ROC is the region in the \\(s\\)-plane to the right of the rightmost pole.\nIf \\(x(t)\\) is left-sided, the ROC is the region in the \\(s\\)-plane to the left of the leftmost pole.\n\nExplanation: This property is a direct consequence of Properties 4, 5, and 7.\n\nRight-sided signals: Their ROCs are right-half planes (Property 4). For rational \\(X(s)\\), the right-half plane must be bounded by the pole with the largest real part (the rightmost pole) to ensure all terms converge.\nLeft-sided signals: Their ROCs are left-half planes (Property 5). For rational \\(X(s)\\), the left-half plane must be bounded by the pole with the smallest real part (the leftmost pole) to ensure all terms converge.\n\n\n\n\n\n\n\nTip\n\n\nThis property provides a quick and practical way to determine the ROC for many common signals in ECE, especially when dealing with system impulse responses.\n\n\n\n\nProperty 8 is incredibly useful for practical application. Once you have the algebraic form of a rational Laplace Transform and you know whether the corresponding time-domain signal is right-sided or left-sided, you can immediately sketch its ROC. This rule simplifies the process significantly and is a key tool in system analysis, particularly for determining causality and stability from pole locations."
  },
  {
    "objectID": "ss_92.html#example-9.8-multiple-rocs-for-same-xs",
    "href": "ss_92.html#example-9.8-multiple-rocs-for-same-xs",
    "title": "Signals and Systems",
    "section": "Example 9.8: Multiple ROCs for Same \\(X(s)\\)",
    "text": "Example 9.8: Multiple ROCs for Same \\(X(s)\\)\nConsider \\(X(s) = \\frac{1}{(s+1)(s+2)}\\). This \\(X(s)\\) has poles at \\(s=-1\\) and \\(s=-2\\).\nBased on the ROC properties and the time-domain characteristics of \\(x(t)\\), there are three possible ROCs for this algebraic expression, each corresponding to a distinct \\(x(t)\\):\n1. Right-Sided Signal (Causal)\n\nROC: \\(\\operatorname{Re}\\{s\\} &gt; -1\\) (right of the rightmost pole).\nTime-domain: \\(x(t) = (e^{-t} - e^{-2t})u(t)\\).\nFourier Transform: Exists, as the ROC includes the \\(j\\omega\\)-axis.\n\n2. Left-Sided Signal (Anti-Causal)\n\nROC: \\(\\operatorname{Re}\\{s\\} &lt; -2\\) (left of the leftmost pole).\nTime-domain: \\(x(t) = (-e^{-t} + e^{-2t})u(-t)\\).\nFourier Transform: Does not exist, as the ROC does not include the \\(j\\omega\\)-axis."
  },
  {
    "objectID": "ss_92.html#interactive-roc-and-time-domain-visualization",
    "href": "ss_92.html#interactive-roc-and-time-domain-visualization",
    "title": "Signals and Systems",
    "section": "Interactive ROC and Time-Domain Visualization",
    "text": "Interactive ROC and Time-Domain Visualization\nExplore the relationship between the ROC and the corresponding time-domain signal for \\(X(s) = \\frac{1}{(s+1)(s+2)}\\).\n\nviewof roc_choice = Inputs.select(\n  [\"Re{s} &gt; -1 (Right-Sided)\", \"Re{s} &lt; -2 (Left-Sided)\", \"-2 &lt; Re{s} &lt; -1 (Two-Sided)\"],\n  {label: \"Select ROC Type\"}\n);\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive element is the culmination of our discussion on ROC properties. By selecting different ROC types, you can directly observe how the same algebraic \\(X(s)\\) corresponds to radically different time-domain signals. Pay close attention to the causality (whether the signal starts at \\(t=0\\)) and the behavior for \\(t&lt;0\\). Also, notice whether the \\(j\\omega\\)-axis is included in the ROC, which tells you if the Fourier Transform exists. This hands-on visualization should solidify your understanding of the profound impact of the ROC."
  },
  {
    "objectID": "ss_92.html#key-takeaways-roc-properties",
    "href": "ss_92.html#key-takeaways-roc-properties",
    "title": "Signals and Systems",
    "section": "Key Takeaways: ROC Properties",
    "text": "Key Takeaways: ROC Properties\n\nVertical Strips: ROCs are always vertical strips in the \\(s\\)-plane.\nNo Poles in ROC: Poles define the boundaries but are never part of the ROC itself.\nFinite Duration: ROC is the entire \\(s\\)-plane.\nRight-Sided: ROC is a right-half plane ($ {s} &gt; _{max_pole} $).\nLeft-Sided: ROC is a left-half plane ($ {s} &lt; _{min_pole} $).\nTwo-Sided: ROC is a strip between poles ($ _L &lt; {s} &lt; _R $).\nRational \\(X(s)\\): ROC is always bounded by poles or extends to infinity.\nSidedness Rule: For rational \\(X(s)\\), sidedness directly dictates the ROC relative to the poles.\n\n\n\n\n\n\n\nImportant\n\n\nThe ROC is as crucial as the algebraic expression of \\(X(s)\\) for uniquely defining a signal and understanding its time-domain characteristics, including causality and stability.\n\n\n\n\n\n\n\n\n\nTip\n\n\nAlways specify the ROC when dealing with Laplace Transforms! It’s not just a mathematical detail; it’s fundamental engineering information.\n\n\n\n\nTo summarize, these eight properties are your go-to rules for understanding and working with the ROC. Remember, poles are the critical points that define the ROC boundaries. The type of signal (finite duration, right-sided, left-sided, or two-sided) directly determines the shape of the ROC. This knowledge is not just academic; it’s essential for correctly interpreting system behavior and designing filters and controllers in ECE."
  },
  {
    "objectID": "ss_92.html#property-1-roc-as-strips-parallel-to-the-jomega-axis-1",
    "href": "ss_92.html#property-1-roc-as-strips-parallel-to-the-jomega-axis-1",
    "title": "Signals and Systems",
    "section": "Property 1: ROC as Strips Parallel to the \\(j\\omega\\)-axis",
    "text": "Property 1: ROC as Strips Parallel to the \\(j\\omega\\)-axis\nVisualization\n\n\n\n\n\ngraph TD\n    subgraph s-Plane\n        direction LR\n        A[\"Re{s}\"] --- B[\"Im{s}\"]\n        B --- C(jω-axis)\n        A --- D(σ-axis)\n        style C fill:#fff,stroke:#fff\n        style D fill:#fff,stroke:#fff\n        ROC([ ])\n        subgraph ROC_Strip\n            alpha(\" \")\n            beta(\" \")\n            gamma(\" \")\n        end\n        alpha -- \" \" --&gt; beta\n        beta -- \" \" --&gt; gamma\n        style ROC_Strip fill:#ADD8E6,stroke:#ADD8E6,stroke-width:0px\n    end\n\n\n\n\n\n\n\nProperty 1 is quite intuitive once you recall the relationship between the Laplace and Fourier Transforms. Since the convergence criterion depends only on the real part of \\(s\\), \\(\\sigma\\), it means that if a particular \\(\\sigma_0\\) works, then any \\(s\\) with that same \\(\\sigma_0\\) will also work, regardless of its imaginary component \\(j\\omega\\). This leads to the characteristic vertical strip shape of the ROC in the s-plane."
  },
  {
    "objectID": "ss_92.html#property-2-roc-does-not-contain-any-poles-1",
    "href": "ss_92.html#property-2-roc-does-not-contain-any-poles-1",
    "title": "Signals and Systems",
    "section": "Property 2: ROC Does Not Contain Any Poles",
    "text": "Property 2: ROC Does Not Contain Any Poles\nVisualization\n\n\n\n\n\ngraph TD\n    subgraph s-Plane\n        direction LR\n        A[\"Re{s}\"] --- B[\"Im{s}\"]\n        Pole(\"X\")\n        ROC_Boundary_Left(\" \")\n        ROC_Boundary_Right(\" \")\n        ROC_Shaded([ ])\n        subgraph ROC_Area\n            alpha(\" \")\n            beta(\" \")\n        end\n        alpha -- \" \" --&gt; beta\n        style ROC_Area fill:#ADD8E6,stroke:#ADD8E6,stroke-width:0px\n    end\n    Pole --- ROC_Boundary_Left\n    Pole --- ROC_Boundary_Right\n    ROC_Boundary_Left -- \" \" --&gt; alpha\n    beta -- \" \" --&gt; ROC_Boundary_Right\n    style Pole fill:#fff,stroke:#ff0000,stroke-width:2px,font-size:16px,font-weight:bold\n\n\n\n\n\n\n\nThis property is fundamental and makes perfect sense. If \\(X(s)\\) goes to infinity at a certain point in the s-plane, then by definition, the integral that defines \\(X(s)\\) cannot converge at that point. Hence, poles are excluded from the ROC. They act as natural boundaries, delineating where the transform converges and where it doesn’t."
  },
  {
    "objectID": "ss_92.html#example-9.6-finite-duration-signal-1",
    "href": "ss_92.html#example-9.6-finite-duration-signal-1",
    "title": "Signals and Systems",
    "section": "Example 9.6: Finite Duration Signal",
    "text": "Example 9.6: Finite Duration Signal\nInteractive Visualization: Magnitude of \\(X(s)\\)\nObserve that \\(X(s)\\) remains finite even when \\(s\\) approaches \\(-a\\).\n\nviewof a_ex6 = Inputs.range([-2, 2], {value: 1, step: 0.1, label: \"Parameter 'a'\"});\nviewof T_ex6 = Inputs.range([0.1, 5], {value: 2, step: 0.1, label: \"Duration T\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 9.6 beautifully illustrates Property 3. Although the algebraic form of \\(X(s)\\) might suggest a pole, the fact that the signal is of finite duration means this “pole” must be removable. The interactive plot shows that the magnitude of \\(X(s)\\) is perfectly finite at \\(s=-a\\), confirming that it’s not a true pole. This cancellation happens because the numerator also becomes zero at \\(s=-a\\), leading to an indeterminate form that evaluates to a finite value."
  },
  {
    "objectID": "ss_92.html#property-4-right-sided-signals-1",
    "href": "ss_92.html#property-4-right-sided-signals-1",
    "title": "Signals and Systems",
    "section": "Property 4: Right-Sided Signals",
    "text": "Property 4: Right-Sided Signals\nVisualization\n\n\n\n\n\ngraph TD\n    subgraph s-Plane\n        direction LR\n        A[\"Re{s}\"] --- B[\"Im{s}\"]\n        Pole1(\"X\")\n        Pole2(\"X\")\n        ROC_Boundary_Right(\" \")\n        ROC_Shaded([ ])\n        subgraph ROC_Area\n            alpha(\" \")\n            beta(\" \")\n            gamma(\" \")\n        end\n        alpha -- \" \" --&gt; beta\n        beta -- \" \" --&gt; gamma\n        style ROC_Area fill:#ADD8E6,stroke:#ADD8E6,stroke-width:0px\n    end\n    Pole1 --- ROC_Boundary_Right\n    Pole2 --- ROC_Boundary_Right\n    ROC_Boundary_Right -- \" \" --&gt; alpha\n    style Pole1 fill:#fff,stroke:#ff0000,stroke-width:2px,font-size:16px,font-weight:bold\n    style Pole2 fill:#fff,stroke:#ff0000,stroke-width:2px,font-size:16px,font-weight:bold\n\n\n\n\n\n\n\nProperty 4 deals with right-sided signals, which are signals that start at a certain point and extend to positive infinity. Think of causal signals like \\(u(t)\\) or \\(e^{-at}u(t)\\). If the integral converges for some \\(\\sigma_0\\), it will converge for any \\(\\sigma\\) greater than \\(\\sigma_0\\). This is because increasing \\(\\sigma\\) makes the \\(e^{-\\sigma t}\\) term decay faster as \\(t \\to \\infty\\), thus helping convergence. This leads to an ROC that is a right-half plane. For rational transforms, this plane starts just to the right of the rightmost pole."
  },
  {
    "objectID": "ss_92.html#property-5-left-sided-signals-1",
    "href": "ss_92.html#property-5-left-sided-signals-1",
    "title": "Signals and Systems",
    "section": "Property 5: Left-Sided Signals",
    "text": "Property 5: Left-Sided Signals\nVisualization\n\n\n\n\n\ngraph TD\n    subgraph s-Plane\n        direction LR\n        A[\"Re{s}\"] --- B[\"Im{s}\"]\n        Pole1(\"X\")\n        Pole2(\"X\")\n        ROC_Boundary_Left(\" \")\n        ROC_Shaded([ ])\n        subgraph ROC_Area\n            alpha(\" \")\n            beta(\" \")\n            gamma(\" \")\n        end\n        alpha -- \" \" --&gt; beta\n        beta -- \" \" --&gt; gamma\n        style ROC_Area fill:#ADD8E6,stroke:#ADD8E6,stroke-width:0px\n    end\n    Pole1 --- ROC_Boundary_Left\n    Pole2 --- ROC_Boundary_Left\n    ROC_Boundary_Left -- \" \" --&gt; alpha\n    style Pole1 fill:#fff,stroke:#ff0000,stroke-width:2px,font-size:16px,font-weight:bold\n    style Pole2 fill:#fff,stroke:#ff0000,stroke-width:2px,font-size:16px,font-weight:bold\n\n\n\n\n\n\n\nProperty 5 is the mirror image of Property 4, applied to left-sided signals. These signals exist from negative infinity up to a certain point, like \\(-e^{-at}u(-t)\\). For these, increasing \\(\\sigma\\) makes \\(e^{-\\sigma t}\\) grow faster as \\(t \\to -\\infty\\), which would hinder convergence. Conversely, decreasing \\(\\sigma\\) makes \\(e^{-\\sigma t}\\) decay faster as \\(t \\to -\\infty\\), aiding convergence. Hence, the ROC is a left-half plane, starting to the left of the leftmost pole for rational transforms."
  },
  {
    "objectID": "ss_92.html#property-6-two-sided-signals-1",
    "href": "ss_92.html#property-6-two-sided-signals-1",
    "title": "Signals and Systems",
    "section": "Property 6: Two-Sided Signals",
    "text": "Property 6: Two-Sided Signals\nVisualization\n\n\n\n\n\ngraph TD\n    subgraph s-Plane\n        direction LR\n        A[\"Re{s}\"] --- B[\"Im{s}\"]\n        PoleL(\"X: $\\sigma_L$\")\n        PoleR(\"X: $\\sigma_R$\")\n        ROC_Strip([ ])\n        subgraph ROC_Area\n            alpha(\" \")\n            beta(\" \")\n        end\n        alpha -- \" \" --&gt; beta\n        style ROC_Area fill:#ADD8E6,stroke:#ADD8E6,stroke-width:0px\n    end\n    PoleL -- \" \" --&gt; ROC_Strip\n    PoleR -- \" \" --&gt; ROC_Strip\n    style PoleL fill:#fff,stroke:#ff0000,stroke-width:2px,font-size:16px,font-weight:bold\n    style PoleR fill:#fff,stroke:#ff0000,stroke-width:2px,font-size:16px,font-weight:bold\n\n\n\n\n\n\n\nTwo-sided signals, like \\(e^{-b|t|}\\), extend in both positive and negative time. We can think of them as a sum of a right-sided and a left-sided component. For the Laplace Transform of the sum to exist, both components must converge, meaning their individual ROCs must overlap. This overlap creates a characteristic vertical strip in the s-plane. A critical condition is that the right-half plane boundary must be to the left of the left-half plane boundary (\\(\\sigma_R &lt; \\sigma_L\\)) for any overlap to exist."
  },
  {
    "objectID": "ss_92.html#example-9.7-two-sided-exponential-1",
    "href": "ss_92.html#example-9.7-two-sided-exponential-1",
    "title": "Signals and Systems",
    "section": "Example 9.7: Two-Sided Exponential",
    "text": "Example 9.7: Two-Sided Exponential\nPole-Zero Plot & ROC\n\nPoles: \\(s = b, s = -b\\) (roots of \\(s^2-b^2=0\\))\nZeros: None in the finite s-plane (numerator is a constant).\n\n\nExample 9.7 is a classic illustration of a two-sided signal and its strip ROC. The signal \\(e^{-b|t|}\\) can be seen as a sum of a causal exponential and an anti-causal exponential. For its Laplace Transform to exist, the ROCs of these two components must overlap. This leads to the condition \\(b &gt; 0\\) and a strip ROC between \\(-b\\) and \\(b\\). If \\(b\\) were negative or zero, these two half-planes would not overlap, and the signal would not have a Laplace Transform. This example clearly shows how the ROC is bounded by the poles of \\(X(s)\\)."
  },
  {
    "objectID": "ss_92.html#example-9.8-multiple-rocs-for-same-xs-cont.",
    "href": "ss_92.html#example-9.8-multiple-rocs-for-same-xs-cont.",
    "title": "Signals and Systems",
    "section": "Example 9.8: Multiple ROCs for Same \\(X(s)\\) (cont.)",
    "text": "Example 9.8: Multiple ROCs for Same \\(X(s)\\) (cont.)\n3. Two-Sided Signal\n\nROC: \\(-2 &lt; \\operatorname{Re}\\{s\\} &lt; -1\\) (strip between poles).\nTime-domain: \\(x(t) = -e^{-t}u(-t) + e^{-2t}u(t)\\).\nFourier Transform: Does not exist, as the ROC does not include the \\(j\\omega\\)-axis.\n\nThis example truly highlights why the ROC is essential for unique signal specification!\n\nExample 9.8 is a cornerstone for understanding the ROC. It shows that an identical algebraic expression for \\(X(s)\\) can represent three entirely different time-domain signals, depending solely on the specified ROC. Each ROC implies a different type of signal (causal, anti-causal, two-sided) and has different implications for the existence of the Fourier Transform. This is why you must always state the ROC along with the algebraic expression of \\(X(s)\\)."
  },
  {
    "objectID": "ss_93.html#the-inverse-laplace-transform",
    "href": "ss_93.html#the-inverse-laplace-transform",
    "title": "Signals and Systems",
    "section": "The Inverse Laplace Transform",
    "text": "The Inverse Laplace Transform\nRecovering Signals from the s-Domain"
  },
  {
    "objectID": "ss_93.html#recap-from-time-to-s-domain",
    "href": "ss_93.html#recap-from-time-to-s-domain",
    "title": "Signals and Systems",
    "section": "Recap: From Time to s-Domain",
    "text": "Recap: From Time to s-Domain\nThe Laplace Transform \\(X(s)\\) of a signal \\(x(t)\\) is defined as:\n\\[\nX(s) = \\int_{-\\infty}^{+\\infty} x(t) e^{-st} dt\n\\]\nWhere \\(s = \\sigma + j\\omega\\). This means \\(X(s)\\) can be interpreted as the Fourier Transform of an exponentially weighted signal:\n\\[\nX(\\sigma+j \\omega)=\\mathcal{F}\\left\\{x(t) e^{-\\sigma t}\\right\\}=\\int_{-\\infty}^{+\\infty} x(t) e^{-\\sigma t} e^{-j \\omega t} d t \\tag{9.53}\n\\]\n\n\n\n\n\n\nNote\n\n\nThe Region of Convergence (ROC) for \\(X(s)\\) is crucial. It defines the range of \\(\\sigma\\) values for which the integral converges.\n\n\n\n\nRemind students that the Laplace transform extends the Fourier transform by allowing for exponentially growing or decaying signals. The real part of ‘s’, sigma, acts as a damping factor, ensuring the integral converges. Emphasize that knowing just \\(X(s)\\) algebraically is not enough; the ROC is equally important for defining the transform."
  },
  {
    "objectID": "ss_93.html#the-inverse-laplace-transform-formula",
    "href": "ss_93.html#the-inverse-laplace-transform-formula",
    "title": "Signals and Systems",
    "section": "The Inverse Laplace Transform Formula",
    "text": "The Inverse Laplace Transform Formula\nWe can recover \\(x(t)\\) from \\(X(s)\\) using the inverse Fourier transform relationship:\n\\[\nx(t) e^{-\\sigma t}=\\mathcal{F}^{-1}\\{X(\\sigma+j \\omega)\\}=\\frac{1}{2 \\pi} \\int_{-\\infty}^{+\\infty} X(\\sigma+j \\omega) e^{j \\omega t} d \\omega \\tag{9.54}\n\\]\nMultiplying by \\(e^{\\sigma t}\\), we get:\n\\[\nx(t)=\\frac{1}{2 \\pi} \\int_{-\\infty}^{+\\infty} X(\\sigma+j \\omega) e^{(\\sigma+j \\omega) t} d \\omega \\tag{9.55}\n\\]\nBy changing the variable of integration from \\(\\omega\\) to \\(s\\) (\\(ds = j d\\omega\\) since \\(\\sigma\\) is constant), we obtain the fundamental inverse Laplace transform equation:\n\\[\nx(t)=\\frac{1}{2 \\pi j} \\int_{\\sigma-j \\infty}^{\\sigma+j \\infty} X(s) e^{s t} d s \\tag{9.56}\n\\]\n\n\n\n\n\n\nWarning\n\n\nThe integral in Eq. (9.56) is a contour integral in the complex \\(s\\)-plane. For most ECE undergraduate courses, direct evaluation is often complex and solved using alternative methods.\n\n\n\n\nExplain that the integration path is a vertical line in the s-plane, where the real part \\(\\sigma\\) is constant and within the ROC. This formula is powerful but computationally intensive. We will focus on a more practical approach for rational functions."
  },
  {
    "objectID": "ss_93.html#practical-inversion-partial-fraction-expansion-pfe",
    "href": "ss_93.html#practical-inversion-partial-fraction-expansion-pfe",
    "title": "Signals and Systems",
    "section": "Practical Inversion: Partial Fraction Expansion (PFE)",
    "text": "Practical Inversion: Partial Fraction Expansion (PFE)\nFor rational Laplace Transforms, \\(X(s) = P(s)/Q(s)\\), we typically use Partial Fraction Expansion (PFE). This method avoids direct evaluation of the complex integral.\nThe procedure involves:\n\nFactorizing the denominator polynomial \\(Q(s)\\).\nExpanding \\(X(s)\\) into a sum of simpler terms (e.g., first-order terms).\nIdentifying the inverse Laplace transform for each term, considering its Region of Convergence (ROC).\n\nFor distinct poles, \\(X(s)\\) can be expanded as:\n\\[\nX(s)=\\sum_{i=1}^{m} \\frac{A_{i}}{s+a_{i}} \\tag{9.57}\n\\]\n\n\n\n\n\n\nImportant\n\n\nThe ROC of the original \\(X(s)\\) is vital for correctly determining the inverse transform of each partial fraction term!"
  },
  {
    "objectID": "ss_93.html#inverse-transform-of-basic-terms",
    "href": "ss_93.html#inverse-transform-of-basic-terms",
    "title": "Signals and Systems",
    "section": "Inverse Transform of Basic Terms",
    "text": "Inverse Transform of Basic Terms\nThe inverse Laplace transform of a first-order term \\(\\frac{1}{s+a}\\) depends entirely on its ROC.\n\n\nCase 1: Right-Sided Signal\nIf the ROC is to the right of the pole \\(s=-a\\) (\\(\\operatorname{Re}\\{s\\} &gt; -a\\)):\n\\[\n\\frac{1}{s+a} \\stackrel{\\mathcal{L}^{-1}}{\\longleftrightarrow} e^{-at}u(t)\n\\]\nThis corresponds to a causal or right-sided exponential.\n\nCase 2: Left-Sided Signal\nIf the ROC is to the left of the pole \\(s=-a\\) (\\(\\operatorname{Re}\\{s\\} &lt; -a\\)):\n\\[\n\\frac{1}{s+a} \\stackrel{\\mathcal{L}^{-1}}{\\longleftrightarrow} -e^{-at}u(-t)\n\\]\nThis corresponds to an anti-causal or left-sided exponential.\n\n\n\n\n\n\n\nTip\n\n\nRemember: The ROC for a right-sided signal is always to the right of its rightmost pole. The ROC for a left-sided signal is always to the left of its leftmost pole. For a two-sided signal, the ROC is a strip between poles.\n\n\n\n\nVisually differentiate between the two cases. Emphasize the sign change for the left-sided signal and the role of \\(u(t)\\) and \\(u(-t)\\). This is a fundamental concept for inverse Laplace transforms."
  },
  {
    "objectID": "ss_93.html#example-9.9-right-sided-signal",
    "href": "ss_93.html#example-9.9-right-sided-signal",
    "title": "Signals and Systems",
    "section": "Example 9.9: Right-Sided Signal",
    "text": "Example 9.9: Right-Sided Signal\nLet’s find the inverse Laplace transform of:\n\\[\nX(s)=\\frac{1}{(s+1)(s+2)}, \\quad \\operatorname{Re}\\{s\\}&gt;-1 \\tag{9.58}\n\\]\n\nPartial-Fraction Expansion: \\(X(s)=\\frac{A}{s+1}+\\frac{B}{s+2}\\)\nUsing the cover-up method: \\(A = \\left.[(s+1)X(s)]\\right|_{s=-1} = \\frac{1}{-1+2} = 1\\) \\(B = \\left.[(s+2)X(s)]\\right|_{s=-2} = \\frac{1}{-2+1} = -1\\)\nSo, \\(X(s)=\\frac{1}{s+1}-\\frac{1}{s+2} \\tag{9.62}\\)\nApply ROC to terms: The overall ROC is \\(\\operatorname{Re}\\{s\\}&gt;-1\\). This is to the right of both poles (\\(s=-1\\) and \\(s=-2\\)). Therefore, both terms correspond to right-sided signals.\n\nFor \\(\\frac{1}{s+1}\\): ROC is \\(\\operatorname{Re}\\{s\\}&gt;-1 \\implies e^{-t}u(t)\\)\nFor \\(\\frac{1}{s+2}\\): ROC is \\(\\operatorname{Re}\\{s\\}&gt;-2 \\implies -e^{-2t}u(t)\\)"
  },
  {
    "objectID": "ss_93.html#example-9.9-pole-zero-plot-and-roc",
    "href": "ss_93.html#example-9.9-pole-zero-plot-and-roc",
    "title": "Signals and Systems",
    "section": "Example 9.9: Pole-Zero Plot and ROC",
    "text": "Example 9.9: Pole-Zero Plot and ROC\nThe pole-zero plot for \\(X(s)=\\frac{1}{(s+1)(s+2)}\\) with ROC \\(\\operatorname{Re}\\{s\\}&gt;-1\\):\nInterpretation of ROC:\nThe overall ROC, \\(\\operatorname{Re}\\{s\\}&gt;-1\\), is to the right of both poles.\n\nFor the term \\(\\frac{1}{s+1}\\): its ROC is \\(\\operatorname{Re}\\{s\\}&gt;-1\\). This implies a right-sided signal: \\(e^{-t}u(t)\\).\nFor the term \\(\\frac{1}{s+2}\\): its ROC is \\(\\operatorname{Re}\\{s\\}&gt;-1\\). Since this is to the right of its pole at \\(-2\\), this also implies a right-sided signal: \\(-e^{-2t}u(t)\\).\n\nThus, both terms contribute right-sided signals.\n\nExplain that for the individual terms, their ROCs are determined by the overall ROC. If the overall ROC is to the right of all poles, then each term’s ROC must also be to the right of its respective pole."
  },
  {
    "objectID": "ss_93.html#example-9.10-left-sided-signal",
    "href": "ss_93.html#example-9.10-left-sided-signal",
    "title": "Signals and Systems",
    "section": "Example 9.10: Left-Sided Signal",
    "text": "Example 9.10: Left-Sided Signal\nConsider the same \\(X(s)\\), but with a different ROC:\n\\[\nX(s)=\\frac{1}{(s+1)(s+2)}, \\quad \\operatorname{Re}\\{s\\}&lt;-2\n\\]\n\nPartial-Fraction Expansion: Still \\(X(s)=\\frac{1}{s+1}-\\frac{1}{s+2}\\)\nApply ROC to terms: The overall ROC is \\(\\operatorname{Re}\\{s\\}&lt;-2\\). This is to the left of both poles (\\(s=-1\\) and \\(s=-2\\)). Therefore, both terms correspond to left-sided signals.\n\nFor \\(\\frac{1}{s+1}\\): The ROC \\(\\operatorname{Re}\\{s\\}&lt;-1\\) (consistent with overall ROC) implies \\(-e^{-t}u(-t)\\).\nFor \\(\\frac{1}{s+2}\\): The ROC \\(\\operatorname{Re}\\{s\\}&lt;-2\\) implies \\(-(-e^{-2t}u(-t)) = e^{-2t}u(-t)\\) (due to the negative sign in PFE).\n\nCombine the terms:\n\\[\nx(t) = [-e^{-t} + e^{-2t}]u(-t) \\tag{9.68}\n\\]\n\n\nEmphasize how the change in ROC drastically changes the time-domain signal. Point out the sign manipulation for the second term due to the PFE and the left-sided rule."
  },
  {
    "objectID": "ss_93.html#example-9.11-two-sided-signal",
    "href": "ss_93.html#example-9.11-two-sided-signal",
    "title": "Signals and Systems",
    "section": "Example 9.11: Two-Sided Signal",
    "text": "Example 9.11: Two-Sided Signal\nOnce again, the same \\(X(s)\\), but with a ROC between the poles:\n\\[\nX(s)=\\frac{1}{(s+1)(s+2)}, \\quad -2 &lt; \\operatorname{Re}\\{s\\} &lt; -1\n\\]\n\nPartial-Fraction Expansion: Still \\(X(s)=\\frac{1}{s+1}-\\frac{1}{s+2}\\)\nApply ROC to terms: The overall ROC is \\(-2 &lt; \\operatorname{Re}\\{s\\} &lt; -1\\).\n\nFor the pole at \\(s=-1\\): The ROC \\(\\operatorname{Re}\\{s\\} &lt; -1\\) (consistent with overall ROC) means it’s to the left of this pole. \\(\\implies \\frac{1}{s+1} \\longleftrightarrow -e^{-t}u(-t)\\)\nFor the pole at \\(s=-2\\): The ROC \\(\\operatorname{Re}\\{s\\} &gt; -2\\) (consistent with overall ROC) means it’s to the right of this pole. \\(\\implies \\frac{1}{s+2} \\longleftrightarrow e^{-2t}u(t)\\)\n\nCombine the terms:\n\\[\nx(t) = -e^{-t}u(-t) - e^{-2t}u(t) \\tag{9.69}\n\\]\n\n\nThis example shows a two-sided signal, which results when the ROC lies between poles. Explain how each term’s ROC is determined by its relation to the overall ROC and its own pole location."
  },
  {
    "objectID": "ss_93.html#interactive-demo-partial-fraction-coefficients",
    "href": "ss_93.html#interactive-demo-partial-fraction-coefficients",
    "title": "Signals and Systems",
    "section": "Interactive Demo: Partial Fraction Coefficients",
    "text": "Interactive Demo: Partial Fraction Coefficients\nLet’s calculate the coefficients for a simple rational function: \\(X(s) = \\frac{1}{(s+a)(s+b)} = \\frac{A}{s+a} + \\frac{B}{s+b}\\)\nInput values for \\(a\\) and \\(b\\) (assume \\(a \\neq b\\)).\n\nviewof pole_a = Inputs.number({value: 1, min: -5, max: 5, step: 0.1, label: \"Pole a (for s+a)\"});\nviewof pole_b = Inputs.number({value: 2, min: -5, max: 5, step: 0.1, label: \"Pole b (for s+b)\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplain the cover-up method for finding A and B. Encourage students to try different values and observe the coefficients. This provides immediate feedback and reinforces the PFE calculation. Point out the potential for errors if poles are not distinct."
  },
  {
    "objectID": "ss_93.html#interactive-demo-visualizing-inverse-laplace-transforms",
    "href": "ss_93.html#interactive-demo-visualizing-inverse-laplace-transforms",
    "title": "Signals and Systems",
    "section": "Interactive Demo: Visualizing Inverse Laplace Transforms",
    "text": "Interactive Demo: Visualizing Inverse Laplace Transforms\nExplore how ROC changes the time-domain signal. For \\(X(s) = \\frac{1}{(s+1)(s+2)}\\), the PFE is \\(\\frac{1}{s+1} - \\frac{1}{s+2}\\).\nAdjust the ROC boundaries below and observe the resulting \\(x(t)\\).\n\nviewof sigma_low = Inputs.number({value: -2.5, min: -3.5, max: 0.5, step: 0.1, label: \"ROC Lower Bound (e.g., -inf = -3.5)\"});\nviewof sigma_high = Inputs.number({value: -0.5, min: -3.5, max: 0.5, step: 0.1, label: \"ROC Upper Bound (e.g., +inf = 0.5)\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nPoles at: \\(s_1 = -1\\), \\(s_2 = -2\\). - Try setting ROC: (-0.5 &lt; Re{s}) for Example 9.9 (Right-sided). - Try setting ROC: (Re{s} &lt; -2.5) for Example 9.10 (Left-sided). - Try setting ROC: (-2.0 &lt; Re{s} &lt; -1.0) for Example 9.11 (Two-sided).\n\n\n\n\n\n\n\n\n\n\nExplain the logic in the Python code: how sigma_low and sigma_high determine if each pole’s contribution is right-sided or left-sided. Encourage students to set the sliders to match the ROCs from Examples 9.9, 9.10, and 9.11 to see the corresponding signals. This is a powerful visualization of the ROC’s importance. Mention that the sigma_low and sigma_high inputs simulate the boundaries of the ROC. For an “infinite” ROC, students should set it to the max/min of the slider range."
  },
  {
    "objectID": "ss_93.html#practical-inversion-partial-fraction-expansion-pfe-1",
    "href": "ss_93.html#practical-inversion-partial-fraction-expansion-pfe-1",
    "title": "Signals and Systems",
    "section": "Practical Inversion: Partial Fraction Expansion (PFE)",
    "text": "Practical Inversion: Partial Fraction Expansion (PFE)\nExample Structure:\nGiven \\(X(s) = \\frac{s+3}{(s+1)(s+2)}\\)\n\nFactorized (already done)\nPFE: \\(X(s) = \\frac{A}{s+1} + \\frac{B}{s+2}\\)\nSolve for \\(A\\) and \\(B\\). \\(A = \\left.[(s+1)X(s)]\\right|_{s=-1}\\) \\(B = \\left.[(s+2)X(s)]\\right|_{s=-2}\\)\nThen, based on ROC, find \\(x(t)\\).\n\n\nHighlight that PFE simplifies the complex \\(X(s)\\) into a sum of basic forms whose inverse transforms are known. The key challenge for students is understanding how the overall ROC applies to each individual term to determine its time-domain characteristic (right-sided, left-sided)."
  },
  {
    "objectID": "ss_93.html#example-9.9-right-sided-signal-cont.",
    "href": "ss_93.html#example-9.9-right-sided-signal-cont.",
    "title": "Signals and Systems",
    "section": "Example 9.9: Right-Sided Signal (cont.)",
    "text": "Example 9.9: Right-Sided Signal (cont.)\n\nCombine the terms:\n\\[\nx(t) = (e^{-t} - e^{-2t})u(t) \\tag{9.65}\n\\]\n\n\nWalk through each step clearly. Explain why both terms are right-sided based on the overall ROC extending to the right of all poles."
  },
  {
    "objectID": "ss_95.html#the-inverse-laplace-transform",
    "href": "ss_95.html#the-inverse-laplace-transform",
    "title": "Signals and Systems",
    "section": "The Inverse Laplace Transform",
    "text": "The Inverse Laplace Transform\nRecovering Signals from the s-Domain"
  },
  {
    "objectID": "ss_95.html#recap-from-time-to-s-domain",
    "href": "ss_95.html#recap-from-time-to-s-domain",
    "title": "Signals and Systems",
    "section": "Recap: From Time to s-Domain",
    "text": "Recap: From Time to s-Domain\nThe Laplace Transform \\(X(s)\\) of a signal \\(x(t)\\) is defined as:\n\\[\nX(s) = \\int_{-\\infty}^{+\\infty} x(t) e^{-st} dt\n\\]\nWhere \\(s = \\sigma + j\\omega\\). This means \\(X(s)\\) can be interpreted as the Fourier Transform of an exponentially weighted signal:\n\\[\nX(\\sigma+j \\omega)=\\mathcal{F}\\left\\{x(t) e^{-\\sigma t}\\right\\}=\\int_{-\\infty}^{+\\infty} x(t) e^{-\\sigma t} e^{-j \\omega t} d t \\tag{9.53}\n\\]\n\n\n\n\n\n\nNote\n\n\nThe Region of Convergence (ROC) for \\(X(s)\\) is crucial. It defines the range of \\(\\sigma\\) values for which the integral converges.\n\n\n\n\nRemind students that the Laplace transform extends the Fourier transform by allowing for exponentially growing or decaying signals. The real part of ‘s’, sigma, acts as a damping factor, ensuring the integral converges. Emphasize that knowing just \\(X(s)\\) algebraically is not enough; the ROC is equally important for defining the transform."
  },
  {
    "objectID": "ss_95.html#the-inverse-laplace-transform-formula",
    "href": "ss_95.html#the-inverse-laplace-transform-formula",
    "title": "Signals and Systems",
    "section": "The Inverse Laplace Transform Formula",
    "text": "The Inverse Laplace Transform Formula\nWe can recover \\(x(t)\\) from \\(X(s)\\) using the inverse Fourier transform relationship:\n\\[\nx(t) e^{-\\sigma t}=\\mathcal{F}^{-1}\\{X(\\sigma+j \\omega)\\}=\\frac{1}{2 \\pi} \\int_{-\\infty}^{+\\infty} X(\\sigma+j \\omega) e^{j \\omega t} d \\omega \\tag{9.54}\n\\]\nMultiplying by \\(e^{\\sigma t}\\), we get:\n\\[\nx(t)=\\frac{1}{2 \\pi} \\int_{-\\infty}^{+\\infty} X(\\sigma+j \\omega) e^{(\\sigma+j \\omega) t} d \\omega \\tag{9.55}\n\\]\nBy changing the variable of integration from \\(\\omega\\) to \\(s\\) (\\(ds = j d\\omega\\) since \\(\\sigma\\) is constant), we obtain the fundamental inverse Laplace transform equation:\n\\[\nx(t)=\\frac{1}{2 \\pi j} \\int_{\\sigma-j \\infty}^{\\sigma+j \\infty} X(s) e^{s t} d s \\tag{9.56}\n\\]\n\n\n\n\n\n\nWarning\n\n\nThe integral in Eq. (9.56) is a contour integral in the complex \\(s\\)-plane. For most ECE undergraduate courses, direct evaluation is often complex and solved using alternative methods.\n\n\n\n\nExplain that the integration path is a vertical line in the s-plane, where the real part \\(\\sigma\\) is constant and within the ROC. This formula is powerful but computationally intensive. We will focus on a more practical approach for rational functions."
  },
  {
    "objectID": "ss_95.html#practical-inversion-partial-fraction-expansion-pfe",
    "href": "ss_95.html#practical-inversion-partial-fraction-expansion-pfe",
    "title": "Signals and Systems",
    "section": "Practical Inversion: Partial Fraction Expansion (PFE)",
    "text": "Practical Inversion: Partial Fraction Expansion (PFE)\nFor rational Laplace Transforms, \\(X(s) = P(s)/Q(s)\\), we typically use Partial Fraction Expansion (PFE). This method avoids direct evaluation of the complex integral.\n\n\nThe procedure involves:\n\nFactorizing the denominator polynomial \\(Q(s)\\).\nExpanding \\(X(s)\\) into a sum of simpler terms (e.g., first-order terms).\nIdentifying the inverse Laplace transform for each term, considering its Region of Convergence (ROC).\n\nFor distinct poles, \\(X(s)\\) can be expanded as:\n\\[\nX(s)=\\sum_{i=1}^{m} \\frac{A_{i}}{s+a_{i}} \\tag{9.57}\n\\]\n\n\n\n\n\n\nImportant\n\n\nThe ROC of the original \\(X(s)\\) is vital for correctly determining the inverse transform of each partial fraction term!\n\n\n\n\nExample Structure:\nGiven \\(X(s) = \\frac{s+3}{(s+1)(s+2)}\\)\n\nFactorized (already done)\nPFE: \\(X(s) = \\frac{A}{s+1} + \\frac{B}{s+2}\\)\nSolve for \\(A\\) and \\(B\\). \\(A = \\left.[(s+1)X(s)]\\right|_{s=-1}\\) \\(B = \\left.[(s+2)X(s)]\\right|_{s=-2}\\)\nThen, based on ROC, find \\(x(t)\\).\n\n\n\nHighlight that PFE simplifies the complex \\(X(s)\\) into a sum of basic forms whose inverse transforms are known. The key challenge for students is understanding how the overall ROC applies to each individual term to determine its time-domain characteristic (right-sided, left-sided)."
  },
  {
    "objectID": "ss_95.html#inverse-transform-of-basic-terms",
    "href": "ss_95.html#inverse-transform-of-basic-terms",
    "title": "Signals and Systems",
    "section": "Inverse Transform of Basic Terms",
    "text": "Inverse Transform of Basic Terms\nThe inverse Laplace transform of a first-order term \\(\\frac{1}{s+a}\\) depends entirely on its ROC.\n\n\nCase 1: Right-Sided Signal\nIf the ROC is to the right of the pole \\(s=-a\\) (\\(\\operatorname{Re}\\{s\\} &gt; -a\\)):\n\\[\n\\frac{1}{s+a} \\stackrel{\\mathcal{L}^{-1}}{\\longleftrightarrow} e^{-at}u(t)\n\\]\nThis corresponds to a causal or right-sided exponential.\n\nCase 2: Left-Sided Signal\nIf the ROC is to the left of the pole \\(s=-a\\) (\\(\\operatorname{Re}\\{s\\} &lt; -a\\)):\n\\[\n\\frac{1}{s+a} \\stackrel{\\mathcal{L}^{-1}}{\\longleftrightarrow} -e^{-at}u(-t)\n\\]\nThis corresponds to an anti-causal or left-sided exponential.\n\n\n\n\n\n\n\nTip\n\n\nRemember: The ROC for a right-sided signal is always to the right of its rightmost pole. The ROC for a left-sided signal is always to the left of its leftmost pole. For a two-sided signal, the ROC is a strip between poles.\n\n\n\n\nVisually differentiate between the two cases. Emphasize the sign change for the left-sided signal and the role of \\(u(t)\\) and \\(u(-t)\\). This is a fundamental concept for inverse Laplace transforms."
  },
  {
    "objectID": "ss_95.html#example-9.9-right-sided-signal",
    "href": "ss_95.html#example-9.9-right-sided-signal",
    "title": "Signals and Systems",
    "section": "Example 9.9: Right-Sided Signal",
    "text": "Example 9.9: Right-Sided Signal\nLet’s find the inverse Laplace transform of:\n\\[\nX(s)=\\frac{1}{(s+1)(s+2)}, \\quad \\operatorname{Re}\\{s\\}&gt;-1 \\tag{9.58}\n\\]\n\nPartial-Fraction Expansion: \\(X(s)=\\frac{A}{s+1}+\\frac{B}{s+2}\\)\nUsing the cover-up method: \\(A = \\left.[(s+1)X(s)]\\right|_{s=-1} = \\frac{1}{-1+2} = 1\\) \\(B = \\left.[(s+2)X(s)]\\right|_{s=-2} = \\frac{1}{-2+1} = -1\\)\nSo, \\(X(s)=\\frac{1}{s+1}-\\frac{1}{s+2} \\tag{9.62}\\)\nApply ROC to terms: The overall ROC is \\(\\operatorname{Re}\\{s\\}&gt;-1\\). This is to the right of both poles (\\(s=-1\\) and \\(s=-2\\)). Therefore, both terms correspond to right-sided signals.\n\nFor \\(\\frac{1}{s+1}\\): ROC is \\(\\operatorname{Re}\\{s\\}&gt;-1 \\implies e^{-t}u(t)\\)\nFor \\(\\frac{1}{s+2}\\): ROC is \\(\\operatorname{Re}\\{s\\}&gt;-2 \\implies -e^{-2t}u(t)\\)\n\nCombine the terms:\n\\[\nx(t) = (e^{-t} - e^{-2t})u(t) \\tag{9.65}\n\\]\n\n\nWalk through each step clearly. Explain why both terms are right-sided based on the overall ROC extending to the right of all poles."
  },
  {
    "objectID": "ss_95.html#example-9.9-pole-zero-plot-and-roc",
    "href": "ss_95.html#example-9.9-pole-zero-plot-and-roc",
    "title": "Signals and Systems",
    "section": "Example 9.9: Pole-Zero Plot and ROC",
    "text": "Example 9.9: Pole-Zero Plot and ROC\nThe pole-zero plot for \\(X(s)=\\frac{1}{(s+1)(s+2)}\\) with ROC \\(\\operatorname{Re}\\{s\\}&gt;-1\\):\n\n\n\n\n\n\n\ngraph TD\n    subgraph S-Plane (Poles and ROC)\n        direction LR\n        Jomega[\"$$j\\omega$$ Axis\"] --.-&gt; Sigma[\"$$\\sigma$$ Axis\"]\n      \n        Pole_at_minus_2[\"X ($$-2$$)\"]\n        Pole_at_minus_1[\"X ($$-1$$)\"]\n      \n        Sigma -- ROC_Boundary(\"$$\\sigma = -1$$\")\n      \n        Pole_at_minus_2 --.-&gt; Pole_at_minus_1\n        Pole_at_minus_1 --.-&gt; ROC_Boundary\n      \n        ROC_Boundary -- \"ROC extends to the right\" --&gt; Right_Region[$$Re\\{s\\} &gt; -1$$]\n      \n        style Pole_at_minus_2 fill:#F00,stroke:#333,stroke-width:2px;\n        style Pole_at_minus_1 fill:#F00,stroke:#333,stroke-width:2px;\n        style ROC_Boundary stroke:#00F,stroke-width:2px,stroke-dasharray: 5 5;\n        style Right_Region fill:#E0E0FF,stroke:#E0E0FF;\n    end\n\n\n\n\n\n\n\nInterpretation of ROC:\nThe overall ROC, \\(\\operatorname{Re}\\{s\\}&gt;-1\\), is to the right of both poles.\n\nFor the term \\(\\frac{1}{s+1}\\): its ROC is \\(\\operatorname{Re}\\{s\\}&gt;-1\\). This implies a right-sided signal: \\(e^{-t}u(t)\\).\nFor the term \\(\\frac{1}{s+2}\\): its ROC is \\(\\operatorname{Re}\\{s\\}&gt;-2\\). Since this is to the right of its pole at \\(-2\\), this also implies a right-sided signal: \\(-e^{-2t}u(t)\\).\n\nThus, both terms contribute right-sided signals.\n\n\nExplain that for the individual terms, their ROCs are determined by the overall ROC. If the overall ROC is to the right of all poles, then each term’s ROC must also be to the right of its respective pole."
  },
  {
    "objectID": "ss_95.html#example-9.10-left-sided-signal",
    "href": "ss_95.html#example-9.10-left-sided-signal",
    "title": "Signals and Systems",
    "section": "Example 9.10: Left-Sided Signal",
    "text": "Example 9.10: Left-Sided Signal\nConsider the same \\(X(s)\\), but with a different ROC:\n\\[\nX(s)=\\frac{1}{(s+1)(s+2)}, \\quad \\operatorname{Re}\\{s\\}&lt;-2\n\\]\n\nPartial-Fraction Expansion: Still \\(X(s)=\\frac{1}{s+1}-\\frac{1}{s+2}\\)\nApply ROC to terms: The overall ROC is \\(\\operatorname{Re}\\{s\\}&lt;-2\\). This is to the left of both poles (\\(s=-1\\) and \\(s=-2\\)). Therefore, both terms correspond to left-sided signals.\n\nFor \\(\\frac{1}{s+1}\\): The ROC \\(\\operatorname{Re}\\{s\\}&lt;-1\\) (consistent with overall ROC) implies \\(-e^{-t}u(-t)\\).\nFor \\(\\frac{1}{s+2}\\): The ROC \\(\\operatorname{Re}\\{s\\}&lt;-2\\) implies \\(-(-e^{-2t}u(-t)) = e^{-2t}u(-t)\\) (due to the negative sign in PFE).\n\nCombine the terms:\n\\[\nx(t) = [-e^{-t} + e^{-2t}]u(-t) \\tag{9.68}\n\\]\n\n\nEmphasize how the change in ROC drastically changes the time-domain signal. Point out the sign manipulation for the second term due to the PFE and the left-sided rule."
  },
  {
    "objectID": "ss_95.html#example-9.11-two-sided-signal",
    "href": "ss_95.html#example-9.11-two-sided-signal",
    "title": "Signals and Systems",
    "section": "Example 9.11: Two-Sided Signal",
    "text": "Example 9.11: Two-Sided Signal\nOnce again, the same \\(X(s)\\), but with a ROC between the poles:\n\\[\nX(s)=\\frac{1}{(s+1)(s+2)}, \\quad -2 &lt; \\operatorname{Re}\\{s\\} &lt; -1\n\\]\n\nPartial-Fraction Expansion: Still \\(X(s)=\\frac{1}{s+1}-\\frac{1}{s+2}\\)\nApply ROC to terms: The overall ROC is \\(-2 &lt; \\operatorname{Re}\\{s\\} &lt; -1\\).\n\nFor the pole at \\(s=-1\\): The ROC \\(\\operatorname{Re}\\{s\\} &lt; -1\\) (consistent with overall ROC) means it’s to the left of this pole. \\(\\implies \\frac{1}{s+1} \\longleftrightarrow -e^{-t}u(-t)\\)\nFor the pole at \\(s=-2\\): The ROC \\(\\operatorname{Re}\\{s\\} &gt; -2\\) (consistent with overall ROC) means it’s to the right of this pole. \\(\\implies \\frac{1}{s+2} \\longleftrightarrow e^{-2t}u(t)\\)\n\nCombine the terms:\n\\[\nx(t) = -e^{-t}u(-t) - e^{-2t}u(t) \\tag{9.69}\n\\]\n\n\nThis example shows a two-sided signal, which results when the ROC lies between poles. Explain how each term’s ROC is determined by its relation to the overall ROC and its own pole location."
  },
  {
    "objectID": "ss_95.html#interactive-demo-partial-fraction-coefficients",
    "href": "ss_95.html#interactive-demo-partial-fraction-coefficients",
    "title": "Signals and Systems",
    "section": "Interactive Demo: Partial Fraction Coefficients",
    "text": "Interactive Demo: Partial Fraction Coefficients\nLet’s calculate the coefficients for a simple rational function: \\(X(s) = \\frac{1}{(s+a)(s+b)} = \\frac{A}{s+a} + \\frac{B}{s+b}\\)\nInput values for \\(a\\) and \\(b\\) (assume \\(a \\neq b\\)).\n\nviewof pfe_pole_a = Inputs.number({value: 1, min: -5, max: 5, step: 0.1, label: \"Pole a (for s+a)\"});\nviewof pfe_pole_b = Inputs.number({value: 2, min: -5, max: 5, step: 0.1, label: \"Pole b (for s+b)\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplain the cover-up method for finding A and B. Encourage students to try different values and observe the coefficients. This provides immediate feedback and reinforces the PFE calculation. Point out the potential for errors if poles are not distinct."
  },
  {
    "objectID": "ss_95.html#interactive-demo-visualizing-inverse-laplace-transforms",
    "href": "ss_95.html#interactive-demo-visualizing-inverse-laplace-transforms",
    "title": "Signals and Systems",
    "section": "Interactive Demo: Visualizing Inverse Laplace Transforms",
    "text": "Interactive Demo: Visualizing Inverse Laplace Transforms\nExplore how ROC changes the time-domain signal. For \\(X(s) = \\frac{1}{(s+1)(s+2)}\\), the PFE is \\(\\frac{1}{s+1} - \\frac{1}{s+2}\\).\nAdjust the ROC boundaries below and observe the resulting \\(x(t)\\).\n\nviewof sigma_low = Inputs.number({value: -2.5, min: -3.5, max: 0.5, step: 0.1, label: \"ROC Lower Bound (e.g., -inf = -3.5)\"});\nviewof sigma_high = Inputs.number({value: -0.5, min: -3.5, max: 0.5, step: 0.1, label: \"ROC Upper Bound (e.g., +inf = 0.5)\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nPoles at: \\(s_1 = -1\\), \\(s_2 = -2\\). - Try setting ROC: (-0.5 &lt; Re{s}) for Example 9.9 (Right-sided). - Try setting ROC: (Re{s} &lt; -2.5) for Example 9.10 (Left-sided). - Try setting ROC: (-2.0 &lt; Re{s} &lt; -1.0) for Example 9.11 (Two-sided).\n\n\n\n\n\n\n\n\n\n\nExplain the logic in the Python code: how sigma_low and sigma_high determine if each pole’s contribution is right-sided or left-sided. Encourage students to set the sliders to match the ROCs from Examples 9.9, 9.10, and 9.11 to see the corresponding signals. This is a powerful visualization of the ROC’s importance. Mention that the sigma_low and sigma_high inputs simulate the boundaries of the ROC. For an “infinite” ROC, students should set it to the max/min of the slider range."
  },
  {
    "objectID": "ss_95.html#leveraging-transform-domain-relationships",
    "href": "ss_95.html#leveraging-transform-domain-relationships",
    "title": "Signals and Systems",
    "section": "Leveraging Transform Domain Relationships",
    "text": "Leveraging Transform Domain Relationships\nJust like with the Fourier Transform, the Laplace Transform has a rich set of properties that simplify analysis and provide deeper insights. These properties are invaluable for:\n\nDeriving new transform pairs.\nAnalyzing LTI systems.\nSolving differential equations.\nChecking calculations.\n\nWe will explore key properties and their implications on both the signal and its ROC.\n\nIntroduce the section by highlighting the utility of properties in Signals and Systems, drawing a parallel to Fourier Transform properties. Emphasize that understanding these properties is crucial for practical applications."
  },
  {
    "objectID": "ss_95.html#linearity-of-the-laplace-transform",
    "href": "ss_95.html#linearity-of-the-laplace-transform",
    "title": "Signals and Systems",
    "section": "Linearity of the Laplace Transform",
    "text": "Linearity of the Laplace Transform\nIf \\(x_1(t) \\stackrel{\\mathcal{L}}{\\longleftrightarrow} X_1(s)\\) with ROC \\(R_1\\), and \\(x_2(t) \\stackrel{\\mathcal{L}}{\\longleftrightarrow} X_2(s)\\) with ROC \\(R_2\\),\nThen, for constants \\(a, b\\):\n\\[\na x_{1}(t)+b x_{2}(t) \\stackrel{\\mathcal{L}}{\\longleftrightarrow} a X_{1}(s)+b X_{2}(s) \\tag{9.82}\n\\]\nROC: At least contains \\(R_1 \\cap R_2\\). The ROC can sometimes be larger than the intersection, especially due to pole-zero cancellation.\n\n\n\n\n\n\nTip\n\n\nIf \\(R_1 \\cap R_2\\) is empty, then the linear combination \\(ax_1(t) + bx_2(t)\\) generally does not have a Laplace Transform.\n\n\n\n\nExplain that linearity holds for both the transform and the inverse transform. The critical point here is the ROC; it’s at least the intersection, meaning it can expand. This expansion is usually due to cancellations."
  },
  {
    "objectID": "ss_95.html#example-9.13-linearity-and-roc-extension",
    "href": "ss_95.html#example-9.13-linearity-and-roc-extension",
    "title": "Signals and Systems",
    "section": "Example 9.13: Linearity and ROC Extension",
    "text": "Example 9.13: Linearity and ROC Extension\nConsider \\(x(t) = x_1(t) - x_2(t)\\) where:\n\\(X_1(s)=\\frac{1}{s+1}\\), with \\(\\operatorname{Re}\\{s\\}&gt;-1\\) (\\(R_1\\)) \\(X_2(s)=\\frac{1}{(s+1)(s+2)}\\), with \\(\\operatorname{Re}\\{s\\}&gt;-1\\) (\\(R_2\\))\nThe intersection \\(R_1 \\cap R_2\\) is \\(\\operatorname{Re}\\{s\\}&gt;-1\\).\nNow, \\(X(s) = X_1(s) - X_2(s)\\):\n\\[\nX(s)=\\frac{1}{s+1}-\\frac{1}{(s+1)(s+2)} = \\frac{(s+2)-1}{(s+1)(s+2)} = \\frac{s+1}{(s+1)(s+2)} = \\frac{1}{s+2} \\tag{9.86}\n\\]\nThe pole at \\(s=-1\\) is canceled by a zero at \\(s=-1\\).\n\n\n\n\n\n\nNote\n\n\nDue to the pole-zero cancellation, the ROC for \\(X(s)\\) extends to \\(\\operatorname{Re}\\{s\\} &gt; -2\\), which is larger than the intersection of \\(R_1\\) and \\(R_2\\).\n\n\n\n\nClearly show how the pole at -1 and zero at -1 cancel in the combined \\(X(s)\\). The Mermaid diagram visually represents the poles and ROCs for \\(X_1(s)\\), \\(X_2(s)\\), and the resulting \\(X(s)\\), highlighting the expanded ROC due to cancellation."
  },
  {
    "objectID": "ss_95.html#time-shifting",
    "href": "ss_95.html#time-shifting",
    "title": "Signals and Systems",
    "section": "Time Shifting",
    "text": "Time Shifting\nIf \\(x(t) \\stackrel{\\mathcal{L}}{\\longleftrightarrow} X(s)\\) with ROC \\(R\\), then:\n\\[\nx\\left(t-t_{0}\\right) \\stackrel{\\mathcal{L}}{\\longleftrightarrow} e^{-s t_{0}} X(s) \\tag{9.87}\n\\]\nROC: Remains \\(R\\).\n\nShifting a signal in the time domain corresponds to multiplying its Laplace Transform by a complex exponential \\(e^{-st_0}\\).\nThe ROC is unchanged because the convergence behavior of \\(x(t-t_0)\\) is the same as \\(x(t)\\).\n\n\nExplain that time shifting doesn’t change the exponential growth/decay rate, hence the ROC remains the same. This is a common property across Fourier and Laplace transforms."
  },
  {
    "objectID": "ss_95.html#shifting-in-the-s-domain",
    "href": "ss_95.html#shifting-in-the-s-domain",
    "title": "Signals and Systems",
    "section": "Shifting in the s-Domain",
    "text": "Shifting in the s-Domain\nIf \\(x(t) \\stackrel{\\mathcal{L}}{\\longleftrightarrow} X(s)\\) with ROC \\(R\\), then:\n\\[\ne^{s_{0} t} x(t) \\stackrel{\\mathcal{L}}{\\longleftrightarrow} X\\left(s-s_{0}\\right) \\tag{9.88}\n\\]\nROC: \\(R + \\operatorname{Re}\\{s_0\\}\\). The ROC of \\(X(s-s_0)\\) is the ROC of \\(X(s)\\) shifted by \\(\\operatorname{Re}\\{s_0\\}\\).\nSpecial Case: Modulation If \\(s_0 = j\\omega_0\\) (pure imaginary), then \\(e^{j\\omega_0 t}x(t) \\stackrel{\\mathcal{L}}{\\longleftrightarrow} X(s-j\\omega_0)\\). - This corresponds to a shift in the \\(s\\)-plane parallel to the \\(j\\omega\\)-axis. - The ROC is unchanged in terms of its real axis boundaries.\n\nIllustrate how multiplying by an exponential in the time domain shifts the entire s-plane representation. Emphasize the ROC shift, which is crucial. Explain the modulation special case and its significance in communication systems."
  },
  {
    "objectID": "ss_95.html#time-scaling",
    "href": "ss_95.html#time-scaling",
    "title": "Signals and Systems",
    "section": "Time Scaling",
    "text": "Time Scaling\nIf \\(x(t) \\stackrel{\\mathcal{L}}{\\longleftrightarrow} X(s)\\) with ROC \\(R\\), then:\n\\[\nx(a t) \\stackrel{\\mathcal{L}}{\\longleftrightarrow} \\frac{1}{|a|} X\\left(\\frac{s}{a}\\right) \\tag{9.90}\n\\]\nROC: \\(aR\\). The ROC is also scaled by \\(a\\).\n\nIf \\(a &gt; 0\\), the ROC is scaled. For \\(0 &lt; a &lt; 1\\), it’s compressed; for \\(a &gt; 1\\), it’s expanded.\nIf \\(a &lt; 0\\) (time reversal), the ROC undergoes a reversal about the \\(j\\omega\\)-axis as well as scaling.\n\nTime Reversal (\\(a=-1\\)):\n\\[\nx(-t) \\stackrel{\\mathcal{L}}{\\longleftrightarrow} X(-s), \\quad \\text { with } \\mathrm{ROC}=-R \\tag{9.91}\n\\]\n\nExplain that time compression/expansion affects the rate of change, thus scaling the s-domain representation and its ROC. Time reversal is a specific case that also flips the ROC."
  },
  {
    "objectID": "ss_95.html#conjugation",
    "href": "ss_95.html#conjugation",
    "title": "Signals and Systems",
    "section": "Conjugation",
    "text": "Conjugation\nIf \\(x(t) \\stackrel{\\mathcal{L}}{\\longleftrightarrow} X(s)\\) with ROC \\(R\\), then:\n\\[\nx^{*}(t) \\stackrel{\\mathcal{L}}{\\longleftrightarrow} X^{*}\\left(s^{*}\\right) \\tag{9.93}\n\\]\nROC: \\(R\\).\nImportant Consequence for Real Signals: If \\(x(t)\\) is a real signal, then \\(X(s) = X^*(s^*)\\). This implies that if \\(X(s)\\) has a pole or zero at \\(s_0\\), it must also have a pole or zero at the complex conjugate point \\(s_0^*\\).\n\nExplain the property and its direct implication for real-world signals. Most physical signals are real, so their Laplace transforms always exhibit conjugate symmetry for poles and zeros."
  },
  {
    "objectID": "ss_95.html#convolution-property",
    "href": "ss_95.html#convolution-property",
    "title": "Signals and Systems",
    "section": "Convolution Property",
    "text": "Convolution Property\nIf \\(x_1(t) \\stackrel{\\mathcal{L}}{\\longleftrightarrow} X_1(s)\\) with ROC \\(R_1\\), and \\(x_2(t) \\stackrel{\\mathcal{L}}{\\longleftrightarrow} X_2(s)\\) with ROC \\(R_2\\), then:\n\\[\nx_{1}(t) * x_{2}(t) \\stackrel{\\mathcal{L}}{\\longleftrightarrow} X_{1}(s) X_{2}(s) \\tag{9.95}\n\\]\nROC: Containing \\(R_1 \\cap R_2\\). Similar to linearity, the ROC can be larger if pole-zero cancellation occurs in the product \\(X_1(s)X_2(s)\\).\n\n\n\n\n\n\nImportant\n\n\nThis is a cornerstone for Linear Time-Invariant (LTI) system analysis. Convolution in the time domain becomes multiplication in the \\(s\\)-domain, greatly simplifying system response calculations.\n\n\n\n\nEmphasize the profound importance of this property for LTI systems. It transforms a complex convolution operation into a simple multiplication, which is why frequency/s-domain analysis is so powerful."
  },
  {
    "objectID": "ss_95.html#differentiation-in-the-time-domain",
    "href": "ss_95.html#differentiation-in-the-time-domain",
    "title": "Signals and Systems",
    "section": "Differentiation in the Time Domain",
    "text": "Differentiation in the Time Domain\nIf \\(x(t) \\stackrel{\\mathcal{L}}{\\longleftrightarrow} X(s)\\) with ROC \\(R\\), then:\n\\[\n\\frac{d x(t)}{d t} \\stackrel{\\mathcal{L}}{\\longleftrightarrow} s X(s) \\tag{9.98}\n\\]\nROC: Containing \\(R\\). The ROC can be larger if \\(X(s)\\) has a first-order pole at \\(s=0\\) that is canceled by the multiplication by \\(s\\).\nExample: - \\(u(t) \\stackrel{\\mathcal{L}}{\\longleftrightarrow} \\frac{1}{s}\\), \\(\\operatorname{Re}\\{s\\}&gt;0\\) - \\(\\frac{d}{dt}u(t) = \\delta(t) \\stackrel{\\mathcal{L}}{\\longleftrightarrow} s \\cdot \\frac{1}{s} = 1\\), ROC is the entire \\(s\\)-plane.\n\nExplain how differentiation in time translates to multiplication by \\(s\\) in the Laplace domain. Highlight the ROC behavior, especially the potential for expansion when a pole at the origin is canceled, as seen with the unit step and impulse."
  },
  {
    "objectID": "ss_95.html#differentiation-in-the-s-domain",
    "href": "ss_95.html#differentiation-in-the-s-domain",
    "title": "Signals and Systems",
    "section": "Differentiation in the s-Domain",
    "text": "Differentiation in the s-Domain\nIf \\(x(t) \\stackrel{\\mathcal{L}}{\\longleftrightarrow} X(s)\\) with ROC \\(R\\), then:\n\\[\n-t x(t) \\stackrel{\\mathcal{L}}{\\longleftrightarrow} \\frac{d X(s)}{d s} \\tag{9.100}\n\\]\nROC: \\(R\\).\nThis property is useful for finding transforms of signals multiplied by \\(t^n\\).\n\nExplain this less intuitive property: multiplication by time in the time domain corresponds to differentiation in the \\(s\\)-domain. This is particularly useful for signals like \\(t \\cdot e^{-at}u(t)\\)."
  },
  {
    "objectID": "ss_95.html#example-9.14-using-s-domain-differentiation",
    "href": "ss_95.html#example-9.14-using-s-domain-differentiation",
    "title": "Signals and Systems",
    "section": "Example 9.14: Using s-Domain Differentiation",
    "text": "Example 9.14: Using s-Domain Differentiation\nLet’s find the Laplace transform of \\(x(t)=t e^{-at} u(t)\\).\nWe know that \\(e^{-at} u(t) \\stackrel{\\mathcal{L}}{\\longleftrightarrow} \\frac{1}{s+a}\\), with \\(\\operatorname{Re}\\{s\\}&gt;-a\\).\nApplying the differentiation in the s-domain property:\n\\[\nt e^{-a t} u(t) \\stackrel{\\mathcal{L}}{\\longleftrightarrow} -\\frac{d}{d s}\\left[\\frac{1}{s+a}\\right] = - \\left( -\\frac{1}{(s+a)^2} \\right) = \\frac{1}{(s+a)^{2}} \\tag{9.102}\n\\]\nROC: \\(\\operatorname{Re}\\{s\\}&gt;-a\\).\nRepeated application yields:\n\\[\n\\frac{t^{n-1}}{(n-1) !} e^{-a t} u(t) \\stackrel{\\mathcal{L}}{\\longleftrightarrow} \\frac{1}{(s+a)^{n}} \\tag{9.104}\n\\]\n\nWalk through the derivation. Emphasize how a complex time-domain signal (multiplied by t) becomes a simpler derivative in the s-domain."
  },
  {
    "objectID": "ss_95.html#interactive-demo-t-e-at-ut",
    "href": "ss_95.html#interactive-demo-t-e-at-ut",
    "title": "Signals and Systems",
    "section": "Interactive Demo: \\(t e^{-at} u(t)\\)",
    "text": "Interactive Demo: \\(t e^{-at} u(t)\\)\nObserve the time-domain signal \\(x(t) = t e^{-at} u(t)\\) and its relation to \\(e^{-at} u(t)\\). Vary the parameter ‘a’ and see how the signals change.\n\nviewof param_a = Inputs.range([0.1, 5.0], {value: 1, step: 0.1, label: \"Parameter 'a'\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\nEncourage students to manipulate the ‘a’ parameter and observe the behavior of both signals. They should see how \\(t e^{-at} u(t)\\) initially grows before decaying, compared to the direct exponential decay. This visually reinforces the mathematical relationship."
  },
  {
    "objectID": "ss_95.html#example-9.15-inverse-transform-with-multiple-poles",
    "href": "ss_95.html#example-9.15-inverse-transform-with-multiple-poles",
    "title": "Signals and Systems",
    "section": "Example 9.15: Inverse Transform with Multiple Poles",
    "text": "Example 9.15: Inverse Transform with Multiple Poles\nConsider \\(X(s)=\\frac{2 s^{2}+5 s+5}{(s+1)^{2}(s+2)}\\), with \\(\\operatorname{Re}\\{s\\}&gt;-1\\).\nUsing partial-fraction expansion (PFE) for multiple-order poles (as discussed in the appendix or advanced PFE techniques):\n\\[\nX(s)=\\frac{2}{(s+1)^{2}}-\\frac{1}{(s+1)}+\\frac{3}{s+2} \\tag{9.105}\n\\]\nSince the ROC is \\(\\operatorname{Re}\\{s\\}&gt;-1\\) (to the right of all poles), all terms correspond to right-sided signals.\n\nFrom Eq. (9.104): \\(\\frac{2}{(s+1)^2} \\stackrel{\\mathcal{L}^{-1}}{\\longleftrightarrow} 2 t e^{-t} u(t)\\)\nFrom Example 9.1: \\(-\\frac{1}{s+1} \\stackrel{\\mathcal{L}^{-1}}{\\longleftrightarrow} -e^{-t} u(t)\\)\nFrom Example 9.1: \\(\\frac{3}{s+2} \\stackrel{\\mathcal{L}^{-1}}{\\longleftrightarrow} 3 e^{-2t} u(t)\\)\n\nCombining these, the inverse transform is:\n\\[\nx(t)=\\left[2 t e^{-t}-e^{-t}+3 e^{-2 t}\\right] u(t)\n\\]\n\nHighlight how the previously derived transform pair for \\(t e^{-at}u(t)\\) is directly applied here. This demonstrates the practical utility of understanding transform properties for solving more complex inverse Laplace transform problems."
  },
  {
    "objectID": "ss_95.html#integration-in-the-time-domain",
    "href": "ss_95.html#integration-in-the-time-domain",
    "title": "Signals and Systems",
    "section": "Integration in the Time Domain",
    "text": "Integration in the Time Domain\nIf \\(x(t) \\stackrel{\\mathcal{L}}{\\longleftrightarrow} X(s)\\) with ROC \\(R\\), then:\n\\[\n\\int_{-\\infty}^{t} x(\\tau) d \\tau \\stackrel{\\mathcal{L}}{\\longleftrightarrow} \\frac{1}{s} X(s) \\tag{9.106}\n\\]\nROC: Containing \\(R \\cap \\{\\operatorname{Re}\\{s\\}&gt;0\\}\\).\n\nThis property is the inverse of time-domain differentiation.\nIt can be derived using the convolution property, as \\(\\int_{-\\infty}^{t} x(\\tau) d \\tau = x(t) * u(t)\\).\nSince \\(u(t) \\stackrel{\\mathcal{L}}{\\longleftrightarrow} \\frac{1}{s}\\) with \\(\\operatorname{Re}\\{s\\}&gt;0\\), the ROC of the integral is the intersection of \\(R\\) and \\(\\operatorname{Re}\\{s\\}&gt;0\\).\n\n\nConnect this property to both differentiation and convolution. Explain why the ROC for integration includes the \\(\\operatorname{Re}\\{s\\}&gt;0\\) constraint from the unit step function."
  },
  {
    "objectID": "ss_95.html#initial-and-final-value-theorems",
    "href": "ss_95.html#initial-and-final-value-theorems",
    "title": "Signals and Systems",
    "section": "Initial and Final-Value Theorems",
    "text": "Initial and Final-Value Theorems\nThese theorems allow us to find the initial and final values of a time-domain signal directly from its Laplace Transform, under certain conditions.\nInitial-Value Theorem: (Conditions: \\(x(t)=0\\) for \\(t&lt;0\\); no impulses or higher-order singularities at \\(t=0\\))\n\\[\nx\\left(0^{+}\\right)=\\lim _{s \\rightarrow \\infty} s X(s) \\tag{9.110}\n\\]\nFinal-Value Theorem: (Conditions: \\(x(t)=0\\) for \\(t&lt;0\\); \\(x(t)\\) has a finite limit as \\(t \\rightarrow \\infty\\))\n\\[\n\\lim _{t \\rightarrow \\infty} x(t)=\\lim _{s \\rightarrow 0} s X(s) \\tag{9.111}\n\\]\n\n\n\n\n\n\nTip\n\n\nThese theorems are excellent tools for checking the correctness of Laplace transform calculations or the steady-state behavior of systems without explicitly finding \\(x(t)\\).\n\n\n\n\nClearly state the conditions for each theorem, as they are crucial for correct application. Emphasize their practical use in verifying results and understanding system behavior."
  },
  {
    "objectID": "ss_95.html#summary-of-laplace-transform-properties",
    "href": "ss_95.html#summary-of-laplace-transform-properties",
    "title": "Signals and Systems",
    "section": "Summary of Laplace Transform Properties",
    "text": "Summary of Laplace Transform Properties\n\n\n\n\n\n\n\n\n\nProperty\nSignal \\(x(t)\\)\nLaplace Transform \\(X(s)\\)\nROC\n\n\n\n\nLinearity\n\\(ax_1(t) + bx_2(t)\\)\n\\(aX_1(s) + bX_2(s)\\)\nAt least \\(R_1 \\cap R_2\\)\n\n\nTime Shifting\n\\(x(t-t_0)\\)\n\\(e^{-st_0}X(s)\\)\n\\(R\\)\n\n\nShifting in \\(s\\)-Domain\n\\(e^{s_0 t}x(t)\\)\n\\(X(s-s_0)\\)\n\\(R + \\operatorname{Re}\\{s_0\\}\\)\n\n\nTime Scaling\n\\(x(at)\\)\n\\(\\frac{1}{|a|}X(\\frac{s}{a})\\)\n\\(aR\\)\n\n\nConjugation\n\\(x^*(t)\\)\n\\(X^*(s^*)\\)\n\\(R\\)\n\n\nConvolution\n\\(x_1(t) * x_2(t)\\)\n\\(X_1(s)X_2(s)\\)\nAt least \\(R_1 \\cap R_2\\)\n\n\nDifferentiation (Time)\n\\(\\frac{d}{dt}x(t)\\)\n\\(sX(s)\\)\nAt least \\(R\\)\n\n\nDifferentiation (\\(s\\)-Domain)\n\\(-tx(t)\\)\n\\(\\frac{d}{ds}X(s)\\)\n\\(R\\)\n\n\nIntegration (Time)\n\\(\\int_{-\\infty}^{t} x(\\tau) d\\tau\\)\n\\(\\frac{1}{s}X(s)\\)\nAt least \\(R \\cap \\{\\operatorname{Re}\\{s\\}&gt;0\\}\\)\n\n\nInitial-Value Theorem\n\\(x(0^+)\\) (for \\(x(t)=0, t&lt;0\\))\n\\(\\lim_{s \\to \\infty} sX(s)\\)\nN/A\n\n\nFinal-Value Theorem\n\\(\\lim_{t \\to \\infty} x(t)\\) (for \\(x(t)=0, t&lt;0\\), finite limit)\n\\(\\lim_{s \\to 0} sX(s)\\)\nN/A\n\n\n\n\nReview the table quickly, highlighting the main takeaway for each property. Emphasize that these properties form the foundation for applying the Laplace Transform to complex engineering problems."
  },
  {
    "objectID": "ss_95.html#some-laplace-transform-pairs",
    "href": "ss_95.html#some-laplace-transform-pairs",
    "title": "Signals and Systems",
    "section": "Some Laplace Transform Pairs",
    "text": "Some Laplace Transform Pairs\nTABLE 9.2 LAPLACE TRANSFORMS OF ELEMENTARY FUNCTIONS\n\n\n\n\n\n\n\n\n\nTransform  pair\nSignal\nTransform\nROC\n\n\n\n\n1\n\\(\\delta(t)\\)\n1\nAll \\(s\\)\n\n\n2\n\\(u(t)\\)\n\\(\\frac{1}{s}\\)\n\\(\\operatorname{Re}\\{s\\}&gt;0\\)\n\n\n3\n\\(-u(-t)\\)\n\\(\\frac{1}{s}\\)\n\\(\\operatorname{Re}\\{s\\}&lt;0\\)\n\n\n4\n\\(\\frac{t^{n-1}}{(n-1) !} u(t)\\)\n\\(\\frac{1}{s^{n}}\\)\n\\(\\operatorname{Re}\\{s\\}&gt;0\\)\n\n\n5\n\\(-\\frac{t^{n-1}}{(n-1) !} u(-t)\\)\n\\(\\frac{1}{s^{n}}\\)\n\\(\\operatorname{Re}\\{s\\}&lt;0\\)\n\n\n6\n\\(e^{-\\alpha t} u(t)\\)\n\\(\\frac{1}{s+\\alpha}\\)\n\\(\\operatorname{Re}\\{s\\}&gt;-\\alpha\\)\n\n\n7\n\\(-e^{-\\alpha t} u(-t)\\)\n\\(\\frac{1}{s+\\alpha}\\)\n\\(\\operatorname{Re}\\{s\\}&lt;-\\alpha\\)\n\n\n8\n\\(\\frac{t^{n-1}}{(n-1) !} e^{-\\alpha t} u(t)\\)\n\\(\\frac{1}{(s+\\alpha)^{n}}\\)\n\\(\\operatorname{Re}\\{s\\}&gt;-\\alpha\\)\n\n\n9\n\\(-\\frac{t^{n-1}}{(n-1) !} e^{-\\alpha t} u(-t)\\)\n\\(\\frac{1}{(s+\\alpha)^{n}}\\)\n\\(\\operatorname{Re}\\{s\\}&lt;-\\alpha\\)\n\n\n10\n\\(\\delta(t-T)\\)\n\\(e^{-s T}\\)\nAll \\(s\\)\n\n\n11\n\\(\\left[\\cos \\omega_{0} t\\right] u(t)\\)\n\\(\\frac{s}{s^{2}+\\omega_{0}^{2}}\\)\n\\(\\operatorname{Re}\\{s\\}&gt;0\\)\n\n\n12\n\\(\\left[\\sin \\omega_{0} t\\right] u(t)\\)\n\\(\\frac{\\omega_{0}}{s^{2}+\\omega_{0}^{2}}\\)\n\\(\\operatorname{Re}\\{s\\}&gt;0\\)\n\n\n13\n\\(\\left[e^{-\\alpha t} \\cos \\omega_{0} t\\right] u(t)\\)\n\\(\\frac{s+\\alpha}{(s+\\alpha)^{2}+\\omega_{0}^{2}}\\)\n\\(\\operatorname{Re}\\{s\\}&gt;-\\alpha\\)\n\n\n14\n\\(\\left[e^{-\\alpha t} \\sin \\omega_{0} t\\right] u(t)\\)\n\\(\\frac{\\omega_{0}}{(s+\\alpha)^{2}+\\omega_{0}^{2}}\\)\n\\(\\operatorname{Re}\\{s\\}&gt;-\\alpha\\)\n\n\n15\n\\(u_{n}(t)=\\frac{d^{n} \\delta(t)}{d t^{n}}\\)\n\\(s^{n}\\)\nAll \\(s\\)\n\n\n16\n\\(u_{-n}(t)=\\underbrace{u(t) * \\cdots * u(t)}_{n \\text { times }}\\)\n\\(\\frac{1}{s^{n}}\\)\n\\(\\operatorname{Re}\\{s\\}&gt;0\\)"
  },
  {
    "objectID": "index.html#week-9",
    "href": "index.html#week-9",
    "title": "Signals and Systems",
    "section": "Week 9",
    "text": "Week 9\n\nSignals and Systems 9.1: THE LAPLACE TRANSFORM\nSignals and Systems 9.2: THE REGION OF CONVERGENCE (ROC)\nSignals and Systems 9.3: THE INVERSE LAPLACE TRANSFORM\nSignals and Systems 9.5: PROPERTIES OF THE LAPLACE TRANSFORM"
  },
  {
    "objectID": "ss_95.html#properties-of-the-laplace-transform",
    "href": "ss_95.html#properties-of-the-laplace-transform",
    "title": "Signals and Systems",
    "section": "9.5 Properties of the Laplace Transform",
    "text": "9.5 Properties of the Laplace Transform\nLeveraging Transform Domain Relationships\nJust like with the Fourier Transform, the Laplace Transform has a rich set of properties that simplify analysis and provide deeper insights. These properties are invaluable for:\n\nDeriving new transform pairs.\nAnalyzing LTI systems.\nSolving differential equations.\nChecking calculations.\n\nWe will explore key properties and their implications on both the signal and its ROC.\n\nIntroduce the section by highlighting the utility of properties in Signals and Systems, drawing a parallel to Fourier Transform properties. Emphasize that understanding these properties is crucial for practical applications."
  },
  {
    "objectID": "ss_97.html#the-system-function-hs",
    "href": "ss_97.html#the-system-function-hs",
    "title": "Signals and Systems",
    "section": "The System Function: \\(H(s)\\)",
    "text": "The System Function: \\(H(s)\\)\nThe Laplace Transform is a powerful tool for analyzing LTI systems because it converts convolution in the time domain into multiplication in the s-domain.\n\\[\n\\begin{equation*}\nY(s)=H(s) X(s) \\tag{9.112}\n\\end{equation*}\n\\]\n\n\\(X(s)\\): Laplace Transform of the input \\(x(t)\\)\n\\(Y(s)\\): Laplace Transform of the output \\(y(t)\\)\n\\(H(s)\\): Laplace Transform of the impulse response \\(h(t)\\)\n\n\n\n\n\n\n\nNote\n\n\n\\(H(s)\\) is commonly referred to as the system function or transfer function. If the ROC of \\(H(s)\\) includes the imaginary axis, then \\(H(j\\omega)\\) is the frequency response.\n\n\n\n\nThe equation \\(Y(s) = H(s)X(s)\\) is fundamental. It’s the Laplace transform counterpart to the convolution integral \\(y(t) = h(t) * x(t)\\). This transformation from convolution to multiplication dramatically simplifies system analysis. \\(H(s)\\) essentially describes how the system transforms an input. When we evaluate \\(H(s)\\) along the imaginary axis, i.e., \\(s=j\\omega\\), we get the system’s frequency response, telling us how different frequency components are affected."
  },
  {
    "objectID": "ss_97.html#causality-of-lti-systems",
    "href": "ss_97.html#causality-of-lti-systems",
    "title": "Signals and Systems",
    "section": "Causality of LTI Systems",
    "text": "Causality of LTI Systems\nA system is causal if its output at any time depends only on present and past inputs. For an LTI system, this means the impulse response \\(h(t)=0\\) for \\(t&lt;0\\).\n\nGeneral Rule: The ROC associated with the system function \\(H(s)\\) for a causal system is a right-half plane.\n\n\n\n\n\n\n\nImportant\n\n\nFor systems with a rational system function \\(H(s)\\): Causality \\(\\iff\\) the ROC is the right-half plane to the right of the rightmost pole.\n\n\n\n\nCausality is a critical property for real-time systems; you can’t predict the future. For LTI systems, this translates directly to the impulse response being zero before \\(t=0\\). The first rule is always true: a causal system’s ROC is a right-half plane. However, the second rule, which establishes a strong “if and only if” condition, is specifically for systems where \\(H(s)\\) is a rational function, meaning it can be expressed as a ratio of polynomials. This distinction is important, as we’ll see in an example."
  },
  {
    "objectID": "ss_97.html#example-causal-system",
    "href": "ss_97.html#example-causal-system",
    "title": "Signals and Systems",
    "section": "Example: Causal System",
    "text": "Example: Causal System\nConsider a system with impulse response:\n\\(h(t) = e^(-t)u(t)\\)\nThe system is causal since \\(h(t)=0\\) for \\(t&lt;0\\).\nIts system function is:\n\\[\n\\begin{equation*}\nH(s)=\\frac{1}{s+1}, \\quad \\operatorname{Re}\\{s\\}&gt;-1 \\tag{9.114}\n\\end{equation*}\n\\]\n\nPole: at \\(s=-1\\)\nROC: \\(\\operatorname{Re}\\{s\\}&gt;-1\\)\n\nThis ROC is to the right of the rightmost (and only) pole, confirming causality for this rational \\(H(s)\\).\n\nHere we have a simple first-order system. The impulse response \\(e^{-t}u(t)\\) is zero for \\(t&lt;0\\), so it’s causal. Its Laplace transform \\(H(s)\\) has a single pole at \\(s=-1\\). The Region of Convergence is \\(\\operatorname{Re}\\{s\\} &gt; -1\\), which is indeed a right-half plane extending to the right of the pole. This perfectly aligns with our rule for rational causal systems. The diagram visually represents the s-plane, showing the pole and the shaded ROC."
  },
  {
    "objectID": "ss_97.html#example-non-causal-system",
    "href": "ss_97.html#example-non-causal-system",
    "title": "Signals and Systems",
    "section": "Example: Non-Causal System",
    "text": "Example: Non-Causal System\nConsider a system with impulse response:\n\\(h(t) = e^(-|t|)\\)\nThis system is not causal since \\(h(t) \\neq 0\\) for \\(t&lt;0\\).\nIts system function is:\n\\[\nH(s)=\\frac{-2}{s^{2}-1}, \\quad-1&lt;\\operatorname{Re}\\{s\\}&lt;+1\n\\]\n\nPoles: at \\(s=-1\\) and \\(s=1\\)\nROC: \\(-1 &lt; \\operatorname{Re}\\{s\\} &lt; 1\\)\n\nThe ROC is a strip, not a right-half plane to the right of the rightmost pole (\\(s=1\\)), which is consistent with the system being non-causal.\n\nIn this example, the impulse response \\(e^{-|t|}\\) is non-zero for \\(t&lt;0\\), making the system non-causal. When we look at its system function, \\(H(s)\\), we find two poles, at \\(s=-1\\) and \\(s=1\\). The ROC is a vertical strip between these two poles. According to our rule, for a causal system with a rational \\(H(s)\\), the ROC must be to the right of the rightmost pole (which is \\(s=1\\) here). Since our ROC is not \\(\\operatorname{Re}\\{s\\}&gt;1\\), it correctly indicates that this system is not causal."
  },
  {
    "objectID": "ss_97.html#causality-caveat-non-rational-hs",
    "href": "ss_97.html#causality-caveat-non-rational-hs",
    "title": "Signals and Systems",
    "section": "Causality Caveat: Non-Rational \\(H(s)\\)",
    "text": "Causality Caveat: Non-Rational \\(H(s)\\)\nThe “rational \\(H(s)\\)” condition for causality is crucial.\nConsider a system function:\n\\[\n\\begin{equation*}\nH(s)=\\frac{e^{s}}{s+1}, \\quad \\operatorname{Re}\\{s\\}&gt;-1 \\tag{9.115}\n\\end{equation*}\n\\]\n\nPole: at \\(s=-1\\)\nROC: \\(\\operatorname{Re}\\{s\\}&gt;-1\\)\n\nHere, the ROC is to the right of the pole.\nHowever, \\(H(s)\\) is not rational due to the \\(e^s\\) term.\nThe impulse response is \\(h(t) = e^(-(t+1))u(t+1)\\).\nSince \\(u(t+1)\\) is non-zero for \\(t &lt; 0\\) (specifically for \\(-1 &lt; t &lt; 0\\)), the system is not causal.\n\n\n\n\n\n\nCaution\n\n\nCausality implies the ROC is a right-half plane. The converse is not generally true unless the system function \\(H(s)\\) is rational.\n\n\n\n\nThis example highlights a common misconception. While a right-half plane ROC is a necessary condition for causality, it’s not always sufficient if \\(H(s)\\) is not a rational function. The \\(e^s\\) term in \\(H(s)\\) corresponds to a time shift in the time domain, specifically \\(h(t) = e^{-(t+1)}u(t+1)\\). Because this impulse response starts before \\(t=0\\), the system is non-causal, even though its ROC is a right-half plane. This emphasizes the importance of the “rational system function” condition in our second rule for causality."
  },
  {
    "objectID": "ss_97.html#stability-of-lti-systems",
    "href": "ss_97.html#stability-of-lti-systems",
    "title": "Signals and Systems",
    "section": "Stability of LTI Systems",
    "text": "Stability of LTI Systems\nAn LTI system is stable if every bounded input produces a bounded output (BIBO stability). This is equivalent to its impulse response being absolutely integrable: \\(\\int_{-\\infty}^{\\infty} |h(t)| dt &lt; \\infty\\).\n\nGeneral Rule: An LTI system is stable \\(\\iff\\) the ROC of its system function \\(H(s)\\) includes the entire \\(j\\omega\\)-axis (\\(\\operatorname{Re}\\{s\\}=0\\)).\n\n\n\n\n\n\n\nImportant\n\n\nFor a causal system with a rational system function \\(H(s)\\): Stability \\(\\iff\\) all poles of \\(H(s)\\) lie in the left-half of the s-plane (i.e., all poles have negative real parts).\n\n\n\n\nStability is another fundamental system property. A stable system won’t “blow up” or produce an unbounded output from a bounded input. The first rule for stability is very general: the ROC must encompass the entire imaginary axis. This directly links to the convergence of the Fourier Transform of \\(h(t)\\). The second rule is a powerful shortcut for a very common class of systems: causal systems with rational transfer functions. In this case, simply checking the pole locations in the s-plane tells you everything you need to know about stability."
  },
  {
    "objectID": "ss_97.html#example-stability-multiple-rocs",
    "href": "ss_97.html#example-stability-multiple-rocs",
    "title": "Signals and Systems",
    "section": "Example: Stability & Multiple ROCs",
    "text": "Example: Stability & Multiple ROCs\nConsider a system with \\(H(s)=\\frac{s-1}{(s+1)(s-2)}\\). Poles at \\(s=-1, s=2\\). Zero at \\(s=1\\).\nDifferent ROCs lead to different system properties:\n\n\n(a) Causal, Unstable\nROC: \\(\\operatorname{Re}\\{s\\} &gt; 2\\)\n\nRight of rightmost pole.\nDoes NOT include \\(j\\omega\\)-axis. mermaid\n\n\n(b) Non-Causal, Stable\nROC: \\(-1 &lt; \\operatorname{Re}\\{s\\} &lt; 2\\)\n\nIncludes \\(j\\omega\\)-axis.\nNot to the right of rightmost pole. mermaid\n\n\n(c) Anticausal, Unstable\nROC: \\(\\operatorname{Re}\\{s\\} &lt; -1\\)\n\nLeft of leftmost pole.\nDoes NOT include \\(j\\omega\\)-axis. mermaid\n\n\n\nThis example beautifully illustrates how the same algebraic expression for \\(H(s)\\) can represent different systems depending on its ROC. We have poles at -1 and 2. For case (a), if the system is causal, its ROC must be \\(\\operatorname{Re}\\{s\\}&gt;2\\). This doesn’t include the \\(j\\omega\\)-axis, so it’s unstable. For case (b), if the system is stable, its ROC must include the \\(j\\omega\\)-axis, meaning it’s the strip \\(-1 &lt; \\operatorname{Re}\\{s\\} &lt; 2\\). This system is stable, but because the ROC is not to the right of the rightmost pole, it’s non-causal. Finally, for case (c), if it’s anticausal, its ROC is \\(\\operatorname{Re}\\{s\\}&lt;-1\\). This is unstable as it doesn’t contain the \\(j\\omega\\)-axis. The key takeaway: ROC is crucial for defining system properties."
  },
  {
    "objectID": "ss_97.html#example-causal-system-stability",
    "href": "ss_97.html#example-causal-system-stability",
    "title": "Signals and Systems",
    "section": "Example: Causal System Stability",
    "text": "Example: Causal System Stability\nConsider causal LTI systems with rational system functions:\n\n\nStable System\n\nh(t) = e^(-t)u(t)\nH(s) = 1/(s+1)\nPole: at \\(s=-1\\) (in the left-half s-plane)\n\nThis system is stable because its pole has a negative real part.\n\nUnstable System\n\nh(t) = e^(2t)u(t)\nH(s) = 1/(s-2)\nPole: at \\(s=2\\) (in the right-half s-plane)\n\nThis system is unstable because its pole has a positive real part.\n\n\n\n\n\n\n\nTip\n\n\nFor causal systems with rational \\(H(s)\\), simply check if all poles are in the LHP for stability.\n\n\n\n\nThese are straightforward examples applying the second rule for stability. For causal systems with rational transfer functions, stability is directly determined by the location of the poles. A pole in the left-half plane, like \\(s=-1\\), leads to a decaying exponential in the impulse response, which is absolutely integrable and thus stable. A pole in the right-half plane, like \\(s=2\\), leads to a growing exponential, which is not absolutely integrable, making the system unstable. This visual check in the s-plane is incredibly useful for engineers."
  },
  {
    "objectID": "ss_97.html#lti-systems-characterized-by-linear-constant-coefficient-differential-equations",
    "href": "ss_97.html#lti-systems-characterized-by-linear-constant-coefficient-differential-equations",
    "title": "Signals and Systems",
    "section": "LTI Systems Characterized by Linear Constant-Coefficient Differential Equations",
    "text": "LTI Systems Characterized by Linear Constant-Coefficient Differential Equations\nThe Laplace Transform simplifies the analysis of LTI systems described by LCCDEs.\nA general LCCDE:\n\\[\n\\begin{equation*}\n\\sum_{k=0}^{N} a_{k} \\frac{d^{k} y(t)}{d t^{k}}=\\sum_{k=0}^{M} b_{k} \\frac{d^{k} x(t)}{d t^{k}} \\tag{9.131}\n\\end{equation*}\n\\]\nApplying the Laplace Transform (using linearity and differentiation properties):\n\\[\n\\begin{equation*}\n\\left(\\sum_{k=0}^{N} a_{k} s^{k}\\right) Y(s)=\\left(\\sum_{k=0}^{M} b_{k} s^{k}\\right) X(s) \\tag{9.132}\n\\end{equation*}\n\\]\nThe system function \\(H(s) = Y(s)/X(s)\\) is always rational:\n\\[\n\\begin{equation*}\nH(s)=\\frac{\\left\\{\\sum_{k=0}^{M} b_{k} s^{k}\\right\\}}{\\left\\{\\sum_{k=0}^{N} a_{k} s^{k}\\right\\}} \\tag{9.133}\n\\end{equation*}\n\\]\n\nZeros: solutions to \\(\\sum_{k=0}^{M} b_{k} s^{k}=0\\)\nPoles: solutions to \\(\\sum_{k=0}^{N} a_{k} s^{k}=0\\)\n\n\nDifferential equations are a natural way to model many physical systems. The beauty of the Laplace transform is how it transforms these complex differential equations into simple algebraic equations in the s-domain. This allows us to easily find the system function \\(H(s)\\) as a ratio of polynomials, which makes it a rational function. The roots of the numerator polynomial are the zeros of the system, and the roots of the denominator polynomial are the poles. Remember, the differential equation itself doesn’t specify the ROC; additional information like causality or stability is needed."
  },
  {
    "objectID": "ss_97.html#example-first-order-lccde",
    "href": "ss_97.html#example-first-order-lccde",
    "title": "Signals and Systems",
    "section": "Example: First-Order LCCDE",
    "text": "Example: First-Order LCCDE\nConsider an LTI system where input \\(x(t)\\) and output \\(y(t)\\) satisfy:\n\\[\n\\begin{equation*}\n\\frac{d y(t)}{d t}+3 y(t)=x(t) \\tag{9.126}\n\\end{equation*}\n\\]\n\nApply Laplace Transform: \\(sY(s) + 3Y(s) = X(s)\\) \\((s+3)Y(s) = X(s)\\)\nDetermine System Function: \\[\n\\begin{equation*}\nH(s)=\\frac{Y(s)}{X(s)}=\\frac{1}{s+3} \\tag{9.128}\n\\end{equation*}\n\\]\nInfer ROC (with additional info):\n\nIf causal: ROC is \\(\\operatorname{Re}\\{s\\}&gt;-3\\). (System is stable).\nIf anticausal: ROC is \\(\\operatorname{Re}\\{s\\}&lt;-3\\). (System is unstable).\n\n\n\nThis example demonstrates the process of finding the system function from a differential equation. We take the Laplace transform of both sides, use the differentiation property, and then simply rearrange the terms to solve for \\(Y(s)/X(s)\\). The result, \\(H(s) = 1/(s+3)\\), has a pole at \\(s=-3\\). As discussed, the differential equation alone doesn’t give the ROC. If we add the condition of causality, we know the ROC must be \\(\\operatorname{Re}\\{s\\}&gt;-3\\), which also makes the system stable since the pole is in the LHP. If it were anticausal, the ROC would be \\(\\operatorname{Re}\\{s\\}&lt;-3\\), making it unstable."
  },
  {
    "objectID": "ss_97.html#application-rlc-circuit-analysis",
    "href": "ss_97.html#application-rlc-circuit-analysis",
    "title": "Signals and Systems",
    "section": "Application: RLC Circuit Analysis",
    "text": "Application: RLC Circuit Analysis\nAn RLC circuit is a classic example of an LTI system described by an LCCDE.\nConsider the series RLC circuit where \\(x(t)\\) is the source voltage and \\(y(t)\\) is the capacitor voltage.\nThe differential equation is: \\[\n\\begin{equation*}\nL C \\frac{d^{2} y(t)}{d t^{2}}+R C \\frac{d y(t)}{d t}+y(t)=x(t) \\tag{9.136}\n\\end{equation*}\n\\]\nApplying Laplace Transform: \\[\n\\begin{equation*}\nH(s)=\\frac{1 / L C}{s^{2}+(R / L) s+(1 / L C)} \\tag{9.137}\n\\end{equation*}\n\\]\n\n\n\n\n\n\nNote\n\n\nFor positive values of \\(R, L, C\\), the poles of this system function will always have negative real parts, ensuring the system is stable.\n\n\n\n\nHere’s a practical application. A series RLC circuit is a fundamental circuit in ECE. If we take the input as the source voltage and the output as the voltage across the capacitor, we can derive a second-order differential equation describing its behavior. Using the Laplace transform, we can easily find its system function \\(H(s)\\). For physically realizable components (positive R, L, C), the poles of this system will always reside in the left-half of the s-plane, meaning the RLC circuit is inherently stable. This is a powerful conclusion derived quickly using Laplace transforms."
  },
  {
    "objectID": "ss_97.html#example-deduce-system-from-io-pair",
    "href": "ss_97.html#example-deduce-system-from-io-pair",
    "title": "Signals and Systems",
    "section": "Example: Deduce System from I/O Pair",
    "text": "Example: Deduce System from I/O Pair\nSuppose we know that for an LTI system:\n\nInput: \\(x(t)=e^{-3 t} u(t)\\)\nOutput: \\(y(t)=\\left[e^{-t}-e^{-2 t}\\right] u(t)\\)\n\nLet’s find \\(H(s)\\) and system properties:\n\nLaplace Transforms of I/O: \\(X(s)=\\frac{1}{s+3}, \\quad \\operatorname{Re}\\{s\\}&gt;-3\\) \\(Y(s)=\\frac{1}{(s+1)(s+2)}, \\quad \\operatorname{Re}\\{s\\}&gt;-1\\)\nSystem Function: \\(H(s)=\\frac{Y(s)}{X(s)}=\\frac{s+3}{(s+1)(s+2)}\\)\nROC of \\(H(s)\\): The ROC of \\(Y(s)\\) must include the intersection of ROCs of \\(X(s)\\) and \\(H(s)\\). Given \\(X(s)\\) has \\(\\operatorname{Re}\\{s\\}&gt;-3\\) and \\(Y(s)\\) has \\(\\operatorname{Re}\\{s\\}&gt;-1\\), the only consistent ROC for \\(H(s)\\) is \\(\\operatorname{Re}\\{s\\}&gt;-1\\).\nDeduced System Properties:\n\nCausal: Yes (ROC is RHP to the right of rightmost pole at \\(s=-1\\)).\nStable: Yes (All poles at \\(s=-1, -2\\) are in LHP, ROC includes \\(j\\omega\\)-axis).\nDifferential Equation: \\(\\frac{d^{2} y(t)}{d t^{2}}+3 \\frac{d y(t)}{d t}+2 y(t)=\\frac{d x(t)}{d t}+3 x(t)\\)\n\n\n\nThis example showcases the power of the Laplace transform to reverse-engineer system properties from a single input-output pair. By transforming \\(x(t)\\) and \\(y(t)\\) to the s-domain, we can easily calculate \\(H(s)\\). Crucially, the ROC of \\(H(s)\\) must be compatible with both \\(X(s)\\) and \\(Y(s)\\). From this, we can deduce causality, stability, and even the underlying differential equation. This is a common problem-solving technique in system identification."
  },
  {
    "objectID": "ss_97.html#example-analyze-system-statements-part-1",
    "href": "ss_97.html#example-analyze-system-statements-part-1",
    "title": "Signals and Systems",
    "section": "Example: Analyze System Statements (Part 1)",
    "text": "Example: Analyze System Statements (Part 1)\nGiven: Causal, stable, rational \\(H(s)\\), pole at \\(s=-2\\), no zero at origin.\nLet’s evaluate these statements:\n(a) \\(\\mathcal{F}\\{h(t)e^{3t}\\}\\) converges.\n\nFalse. This corresponds to \\(H(s)\\) evaluated at \\(s=-3\\).\nSince the system is causal and stable, its ROC is \\(\\operatorname{Re}\\{s\\} &gt; -2\\).\n\\(s=-3\\) is not in this ROC, so the Fourier transform does not converge.\n\n(b) \\(\\int_{-\\infty}^{+\\infty} h(t) d t=0\\).\n\nFalse. This integral is equivalent to \\(H(0)\\).\nThe problem states there is no zero at the origin, so \\(H(0) \\neq 0\\).\n\n\nThis is a good exercise to test your understanding of the interplay between Laplace transform properties and system characteristics. For statement (a), recall that multiplication by \\(e^{at}\\) in the time domain corresponds to a shift in the s-domain by \\(s-a\\). So, \\(h(t)e^{3t}\\) has a Laplace transform \\(H(s-3)\\). If we then take the Fourier transform, it’s \\(H(j\\omega-3)\\), which means we’re evaluating \\(H(s)\\) at \\(s=j\\omega-3\\). For convergence, this \\(s\\) must be in the ROC of \\(H(s)\\). The ROC for a causal, stable system with a pole at -2 is \\(\\operatorname{Re}\\{s\\}&gt;-2\\). Since \\(s=-3\\) is not in this region, the Fourier transform does not converge. For statement (b), the integral of \\(h(t)\\) is simply \\(H(0)\\). The problem explicitly states there’s no zero at the origin, meaning \\(H(0)\\) is not zero."
  },
  {
    "objectID": "ss_97.html#example-analyze-system-statements-part-2",
    "href": "ss_97.html#example-analyze-system-statements-part-2",
    "title": "Signals and Systems",
    "section": "Example: Analyze System Statements (Part 2)",
    "text": "Example: Analyze System Statements (Part 2)\nGiven: Causal, stable, rational \\(H(s)\\), pole at \\(s=-2\\), no zero at origin.\nLet’s evaluate these statements:\n\n\n(c) \\(t h(t)\\) is the impulse response of a causal and stable system.\n\nTrue. The Laplace transform of \\(t h(t)\\) is \\(-\\frac{d}{ds}H(s)\\), which has the same ROC as \\(H(s)\\).\nSince \\(H(s)\\) is causal and stable, its ROC includes the \\(j\\omega\\)-axis.\nAlso, if \\(h(t)=0\\) for \\(t&lt;0\\), then \\(t h(t)=0\\) for \\(t&lt;0\\), so it’s causal.\n\n\n(d) \\(d h(t) / d t\\) contains at least one pole in its Laplace transform.\n\nTrue. The Laplace transform of \\(d h(t) / d t\\) is \\(s H(s)\\).\nMultiplying by \\(s\\) does not remove the pole at \\(s=-2\\).\n(It could remove a pole at \\(s=0\\), but \\(s=-2\\) remains)."
  },
  {
    "objectID": "ss_97.html#example-analyze-system-statements-part-3",
    "href": "ss_97.html#example-analyze-system-statements-part-3",
    "title": "Signals and Systems",
    "section": "Example: Analyze System Statements (Part 3)",
    "text": "Example: Analyze System Statements (Part 3)\nGiven: Causal, stable, rational \\(H(s)\\), pole at \\(s=-2\\), no zero at origin.\nLet’s evaluate these statements:\n\n\n(e) \\(h(t)\\) has finite duration.\n\nFalse.\nIf \\(h(t)\\) has finite duration, its ROC must be the entire \\(s\\)-plane.\nThis is inconsistent with \\(H(s)\\) having a pole at \\(s=-2\\).\n(Poles imply the ROC cannot be the entire s-plane).\n\n\n(f) \\(H(s)=H(-s)\\).\n\nFalse.\nIf \\(H(s)\\) has a pole at \\(s=-2\\), then \\(H(-s)\\) would have a pole at \\(s=2\\).\nFor \\(H(s)=H(-s)\\) to hold, \\(H(s)\\) would need a pole at \\(s=2\\).\nThis contradicts the condition that the system is causal and stable (all poles in LHP)."
  },
  {
    "objectID": "ss_97.html#example-analyze-system-statements-part-4",
    "href": "ss_97.html#example-analyze-system-statements-part-4",
    "title": "Signals and Systems",
    "section": "Example: Analyze System Statements (Part 4)",
    "text": "Example: Analyze System Statements (Part 4)\nGiven: Causal, stable, rational \\(H(s)\\), pole at \\(s=-2\\), no zero at origin.\nLet’s evaluate this statement:\n(g) \\(\\lim _{s \\rightarrow \\infty} H(s)=2\\).\n\n\n\n\n\n\nNote\n\n\nInsufficient Information.\nFor this limit to be a finite non-zero value, the degree of the numerator polynomial must equal the degree of the denominator polynomial in \\(H(s)\\). We do not have enough information about the other poles and zeros of \\(H(s)\\) to determine this.\n\n\n\n\nFor (c), the differentiation in the s-domain (multiplication by \\(s\\)) doesn’t remove poles unless they are at \\(s=0\\). So the pole at \\(s=-2\\) remains. For (d), the property of \\(t h(t)\\) having the same ROC as \\(H(s)\\) is a direct Laplace transform property. Since \\(h(t)\\) is causal, \\(th(t)\\) is also causal. Since \\(H(s)\\) is stable, its ROC includes the \\(j\\omega\\)-axis, so the ROC of \\(-\\frac{d}{ds}H(s)\\) also includes the \\(j\\omega\\)-axis, making \\(th(t)\\) stable. For (e), finite duration signals have ROCs that are the entire s-plane. If there’s a pole at \\(s=-2\\), the ROC cannot be the entire s-plane, so \\(h(t)\\) cannot be finite duration. For (f), if \\(H(s)=H(-s)\\), it means the pole-zero plot must be symmetric with respect to the \\(j\\omega\\)-axis. If there’s a pole at \\(s=-2\\), there must also be one at \\(s=2\\). A pole at \\(s=2\\) would make a causal system unstable, contradicting the given condition. For (g), the limit of \\(H(s)\\) as \\(s \\to \\infty\\) depends on the relative degrees of the numerator and denominator polynomials. If they are equal, the limit is the ratio of their leading coefficients. We don’t have this information, so we can’t determine the truth of the statement."
  },
  {
    "objectID": "ss_97.html#application-butterworth-filters",
    "href": "ss_97.html#application-butterworth-filters",
    "title": "Signals and Systems",
    "section": "Application: Butterworth Filters",
    "text": "Application: Butterworth Filters\nButterworth filters are a widely used class of LTI systems known for their maximally flat passband frequency response.\nAn \\(N\\)-th order lowpass Butterworth filter has a frequency response magnitude squared given by:\n\\[\n\\begin{equation*}\n|B(j \\omega)|^{2}=\\frac{1}{1+\\left(j \\omega / j \\omega_{c}\\right)^{2 N}} \\tag{9.140}\n\\end{equation*}\n\\]\nTo find the system function \\(B(s)\\):\n\nUse the property \\(|B(j \\omega)|^{2}=B(j \\omega) B^{*}(j \\omega)\\).\nFor real impulse response, \\(B^{*}(j \\omega)=B(-j \\omega)\\). So, \\(|B(j \\omega)|^{2}=B(j \\omega) B(-j \\omega)\\).\nSubstitute \\(s=j\\omega\\):\n\\[\n\\begin{equation*}\nB(s) B(-s)=\\frac{1}{1+\\left(s / j \\omega_{c}\\right)^{2 N}} \\tag{9.144}\n\\end{equation*}\n\\]\n\n\n\n\n\n\n\nTip\n\n\nFrom \\(B(s)B(-s)\\), we find the poles of the filter. To ensure a causal and stable filter, we select the poles that lie in the left-half of the s-plane.\n\n\n\n\nButterworth filters are a cornerstone of analog and digital filter design due to their flat passband and monotonic rolloff. We begin with the definition of its squared magnitude response. The key step here is to relate \\(|B(j\\omega)|^2\\) to \\(B(s)B(-s)\\) by replacing \\(j\\omega\\) with \\(s\\). This gives us an expression for \\(B(s)B(-s)\\), from which we can find all \\(2N\\) poles. To construct the stable and causal filter \\(B(s)\\), we must select only those poles that lie in the left-half of the s-plane. This process directly links the frequency domain specification to the s-domain system function."
  },
  {
    "objectID": "ss_97.html#butterworth-filter-pole-locations-interactive",
    "href": "ss_97.html#butterworth-filter-pole-locations-interactive",
    "title": "Signals and Systems",
    "section": "Butterworth Filter Pole Locations (Interactive)",
    "text": "Butterworth Filter Pole Locations (Interactive)\nThe poles of \\(B(s)B(-s)\\) are located on a circle of radius \\(\\omega_c\\) in the s-plane.\nSpecifically, \\(s_p = \\omega_c \\exp\\left(j\\left[\\frac{\\pi(2k+1)}{2N}+\\frac{\\pi}{2}\\right]\\right)\\) for \\(k=0, \\dots, 2N-1\\).\nUse the sliders to explore how the pole locations change with filter order \\(N\\) and cutoff frequency \\(\\omega_c\\).\n\nviewof N_butter = Inputs.range([1, 6], {value: 2, step: 1, label: \"Filter Order (N)\"});\nviewof wc_butter = Inputs.range([0.5, 2.0], {value: 1.0, step: 0.1, label: \"Cutoff Frequency (ωc)\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive plot allows you to visualize the pole locations for Butterworth filters. The red ‘x’ marks show all \\(2N\\) poles of \\(B(s)B(-s)\\), which are symmetrically distributed on a circle of radius \\(\\omega_c\\). The blue circles represent the \\(N\\) poles selected for \\(B(s)\\) to ensure causality and stability – these are the poles exclusively in the left-half of the s-plane. Notice how increasing \\(N\\) adds more poles, making the filter sharper, and changing \\(\\omega_c\\) scales the entire pole constellation."
  },
  {
    "objectID": "ss_97.html#butterworth-transfer-functions-differential-equations",
    "href": "ss_97.html#butterworth-transfer-functions-differential-equations",
    "title": "Signals and Systems",
    "section": "Butterworth Transfer Functions & Differential Equations",
    "text": "Butterworth Transfer Functions & Differential Equations\nOnce the poles for \\(B(s)\\) are identified, the transfer function can be constructed.\n\nN=1: \\(B(s)=\\frac{\\omega_{c}}{s+\\omega_{c}}\\) Differential Equation: \\(\\frac{d y(t)}{d t}+\\omega_{c} y(t)=\\omega_{c} x(t)\\)\nN=2: \\(B(s)=\\frac{\\omega_{c}^{2}}{s^{2}+\\sqrt{2} \\omega_{c} s+\\omega_{c}^{2}}\\) Differential Equation: \\(\\frac{d^{2} y(t)}{d t^{2}}+\\sqrt{2} \\omega_{c} \\frac{d y(t)}{d t}+\\omega_{c}^{2} y(t)=\\omega_{c}^{2} x(t)\\)\nN=3: \\(B(s)=\\frac{\\omega_{c}^{3}}{s^{3}+2 \\omega_{c} s^{2}+2 \\omega_{c}^{2} s+\\omega_{c}^{3}}\\) Differential Equation: \\(\\frac{d^{3} y(t)}{d t^{3}}+2 \\omega_{c} \\frac{d^{2} y(t)}{d t^{2}}+2 \\omega_{c}^{2} \\frac{d y(t)}{d t}+\\omega_{c}^{3} y(t)=\\omega_{c}^{3} x(t)\\)\n\n\n\n\n\n\n\nNote\n\n\nHigher-order filters (larger \\(N\\)) result in more complex differential equations but offer sharper cutoff characteristics in the frequency domain.\n\n\n\n\nHere we see the actual transfer functions and their corresponding differential equations for the first few orders of Butterworth filters. As the order \\(N\\) increases, the denominator polynomial gets more complex, leading to higher-order differential equations. This directly translates to more components in an analog circuit implementation or more complex algorithms in a digital filter. The trade-off is improved filter performance, such as a steeper roll-off in the stopband, at the cost of increased system complexity."
  },
  {
    "objectID": "ss_98.html#introduction-system-function-algebra",
    "href": "ss_98.html#introduction-system-function-algebra",
    "title": "Signals and Systems",
    "section": "Introduction: System Function Algebra",
    "text": "Introduction: System Function Algebra\nThe Laplace Transform simplifies LTI system analysis by converting time-domain operations into algebraic ones. This is particularly useful for:\n\nAnalyzing interconnections of LTI systems.\nSynthesizing systems from elementary building blocks.\n\n\n\n\n\n\n\nTip\n\n\nKey Idea:\nConvolution in the time domain becomes multiplication in the s-domain. Differentiation becomes multiplication by \\(s\\). Integration becomes multiplication by \\(1/s\\)."
  },
  {
    "objectID": "ss_98.html#parallel-interconnection",
    "href": "ss_98.html#parallel-interconnection",
    "title": "Signals and Systems",
    "section": "Parallel Interconnection",
    "text": "Parallel Interconnection\nConsider two LTI systems, \\(H_1(s)\\) and \\(H_2(s)\\), connected in parallel.\nTime Domain:\nThe impulse response of the overall system is the sum of individual impulse responses:\n\\[h(t) = h_1(t) + h_2(t)\\]\nS-Domain:\nDue to the linearity of the Laplace transform, the overall system function is the sum of individual system functions:\n\\[H(s) = H_1(s) + H_2(s)\\]\n\nParallel InterconnectionFigure 9.30(a): Parallel interconnection of two LTI systems.\n\nIn a parallel interconnection, the input signal \\(x(t)\\) is applied to both systems simultaneously. The outputs of the individual systems, \\(y_1(t)\\) and \\(y_2(t)\\), are then summed to produce the overall output \\(y(t)\\). This is analogous to connecting two resistors in parallel, where the total conductance is the sum of individual conductances. This setup is common in audio mixing or parallel processing."
  },
  {
    "objectID": "ss_98.html#interactive-example-parallel-systems",
    "href": "ss_98.html#interactive-example-parallel-systems",
    "title": "Signals and Systems",
    "section": "Interactive Example: Parallel Systems",
    "text": "Interactive Example: Parallel Systems\nLet’s explore the effect of parallel interconnection on system response. Consider two first-order systems: \\(H_1(s) = \\frac{1}{s+a}\\) and \\(H_2(s) = \\frac{1}{s+b}\\).\n\nviewof pole_a = Inputs.range([0.1, 5.0], {value: 1.0, step: 0.1, label: \"Pole 1 (a)\"});\nviewof pole_b = Inputs.range([0.1, 5.0], {value: 2.0, step: 0.1, label: \"Pole 2 (b)\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjust the pole values ‘a’ and ‘b’ using the sliders. Observe how the individual step responses of \\(H_1(s)\\) and \\(H_2(s)\\) combine to form the step response of the overall parallel system. Notice that the parallel system’s response is simply the sum of the individual responses, demonstrating the linearity of the system function algebra. This interactive plot helps visualize the direct relationship between individual system characteristics and the combined system’s behavior."
  },
  {
    "objectID": "ss_98.html#series-interconnection",
    "href": "ss_98.html#series-interconnection",
    "title": "Signals and Systems",
    "section": "Series Interconnection",
    "text": "Series Interconnection\nConsider two LTI systems, \\(H_1(s)\\) and \\(H_2(s)\\), connected in series (cascade).\nTime Domain:\nThe impulse response of the overall system is the convolution of individual impulse responses:\n\\[h(t) = h_1(t) * h_2(t)\\]\nS-Domain:\nThe overall system function is the product of individual system functions:\n\\[H(s) = H_1(s) H_2(s)\\]\n\nSeries InterconnectionFigure 9.30(b): Series combination of two LTI systems.\n\nIn a series interconnection, the output of the first system, \\(y_1(t)\\), becomes the input to the second system, \\(x_2(t)\\). The final output \\(y(t)\\) is then the output of the second system. This setup is fundamental in signal processing chains, where multiple filters or processing stages are applied sequentially. For example, an audio signal might pass through an equalizer (filter 1) and then a compressor (filter 2)."
  },
  {
    "objectID": "ss_98.html#interactive-example-series-systems",
    "href": "ss_98.html#interactive-example-series-systems",
    "title": "Signals and Systems",
    "section": "Interactive Example: Series Systems",
    "text": "Interactive Example: Series Systems\nLet’s see how cascading two first-order systems affects the overall step response.\nSystems: \\(H_1(s) = \\frac{1}{s+a}\\) and \\(H_2(s) = \\frac{1}{s+b}\\).\n\nviewof pole_c = Inputs.range([0.1, 5.0], {value: 1.0, step: 0.1, label: \"Pole 1 (c)\"});\nviewof pole_d = Inputs.range([0.1, 5.0], {value: 2.0, step: 0.1, label: \"Pole 2 (d)\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimilar to the parallel case, adjust the pole values ‘c’ and ‘d’. Notice how the series connection results in a system that is often “slower” or more filtered than the individual components, especially if the poles are close to the origin. The overall pole locations are determined by the product of the individual system functions. This is crucial for designing multi-stage filters or control systems where each stage contributes to the overall dynamics."
  },
  {
    "objectID": "ss_98.html#feedback-interconnection",
    "href": "ss_98.html#feedback-interconnection",
    "title": "Signals and Systems",
    "section": "Feedback Interconnection",
    "text": "Feedback Interconnection\nFeedback is a powerful concept in engineering, fundamental to control systems, amplifiers, and oscillators.\nSystem Equations:\nFrom the diagram:\n\\(Y(s) = H_1(s) E(s)\\)\n\\(E(s) = X(s) - Z(s)\\)\n\\(Z(s) = H_2(s) Y(s)\\)\nSubstituting and solving for \\(Y(s)/X(s)\\):\n\\(Y(s) = H_1(s) [X(s) - H_2(s) Y(s)]\\)\n\\(Y(s) = H_1(s) X(s) - H_1(s) H_2(s) Y(s)\\)\n\\(Y(s) [1 + H_1(s) H_2(s)] = H_1(s) X(s)\\)\nOverall System Function: \\[H(s) = \\frac{Y(s)}{X(s)} = \\frac{H_1(s)}{1 + H_1(s) H_2(s)}\\]\n\nFeedback InterconnectionFigure 9.31: Feedback interconnection of two LTI systems.\n\nFeedback systems are ubiquitous in engineering. A simple example is a thermostat controlling room temperature. The sensor (\\(H_2(s)\\)) measures the actual temperature (\\(Y(s)\\)), compares it to the desired temperature (\\(X(s)\\)), and sends an error signal (\\(E(s)\\)) to the heating/cooling system (\\(H_1(s)\\)). The primary goal of feedback is to regulate or stabilize a system, improve its performance, or make it less sensitive to disturbances. The algebraic derivation shows how feedback fundamentally alters the system’s poles, which directly impacts stability and response characteristics."
  },
  {
    "objectID": "ss_98.html#block-diagram-representations",
    "href": "ss_98.html#block-diagram-representations",
    "title": "Signals and Systems",
    "section": "Block Diagram Representations",
    "text": "Block Diagram Representations\nBlock diagrams are visual tools to represent LTI systems, especially those described by differential equations. They use three basic operations:\n\nAddition: Summing junction.\nMultiplication by a coefficient: Gain block.\nIntegration: Integrator block (\\(1/s\\)).\n\n\n\n\n\n\n\nImportant\n\n\nThese building blocks allow us to visualize the internal structure and signal flow within a system."
  },
  {
    "objectID": "ss_98.html#example-9.28-first-order-system",
    "href": "ss_98.html#example-9.28-first-order-system",
    "title": "Signals and Systems",
    "section": "Example 9.28: First-Order System",
    "text": "Example 9.28: First-Order System\nConsider a causal LTI system with system function \\(H(s)=\\frac{1}{s+3}\\).\nThis corresponds to the differential equation: \\(\\frac{d y(t)}{d t}+3 y(t)=x(t)\\).\nWe can rearrange the differential equation as:\n\\(\\frac{d y(t)}{d t} = x(t) - 3 y(t)\\)\nIntegrating both sides:\n\\(y(t) = \\int [x(t) - 3 y(t)] dt\\)\nThis suggests a feedback structure where the input to the integrator is \\(x(t) - 3y(t)\\).\nLet’s visualize this using a block diagram.\n\nFigure 9.32(a)Figure 9.32(a): Block diagram representation.\n\nThis example demonstrates how a simple first-order differential equation can be translated into a block diagram using an integrator, a gain block, and a summing junction. The term \\(1/s\\) represents the integrator, which performs the operation of integration in the s-domain. The feedback path with gain -3 represents the term \\(3y(t)\\) being subtracted from the input. This direct translation from a differential equation to a block diagram is a fundamental skill in system analysis and design."
  },
  {
    "objectID": "ss_98.html#example-9.28-block-diagram-mermaid",
    "href": "ss_98.html#example-9.28-block-diagram-mermaid",
    "title": "Signals and Systems",
    "section": "Example 9.28: Block Diagram (Mermaid)",
    "text": "Example 9.28: Block Diagram (Mermaid)\nHere’s the block diagram for \\(H(s)=\\frac{1}{s+3}\\) using Mermaid.\n\n\n\n\n\ngraph LR\n    X(\"x(t)\") --&gt; Sum(Sum)\n    Sum --&gt; Integrator(\"1/s\")\n    Integrator --&gt; Y(\"y(t)\")\n    Y --&gt; Gain(\"-3\")\n    Gain --&gt; Sum\n    style Sum fill:#fff,stroke:#333,stroke-width:2px,rx:5px,ry:5px\n    style Integrator fill:#fff,stroke:#333,stroke-width:2px,rx:5px,ry:5px\n    style Gain fill:#fff,stroke:#333,stroke-width:2px,rx:5px,ry:5px\n\n\n\n\n\n\n\nThe diagram visually confirms the feedback structure. The input \\(x(t)\\) is added to the feedback signal. The result is integrated (represented by \\(1/s\\)), producing \\(y(t)\\). A portion of \\(y(t)\\) is fed back through a gain of -3 and subtracted from the input. This is a direct implementation of the differential equation \\(\\frac{d y(t)}{d t} = x(t) - 3 y(t)\\)."
  },
  {
    "objectID": "ss_98.html#example-9.29-first-order-system-with-zero",
    "href": "ss_98.html#example-9.29-first-order-system-with-zero",
    "title": "Signals and Systems",
    "section": "Example 9.29: First-Order System with Zero",
    "text": "Example 9.29: First-Order System with Zero\nConsider \\(H(s)=\\frac{s+2}{s+3}\\).\nThis can be written as \\(H(s)=\\left(\\frac{1}{s+3}\\right)(s+2)\\), suggesting a cascade of two systems.\nThe first system is \\(H_A(s) = \\frac{1}{s+3}\\), which we just saw. The second system is \\(H_B(s) = s+2\\).\nIf \\(Z(s)\\) is the output of \\(H_A(s)\\), then \\(Y(s) = (s+2)Z(s)\\).\nIn time domain: \\(y(t) = \\frac{d z(t)}{d t} + 2 z(t)\\).\nThe input to the integrator in the \\(H_A(s)\\) block is \\(e(t) = \\frac{d z(t)}{d t}\\).\nSo, \\(y(t) = e(t) + 2 z(t)\\).\nThis avoids explicit differentiation, leading to a “direct form” representation.\n\nFigure 9.33(b)Figure 9.33(b): Equivalent block diagram representation.\n\nThis example shows how a system with both poles and zeros can be represented. The “tapping” of signals from within the first stage (specifically the input to the integrator, \\(e(t)\\), which is \\(dz(t)/dt\\)) is a common technique to avoid explicit differentiators, which are often problematic in practical implementations due to noise amplification. This direct-form structure is efficient and widely used in digital filter implementations."
  },
  {
    "objectID": "ss_98.html#example-9.30-second-order-system",
    "href": "ss_98.html#example-9.30-second-order-system",
    "title": "Signals and Systems",
    "section": "Example 9.30: Second-Order System",
    "text": "Example 9.30: Second-Order System\nConsider \\(H(s)=\\frac{1}{(s+1)(s+2)}=\\frac{1}{s^{2}+3 s+2}\\).\nThis corresponds to the differential equation: \\(\\frac{d^{2} y(t)}{d t^{2}}+3 \\frac{d y(t)}{d t}+2 y(t)=x(t)\\).\nDirect Form:\nRearranging the differential equation:\n\\(\\frac{d^{2} y(t)}{d t^{2}} = x(t) - 3 \\frac{d y(t)}{d t} - 2 y(t)\\) We can use two integrators in cascade to obtain \\(y(t)\\) from \\(\\frac{d^{2} y(t)}{d t^{2}}\\).\nThe outputs of these integrators provide \\(\\frac{d y(t)}{d t}\\) and \\(y(t)\\).\n\nFigure 9.34(a)Figure 9.34(a): Direct form representation.\n\nThe direct form for a second-order system extends the ideas from the first-order case. Two cascaded integrators are used, where the output of the first integrator is the first derivative of the output, and the output of the second is the output itself. Feedback paths from these outputs, scaled by the coefficients of the differential equation, are summed with the input to provide the second derivative, closing the loop. This structure is very straightforward to derive from the differential equation or system function."
  },
  {
    "objectID": "ss_98.html#example-9.30-cascade-and-parallel-forms",
    "href": "ss_98.html#example-9.30-cascade-and-parallel-forms",
    "title": "Signals and Systems",
    "section": "Example 9.30: Cascade and Parallel Forms",
    "text": "Example 9.30: Cascade and Parallel Forms\nFor \\(H(s)=\\frac{1}{(s+1)(s+2)}\\), other representations exist:\nCascade Form:\n\\(H(s)=\\left(\\frac{1}{s+1}\\right)\\left(\\frac{1}{s+2}\\right)\\) This is a series connection of two first-order systems.\n\n\n\nFigure 9.34(b)\n\n\nFigure 9.34(b): Cascade form.\nParallel Form:\nUsing partial-fraction expansion:\n\\(H(s)=\\frac{1}{s+1}-\\frac{1}{s+2}\\)\nThis is a parallel connection of two first-order systems.\n\n\n\nFigure 9.34(c)\n\n\nFigure 9.34(c): Parallel form.\n\nThe cascade and parallel forms offer alternative ways to implement the same system. The cascade form breaks down the system into a sequence of simpler, often first-order, systems. This can be advantageous for modular design and analysis. The parallel form decomposes the system into a sum of simpler systems, typically through partial fraction expansion. This form can be useful when different modes of the system (corresponding to the poles) need to be processed independently, or for stability analysis. All three forms (direct, cascade, parallel) are equivalent in terms of their input-output behavior for LTI systems, but they differ in internal structure, which can impact sensitivity to component variations, numerical stability in digital implementations, and computational complexity."
  },
  {
    "objectID": "ss_98.html#interactive-example-second-order-system-response",
    "href": "ss_98.html#interactive-example-second-order-system-response",
    "title": "Signals and Systems",
    "section": "Interactive Example: Second-Order System Response",
    "text": "Interactive Example: Second-Order System Response\nLet’s observe the step response of a second-order system and how its poles affect it.\nConsider \\(H(s) = \\frac{1}{s^2 + (p_1+p_2)s + p_1 p_2}\\).\n\nviewof pole_p1 = Inputs.range([0.1, 5.0], {value: 1.0, step: 0.1, label: \"Pole 1 (p1)\"});\nviewof pole_p2 = Inputs.range([0.1, 5.0], {value: 2.0, step: 0.1, label: \"Pole 2 (p2)\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjust the pole locations \\(p_1\\) and \\(p_2\\) using the sliders. Observe how changing these values affects the transient response characteristics such as rise time, settling time, and overshoot (though this system is overdamped, so no overshoot here). For instance, moving poles closer to the origin (smaller positive values) generally makes the system slower, while moving them further away makes it faster. This interactive tool helps build an intuitive understanding of the relationship between pole locations and time-domain behavior, which is critical in control system design."
  },
  {
    "objectID": "ss_98.html#example-9.31-more-complex-second-order-system",
    "href": "ss_98.html#example-9.31-more-complex-second-order-system",
    "title": "Signals and Systems",
    "section": "Example 9.31: More Complex Second-Order System",
    "text": "Example 9.31: More Complex Second-Order System\nConsider \\(H(s)=\\frac{2 s^{2}+4 s-6}{s^{2}+3 s+2}\\).\nThis system includes zeros in addition to poles.\nDirect Form:\nSimilar to Example 9.29, we can tap signals from the integrators to realize the numerator polynomial.\nThis method allows the coefficients of the system function to directly appear in the block diagram.\n\nFigure 9.35Figure 9.35: Direct-form representation.\n\nThis direct-form representation is very general and can be applied to any rational system function. The denominator coefficients determine the feedback path (poles), and the numerator coefficients determine the feed-forward path (zeros). This structure is widely used in digital signal processing for implementing IIR (Infinite Impulse Response) filters due to its computational efficiency. Understanding how to derive and interpret these block diagrams is crucial for both analyzing existing systems and synthesizing new ones."
  },
  {
    "objectID": "ss_98.html#conclusion-power-of-system-function-algebra",
    "href": "ss_98.html#conclusion-power-of-system-function-algebra",
    "title": "Signals and Systems",
    "section": "Conclusion: Power of System Function Algebra",
    "text": "Conclusion: Power of System Function Algebra\n\nAlgebraic Simplification: Laplace transform converts complex time-domain operations (convolution, differentiation) into simpler algebraic manipulations.\nSystem Interconnections: Provides straightforward methods to find the overall system function for parallel, series, and feedback configurations.\nBlock Diagram Synthesis: Enables visual representation of LTI systems using basic building blocks (integrators, gains, summers).\nMultiple Implementations: A single system function can have various block diagram forms (direct, cascade, parallel), each with practical implications for design and implementation.\n\n\n\n\n\n\n\nNote\n\n\nMastering system function algebra and block diagrams is fundamental for analyzing, designing, and implementing LTI systems in various ECE applications, from control systems to signal processing and communications."
  },
  {
    "objectID": "ss_71.html#introduction-to-sampling-the-digital-frontier",
    "href": "ss_71.html#introduction-to-sampling-the-digital-frontier",
    "title": "Signals and Systems",
    "section": "Introduction to Sampling: The Digital Frontier",
    "text": "Introduction to Sampling: The Digital Frontier\n\n\nSignals in the real world are often continuous-time (analog). However, modern systems (computers, digital communication) operate on discrete-time (digital) signals.\nSampling is the process of converting a continuous-time signal into a discrete-time sequence.\n\nWhy Sample?\n\nDigital processing advantages (noise immunity, flexibility).\nStorage and transmission efficiency.\nEnables powerful digital signal processing (DSP) algorithms.\n\n\n\n\n\n\n\n\nNote\n\n\nKey Question: How do we convert an analog signal to digital without losing information?\n\n\n\n\n\nFigure 7.1: Three continuous-time signals with identical values at integer multiples of \\(T\\).\n\n\nGood morning, everyone! Today we’re diving into a fundamental concept in Signals and Systems: the Sampling Theorem. This theorem is the bridge between the analog world we live in and the digital world our computers and smartphones operate in. We’ll explore why sampling is necessary, how it works, and what conditions are required to ensure we don’t lose crucial information during this conversion. As you can see in Figure 7.1, simply taking samples isn’t enough; many different continuous signals can produce the same set of discrete samples. Our goal is to understand how to make this conversion unique and reversible."
  },
  {
    "objectID": "ss_71.html#the-challenge-ambiguity-in-sampling",
    "href": "ss_71.html#the-challenge-ambiguity-in-sampling",
    "title": "Signals and Systems",
    "section": "The Challenge: Ambiguity in Sampling",
    "text": "The Challenge: Ambiguity in Sampling\nFrom Figure 7.1, it’s clear:\n\nAn infinite number of continuous-time signals can pass through the same discrete samples.\nWithout additional information, samples alone don’t uniquely define the original signal.\n\nOur Goal: Find conditions under which a signal can be uniquely specified by its samples, allowing perfect reconstruction.\n\n\n\n\n\n\nImportant\n\n\nThis unique specification is possible if the signal is band-limited and sampled at a sufficiently high rate."
  },
  {
    "objectID": "ss_71.html#impulse-train-sampling-the-model",
    "href": "ss_71.html#impulse-train-sampling-the-model",
    "title": "Signals and Systems",
    "section": "Impulse-Train Sampling: The Model",
    "text": "Impulse-Train Sampling: The Model\nTo analyze sampling mathematically, we use impulse-train sampling.\n\n\n\nContinuous Signal: \\(x(t)\\)\nSampling Function: A periodic impulse train \\(p(t)\\). \\[\np(t)=\\sum_{n=-\\infty}^{+\\infty} \\delta(t-n T)\n\\]\n\n\\(T\\): Sampling Period\n\\(\\omega_s = 2\\pi/T\\): Sampling Frequency\n\nSampled Signal: \\(x_p(t) = x(t)p(t)\\)\n\nUsing the sampling property of the impulse: \\[\nx_p(t)=\\sum_{n=-\\infty}^{+\\infty} x(n T) \\delta(t-n T)\n\\] This produces an impulse train where each impulse’s amplitude is the value of \\(x(t)\\) at the sampling instant \\(nT\\).\n\n\nFigure 7.2: Impulse-train sampling.\n\n\nImpulse-train sampling is a theoretical model that simplifies the analysis of sampling. We multiply our continuous signal \\(x(t)\\) by a train of impulses. Each impulse is like a tiny “snapshot” of the signal’s value at that specific time instant. The resulting signal, \\(x_p(t)\\), is a series of weighted impulses, where the weights are precisely the samples of the original signal. This representation is crucial because it allows us to analyze the sampling process in the frequency domain using the Fourier Transform."
  },
  {
    "objectID": "ss_71.html#sampling-in-the-frequency-domain-replicas",
    "href": "ss_71.html#sampling-in-the-frequency-domain-replicas",
    "title": "Signals and Systems",
    "section": "Sampling in the Frequency Domain: Replicas",
    "text": "Sampling in the Frequency Domain: Replicas\nThe magic happens in the frequency domain.\n\nMultiplication in the time domain corresponds to convolution in the frequency domain. \\[\nX_p(j\\omega) = \\frac{1}{2\\pi} [X(j\\omega) * P(j\\omega)]\n\\]\nThe Fourier Transform of the impulse train \\(p(t)\\) is also an impulse train in frequency: \\[\nP(j\\omega) = \\frac{2\\pi}{T} \\sum_{k=-\\infty}^{+\\infty} \\delta(\\omega - k\\omega_s)\n\\]\nConvolving \\(X(j\\omega)\\) with \\(P(j\\omega)\\) gives: \\[\nX_p(j\\omega) = \\frac{1}{T} \\sum_{k=-\\infty}^{+\\infty} X(j(\\omega - k\\omega_s))\n\\]\n\n\n\n\n\n\n\nTip\n\n\nInterpretation: The spectrum of the sampled signal, \\(X_p(j\\omega)\\), is an infinite sum of shifted and scaled replicas of the original signal’s spectrum, \\(X(j\\omega)\\). Each replica is centered at multiples of the sampling frequency \\(\\omega_s\\).\n\n\n\n\nWhen we move to the frequency domain, the multiplication of \\(x(t)\\) and \\(p(t)\\) becomes a convolution of their Fourier Transforms. Since \\(p(t)\\) is an impulse train, its Fourier Transform \\(P(j\\omega)\\) is also an impulse train, but in the frequency domain. The convolution operation then effectively creates multiple copies of the original signal’s spectrum, \\(X(j\\omega)\\), shifted by integer multiples of the sampling frequency, \\(\\omega_s\\). These are called spectral replicas. The key insight here is how these replicas interact."
  },
  {
    "objectID": "ss_71.html#aliasing-when-replicas-overlap",
    "href": "ss_71.html#aliasing-when-replicas-overlap",
    "title": "Signals and Systems",
    "section": "Aliasing: When Replicas Overlap",
    "text": "Aliasing: When Replicas Overlap\nThe relationship between the sampling frequency (\\(\\omega_s\\)) and the signal’s bandwidth (\\(\\omega_M\\)) is critical.\n\n\nCase 1: No Overlap (\\(\\omega_s &gt; 2\\omega_M\\))\n\nThe replicas of \\(X(j\\omega)\\) are perfectly separated.\nThe original spectrum \\(X(j\\omega)\\) can be recovered by an ideal lowpass filter.\n\nCase 2: Overlap (\\(\\omega_s &lt; 2\\omega_M\\))\n\nThe replicas overlap, causing aliasing.\nInformation is lost; the original signal cannot be perfectly reconstructed.\n\n\n\n\n\nFigure 7.3 (c) and (d): Spectrum of sampled signal with \\(\\omega_{s}&gt;2 \\omega_{M}\\) (no aliasing) and \\(\\omega_{s}&lt;2 \\omega_{M}\\) (aliasing).\n\n\n\n\nThis is where the concept of aliasing comes into play. If our sampling frequency \\(\\omega_s\\) is high enough, specifically greater than twice the highest frequency component in our original signal, \\(\\omega_M\\), then the spectral replicas are well-separated. We can then use a simple lowpass filter to isolate the original spectrum and perfectly reconstruct the signal. However, if \\(\\omega_s\\) is too low, the replicas overlap. This overlap is called aliasing, and it causes higher frequencies in the original signal to appear as lower frequencies in the sampled signal, leading to irreversible distortion."
  },
  {
    "objectID": "ss_71.html#interactive-exploring-aliasing",
    "href": "ss_71.html#interactive-exploring-aliasing",
    "title": "Signals and Systems",
    "section": "Interactive: Exploring Aliasing",
    "text": "Interactive: Exploring Aliasing\nAdjust the signal bandwidth (\\(\\omega_M\\)) and sampling frequency (\\(\\omega_s\\)) to observe aliasing.\n\nviewof signal_bandwidth = Inputs.range([0.1, 5.0], {value: 1.0, step: 0.1, label: \"Signal Bandwidth (ω_M)\"});\nviewof sampling_frequency = Inputs.range([0.1, 10.0], {value: 3.0, step: 0.1, label: \"Sampling Frequency (ω_s)\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive plot helps visualize the core concept of aliasing. The blue line represents the Fourier Transform of our original band-limited signal. The red dashed line shows the spectrum after impulse-train sampling. As you adjust the “Signal Bandwidth” (\\(\\omega_M\\)) and “Sampling Frequency” (\\(\\omega_s\\)), you can see how the replicas of the original spectrum shift and potentially overlap. Pay close attention to when the “Aliasing Occurring!” message appears – this is when \\(\\omega_s\\) drops below the Nyquist rate, \\(2\\omega_M\\), and reconstruction becomes impossible."
  },
  {
    "objectID": "ss_71.html#the-sampling-theorem",
    "href": "ss_71.html#the-sampling-theorem",
    "title": "Signals and Systems",
    "section": "The Sampling Theorem",
    "text": "The Sampling Theorem\nFormal Statement:\nLet \\(x(t)\\) be a band-limited signal with \\(X(j\\omega)=0\\) for \\(|\\omega| &gt; \\omega_M\\). Then \\(x(t)\\) is uniquely determined by its samples \\(x(nT), n=0, \\pm 1, \\pm 2, \\ldots\\), if\n\\[\n\\omega_s &gt; 2\\omega_M\n\\]\nwhere \\(\\omega_s = 2\\pi/T\\) is the sampling frequency.\n\n\n\n\n\n\nNote\n\n\nThe frequency \\(2\\omega_M\\) is known as the Nyquist rate. \\(\\omega_M\\) is often called the Nyquist frequency.\n\n\n\nReconstruction Process\n\nGenerate an impulse train \\(x_p(t)\\) from the samples.\nPass \\(x_p(t)\\) through an ideal lowpass filter with:\n\nGain \\(T\\)\nCutoff frequency \\(\\omega_c\\) such that \\(\\omega_M &lt; \\omega_c &lt; \\omega_s - \\omega_M\\).\n\n\nThe output signal will exactly equal \\(x(t)\\).\n\nThis is the formal statement of the Sampling Theorem. It provides the crucial condition for perfect reconstruction: the sampling frequency must be strictly greater than twice the highest frequency component in the signal. The term \\(2\\omega_M\\) is so important that it has its own name: the Nyquist rate. To reconstruct the signal, we essentially reverse the sampling process in the frequency domain. We take our sampled impulse train, whose spectrum contains the original spectrum and its replicas, and pass it through an ideal lowpass filter. This filter acts like a “sieve,” isolating just the original baseband spectrum and filtering out all the higher-frequency replicas. The gain \\(T\\) is necessary to restore the original amplitude scaling."
  },
  {
    "objectID": "ss_71.html#ideal-reconstruction-the-lowpass-filter",
    "href": "ss_71.html#ideal-reconstruction-the-lowpass-filter",
    "title": "Signals and Systems",
    "section": "Ideal Reconstruction: The Lowpass Filter",
    "text": "Ideal Reconstruction: The Lowpass Filter\n\n\nThe ideal reconstruction system:\n\n\n\n\n\ngraph LR\n    A[\"x(t)\"] --&gt; B{Sampler};\n    B --&gt; C[\"x_p(t)\"];\n    C --&gt; D{Ideal Lowpass Filter};\n    D --&gt; E[\"x_r(t) = x(t)\"];\n\n\n\n\n\n\nThe ideal lowpass filter \\(H(j\\omega)\\) has: \\[\nH(j\\omega) = \\begin{cases} T & |\\omega| &lt; \\omega_c \\\\ 0 & |\\omega| \\ge \\omega_c \\end{cases}\n\\] where \\(\\omega_M &lt; \\omega_c &lt; \\omega_s - \\omega_M\\).\nThis filter effectively isolates the baseband spectrum \\(X(j\\omega)\\) from its replicas, scaled by \\(T\\) to restore the original amplitude.\n\n\n\n\nFigure 7.4 (a) and (d): System for sampling and reconstruction, and the ideal lowpass filter.\n\n\n\n\nFigure 7.4 illustrates the complete system for sampling and reconstruction. The sampled signal \\(x_p(t)\\) goes through an ideal lowpass filter. This filter’s job is to perfectly cut off all the spectral replicas, leaving only the original signal’s spectrum in its passband. The gain of \\(T\\) is applied to counteract the \\(1/T\\) scaling introduced during sampling, ensuring the reconstructed signal has the correct amplitude. While ideal filters are not physically realizable, they provide a theoretical benchmark and are approximated by practical filters in real-world applications."
  },
  {
    "objectID": "ss_71.html#practical-sampling-zero-order-hold",
    "href": "ss_71.html#practical-sampling-zero-order-hold",
    "title": "Signals and Systems",
    "section": "Practical Sampling: Zero-Order Hold",
    "text": "Practical Sampling: Zero-Order Hold\nImpulse-train sampling is theoretical. In practice, a zero-order hold is often used.\n\n\n\nSamples \\(x(t)\\) at an instant and holds that value until the next sample.\nOutput \\(x_0(t)\\) is a staircase-like approximation of \\(x(t)\\).\n\n\n\n\nFigure 7.5: Sampling utilizing a zero-order hold.\n\n\nRepresentation: A zero-order hold can be modeled as impulse-train sampling followed by an LTI system with a rectangular impulse response \\(h_0(t)\\): \\[\nh_0(t) = \\begin{cases} 1 & 0 \\le t &lt; T \\\\ 0 & \\text{otherwise} \\end{cases}\n\\]\n\n\n\n\nFigure 7.6: Zero-order hold as impulse-train sampling followed by an LTI system with a rectangular impulse response.\n\n\n\n\nWhile impulse-train sampling is great for theoretical analysis, it’s not what physically happens in most Analog-to-Digital Converters (ADCs). A more practical approach is the zero-order hold. Imagine a sample-and-hold circuit: it takes a sample and then holds that voltage level constant until the next sample arrives. This creates a staircase-like approximation of the original signal. We can model this by first performing impulse-train sampling and then passing the resulting impulses through a system with a rectangular impulse response, essentially “stretching” each impulse into a pulse of duration \\(T\\)."
  },
  {
    "objectID": "ss_71.html#zero-order-hold-reconstruction",
    "href": "ss_71.html#zero-order-hold-reconstruction",
    "title": "Signals and Systems",
    "section": "Zero-Order Hold: Reconstruction",
    "text": "Zero-Order Hold: Reconstruction\nTo reconstruct \\(x(t)\\) from \\(x_0(t)\\), we need a specific filter.\n\n\nThe Fourier Transform of \\(h_0(t)\\) is: \\[\nH_0(j\\omega) = e^{-j\\omega T/2} \\left[ \\frac{2\\sin(\\omega T/2)}{\\omega} \\right]\n\\] This is a sinc function in frequency.\nTo reconstruct \\(x(t)\\), we need to compensate for \\(H_0(j\\omega)\\). The reconstruction filter \\(H_r(j\\omega)\\) must satisfy: \\[\nH_r(j\\omega) H_0(j\\omega) = H_{ideal}(j\\omega)\n\\] Where \\(H_{ideal}(j\\omega)\\) is the ideal lowpass filter with gain \\(T\\).\nThus, the required reconstruction filter is: \\[\nH_r(j\\omega) = \\frac{e^{j\\omega T/2} H_{ideal}(j\\omega)}{\\frac{2\\sin(\\omega T/2)}{\\omega}}\n\\]\n\n\n\n\nFigure 7.8: Magnitude and phase for the reconstruction filter for a zero-order hold.\n\n\n\n\n\n\n\n\nCaution\n\n\nThis reconstruction filter is complex to implement due to the sinc inverse in the denominator and the non-constant gain in the passband.\n\n\n\n\n\nSince the zero-order hold introduces its own frequency response, \\(H_0(j\\omega)\\), during the sampling process, our reconstruction filter needs to undo that effect in addition to performing the lowpass filtering. This means the reconstruction filter \\(H_r(j\\omega)\\) will have a more complex characteristic than a simple ideal lowpass filter. As shown in Figure 7.8, its magnitude is not flat, and its phase is not linear, making it more challenging to implement in practice compared to the ideal lowpass filter for impulse-train sampling. Often, in many applications, the output of the zero-order hold itself is considered a sufficient approximation, or more sophisticated interpolation methods are used."
  },
  {
    "objectID": "ss_71.html#interactive-zero-order-hold-in-time-domain",
    "href": "ss_71.html#interactive-zero-order-hold-in-time-domain",
    "title": "Signals and Systems",
    "section": "Interactive: Zero-Order Hold in Time Domain",
    "text": "Interactive: Zero-Order Hold in Time Domain\nObserve how a continuous signal is approximated by a zero-order hold for different sampling periods.\n\nviewof signal_freq = Inputs.range([0.1, 2.0], {value: 0.5, step: 0.1, label: \"Signal Frequency (Hz)\"});\nviewof sampling_period = Inputs.range([0.05, 1.0], {value: 0.2, step: 0.01, label: \"Sampling Period (T)\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive plot demonstrates the zero-order hold in the time domain. The blue curve is our original continuous-time signal, a simple sine wave. The red markers are the discrete samples taken at intervals of \\(T\\). The green dotted line shows the output of the zero-order hold. As you decrease the “Sampling Period (T)”, which means increasing the sampling frequency, you’ll notice that the staircase-like approximation of the zero-order hold becomes much smoother and more closely resembles the original signal. This visually reinforces the idea that a higher sampling rate generally leads to a better approximation and easier reconstruction."
  },
  {
    "objectID": "ss_71.html#conclusion-applications",
    "href": "ss_71.html#conclusion-applications",
    "title": "Signals and Systems",
    "section": "Conclusion & Applications",
    "text": "Conclusion & Applications\nKey Takeaways:\n\nSampling Theory provides the mathematical foundation for converting analog to digital signals.\nThe Sampling Theorem states that a band-limited signal can be perfectly reconstructed from its samples if the sampling frequency \\(\\omega_s\\) is greater than the Nyquist rate (\\(2\\omega_M\\)).\nAliasing occurs when the sampling rate is too low, leading to irreversible loss of information.\nZero-Order Hold is a practical method for sampling, requiring a more complex reconstruction filter.\n\nReal-World ECE Applications:\n\nAnalog-to-Digital Converters (ADCs): Crucial components in almost all digital systems (audio, video, sensors).\nDigital Audio: CDs (44.1 kHz sampling rate) and high-resolution audio.\nDigital Image Processing: Pixels are samples of continuous light intensity.\nTelecommunications: Digital modulation and demodulation.\nControl Systems: Sampling sensor data for feedback.\n\n\n\n\n\n\n\nTip\n\n\nUnderstanding the Sampling Theorem is fundamental for designing robust and accurate digital signal processing systems.\n\n\n\n\nIn summary, the Sampling Theorem is a cornerstone of modern ECE. It tells us not just that we can convert analog signals to digital, but how to do it correctly to avoid losing information. From the music you listen to, to the images on your screen, to the complex control systems in modern machinery, the principles of sampling are at play everywhere. Ignoring the Nyquist rate can lead to significant problems, such as distorted audio or inaccurate sensor readings. So, while the math can seem abstract, its practical implications are vast and critical to virtually every aspect of electrical and computer engineering."
  },
  {
    "objectID": "index.html#week-10",
    "href": "index.html#week-10",
    "title": "Signals and Systems",
    "section": "Week 10",
    "text": "Week 10\n\nSignals and Systems 9.7: ANALYSIS AND CHARACTERIZATION OF LTI SYSTEMS USING THE LAPLACE TRANSFORM\nSignals and Systems 9.8: SYSTEM FUNCTION ALGEBRA AND BLOCK DIAGRAM REPRESENTATIONS\nSignals and Systems 7.1: REPRESENTATION OF A CONTINUOUS-TIME SIGNAL BY ITS SAMPLES: THE SAMPLING THEOREM"
  },
  {
    "objectID": "index.html#week-11",
    "href": "index.html#week-11",
    "title": "Signals and Systems",
    "section": "Week 11",
    "text": "Week 11\n\nUTS - Asesmen CPMK 2"
  }
]