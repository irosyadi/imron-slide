[
  {
    "objectID": "ss_3x.html#fourier-analysis-deconstructing-signals",
    "href": "ss_3x.html#fourier-analysis-deconstructing-signals",
    "title": "Signals and Systems",
    "section": "Fourier Analysis: Deconstructing Signals",
    "text": "Fourier Analysis: Deconstructing Signals\nImron Rosyadi"
  },
  {
    "objectID": "ss_3x.html#the-power-of-complex-exponentials",
    "href": "ss_3x.html#the-power-of-complex-exponentials",
    "title": "Signals and Systems",
    "section": "1. The Power of Complex Exponentials",
    "text": "1. The Power of Complex Exponentials\nRecall from our previous discussion that complex exponentials are eigenfunctions of LTI systems.\n\n\nContinuous-Time \\(x(t) = e^{st} \\quad \\xrightarrow{\\text{LTI System}} \\quad y(t) = H(s) e^{st}\\)\n\nDiscrete-Time \\(x[n] = z^n \\quad \\xrightarrow{\\text{LTI System}} \\quad y[n] = H(z) z^n\\)\n\nThis means:\n\nLTI systems only scale complex exponentials, they don’t change their fundamental form.\nThe system’s behavior is fully characterized by \\(H(s)\\) or \\(H(z)\\) at specific values of \\(s\\) or \\(z\\).\n\nThe BIG Question: If we can represent any signal as a sum (or integral) of these simple complex exponentials, then analyzing complex LTI systems becomes a simple matter of multiplication.\nThis is exactly what Fourier Analysis enables!\n\nStart by reminding students of the crucial eigenfunction property. This sets the stage for why Fourier analysis is so powerful. It’s not just a mathematical curiosity; it’s a direct consequence of how LTI systems behave. The fundamental insight is that if we can decompose any signal into these elementary complex exponential building blocks, then the analysis of complex signals through LTI systems transforms from difficult convolution operations in the time domain to simple algebraic multiplications in the frequency domain. This is the cornerstone of frequency-domain analysis in ECE."
  },
  {
    "objectID": "ss_3x.html#a-glimpse-into-history-joseph-fourier",
    "href": "ss_3x.html#a-glimpse-into-history-joseph-fourier",
    "title": "Signals and Systems",
    "section": "2. A Glimpse into History: Joseph Fourier",
    "text": "2. A Glimpse into History: Joseph Fourier\nJoseph Fourier (1768-1830)\n\n\n\n\n\n\n\nMathematica, quae ad calorem deduxi, sunt mihi graviora. (The mathematics that I have derived concerning heat, are to me of greater importance.) — Joseph Fourier\n\n\nFrench mathematician and physicist.\nBest known for initiating the investigation of Fourier series and their applications to problems of heat transfer and vibrations.\nOriginal Motivation: Solving the one-dimensional heat equation (1807). He proposed that any continuous function could be represented as a sum of sines and cosines.\nControversial Idea: His assertion that any function could be represented as such was highly controversial among his peers (Lagrange, Laplace), as the concept of “function” was very narrow at the time.\n\nImpact: Revolutionized mathematics and physics, enabling frequency-domain analysis.\n\n\nIt’s important to provide historical context. Understanding that Fourier’s ideas were revolutionary and even controversial helps students appreciate the depth and impact of his work. His motivation came from a practical physical problem (heat conduction), illustrating how theoretical breakthroughs often arise from engineering challenges. Emphasize that the general idea of decomposing a function into a sum of simple terms was not new, but Fourier’s specific claim about trigonometric functions for any function was radical at the time."
  },
  {
    "objectID": "ss_3x.html#fourier-series-representing-periodic-signals",
    "href": "ss_3x.html#fourier-series-representing-periodic-signals",
    "title": "Signals and Systems",
    "section": "3. Fourier Series: Representing Periodic Signals",
    "text": "3. Fourier Series: Representing Periodic Signals\nThe Fourier Series allows us to represent a periodic signal \\(x(t)\\) with period \\(T_0\\) (and fundamental frequency \\(\\omega_0 = 2\\pi/T_0\\)) as a weighted sum of harmonically related complex exponentials.\n3.1 Complex Exponential Fourier Series\n\\[\nx(t) = \\sum_{k=-\\infty}^{\\infty} c_k e^{jk\\omega_0 t}\n\\] where the coefficients \\(c_k\\) are given by: \\[\nc_k = \\frac{1}{T_0} \\int_{T_0} x(t) e^{-jk\\omega_0 t} dt\n\\] (The integral is over any single period \\(T_0\\))."
  },
  {
    "objectID": "ss_3x.html#fourier-series-representing-periodic-signals-1",
    "href": "ss_3x.html#fourier-series-representing-periodic-signals-1",
    "title": "Signals and Systems",
    "section": "3. Fourier Series: Representing Periodic Signals",
    "text": "3. Fourier Series: Representing Periodic Signals\n3.2 Trigonometric Fourier Series (Alternative Form)\n\\[\nx(t) = a_0 + \\sum_{k=1}^{\\infty} (a_k \\cos(k\\omega_0 t) + b_k \\sin(k\\omega_0 t))\n\\] where \\(a_0 = c_0 = \\frac{1}{T_0} \\int_{T_0} x(t) dt\\) and \\(c_k = \\frac{1}{2}(a_k - jb_k)\\). (For real signals: \\(a_k = 2 \\text{Re}\\{c_k\\}\\), \\(b_k = -2 \\text{Im}\\{c_k\\}\\)).\nKey Idea: Any periodic signal can be decomposed into a sum of a DC component, a fundamental frequency component, and components at integer multiples (harmonics) of the fundamental frequency.\n\nIntroduce both the complex exponential and trigonometric forms. Emphasize that the complex exponential form is often more mathematically convenient, especially when dealing with LTI systems due to the eigenfunction property. Explain the concept of “harmonics” – components at integer multiples of the fundamental frequency. The coefficients \\(c_k\\), \\(a_k\\), \\(b_k\\) represent the “strength” or “amplitude” of each frequency component present in the signal. The integral formula for \\(c_k\\) is essentially a correlation, telling us how much of \\(e^{jk\\omega_0t}\\) is “in” \\(x(t)\\)."
  },
  {
    "objectID": "ss_3x.html#interactive-demo-fourier-series-synthesis-of-a-square-wave",
    "href": "ss_3x.html#interactive-demo-fourier-series-synthesis-of-a-square-wave",
    "title": "Signals and Systems",
    "section": "4. Interactive Demo: Fourier Series Synthesis of a Square Wave",
    "text": "4. Interactive Demo: Fourier Series Synthesis of a Square Wave\nLet’s synthesize a square wave by summing its Fourier Series components. A square wave is rich in odd harmonics.\n\\[\nx(t)_{\\text{square}} = \\frac{4}{\\pi} \\sum_{k \\text{ odd}, k \\ge 1} \\frac{1}{k} \\sin(k\\omega_0 t)\n\\]\nAdjust the number of Harmonics to see how well the approximation matches the ideal square wave.\n\n\n                            \n                                            \n\n\n\nThis interactive demo is crucial for building intuition. Students can visually see how adding more harmonics progressively refines the approximation of the square wave. Point out the Gibbs phenomenon (overshoots at discontinuities) as an interesting artifact. Explain that signals with sharp transitions or discontinuities require many harmonics to be accurately represented, while smooth signals might require fewer. This provides a direct link between a signal’s time-domain characteristics and its frequency-domain representation."
  },
  {
    "objectID": "ss_3x.html#fourier-transform-for-aperiodic-signals",
    "href": "ss_3x.html#fourier-transform-for-aperiodic-signals",
    "title": "Signals and Systems",
    "section": "5. Fourier Transform: For Aperiodic Signals",
    "text": "5. Fourier Transform: For Aperiodic Signals\nFourier Series applies only to periodic signals. What about non-periodic, transient signals? The Fourier Transform (FT) extends this concept to aperiodic signals.\n5.1 From Fourier Series to Fourier Transform (Conceptual)\nImagine a periodic signal whose period \\(T_0\\) approaches infinity (\\(T_0 \\to \\infty\\)). As \\(T_0 \\to \\infty\\):\n\nThe fundamental frequency \\(\\omega_0 = 2\\pi/T_0 \\to 0\\).\nThe discrete sum over harmonics becomes a continuous integral over frequency.\nThe Fourier coefficients \\(c_k\\) become a continuous function of frequency, \\(X(j\\omega)\\)."
  },
  {
    "objectID": "ss_3x.html#fourier-transform-for-aperiodic-signals-1",
    "href": "ss_3x.html#fourier-transform-for-aperiodic-signals-1",
    "title": "Signals and Systems",
    "section": "5. Fourier Transform: For Aperiodic Signals",
    "text": "5. Fourier Transform: For Aperiodic Signals\n5.2 The Fourier Transform Pair\nForward Fourier Transform: Transforms a time-domain signal \\(x(t)\\) into its frequency-domain representation \\(X(j\\omega)\\). \\[\nX(j\\omega) = \\int_{-\\infty}^{\\infty} x(t) e^{-j\\omega t} dt\n\\]\nInverse Fourier Transform: Transforms a frequency-domain spectrum \\(X(j\\omega)\\) back into its time-domain signal \\(x(t)\\). \\[\nx(t) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} X(j\\omega) e^{j\\omega t} d\\omega\n\\]\nInterpretation: \\(X(j\\omega)\\) is the spectrum of the signal \\(x(t)\\), showing how much of each frequency \\(\\omega\\) is present in the signal.\n\nTransition from FS to FT by explaining the limiting process where the period goes to infinity. This helps students grasp the conceptual continuity. Emphasize the interpretation of \\(X(j\\omega)\\) as the “frequency spectrum” – it’s crucial for understanding applications. \\(X(j\\omega)\\) is a complex-valued function and can be visualized by its magnitude \\(|X(j\\omega)|\\) (how much of that frequency is present) and its phase \\(\\angle X(j\\omega)\\) (relative phase of that frequency component)."
  },
  {
    "objectID": "ss_3x.html#key-properties-of-the-fourier-transform-in-lti-systems",
    "href": "ss_3x.html#key-properties-of-the-fourier-transform-in-lti-systems",
    "title": "Signals and Systems",
    "section": "6. Key Properties of the Fourier Transform in LTI Systems",
    "text": "6. Key Properties of the Fourier Transform in LTI Systems\nThe FT reveals powerful properties crucial for ECE:\n\nLinearity: If \\(\\mathcal{F}\\{x_1(t)\\} = X_1(j\\omega)\\) and \\(\\mathcal{F}\\{x_2(t)\\} = X_2(j\\omega)\\), then \\(\\mathcal{F}\\{ax_1(t) + bx_2(t)\\} = aX_1(j\\omega) + bX_2(j\\omega)\\)\nTime Shift: If \\(\\mathcal{F}\\{x(t)\\} = X(j\\omega)\\), then \\(\\mathcal{F}\\{x(t-t_0)\\} = e^{-j\\omega t_0} X(j\\omega)\\) (A time shift in the time domain corresponds to a phase shift in the frequency domain).\nConvolution Property (THE MOST IMPORTANT FOR LTI): If \\(y(t) = x(t) * h(t)\\) (convolution in time domain), then \\(\\mathcal{F}\\{y(t)\\} = Y(j\\omega) = X(j\\omega) H(j\\omega)\\) (Convolution in time domain becomes multiplication in frequency domain!)\nHere, \\(H(j\\omega) = \\mathcal{F}\\{h(t)\\}\\) is the frequency response of the LTI system."
  },
  {
    "objectID": "ss_3x.html#key-properties-of-the-fourier-transform-in-lti-systems-1",
    "href": "ss_3x.html#key-properties-of-the-fourier-transform-in-lti-systems-1",
    "title": "Signals and Systems",
    "section": "6. Key Properties of the Fourier Transform in LTI Systems",
    "text": "6. Key Properties of the Fourier Transform in LTI Systems\n\n\n\n\n\ngraph TD\n    A[Time Domain] -- \"Convolution: x(t) * h(t)\" --&gt; B[\"Time Domain Result: y(t)\"]\n\n    C[Frequency Domain] -- \"FT: X(j&omega;), H(j&omega;)\" --&gt; D[\"Frequency Domain Result: Y(j&omega;)\"]\n\n    A -- FT --&gt; C\n    B -- FT --&gt; D\n  \n    C -- Multiplication: X(j&omega;) * H(j&omega;) --&gt; D\n\n    style A fill:#a2c4c9,stroke:#333,stroke-width:2px;\n    style B fill:#a2c4c9,stroke:#333,stroke-width:2px;\n    style C fill:#fce5cd,stroke:#333,stroke-width:2px;\n    style D fill:#fce5cd,stroke:#333,stroke-width:2px;\n\n    linkStyle 0 stroke:#666,stroke-width:2px;\n    linkStyle 1 stroke:#333,stroke-width:3px,color:red;\n    linkStyle 2 stroke:#666,stroke-width:2px;\n    linkStyle 3 stroke:#666,stroke-width:2px;\n    linkStyle 4 stroke:#333,stroke-width:3px,color:blue;\n\n\n\n\n\n\n\nHighlight these properties, especially the convolution property. This is the “AHA!” moment for LTI systems. Explain that calculating convolution in the time domain (an integral or sum) can be computationally intensive and conceptually opaque. Transforming to the frequency domain allows us to simply multiply the input spectrum by the system’s frequency response, and then inverse transform the result. This transforms a complex operation into two transforms and a simple multiplication, providing immense analytical and computational benefits."
  },
  {
    "objectID": "ss_3x.html#interactive-demo-filtering-in-the-frequency-domain",
    "href": "ss_3x.html#interactive-demo-filtering-in-the-frequency-domain",
    "title": "Signals and Systems",
    "section": "7. Interactive Demo: Filtering in the Frequency Domain",
    "text": "7. Interactive Demo: Filtering in the Frequency Domain\nLet’s apply a simple Low-Pass Filter (LPF) to a signal composed of two sine waves with different frequencies.\nObserve:\n\nThe input signal’s time-domain plot and its frequency spectrum.\nThe filter’s frequency response (magnitude).\nThe output signal’s time-domain plot and its frequency spectrum.\n\nAdjust the Cutoff Frequency of the filter. See how the filter attenuates higher frequencies while passing lower frequencies.\n\n\n                            \n                                            \n\n\n\nThis is another critical interactive demo. It visually demonstrates the action of a filter directly in both the time and frequency domains. As students move the cutoff frequency, they should observe: 1. The input signal contains both low (0.5 Hz) and high (3.0 Hz) frequency components. 2. The filter’s frequency response is a “gate” that passes frequencies below the cutoff and blocks those above. 3. The output signal’s spectrum clearly shows which frequencies were passed and which were attenuated. 4. The output signal in the time domain visibly changes, retaining the lower frequency components and smoothing out the higher ones. This directly illustrates the meaning of \\(Y(j\\omega) = X(j\\omega)H(j\\omega)\\) in a dynamic way."
  },
  {
    "objectID": "ss_3x.html#ece-applications-of-fourier-analysis",
    "href": "ss_3x.html#ece-applications-of-fourier-analysis",
    "title": "Signals and Systems",
    "section": "8. ECE Applications of Fourier Analysis",
    "text": "8. ECE Applications of Fourier Analysis\nFourier Analysis is fundamental to almost every area of Electrical and Computer Engineering.\n1. Communication Systems\n\nModulation/Demodulation: Shifting a signal’s spectrum to a different carrier frequency for transmission (radio, WiFi).\nMultiplexing: Combining multiple signals into one channel by allocating different frequency bands.\nSpectrum Analysis: Understanding bandwidth requirements, interference.\n\n2. Audio & Speech Processing\n\nEqualizers: Boosting or cutting specific frequency ranges.\nNoise Reduction: Filtering out unwanted frequency components.\nCompression (e.g., MP3): Discarding inaudible frequency components.\n\n3. Image & Video Processing\n\nFiltering: Sharpening (high-pass), blurring (low-pass), edge detection.\nCompression (e.g., JPEG): Representing images efficiently in the frequency domain."
  },
  {
    "objectID": "ss_3x.html#ece-applications-of-fourier-analysis-1",
    "href": "ss_3x.html#ece-applications-of-fourier-analysis-1",
    "title": "Signals and Systems",
    "section": "8. ECE Applications of Fourier Analysis",
    "text": "8. ECE Applications of Fourier Analysis\n4. Circuit Analysis\n\nAC Steady-State Analysis: Transforming differential equations into algebraic equations in the frequency domain (phasors).\nFilter Design: Designing circuits that pass or block specific frequencies (e.g., Butterworth, Chebyshev filters).\n\n5. Control Systems\n\nFrequency Response Analysis: Assessing system stability and performance by analyzing how different input frequencies are processed.\nSystem Identification: Determining a system’s characteristics by observing its response to various frequencies.\n\n6. Digital Signal Processing (DSP)\n\nDiscrete Fourier Transform (DFT) / Fast Fourier Transform (FFT): Efficient algorithms for computing the FT on discrete (sampled) data, enabling real-time applications.\n\n\nDedicate a slide to the sheer breadth of applications. This motivates students and shows them how theoretical concepts translate directly into real-world technologies they encounter daily. Emphasize that Fourier analysis is not just a mathematical tool, but a core component of how these systems are designed, analyzed, and implemented. Mention the FFT as the computational backbone that made many of these applications practical."
  },
  {
    "objectID": "ss_3x.html#conclusion-the-ubiquity-of-fourier",
    "href": "ss_3x.html#conclusion-the-ubiquity-of-fourier",
    "title": "Signals and Systems",
    "section": "9. Conclusion: The Ubiquity of Fourier",
    "text": "9. Conclusion: The Ubiquity of Fourier\n\nDecomposition: Fourier analysis provides a powerful framework to decompose complex signals into simpler, understandable frequency components (complex exponentials).\nSimplification: It transforms complex time-domain operations (like convolution) into simpler frequency-domain operations (like multiplication) for LTI systems.\nInsight: It offers deep insights into phenomena related to frequency, resonance, filtering, and system behavior.\n\nIn essence, Fourier Analysis is the lens through which engineers view and manipulate signals and systems in the frequency domain.\nIt is a cornerstone of modern electrical and computer engineering, underpinning technologies from your smartphone to medical imaging, and from radar to robotics."
  },
  {
    "objectID": "ss_3x.html#conclusion-the-ubiquity-of-fourier-1",
    "href": "ss_3x.html#conclusion-the-ubiquity-of-fourier-1",
    "title": "Signals and Systems",
    "section": "9. Conclusion: The Ubiquity of Fourier",
    "text": "9. Conclusion: The Ubiquity of Fourier\n\n\n\n\n\ngraph TD\n    A[Complex Time-Domain Signal] --&gt; B{Fourier Transform}\n    B --&gt; C[Simple Frequency Components]\n    C --&gt; D[\"LTI System Analysis: Multiplication by H(j&omega;)\"]\n    D --&gt; E[Output Frequency Components]\n    E --&gt; F{Inverse Fourier Transform}\n    F --&gt; G[System Output in Time-Domain]\n\n    style A fill:#DDA0DD,stroke:#333,stroke-width:2px;\n    style G fill:#DDA0DD,stroke:#333,stroke-width:2px;\n    style C fill:#ADD8E6,stroke:#333,stroke-width:2px;\n    style E fill:#ADD8E6,stroke:#333,stroke-width:2px;\n    style B fill:#8FBC8F,stroke:#333,stroke-width:2px;\n    style F fill:#8FBC8F,stroke:#333,stroke-width:2px;\n    style D fill:#F4A460,stroke:#333,stroke-width:2px;\n\n\n\n\n\n\n\nSummarize the key takeaways and reinforce why Fourier analysis is indispensable. Reiterate that it’s a fundamental paradigm shift in how we approach signals and systems. The final Mermaid diagram provides a visual conceptual flow from time to frequency and back, showing where the ‘magic’ of simplification happens. End on an inspiring note connecting the theory back to everyday technology."
  },
  {
    "objectID": "ss_35.html#introduction-to-fourier-series-properties",
    "href": "ss_35.html#introduction-to-fourier-series-properties",
    "title": "Signals and Systems",
    "section": "Introduction to Fourier Series Properties",
    "text": "Introduction to Fourier Series Properties\nFourier series representations possess important properties useful for conceptual insights and simplifying calculations.\nThese properties help us understand how operations on a signal in the time domain affect its Fourier series coefficients.\n\nToday, we’ll dive into the fundamental properties of Continuous-Time Fourier Series. Understanding these properties is crucial because they not only provide deeper insights into signal behavior but also offer powerful shortcuts for analyzing complex signals, often avoiding direct, tedious calculations."
  },
  {
    "objectID": "ss_35.html#shorthand-notation-for-fourier-series",
    "href": "ss_35.html#shorthand-notation-for-fourier-series",
    "title": "Signals and Systems",
    "section": "Shorthand Notation for Fourier Series",
    "text": "Shorthand Notation for Fourier Series\nTo simplify discussion, we use a shorthand notation.\nIf \\(x(t)\\) is a periodic signal with period \\(T\\) and fundamental frequency \\(\\omega_0 = 2\\pi/T\\), and its Fourier series coefficients are \\(a_k\\), we write:\n\\[\nx(t) \\stackrel{\\mathcal{F S}}{\\longleftrightarrow} a_{k}\n\\]\n\nThis notation, \\(x(t)\\) double-arrow FS \\(a_k\\), simply means that \\(a_k\\) are the complex Fourier series coefficients corresponding to the periodic signal \\(x(t)\\). It’s a convenient way to express the Fourier series pair without writing out the full synthesis or analysis equations every time."
  },
  {
    "objectID": "ss_35.html#linearity",
    "href": "ss_35.html#linearity",
    "title": "Signals and Systems",
    "section": "Linearity",
    "text": "Linearity\nIf \\(x(t)\\) and \\(y(t)\\) are periodic with period \\(T\\), with coefficients \\(a_k\\) and \\(b_k\\) respectively:\n\\[\n\\begin{aligned}\n& x(t) \\stackrel{\\mathcal{F S}}{\\longleftrightarrow} a_{k}, \\\\\n& y(t) \\stackrel{\\mathcal{F S}}{\\longleftrightarrow} b_{k} .\n\\end{aligned}\n\\]\nThen, a linear combination of these signals, \\(z(t) = A x(t) + B y(t)\\), will have Fourier series coefficients \\(c_k\\):\n\\[\nz(t)=A x(t)+B y(t) \\stackrel{\\mathcal{F S}}{\\longleftrightarrow} c_{k}=A a_{k}+B b_{k}\n\\]\n\nLinearity is one of the most fundamental properties. It tells us that if we combine signals in a linear way, their Fourier coefficients combine in the exact same linear way. This is incredibly useful because it allows us to break down complex signals into simpler components, analyze each component separately, and then combine the results. The proof follows directly from the definition of the Fourier series analysis equation."
  },
  {
    "objectID": "ss_35.html#linearity-interactive-demonstration",
    "href": "ss_35.html#linearity-interactive-demonstration",
    "title": "Signals and Systems",
    "section": "Linearity: Interactive Demonstration",
    "text": "Linearity: Interactive Demonstration\nLet’s see linearity in action with two simple periodic signals.\n\n\n\n\n\n\n\nIn this Python example, we simulate the Fourier coefficients for two simple signals, a sine wave and a cosine wave. We then apply arbitrary scaling factors, A and B, to combine these signals. The output clearly shows that the coefficients of the combined signal are simply the scaled sum of the individual coefficients, confirming the linearity property. While this uses hypothetical coefficients for simplicity, it captures the essence."
  },
  {
    "objectID": "ss_35.html#time-shifting",
    "href": "ss_35.html#time-shifting",
    "title": "Signals and Systems",
    "section": "Time Shifting",
    "text": "Time Shifting\nWhen a periodic signal \\(x(t)\\) is shifted in time by \\(t_0\\) to become \\(y(t) = x(t-t_0)\\), its period \\(T\\) remains unchanged.\nThe new Fourier series coefficients \\(b_k\\) are related to the original coefficients \\(a_k\\) by:\n\\[\nx\\left(t-t_{0}\\right) \\stackrel{\\mathcal{F S}}{\\longleftrightarrow} e^{-j k \\omega_{0} t_{0}} a_{k}=e^{-j k(2 \\pi / T) t_{0}} a_{k}\n\\]\n\n\n\n\n\n\nTip\n\n\nKey Insight: Time shifting a signal only changes the phase of its Fourier coefficients, not their magnitudes.\n\n\n\n\nTime shifting is a very intuitive operation. If you take a periodic signal and just slide it along the time axis, its fundamental frequency and the ‘strength’ of its harmonics don’t change. What does change is the starting phase of each harmonic component. The exponential term, \\(e^{-j k \\omega_{0} t_{0}}\\), represents this phase shift, which is linear with the harmonic index \\(k\\). This property is crucial for understanding delays and propagation effects in systems."
  },
  {
    "objectID": "ss_35.html#time-shifting-interactive-visualization",
    "href": "ss_35.html#time-shifting-interactive-visualization",
    "title": "Signals and Systems",
    "section": "Time Shifting: Interactive Visualization",
    "text": "Time Shifting: Interactive Visualization\nObserve a square wave and its time-shifted version. Notice how the magnitude of coefficients remains the same, but the phase changes.\n\nviewof t0_slider = Inputs.range([-2, 2], {\n  label: \"Shift Amount (t0)\",\n  step: 0.1,\n  value: 0\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive plot demonstrates the time-shifting property. The top panel shows the original square wave and its shifted version. As you adjust the Shift Amount (t0) slider, you’ll see the red dashed line (the shifted signal) move. Crucially, look at the middle panel: the magnitudes of the Fourier coefficients for both signals are identical. However, in the bottom panel, the phases of the shifted signal’s coefficients exhibit a linear change with respect to the harmonic index \\(k\\), which is exactly what the formula \\(e^{-j k \\omega_{0} t_{0}}\\) predicts."
  },
  {
    "objectID": "ss_35.html#time-reversal",
    "href": "ss_35.html#time-reversal",
    "title": "Signals and Systems",
    "section": "Time Reversal",
    "text": "Time Reversal\nWhen a periodic signal \\(x(t)\\) undergoes time reversal to become \\(y(t) = x(-t)\\), its period \\(T\\) remains unchanged.\nThe new Fourier series coefficients \\(b_k\\) are related to the original coefficients \\(a_k\\) by:\n\\[\nx(-t) \\stackrel{\\mathcal{F S}}{\\longleftrightarrow} a_{-k}\n\\]\n\n\n\n\n\n\nNote\n\n\nSymmetry Implication:\nIf \\(x(t)\\) is even (\\(x(-t) = x(t)\\)), then \\(a_k = a_{-k}\\).\nIf \\(x(t)\\) is odd (\\(x(-t) = -x(t)\\)), then \\(a_k = -a_{-k}\\).\n\n\n\n\nTime reversal flips the signal about the vertical axis at \\(t=0\\). The property states that the Fourier coefficients are also ‘flipped’ in their index. So the \\(k\\)-th coefficient of \\(x(-t)\\) is the negative \\(k\\)-th coefficient of \\(x(t)\\). This has direct implications for symmetric signals. For an even signal, the coefficients are also even; for an odd signal, the coefficients are odd. This can save a lot of calculation if you recognize the symmetry."
  },
  {
    "objectID": "ss_35.html#time-reversal-interactive-visualization",
    "href": "ss_35.html#time-reversal-interactive-visualization",
    "title": "Signals and Systems",
    "section": "Time Reversal: Interactive Visualization",
    "text": "Time Reversal: Interactive Visualization\nObserve an asymmetric periodic signal and its time-reversed version.\n\n\n\n\n\n\n\nHere, we visualize time reversal using a sawtooth wave, which is an asymmetric signal. The top plot shows the original signal and its flipped version. In the coefficient plots, you’ll notice that the magnitudes are symmetric (\\(|a_k| = |a_{-k}|\\)), but the phases are anti-symmetric (\\(\\angle a_k = -\\angle a_{-k}\\)), reflecting the \\(a_{-k}\\) relationship. This clearly illustrates that reversing a signal in time corresponds to ‘reversing’ the indices of its Fourier coefficients."
  },
  {
    "objectID": "ss_35.html#time-scaling",
    "href": "ss_35.html#time-scaling",
    "title": "Signals and Systems",
    "section": "Time Scaling",
    "text": "Time Scaling\nIf \\(x(t)\\) is periodic with period \\(T\\) and fundamental frequency \\(\\omega_0\\), then \\(x(\\alpha t)\\) (for \\(\\alpha &gt; 0\\)) is periodic with period \\(T/\\alpha\\) and fundamental frequency \\(\\alpha \\omega_0\\).\nThe Fourier coefficients for \\(x(\\alpha t)\\) are the same as for \\(x(t)\\), i.e., \\(a_k\\).\n\\[\nx(\\alpha t)=\\sum_{k=-\\infty}^{+\\infty} a_{k} e^{j k\\left(\\alpha \\omega_{0}\\right) t}\n\\]\n\n\n\n\n\n\nCaution\n\n\nImportant: While the coefficients \\(a_k\\) remain the same, the series representation changes because the fundamental frequency is now \\(\\alpha \\omega_0\\).\n\n\n\n\nTime scaling stretches or compresses the signal along the time axis. If you compress it (alpha &gt; 1), it speeds up, and its period shortens. If you stretch it (alpha &lt; 1), it slows down, and its period lengthens. The key here is that the weights or amplitudes of the harmonic components (the \\(a_k\\)’s) don’t change. What changes is the frequency at which each harmonic occurs, effectively scaling the fundamental frequency by \\(\\alpha\\)."
  },
  {
    "objectID": "ss_35.html#time-scaling-interactive-visualization",
    "href": "ss_35.html#time-scaling-interactive-visualization",
    "title": "Signals and Systems",
    "section": "Time Scaling: Interactive Visualization",
    "text": "Time Scaling: Interactive Visualization\nObserve how scaling affects the signal’s period and fundamental frequency, while the coefficients themselves remain invariant.\n\nviewof alpha_slider = Inputs.range([0.5, 2], {\n  label: \"Scaling Factor (α)\",\n  step: 0.1,\n  value: 1\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive plot helps illustrate the effect of time scaling. As you adjust the Scaling Factor (α), you’ll see the dashed red line (the scaled signal) either compress or expand. Notice that the period of the scaled signal changes to \\(T/\\alpha\\). However, if you look at the bottom panel, the Fourier coefficients a_k themselves (their magnitudes) remain exactly the same. What changes is the mapping of these coefficients to the actual frequencies in the synthesis equation, as the fundamental frequency becomes \\(\\alpha \\omega_0\\)."
  },
  {
    "objectID": "ss_35.html#multiplication",
    "href": "ss_35.html#multiplication",
    "title": "Signals and Systems",
    "section": "Multiplication",
    "text": "Multiplication\nIf \\(x(t)\\) and \\(y(t)\\) are periodic with period \\(T\\), with coefficients \\(a_k\\) and \\(b_k\\) respectively:\n\\[\n\\begin{aligned}\n& x(t) \\stackrel{\\mathcal{F S}}{\\longleftrightarrow} a_{k}, \\\\\n& y(t) \\stackrel{\\mathcal{F S}}{\\longleftrightarrow} b_{k} .\n\\end{aligned}\n\\]\nThe Fourier series coefficients \\(h_k\\) of their product \\(x(t)y(t)\\) are given by the discrete-time convolution of \\(a_k\\) and \\(b_k\\):\n\\[\nx(t) y(t) \\stackrel{\\mathcal{F S}}{\\longleftrightarrow} h_{k}=\\sum_{l=-\\infty}^{\\infty} a_{l} b_{k-l}\n\\]\n\nThe multiplication property is often called the convolution property in the frequency domain. Just as convolution in the time domain corresponds to multiplication in the frequency domain for Fourier transforms, here, multiplication of two periodic signals in the time domain corresponds to the convolution of their discrete Fourier series coefficients. This is a powerful concept, especially when analyzing systems where signals are modulated or mixed."
  },
  {
    "objectID": "ss_35.html#conjugation-and-conjugate-symmetry",
    "href": "ss_35.html#conjugation-and-conjugate-symmetry",
    "title": "Signals and Systems",
    "section": "Conjugation and Conjugate Symmetry",
    "text": "Conjugation and Conjugate Symmetry\nTaking the complex conjugate of a periodic signal \\(x(t)\\) has the effect of complex conjugation and time reversal on its Fourier series coefficients:\n\\[\nx^{*}(t) \\stackrel{\\mathcal{F S}}{\\longleftrightarrow} a_{-k}^{*}\n\\]\nA significant consequence for real signals (\\(x(t) = x^*(t)\\)) is conjugate symmetry:\n\\[\na_{-k}=a_{k}^{*}\n\\]\nThis implies various symmetry properties for magnitudes, phases, real, and imaginary parts of coefficients for real signals.\n\nThis property is fundamental when dealing with real-world signals, which are always real-valued. If a signal is real, its Fourier coefficients must satisfy conjugate symmetry. This means \\(a_0\\) must be real, the magnitudes \\(|a_k|\\) are even (\\(|a_k| = |a_{-k}|\\)), and the phases \\(\\angle a_k\\) are odd (\\(\\angle a_k = -\\angle a_{-k}\\)). These symmetries simplify calculations and provide a way to check for errors."
  },
  {
    "objectID": "ss_35.html#parsevals-relation-for-continuous-time-periodic-signals",
    "href": "ss_35.html#parsevals-relation-for-continuous-time-periodic-signals",
    "title": "Signals and Systems",
    "section": "Parseval’s Relation for Continuous-Time Periodic Signals",
    "text": "Parseval’s Relation for Continuous-Time Periodic Signals\nParseval’s relation states that the average power of a periodic signal in the time domain equals the sum of the average powers of its harmonic components in the frequency domain.\n\\[\n\\frac{1}{T} \\int_{T}|x(t)|^{2} d t=\\sum_{k=-\\infty}^{+\\infty}\\left|a_{k}\\right|^{2}\n\\]\nThe left side is the average power in one period of \\(x(t)\\). The term \\(|a_k|^2\\) represents the average power in the \\(k\\)-th harmonic component.\n\n\n\n\n\n\nImportant\n\n\nConservation of Energy: This relation highlights the conservation of power between the time and frequency domains.\n\n\n\n\nParseval’s relation is a cornerstone of signal analysis. It tells us that we can quantify the total energy or power in a signal by either integrating its squared magnitude over time or by summing the squared magnitudes of its Fourier coefficients. This is incredibly useful for power calculations, noise analysis, and understanding how power is distributed across different frequency components."
  },
  {
    "objectID": "ss_35.html#parsevals-relation-interactive-demonstration",
    "href": "ss_35.html#parsevals-relation-interactive-demonstration",
    "title": "Signals and Systems",
    "section": "Parseval’s Relation: Interactive Demonstration",
    "text": "Parseval’s Relation: Interactive Demonstration\nLet’s verify Parseval’s relation for a simple square wave.\n\n\n\n\n\n\n\nIn this example, we calculate the average power of a square wave in two ways. First, directly from its time-domain definition by integrating its squared magnitude over one period. Second, by summing the squared magnitudes of its Fourier series coefficients. Notice how closely these two values match, especially as we increase the number of harmonics considered. This vividly demonstrates Parseval’s relation, confirming that power is conserved across domains."
  },
  {
    "objectID": "ss_35.html#summary-of-properties",
    "href": "ss_35.html#summary-of-properties",
    "title": "Signals and Systems",
    "section": "Summary of Properties",
    "text": "Summary of Properties\nMany important properties of continuous-time Fourier series are summarized in the table below.\n\n\n\nProperty\nPeriodic Signal\nFourier Series Coeffs\n\n\n\n\nLinearity\n\\(Ax(t) + By(t)\\)\n\\(Aa_k + Bb_k\\)\n\n\nTime Shift\n\\(x(t-t_0)\\)\n\\(a_k e^{-jk\\omega_0 t_0}\\)\n\n\nFreq Shift\n\\(e^{jM\\omega_0 t}x(t)\\)\n\\(a_{k-M}\\)\n\n\nConjugation\n\\(x^*(t)\\)\n\\(a_{-k}^*\\)\n\n\nTime Reversal\n\\(x(-t)\\)\n\\(a_{-k}\\)\n\n\nTime Scaling\n\\(x(\\alpha t)\\)\n\\(a_k\\) (with new \\(\\omega_0\\))"
  },
  {
    "objectID": "ss_35.html#summary-of-properties-1",
    "href": "ss_35.html#summary-of-properties-1",
    "title": "Signals and Systems",
    "section": "Summary of Properties",
    "text": "Summary of Properties\n\n\n\nProperty\nPeriodic Signal\nFourier Series Coeffs\n\n\n\n\nPeriodic Convolution\n\\(\\int_T x(\\tau)y(t-\\tau)d\\tau\\)\n\\(T a_k b_k\\)\n\n\nMultiplication\n\\(x(t)y(t)\\)\n\\(\\sum_{l=-\\infty}^\\infty a_l b_{k-l}\\)\n\n\nDifferentiation\n\\(dx(t)/dt\\)\n\\(jk\\omega_0 a_k\\)\n\n\nIntegration\n\\(\\int x(\\tau)d\\tau\\)\n\\((1/(jk\\omega_0)) a_k\\)\n\n\nReal Signal\n\\(x(t)\\) real\n\\(a_k = a_{-k}^*\\)\n\n\nReal & Even\n\\(x(t)\\) real & even\n\\(a_k\\) real & even\n\n\nReal & Odd\n\\(x(t)\\) real & odd\n\\(a_k\\) purely imag. & odd\n\n\n\n\\[\n\\frac{1}{T} \\int_{T}|x(t)|^{2} d t=\\sum_{k=-\\infty}^{+\\infty}\\left|a_{k}\\right|^{2} \\quad \\text{(Parseval's Relation)}\n\\]\n\nThis table provides a comprehensive overview of all the key properties we’ve discussed and some additional ones. It’s an invaluable reference for any Fourier series problem. Notice how operations in one domain (time) often have a dual or inverse effect in the other domain (frequency). For instance, differentiation in time corresponds to multiplication by \\(jk\\omega_0\\) in the frequency domain. These dualities are a recurring theme in signal processing."
  },
  {
    "objectID": "ss_35.html#example-3.6-using-linearity-and-time-shifting",
    "href": "ss_35.html#example-3.6-using-linearity-and-time-shifting",
    "title": "Signals and Systems",
    "section": "Example 3.6: Using Linearity and Time Shifting",
    "text": "Example 3.6: Using Linearity and Time Shifting\nConsider the signal \\(g(t)\\) with period 4.\n\n\n\n\n\ngraph TD\n    A[\"x(t) - Square Wave T=4, T1=1\"] --&gt; B{\"Shift by 1: x(t-1)\"}\n    C[\"Constant -1/2\"] --&gt; D{Add to B}\n    B --&gt; E[\"g(t) = x(t-1) - 1/2\"]\n    D --&gt; E\n\n\n\n\n\n\nWe know \\(g(t) = x(t-1) - 1/2\\), where \\(x(t)\\) is a symmetric square wave with \\(T=4, T_1=1\\).\nThe coefficients \\(a_k\\) for \\(x(t)\\) are \\(\\frac{\\sin(\\pi k/2)}{k\\pi}\\) (for \\(k \\neq 0\\)) and \\(a_0 = 1/2\\).\n\nThis example shows how to leverage properties to find Fourier coefficients without direct integration. We’re given a signal \\(g(t)\\) that looks like a shifted and DC-offset version of a standard square wave \\(x(t)\\). The square wave \\(x(t)\\) has easily derivable coefficients. By recognizing \\(g(t)\\) as a transformation of \\(x(t)\\), we can use the linearity and time-shifting properties."
  },
  {
    "objectID": "ss_35.html#example-3.6-deriving-coefficients",
    "href": "ss_35.html#example-3.6-deriving-coefficients",
    "title": "Signals and Systems",
    "section": "Example 3.6: Deriving Coefficients",
    "text": "Example 3.6: Deriving Coefficients\n\nTime Shifting: Coefficients \\(b_k\\) for \\(x(t-1)\\) are \\(a_k e^{-j k \\omega_0 t_0}\\).\nHere, \\(\\omega_0 = 2\\pi/T = 2\\pi/4 = \\pi/2\\) and \\(t_0 = 1\\).\nSo, \\(b_k = a_k e^{-j k (\\pi/2)(1)} = a_k e^{-j k \\pi/2}\\).\nDC Offset: The term \\(-1/2\\) has coefficients \\(c_k = -1/2\\) for \\(k=0\\) and \\(0\\) for \\(k \\neq 0\\).\nLinearity: Coefficients \\(d_k\\) for \\(g(t) = x(t-1) - 1/2\\) are \\(b_k + c_k\\).\n\\[\nd_{k}=\\left\\{\\begin{array}{ll}\n\\frac{\\sin (\\pi k / 2)}{k \\pi} e^{-j k \\pi / 2}, & \\text { for } k \\neq 0 \\\\\n0, & \\text { for } k=0\n\\end{array}\\right.\n\\]\n\n\nFirst, we apply the time-shifting property. We know the coefficients of the unshifted square wave, \\(x(t)\\). Shifting it by 1 in time simply multiplies each coefficient \\(a_k\\) by a phase factor \\(e^{-j k \\pi/2}\\). Next, we consider the DC offset. A constant value in the time domain only contributes to the \\(a_0\\) coefficient in the frequency domain. Finally, we use linearity: the coefficients of the sum are the sum of the coefficients. We combine the shifted square wave’s coefficients with the DC offset’s coefficients to get the coefficients for \\(g(t)\\). Notice how the \\(a_0\\) term for \\(x(t)\\) (\\(1/2\\)) combined with the DC offset (\\(-1/2\\)) to yield \\(d_0=0\\)."
  },
  {
    "objectID": "ss_35.html#example-3.7-using-differentiation",
    "href": "ss_35.html#example-3.7-using-differentiation",
    "title": "Signals and Systems",
    "section": "Example 3.7: Using Differentiation",
    "text": "Example 3.7: Using Differentiation\nConsider the triangular wave signal \\(x(t)\\) with period \\(T=4\\).\nIts derivative, \\(dx(t)/dt\\), is the square wave \\(g(t)\\) from Example 3.6.\nIf \\(e_k\\) are the coefficients for \\(x(t)\\) and \\(d_k\\) for \\(g(t)\\), the differentiation property states:\n\\[\nd_{k}=j k \\omega_{0} e_{k}\n\\]\nSince \\(\\omega_0 = \\pi/2\\):\n\\[\nd_{k}=j k (\\pi/2) e_{k}\n\\]\n\nThis example showcases the differentiation property. We have a triangular wave, and its derivative is a square wave, which we just analyzed in Example 3.6. Instead of directly calculating the Fourier series for the triangular wave (which involves integration by parts), we can use the differentiation property. This property states that differentiating in the time domain corresponds to multiplying the Fourier coefficients by \\(j k \\omega_0\\)."
  },
  {
    "objectID": "ss_35.html#example-3.7-deriving-coefficients",
    "href": "ss_35.html#example-3.7-deriving-coefficients",
    "title": "Signals and Systems",
    "section": "Example 3.7: Deriving Coefficients",
    "text": "Example 3.7: Deriving Coefficients\nFrom \\(d_k = j k (\\pi/2) e_k\\), we can find \\(e_k\\):\n\\[\ne_{k}=\\frac{2 d_{k}}{j k \\pi}, \\quad k \\neq 0\n\\]\nSubstituting \\(d_k\\) from Example 3.6:\n\\[\ne_{k}=\\frac{2 \\sin (\\pi k / 2)}{j(k \\pi)^{2}} e^{-j k \\pi / 2}, \\quad k \\neq 0\n\\]\nFor \\(k=0\\), \\(e_0\\) is the average value of \\(x(t)\\) over one period:\n\\[\ne_{0}=\\frac{1}{T} \\int_T x(t) dt = \\frac{1}{4} \\times (\\text{Area of triangle}) = \\frac{1}{4} \\times (2) = \\frac{1}{2}\n\\]\n\nWe rearrange the differentiation formula to solve for \\(e_k\\) in terms of \\(d_k\\). We already know \\(d_k\\) from the previous example. For the DC component, \\(e_0\\), we cannot use the differentiation formula because it would involve division by zero. Instead, \\(e_0\\) is simply the average value of the signal, which we can find by calculating the area under one period of the triangular wave and dividing by the period. This demonstrates how properties can be combined with basic definitions to solve problems efficiently."
  },
  {
    "objectID": "ss_35.html#example-3.8-impulse-train-coefficients",
    "href": "ss_35.html#example-3.8-impulse-train-coefficients",
    "title": "Signals and Systems",
    "section": "Example 3.8: Impulse Train Coefficients",
    "text": "Example 3.8: Impulse Train Coefficients\nThe impulse train \\(x(t) = \\sum_{k=-\\infty}^{\\infty} \\delta(t-kT)\\) with period \\(T\\).\n\n\n\n\n\ngraph LR\n    A[…] --- B{…}\n    B -- T --&gt; C{\"δ(t)\"}\n    C -- T --&gt; D{\"δ(t-T)\"}\n    D -- T --&gt; E{\"δ(t-2T)\"}\n    E -- T --&gt; B\n    style C fill:#f9f,stroke:#333,stroke-width:2px\n\n\n\n\n\n\nThe Fourier series coefficients \\(a_k\\) are found by integrating over one period (e.g., \\([-T/2, T/2]\\)):\n\\[\na_{k}=\\frac{1}{T} \\int_{-T / 2}^{T / 2} \\delta(t) e^{-j k 2 \\pi / T} d t=\\frac{1}{T}\n\\]\nAll coefficients are equal: \\(a_k = 1/T\\).\n\nThe impulse train is a very important signal in signal processing. It consists of a series of Dirac delta functions equally spaced in time. To find its Fourier coefficients, we use the analysis equation and integrate over a single period that contains one impulse, typically at \\(t=0\\). Due to the sifting property of the delta function, the integral simply evaluates to the value of the exponential at \\(t=0\\), which is 1. Thus, all Fourier coefficients for an impulse train are constant, \\(1/T\\). This also aligns with the property that a real and even signal (like the impulse train) has real and even coefficients."
  },
  {
    "objectID": "ss_35.html#example-3.8-relating-signals-using-properties",
    "href": "ss_35.html#example-3.8-relating-signals-using-properties",
    "title": "Signals and Systems",
    "section": "Example 3.8: Relating Signals using Properties",
    "text": "Example 3.8: Relating Signals using Properties\nThe derivative of a square wave \\(g(t)\\) is \\(q(t)\\), which can be expressed as a difference of two shifted impulse trains:\n\\[\nq(t)=x\\left(t+T_{1}\\right)-x\\left(t-T_{1}\\right)\n\\]\n\n\n\n\n\ngraph TD\n    A[\"Square Wave g(t)\"]\n    B[\"Derivative d/dt\"]\n    C[\"Impulse Train x(t)\"]\n    D[\"Shifted x(t+T1)\"]\n    E[\"Shifted x(t-T1)\"]\n    F[\"q(t) = D - E\"]\n\n    A -- B --&gt; F\n    C -- Shift --&gt; D\n    C -- Shift --&gt; E\n    D -- Subtract --&gt; F\n    E -- Subtract --&gt; F\n\n\n\n\n\n\n\nCoefficients for \\(x(t)\\) are \\(a_k = 1/T\\).\nBy time-shifting and linearity, coefficients \\(b_k\\) for \\(q(t)\\) are:\n\\(b_k = a_k e^{j k \\omega_0 T_1} - a_k e^{-j k \\omega_0 T_1}\\)\n\\(b_k = \\frac{1}{T} (e^{j k \\omega_0 T_1} - e^{-j k \\omega_0 T_1}) = \\frac{2j \\sin(k \\omega_0 T_1)}{T}\\)\n\n\nThis part of the example cleverly connects several properties. We observe that the derivative of a square wave is a series of positive and negative impulses. This q(t) can be seen as the difference between two time-shifted impulse trains. We already know the coefficients for a basic impulse train. By applying the time-shifting property to get the coefficients for \\(x(t+T_1)\\) and \\(x(t-T_1)\\), and then the linearity property for their difference, we can quickly derive the Fourier coefficients \\(b_k\\) for \\(q(t)\\)."
  },
  {
    "objectID": "ss_35.html#example-3.8-connecting-back-to-square-wave",
    "href": "ss_35.html#example-3.8-connecting-back-to-square-wave",
    "title": "Signals and Systems",
    "section": "Example 3.8: Connecting Back to Square Wave",
    "text": "Example 3.8: Connecting Back to Square Wave\nSince \\(q(t)\\) is the derivative of \\(g(t)\\), we use the differentiation property:\n\\[\nb_{k}=j k \\omega_{0} c_{k}\n\\]\nWhere \\(c_k\\) are the Fourier coefficients of \\(g(t)\\).\nSolving for \\(c_k\\) (for \\(k \\neq 0\\)):\n\\[\nc_{k}=\\frac{b_{k}}{j k \\omega_{0}}=\\frac{2 j \\sin \\left(k \\omega_{0} T_{1}\\right)}{j k \\omega_{0} T}=\\frac{\\sin \\left(k \\omega_{0} T_{1}\\right)}{k \\pi}, \\quad k \\neq 0\n\\]\nFor \\(c_0\\), the average value of \\(g(t)\\) from inspection of a square wave with width \\(2T_1\\):\n\\[\nc_{0}=\\frac{2 T_{1}}{T}\n\\]\nThese are the same coefficients derived directly for a square wave!\n\nFinally, we tie it all together. Since \\(q(t)\\) is the derivative of \\(g(t)\\), we can use the differentiation property in reverse. We divide the coefficients \\(b_k\\) (which we just found for \\(q(t)\\)) by \\(jk\\omega_0\\) to get the coefficients \\(c_k\\) for \\(g(t)\\). The \\(k=0\\) term (DC component) needs to be found separately by calculating the average value of the square wave. This confirms that the Fourier coefficients obtained using these properties match those found by direct integration, demonstrating the power and consistency of these properties."
  },
  {
    "objectID": "ss_35.html#example-3.9-characterizing-a-signal",
    "href": "ss_35.html#example-3.9-characterizing-a-signal",
    "title": "Signals and Systems",
    "section": "Example 3.9: Characterizing a Signal",
    "text": "Example 3.9: Characterizing a Signal\nGiven facts about \\(x(t)\\):\n\n\\(x(t)\\) is a real signal.\n\\(x(t)\\) is periodic with \\(T=4\\), coefficients \\(a_k\\).\n\\(a_k = 0\\) for \\(|k|&gt;1\\).\nThe signal with coefficients \\(b_k = e^{-j\\pi k/2} a_{-k}\\) is odd.\n\\(\\frac{1}{4} \\int_{4}|x(t)|^{2} d t = 1/2\\).\n\nLet’s determine \\(x(t)\\) to within a sign factor.\n\nThis is a comprehensive problem that requires applying multiple properties to deduce the form of an unknown signal. We are given five distinct facts about a signal \\(x(t)\\) and its Fourier coefficients. Our goal is to use these facts to uniquely identify \\(x(t)\\) up to a sign. This is a great example of how understanding these properties allows for powerful deductive reasoning in signal analysis."
  },
  {
    "objectID": "ss_35.html#example-3.9-step-by-step-deduction",
    "href": "ss_35.html#example-3.9-step-by-step-deduction",
    "title": "Signals and Systems",
    "section": "Example 3.9: Step-by-Step Deduction",
    "text": "Example 3.9: Step-by-Step Deduction\nFact 3: \\(a_k = 0\\) for \\(|k|&gt;1\\).\n\\(\\implies x(t) = a_0 + a_1 e^{j\\pi t/2} + a_{-1} e^{-j\\pi t/2}\\) (since \\(\\omega_0 = 2\\pi/4 = \\pi/2\\)).\nFact 1: \\(x(t)\\) is real.\n\\(\\implies a_0\\) is real, and \\(a_1 = a_{-1}^*\\).\nSo, \\(x(t) = a_0 + 2\\operatorname{Re}\\{a_1 e^{j\\pi t/2}\\}\\).\nFact 4: Signal with \\(b_k = e^{-j\\pi k/2} a_{-k}\\) is odd.\n\n\\(a_{-k}\\) corresponds to \\(x(-t)\\) (Time Reversal).\n\\(e^{-j\\pi k/2}\\) corresponds to a time shift of \\(t_0=1\\) (since \\(e^{-jk\\omega_0 t_0}\\) with \\(\\omega_0=\\pi/2, t_0=1\\) gives \\(e^{-jk\\pi/2}\\)).\n\n\\(\\implies b_k\\) correspond to \\(x(-(t-1)) = x(-t+1)\\).\nSince \\(x(-t+1)\\) is odd and real (Fact 1), its Fourier coefficients \\(b_k\\) must be purely imaginary and odd. \\(\\implies b_0 = 0\\) and \\(b_{-1} = -b_1\\).\n\nWe start by using Fact 3 to limit the number of non-zero coefficients to \\(a_0, a_1, a_{-1}\\). Then, Fact 1 (x(t) is real) tells us about the symmetry of these coefficients: \\(a_0\\) is real and \\(a_1\\) is the complex conjugate of \\(a_{-1}\\). This simplifies the time-domain expression for \\(x(t)\\). Fact 4 is where it gets interesting. We recognize \\(a_{-k}\\) as the coefficients for \\(x(-t)\\) (time reversal) and the exponential term as a time shift. So, \\(b_k\\) are the coefficients for \\(x(-t+1)\\). Since this signal is explicitly stated to be odd AND we know \\(x(t)\\) is real (making \\(x(-t+1)\\) also real), its coefficients \\(b_k\\) must be purely imaginary and odd. This immediately gives us \\(b_0=0\\) and \\(b_{-1}=-b_1\\)."
  },
  {
    "objectID": "ss_35.html#example-3.9-final-steps",
    "href": "ss_35.html#example-3.9-final-steps",
    "title": "Signals and Systems",
    "section": "Example 3.9: Final Steps",
    "text": "Example 3.9: Final Steps\nFrom \\(b_0 = 0\\):\n\\(b_0 = e^{-j\\pi (0)/2} a_{-0} = a_0 = 0\\).\nFrom \\(b_{-1} = -b_1\\):\nWe also know \\(b_k = e^{-j\\pi k/2} a_{-k}\\).\nFor \\(k=1\\): \\(b_1 = e^{-j\\pi /2} a_{-1} = -j a_{-1}\\).\nSince \\(a_{-1} = a_1^*\\), we have \\(b_1 = -j a_1^*\\).\nSince \\(b_k\\) are purely imaginary, \\(b_1\\) must be purely imaginary. This is consistent with \\(-j a_1^*\\)."
  },
  {
    "objectID": "ss_35.html#example-3.9-final-steps-1",
    "href": "ss_35.html#example-3.9-final-steps-1",
    "title": "Signals and Systems",
    "section": "Example 3.9: Final Steps",
    "text": "Example 3.9: Final Steps\nFact 5: Parseval’s Relation.\nThe average power of \\(x(-t+1)\\) is also \\(1/2\\).\n\\(\\frac{1}{4} \\int_{4}|x(-t+1)|^{2} d t=1/2\\).\nUsing Parseval’s: \\(\\sum_{k=-\\infty}^\\infty |b_k|^2 = |b_{-1}|^2 + |b_0|^2 + |b_1|^2 = 1/2\\).\nSince \\(b_0=0\\) and \\(b_{-1}=-b_1\\), we have \\(2|b_1|^2 = 1/2 \\implies |b_1|^2 = 1/4 \\implies |b_1| = 1/2\\).\nSince \\(b_1\\) is purely imaginary, \\(b_1 = j/2\\) or \\(b_1 = -j/2\\).\n\nWe use the deduced properties of \\(b_k\\). From \\(b_0 = 0\\), we find that \\(a_0\\) must be zero. This significantly simplifies \\(x(t)\\) to just two exponential terms. Next, we use Parseval’s relation. Since time reversal and time shifting don’t change the average power, the average power of \\(x(-t+1)\\) is also \\(1/2\\). Applying Parseval’s to the \\(b_k\\) coefficients, and recalling that \\(b_0=0\\) and \\(b_{-1}=-b_1\\), we find that \\(|b_1|\\) must be \\(1/2\\). Coupled with the fact that \\(b_1\\) is purely imaginary, this means \\(b_1\\) can only be \\(j/2\\) or \\(-j/2\\)."
  },
  {
    "objectID": "ss_35.html#example-3.9-determining-xt",
    "href": "ss_35.html#example-3.9-determining-xt",
    "title": "Signals and Systems",
    "section": "Example 3.9: Determining \\(x(t)\\)",
    "text": "Example 3.9: Determining \\(x(t)\\)\nWe have \\(a_0=0\\) and \\(b_1 = -j a_1^*\\).\nCase 1: \\(b_1 = j/2\\).\n\\(j/2 = -j a_1^* \\implies a_1^* = -1/2 \\implies a_1 = -1/2\\).\nThen \\(x(t) = 2\\operatorname{Re}\\{(-1/2) e^{j\\pi t/2}\\} = 2(-1/2 \\cos(\\pi t/2)) = -\\cos(\\pi t/2)\\).\nCase 2: \\(b_1 = -j/2\\).\n\\(-j/2 = -j a_1^* \\implies a_1^* = 1/2 \\implies a_1 = 1/2\\).\nThen \\(x(t) = 2\\operatorname{Re}\\{(1/2) e^{j\\pi t/2}\\} = 2(1/2 \\cos(\\pi t/2)) = \\cos(\\pi t/2)\\).\nThus, \\(x(t) = \\pm \\cos(\\pi t/2)\\).\n\nFinally, we substitute the possible values of \\(b_1\\) back into our relationship between \\(a_1\\) and \\(b_1\\). This allows us to determine the possible values for \\(a_1\\). With \\(a_0=0\\) and the determined \\(a_1\\), we can reconstruct \\(x(t)\\) using the synthesis equation. As the problem stated, we arrive at \\(x(t)\\) being \\(\\cos(\\pi t/2)\\) or \\(-\\cos(\\pi t/2)\\), demonstrating that the given facts are sufficient to determine the signal up to a sign factor. This is a powerful illustration of how properties can be used in reverse, from frequency domain characteristics to time domain signal reconstruction."
  },
  {
    "objectID": "ss_33.html#introduction-to-periodic-signals",
    "href": "ss_33.html#introduction-to-periodic-signals",
    "title": "Signals and Systems",
    "section": "1. Introduction to Periodic Signals",
    "text": "1. Introduction to Periodic Signals\nA signal \\(x(t)\\) is periodic if, for some positive value of \\(T\\),\n\\[\nx(t)=x(t+T) \\quad \\text { for all } t \\tag{3.21}\n\\]\nThe fundamental period \\(T\\) is the minimum positive, nonzero value satisfying this condition. The fundamental frequency \\(\\omega_{0}\\) is related by \\(\\omega_{0} = 2\\pi/T\\).\n\nRecall from Chapter 1 that periodic signals repeat themselves over a fixed interval. The fundamental period is the shortest time for one complete cycle, and the fundamental frequency tells us how often that cycle occurs in radians per second."
  },
  {
    "objectID": "ss_33.html#basic-periodic-signals",
    "href": "ss_33.html#basic-periodic-signals",
    "title": "Signals and Systems",
    "section": "Basic Periodic Signals",
    "text": "Basic Periodic Signals\nThe building blocks of Fourier Series are complex exponentials.\nSinusoidal Signal:\n\\[\nx(t)=\\cos \\omega_{0} t \\tag{3.22}\n\\]\nComplex Exponential:\n\\[\nx(t)=e^{j \\omega_{0} t} \\tag{3.23}\n\\]\nBoth are periodic with fundamental frequency \\(\\omega_{0}\\) and period \\(T=2\\pi/\\omega_{0}\\).\nHarmonically Related Complex Exponentials:\n\\[\n\\phi_{k}(t)=e^{j k \\omega_{0} t}=e^{j k(2 \\pi / T) t}, \\quad k=0, \\pm 1, \\pm 2, \\ldots \\tag{3.24}\n\\] Each \\(\\phi_k(t)\\) is periodic with period \\(T\\).\n\nWhile real sinusoids are intuitive, complex exponentials are mathematically more convenient for analysis. Notice that each harmonically related exponential φ_k(t) has a frequency that is an integer multiple of the fundamental frequency ω₀. This is key to building complex signals."
  },
  {
    "objectID": "ss_33.html#the-fourier-series-representation",
    "href": "ss_33.html#the-fourier-series-representation",
    "title": "Signals and Systems",
    "section": "2. The Fourier Series Representation",
    "text": "2. The Fourier Series Representation\nA periodic signal \\(x(t)\\) can be represented as a linear combination of harmonically related complex exponentials:\n\\[\nx(t)=\\sum_{k=-\\infty}^{+\\infty} a_{k} e^{j k \\omega_{0} t}=\\sum_{k=-\\infty}^{+\\infty} a_{k} e^{j k(2 \\pi / T) t} \\tag{3.25}\n\\]\n\nThe coefficients \\(a_k\\) are called Fourier Series Coefficients.\n\\(k=0\\): DC component (constant term).\n\\(k=\\pm 1\\): Fundamental components (first harmonic).\n\\(k=\\pm N\\): \\(N\\)th harmonic components.\n\n\nThis equation is the heart of the Fourier Series. It states that any periodic signal can be decomposed into a sum of simple sinusoids (or complex exponentials) at integer multiples of the fundamental frequency. The coefficients a_k tell us the amplitude and phase of each frequency component. Think of it like a recipe, where a_k are the ingredients and e^(jkω₀t) are the basic flavors."
  },
  {
    "objectID": "ss_33.html#example-3.2-constructing-a-signal-part-1",
    "href": "ss_33.html#example-3.2-constructing-a-signal-part-1",
    "title": "Signals and Systems",
    "section": "Example 3.2: Constructing a Signal (Part 1)",
    "text": "Example 3.2: Constructing a Signal (Part 1)\nConsider a periodic signal \\(x(t)\\) with fundamental frequency \\(2\\pi\\) (so \\(T=1\\)). It is expressed as:\n\\[\nx(t)=\\sum_{k=-3}^{+3} a_{k} e^{j k 2 \\pi t} \\tag{3.26}\n\\]\nWith coefficients:\n\n\\(a_{0}=1\\)\n\\(a_{1}=a_{-1}=\\frac{1}{4}\\)\n\\(a_{2}=a_{-2}=\\frac{1}{2}\\)\n\\(a_{3}=a_{-3}=\\frac{1}{3}\\)\n\n\nHere, we are given the Fourier series coefficients and are going to reconstruct the time-domain signal. The fundamental frequency is 2π, meaning the fundamental period is 1 second. We only have components up to the 3rd harmonic."
  },
  {
    "objectID": "ss_33.html#example-3.2-constructing-a-signal-part-2",
    "href": "ss_33.html#example-3.2-constructing-a-signal-part-2",
    "title": "Signals and Systems",
    "section": "Example 3.2: Constructing a Signal (Part 2)",
    "text": "Example 3.2: Constructing a Signal (Part 2)\nRewriting the sum and grouping terms:\n\\[\n\\begin{align*}\nx(t)= & 1+\\frac{1}{4}\\left(e^{j 2 \\pi t}+e^{-j 2 \\pi t}\\right)+\\frac{1}{2}\\left(e^{j 4 \\pi t}+e^{-j 4 \\pi t}\\right) \\tag{3.27} \\\\\n& +\\frac{1}{3}\\left(e^{j 6 \\pi t}+e^{-j 6 \\pi t}\\right)\n\\end{align*}\n\\]\nUsing Euler’s relation (\\(e^{j\\theta} + e^{-j\\theta} = 2\\cos\\theta\\)), this simplifies to:\n\\[\nx(t)=1+\\frac{1}{2} \\cos 2 \\pi t+\\cos 4 \\pi t+\\frac{2}{3} \\cos 6 \\pi t \\tag{3.28}\n\\]\n\nBy grouping the complex conjugate exponential terms, we can convert them into real cosine functions. This shows how complex exponentials combine to form real sinusoids, which are often easier to visualize in the time domain."
  },
  {
    "objectID": "ss_33.html#example-3.2-interactive-signal-synthesis",
    "href": "ss_33.html#example-3.2-interactive-signal-synthesis",
    "title": "Signals and Systems",
    "section": "Example 3.2: Interactive Signal Synthesis",
    "text": "Example 3.2: Interactive Signal Synthesis\nSee how a signal is built from its harmonics.\nThis interactive plot shows the individual harmonic components and their sum. Adjust the checkboxes to see how each harmonic contributes to the overall signal.\n\nviewof dc_on = Inputs.toggle({label: \"DC\", value: true})\nviewof fundamental_on = Inputs.toggle({label: \"Fund.\", value: true})\nviewof second_harmonic_on = Inputs.toggle({label: \"2nd Harm.\", value: true})\nviewof third_harmonic_on = Inputs.toggle({label: \"3rd Harm.\", value: true})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive plot helps visualize the concept of signal synthesis. You can enable or disable individual harmonic components to see how they add up to form the final signal x(t). Notice how the signal becomes more complex as higher harmonics are added. This demonstrates the power of Fourier series in representing any periodic signal."
  },
  {
    "objectID": "ss_33.html#properties-for-real-periodic-signals",
    "href": "ss_33.html#properties-for-real-periodic-signals",
    "title": "Signals and Systems",
    "section": "3. Properties for Real Periodic Signals",
    "text": "3. Properties for Real Periodic Signals\nIf \\(x(t)\\) is a real periodic signal, its Fourier series coefficients \\(a_k\\) have a specific property:\n\\[\na_{k}^{*}=a_{-k} \\tag{3.29}\n\\] This means the coefficients for negative frequencies are the complex conjugates of the coefficients for positive frequencies.\nThis property leads to alternative forms for the Fourier Series.\n\nThis property is very useful for simplifying calculations and understanding the symmetry of the spectrum for real-world signals. It implies that the magnitude spectrum is even, and the phase spectrum is odd."
  },
  {
    "objectID": "ss_33.html#alternative-forms-for-real-signals",
    "href": "ss_33.html#alternative-forms-for-real-signals",
    "title": "Signals and Systems",
    "section": "Alternative Forms for Real Signals",
    "text": "Alternative Forms for Real Signals\nIf \\(x(t)\\) is real, we can express its Fourier series in terms of real sinusoids:\nForm 1 (Amplitude-Phase): If \\(a_{k}=A_{k} e^{j \\theta_{k}}\\), then:\n\\[\nx(t)=a_{0}+2 \\sum_{k=1}^{\\infty} A_{k} \\cos \\left(k \\omega_{0} t+\\theta_{k}\\right) \\tag{3.31}\n\\]\nForm 2 (Cosine-Sine): If \\(a_{k}=B_{k}+j C_{k}\\), then:\n\\[\nx(t)=a_{0}+2 \\sum_{k=1}^{\\infty}\\left[B_{k} \\cos k \\omega_{0} t-C_{k} \\sin k \\omega_{0} t\\right] \\tag{3.32}\n\\]\n\n\n\n\n\n\nNote\n\n\nThe complex exponential form (Eq. 3.25) is generally more convenient for analysis and manipulation in ECE, despite these real-valued alternatives.\n\n\n\n\nThese trigonometric forms are often used in introductory texts, but the complex exponential form is more powerful for advanced analysis, especially when dealing with system responses. It unifies the representation and simplifies mathematical operations."
  },
  {
    "objectID": "ss_33.html#determination-of-fourier-series-coefficients",
    "href": "ss_33.html#determination-of-fourier-series-coefficients",
    "title": "Signals and Systems",
    "section": "4. Determination of Fourier Series Coefficients",
    "text": "4. Determination of Fourier Series Coefficients\nThe Analysis Equation\nTo find the coefficients \\(a_k\\) for a given \\(x(t)\\), we use the following derivation:\n\nMultiply \\(x(t)\\) by \\(e^{-j n \\omega_{0} t}\\): \\[\nx(t) e^{-j n \\omega_{0} t}=\\sum_{k=-\\infty}^{+\\infty} a_{k} e^{j k \\omega_{0} t} e^{-j n \\omega_{0} t} \\tag{3.33}\n\\]\nIntegrate both sides over one period \\(T\\): \\[\n\\int_{0}^{T} x(t) e^{-j n \\omega_{0} t} d t=\\int_{0}^{T} \\sum_{k=-\\infty}^{+\\infty} a_{k} e^{j(k-n) \\omega_{0} t} d t \\tag{3.34}\n\\]\nInterchange summation and integration: \\[\n\\int_{0}^{T} x(t) e^{-j n \\omega_{0} t} d t=\\sum_{k=-\\infty}^{+\\infty} a_{k}\\left[\\int_{0}^{T} e^{j(k-n) \\omega_{0} t} d t\\right]\n\\]\n\n\nThis derivation is crucial. It shows how we “extract” each individual coefficient a_k from the signal x(t). The key step is the orthogonality property of complex exponentials over a period, which makes all terms in the summation zero except when k=n."
  },
  {
    "objectID": "ss_33.html#the-orthogonality-of-complex-exponentials",
    "href": "ss_33.html#the-orthogonality-of-complex-exponentials",
    "title": "Signals and Systems",
    "section": "The Orthogonality of Complex Exponentials",
    "text": "The Orthogonality of Complex Exponentials\nThe integral \\(\\int_{0}^{T} e^{j(k-n) \\omega_{0} t} d t\\) evaluates as:\n\\[\n\\int_{0}^{T} e^{j(k-n) \\omega_{0} t} d t=\\left\\{\\begin{array}{ll}\nT, & k=n \\\\\n0, & k \\neq n\n\\end{array}\\right.\n\\]\nThis property is called orthogonality. Applying this to the summation yields \\(T a_n\\).\nTherefore, the formula for the Fourier Series coefficients is:\n\\[\na_{n}=\\frac{1}{T} \\int_{T} x(t) e^{-j n \\omega_{0} t} d t \\tag{3.37}\n\\] The integration can be over any interval of length \\(T\\).\n\nThe orthogonality property is what makes Fourier series analysis possible. It allows us to isolate each harmonic component. The integral effectively acts as a “filter” that picks out the n-th harmonic."
  },
  {
    "objectID": "ss_33.html#the-fourier-series-pair",
    "href": "ss_33.html#the-fourier-series-pair",
    "title": "Signals and Systems",
    "section": "The Fourier Series Pair",
    "text": "The Fourier Series Pair\nThe Fourier Series is defined by a pair of equations:\n\n\nSynthesis Equation (Time Domain to Frequency Domain):\nReconstructs \\(x(t)\\) from its coefficients.\n\\[\nx(t)=\\sum_{k=-\\infty}^{+\\infty} a_{k} e^{j k \\omega_{0} t} \\tag{3.38}\n\\]\n\nAnalysis Equation (Frequency Domain to Time Domain):\nDetermines the coefficients \\(a_k\\) from \\(x(t)\\).\n\\[\na_{k}=\\frac{1}{T} \\int_{T} x(t) e^{-j k \\omega_{0} t} d t \\tag{3.39}\n\\]\n\n\n\n\n\n\n\nImportant\n\n\nThese two equations are fundamental to understanding and applying Fourier Series. They represent the bridge between the time domain and the frequency domain.\n\n\n\n\nThe a_k coefficients are also called spectral coefficients. They describe the signal’s content at different frequencies. The coefficient a₀ (when k=0) is simply the average value of \\(x(t)\\) over one period. \\[\na_{0}=\\frac{1}{T} \\int_{T} x(t) d t \\tag{3.40}\n\\]"
  },
  {
    "objectID": "ss_33.html#example-3.3-fourier-series-of-a-simple-sinusoid",
    "href": "ss_33.html#example-3.3-fourier-series-of-a-simple-sinusoid",
    "title": "Signals and Systems",
    "section": "Example 3.3: Fourier Series of a Simple Sinusoid",
    "text": "Example 3.3: Fourier Series of a Simple Sinusoid\nConsider the signal \\(x(t)=\\sin \\omega_{0} t\\).\nWe can determine its Fourier series coefficients by inspection using Euler’s formula:\n\\[\n\\sin \\omega_{0} t=\\frac{1}{2 j} e^{j \\omega_{0} t}-\\frac{1}{2 j} e^{-j \\omega_{0} t}\n\\]\nComparing this to the synthesis equation (Eq. 3.38), we find:\n\n\\(a_{1}=\\frac{1}{2 j}\\)\n\\(a_{-1}=-\\frac{1}{2 j}\\)\n\\(a_{k}=0\\), for \\(k \\neq +1\\) or \\(-1\\).\n\n\nThis example shows that a pure sinusoid only has two non-zero frequency components: one at +ω₀ and one at -ω₀. This is a very clean spectrum. Notice the j in the denominator, indicating a phase shift."
  },
  {
    "objectID": "ss_33.html#example-3.4-more-complex-sum-of-sinusoids-part-1",
    "href": "ss_33.html#example-3.4-more-complex-sum-of-sinusoids-part-1",
    "title": "Signals and Systems",
    "section": "Example 3.4: More Complex Sum of Sinusoids (Part 1)",
    "text": "Example 3.4: More Complex Sum of Sinusoids (Part 1)\nLet \\(x(t)=1+\\sin \\omega_{0} t+2 \\cos \\omega_{0} t+\\cos \\left(2 \\omega_{0} t+\\frac{\\pi}{4}\\right)\\).\nExpanding into complex exponentials and collecting terms:\n\\[\nx(t)=1+\\left(1-\\frac{1}{2} j\\right) e^{j \\omega_{0} t}+\\left(1+\\frac{1}{2} j\\right) e^{-j \\omega_{0} t}+\\left(\\frac{1}{2} e^{j(\\pi / 4)}\\right) e^{j 2 \\omega_{0} t}+\\left(\\frac{1}{2} e^{-j(\\pi / 4)}\\right) e^{-j 2 \\omega_{0} t}\n\\]\nThe Fourier series coefficients are:\n\n\\(a_{0}=1\\)\n\\(a_{1}=1-\\frac{1}{2} j\\)\n\\(a_{-1}=1+\\frac{1}{2} j\\)\n\\(a_{2}=\\frac{1}{2} e^{j(\\pi / 4)}=\\frac{\\sqrt{2}}{4}(1+j)\\)\n\\(a_{-2}=\\frac{1}{2} e^{-j(\\pi / 4)}=\\frac{\\sqrt{2}}{4}(1-j)\\)\n\\(a_{k}=0\\), for \\(|k|&gt;2\\).\n\n\nHere, we’re combining different sinusoids with varying amplitudes and phases. The resulting a_k coefficients are complex, reflecting both the amplitude and phase contribution of each harmonic. Notice how a_k and a_-k are complex conjugates, as expected for a real signal."
  },
  {
    "objectID": "ss_33.html#example-3.4-interactive-spectrum-visualization",
    "href": "ss_33.html#example-3.4-interactive-spectrum-visualization",
    "title": "Signals and Systems",
    "section": "Example 3.4: Interactive Spectrum Visualization",
    "text": "Example 3.4: Interactive Spectrum Visualization\nMagnitude and Phase Spectrum\nThis plot shows the magnitude and phase of the Fourier coefficients \\(a_k\\) for Example 3.4. Interact with the plot to examine the spectral content.\n\n\n\n\n\n\n\nThe magnitude spectrum shows how much “energy” each harmonic contributes to the signal. The phase spectrum shows the relative phase shift of each harmonic. Notice the symmetry for real signals: magnitude is even (|a_k| = |a_-k|) and phase is odd (∠a_k = -∠a_-k). The DC component a₀ has zero phase, as it’s a real constant."
  },
  {
    "objectID": "ss_33.html#example-3.5-periodic-square-wave-part-1",
    "href": "ss_33.html#example-3.5-periodic-square-wave-part-1",
    "title": "Signals and Systems",
    "section": "Example 3.5: Periodic Square Wave (Part 1)",
    "text": "Example 3.5: Periodic Square Wave (Part 1)\nThe periodic square wave is a canonical signal in ECE.\nDefined over one period as:\n\\[\nx(t)= \\begin{cases}1, & |t|&lt;T_{1} \\\\ 0, & T_{1}&lt;|t|&lt;T / 2\\end{cases} \\tag{3.41}\n\\]\nThis signal is periodic with fundamental period \\(T\\) and fundamental frequency \\(\\omega_{0}=2 \\pi / T\\).\n\nThe square wave is a fundamental signal for understanding Fourier series because it’s simple yet contains an infinite number of harmonics. It’s used in digital communications, clock signals, and many other areas. We’ll derive its Fourier series coefficients."
  },
  {
    "objectID": "ss_33.html#example-3.5-periodic-square-wave-part-2",
    "href": "ss_33.html#example-3.5-periodic-square-wave-part-2",
    "title": "Signals and Systems",
    "section": "Example 3.5: Periodic Square Wave (Part 2)",
    "text": "Example 3.5: Periodic Square Wave (Part 2)\nCalculating \\(a_0\\) and \\(a_k\\)\nFor \\(k=0\\) (DC component): \\[\na_{0}=\\frac{1}{T} \\int_{-T_{1}}^{T_{1}} 1 \\, dt=\\frac{2 T_{1}}{T} \\tag{3.42}\n\\] \\(a_0\\) is the average value of \\(x(t)\\) over one period.\nFor \\(k \\neq 0\\): \\[\na_{k}=\\frac{1}{T} \\int_{-T_{1}}^{T_{1}} e^{-j k \\omega_{0} t} d t = \\frac{1}{T} \\left[ \\frac{e^{-j k \\omega_{0} t}}{-j k \\omega_{0}} \\right]_{-T_{1}}^{T_{1}}\n\\] \\[\na_{k}=\\frac{2}{k \\omega_{0} T}\\left[\\frac{e^{j k \\omega_{0} T_{1}}-e^{-j k \\omega_{0} T_{1}}}{2 j}\\right] \\tag{3.43}\n\\] Recognizing the term in brackets as \\(\\sin(k\\omega_0 T_1)\\), and using \\(\\omega_0 T = 2\\pi\\):\n\\[\na_{k}=\\frac{2 \\sin \\left(k \\omega_{0} T_{1}\\right)}{k \\omega_{0} T}=\\frac{\\sin \\left(k \\omega_{0} T_{1}\\right)}{k \\pi}, \\quad k \\neq 0 \\tag{3.44}\n\\]\n\nThe calculation of a₀ is straightforward: it’s simply the fraction of the period for which the signal is ‘on’ (at value 1). For a_k where k≠0, the integration results in a sinc function shape in the frequency domain. This is a very common and important result."
  },
  {
    "objectID": "ss_33.html#example-3.5-periodic-square-wave-part-3",
    "href": "ss_33.html#example-3.5-periodic-square-wave-part-3",
    "title": "Signals and Systems",
    "section": "Example 3.5: Periodic Square Wave (Part 3)",
    "text": "Example 3.5: Periodic Square Wave (Part 3)\nSpectrum for a 50% Duty Cycle\nConsider the case where \\(T=4T_1\\) (a 50% duty cycle, i.e., \\(x(t)=1\\) for half the period). In this case, \\(\\omega_0 T_1 = \\frac{2\\pi}{T} T_1 = \\frac{2\\pi}{4T_1} T_1 = \\frac{\\pi}{2}\\).\n\n\\(a_{0}=\\frac{1}{2}\\)\n\\(a_{k}=\\frac{\\sin (k \\pi / 2)}{k \\pi}, \\quad k \\neq 0\\)\n\nThis implies:\n\n\\(a_k = 0\\) for \\(k\\) even and non-zero.\n\\(a_k\\) alternates sign for odd \\(k\\):\n\n\\(a_1 = a_{-1} = \\frac{1}{\\pi}\\)\n\\(a_3 = a_{-3} = -\\frac{1}{3\\pi}\\)\n\\(a_5 = a_{-5} = \\frac{1}{5\\pi}\\)\n\n\n\nFor a symmetric square wave, all even harmonics (except DC) are zero. This is a characteristic of signals with odd symmetry around the midpoint of the pulse. The coefficients decrease in magnitude as 1/k, which is typical for signals with discontinuities."
  },
  {
    "objectID": "ss_33.html#example-3.5-interactive-square-wave-spectrum",
    "href": "ss_33.html#example-3.5-interactive-square-wave-spectrum",
    "title": "Signals and Systems",
    "section": "Example 3.5: Interactive Square Wave Spectrum",
    "text": "Example 3.5: Interactive Square Wave Spectrum\nObserve the effect of pulse width on the spectrum.\nAdjust the duty cycle (ratio of pulse width \\(2T_1\\) to period \\(T\\)) to see how the frequency spectrum changes. The envelope of the spectrum is a \\(\\text{sinc}\\) function.\n\nviewof duty_cycle = Inputs.range([0.1, 0.9], {step: 0.05, value: 0.5, label: \"Duty Cycle\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive plot vividly demonstrates the relationship between the time-domain characteristics (pulse width/duty cycle) and the frequency-domain spectrum. As you change the duty cycle, observe how the nulls of the sinc envelope shift. A narrower pulse spreads the energy across more frequencies (wider sinc), while a wider pulse concentrates energy (narrower sinc). This is a fundamental concept in signal analysis."
  },
  {
    "objectID": "ss_33.html#summary-applications",
    "href": "ss_33.html#summary-applications",
    "title": "Signals and Systems",
    "section": "Summary & Applications",
    "text": "Summary & Applications\nKey Takeaways:\n\nPeriodic signals can be uniquely represented by their Fourier series coefficients.\nThe Fourier Series provides a powerful tool to move between time and frequency domains.\nComplex exponentials are the fundamental building blocks.\n\nReal-World Engineering Applications:\n\nAudio Processing: Equalization, compression (MP3).\nImage Processing: JPEG compression, filtering.\nCommunication Systems: Modulation, multiplexing, channel analysis.\nFilter Design: Understanding how circuits respond to different frequencies.\nVibration Analysis: Identifying resonant frequencies in mechanical systems.\n\n\n\n\n\n\n\nTip\n\n\nPractice deriving Fourier Series for different signals and interpreting their spectra. Understanding the relationship between time-domain features and frequency-domain characteristics is crucial!\n\n\n\n\nFourier Series is not just a theoretical concept; it’s a cornerstone of modern engineering. From the music you listen to, to the images you see, and the wireless communication you use, Fourier analysis plays a critical role. Mastering this topic will give you a powerful lens through which to view and design systems."
  },
  {
    "objectID": "ss_24.html#causal-lti-systems-described-by-differential-and-difference-equations",
    "href": "ss_24.html#causal-lti-systems-described-by-differential-and-difference-equations",
    "title": "Signals and Systems",
    "section": "2.4 Causal LTI Systems Described by Differential and Difference Equations",
    "text": "2.4 Causal LTI Systems Described by Differential and Difference Equations"
  },
  {
    "objectID": "ss_24.html#system-description-using-equations",
    "href": "ss_24.html#system-description-using-equations",
    "title": "Signals and Systems",
    "section": "System Description using Equations",
    "text": "System Description using Equations\n\nContinuous-time systems are often described by Linear Constant-Coefficient Differential Equations (LCCDEs).\n\nExamples: RC circuits (Figure 1.1), vehicle motion (Figure 1.2), mechanical systems with restoring and damping forces.\n\nDiscrete-time systems are often described by Linear Constant-Coefficient Difference Equations (LCCDEs).\n\nExamples: Bank account accumulation (Example 1.10), digital simulations, signal processing filters (e.g., differencing, averaging filters).\n\n\n\nToday, we delve into a foundational aspect of Signals and Systems: how we mathematically model systems using differential and difference equations. These equations are not just abstract mathematical constructs; they are the language we use to describe a vast array of real-world engineering phenomena. Think of an RC circuit, where the capacitor’s voltage changes based on the input current and its own past charge, or a vehicle’s speed evolving under the influence of applied forces and friction. All these are elegantly captured by differential equations. Similarly, in the digital realm, from a simple moving average filter to complex control algorithms, difference equations provide the framework for understanding and designing discrete-time systems. Understanding how to interpret and solve these equations is crucial for analyzing system behavior."
  },
  {
    "objectID": "ss_24.html#implicit-system-specification",
    "href": "ss_24.html#implicit-system-specification",
    "title": "Signals and Systems",
    "section": "Implicit System Specification",
    "text": "Implicit System Specification\n\nDifferential and difference equations provide an implicit specification of system behavior.\n\nThey describe a relationship between the input (\\(x(t)\\) or \\(x[n]\\)) and the output (\\(y(t)\\) or \\(y[n]\\)), rather than an explicit expression.\n\nExample: First-order LCCDE \\[\n\\frac{d y(t)}{d t}+2 y(t)=x(t) \\tag{2.95}\n\\]\n\nThis equation relates the rate of change of \\(y(t)\\) and \\(y(t)\\) itself to \\(x(t)\\).\nIt doesn’t directly tell us what \\(y(t)\\) is for a given \\(x(t)\\).\n\n\n\nIt’s important to understand that these equations are not direct formulas for the output. They don’t simply say “y equals this function of x”. Instead, they describe a constraint or a rule that the input and output must satisfy together. For instance, the equation shown on the slide states that the sum of the output’s rate of change and twice the output itself must equal the input. To get an explicit formula for \\(y(t)\\), we need to solve this differential equation. This implicit nature is key to understanding why solving these equations requires specific techniques and additional information."
  },
  {
    "objectID": "ss_24.html#auxiliary-conditions-and-initial-rest",
    "href": "ss_24.html#auxiliary-conditions-and-initial-rest",
    "title": "Signals and Systems",
    "section": "Auxiliary Conditions and Initial Rest",
    "text": "Auxiliary Conditions and Initial Rest\n\nTo find an explicit expression for the output, we must solve the differential or difference equation.\nSolving requires more information than the equation alone: auxiliary conditions.\n\nE.g., initial speed of a car, initial voltage across a capacitor.\n\nDifferent auxiliary conditions lead to different input-output relationships.\nFor Causal LTI systems, the auxiliary conditions typically take the form of the condition of initial rest.\n\nIf the input \\(x(t)=0\\) for \\(t &lt; t_0\\), then the output \\(y(t)\\) must also be 0 for \\(t &lt; t_0\\).\nThis implies specific initial conditions at \\(t_0\\), e.g., \\(y(t_0)=0\\) (and its derivatives for higher-order systems).\n\n\n\nImagine trying to predict where a car will be after 10 seconds of constant acceleration. You can’t just know the acceleration; you also need to know its starting position and speed. Similarly, for an RC circuit, knowing the applied voltage isn’t enough; you need the initial capacitor voltage. These “starting values” are our auxiliary conditions. For most systems we study in Signals and Systems, particularly those that are Linear, Time-Invariant, and Causal – which we’ll call LTI-C systems for short – we employ a standard auxiliary condition called “initial rest.” This condition essentially states that if the system has not been acted upon by an input before a certain time, its output, and all its internal “memory” elements, must be zero up to that time. This condition is crucial because it makes the system unique, causal, and LTI. It ensures that the system’s response only begins when the input begins, and that the system “remembers” its past only via the input it has received, not through some arbitrary pre-existing state."
  },
  {
    "objectID": "ss_24.html#solving-first-order-lccde-example-2.14",
    "href": "ss_24.html#solving-first-order-lccde-example-2.14",
    "title": "Signals and Systems",
    "section": "Solving First-Order LCCDE: Example 2.14",
    "text": "Solving First-Order LCCDE: Example 2.14\nConsider the system described by: \\(\\frac{d y(t)}{d t}+2 y(t)=x(t)\\) Let the input signal be: \\(x(t)=K e^{3 t} u(t)\\)\n\nThe complete solution to a differential equation consists of two parts: \\[\ny(t)=y_{p}(t)+y_{h}(t) \\tag{2.97}\n\\]\n\n\\(y_p(t)\\): Particular solution (or forced response). This part satisfies the full differential equation with the given input.\n\\(y_h(t)\\): Homogeneous solution (or natural response). This part is a solution to the homogeneous differential equation (with input set to zero): \\[\n\\frac{d y(t)}{d t}+2 y(t)=0 \\tag{2.98}\n\\]\n\n\n\nLet’s walk through an example to see how we solve these. We have a first-order differential equation modeling a system, and we’re applying an exponential input x(t). The general approach to solving such an equation is to break the problem into two parts. First, we find the particular solution, which directly responds to the input signal. This is the forced part of the response. Second, we find the homogeneous solution, which describes the system’s inherent behavior—how it would react without any external input, or how it would ‘ring out’ if disturbed. This is often called the natural response. The total solution is simply the sum of these two components."
  },
  {
    "objectID": "ss_24.html#example-2.14-finding-the-particular-solution",
    "href": "ss_24.html#example-2.14-finding-the-particular-solution",
    "title": "Signals and Systems",
    "section": "Example 2.14: Finding the Particular Solution",
    "text": "Example 2.14: Finding the Particular Solution\n\nFor \\(t&gt;0\\), the input is \\(x(t)=K e^{3 t}\\).\nWe hypothesize that the particular solution \\(y_p(t)\\) will have the same exponential form as the input for \\(t&gt;0\\): \\[\ny_{p}(t)=Y e^{3 t} \\tag{2.99}\n\\] where \\(Y\\) is a coefficient to be determined.\nSubstitute \\(y_p(t)\\) and \\(x(t)\\) into the original differential equation (\\(\\frac{d y(t)}{d t}+2 y(t)=x(t)\\)) for \\(t&gt;0\\): \\[\n\\frac{d}{dt}(Y e^{3 t}) + 2 (Y e^{3 t}) = K e^{3 t}  \\quad \\implies \\quad 3 Y e^{3 t}+2 Y e^{3 t}=K e^{3 t} \\tag{2.100}\n\\]\nCancel \\(e^{3 t}\\) from both sides, then solve for \\(Y\\): \\[\n5 Y = K \\implies Y = \\frac{K}{5} \\tag{2.101, 2.102}\n\\]\nThus, the particular solution for \\(t&gt;0\\) is: \\[\ny_{p}(t)=\\frac{K}{5} e^{3 t}, \\quad t&gt;0 \\tag{2.103}\n\\]\n\n\nTo find the particular solution when the input is an exponential, a common and effective strategy is to assume that the particular solution itself will be an exponential of the same form. So, for an input K * e^(3t), we propose a particular solution Y * e^(3t). We then substitute this proposed solution and the input back into the original differential equation. The derivative of Y * e^(3t) is 3Y * e^(3t). After substitution, we can factor out e^(3t) and solve for the unknown coefficient Y. In this case, we find Y = K/5. This gives us the part of the output that is directly “forced” by the input signal."
  },
  {
    "objectID": "ss_24.html#example-2.14-finding-the-homogeneous-solution",
    "href": "ss_24.html#example-2.14-finding-the-homogeneous-solution",
    "title": "Signals and Systems",
    "section": "Example 2.14: Finding the Homogeneous Solution",
    "text": "Example 2.14: Finding the Homogeneous Solution\n\nNow, we need to find the homogeneous solution \\(y_h(t)\\), which satisfies the homogeneous differential equation: \\[\n\\frac{d y(t)}{d t}+2 y(t)=0 \\tag{2.98}\n\\]\nWe hypothesize an exponential form for the homogeneous solution: \\[\ny_{h}(t)=A e^{s t} \\tag{2.104}\n\\] where \\(A\\) and \\(s\\) are constants.\nSubstitute \\(y_h(t)\\) into the homogeneous equation: \\[\n\\frac{d}{dt}(A e^{s t}) + 2 (A e^{s t}) = 0 \\quad \\implies \\quad A s e^{s t}+2 A e^{s t}=0 \\tag{2.105}\n\\]\nFactor out \\(A e^{s t}\\): \\[\nA e^{s t}(s+2)=0\n\\]\nFor this to be true for all \\(t\\), we must have \\(s+2=0\\), which implies \\(s = -2\\).\nTherefore, the homogeneous solution is: \\[\ny_{h}(t)=A e^{-2 t}\n\\] where \\(A\\) is an arbitrary constant determined by auxiliary conditions.\n\n\nNext, let’s find the homogeneous solution. This represents the system’s “natural” way of responding, independent of the specific input. We again assume an exponential form, A * e^(st). When we substitute this into the homogeneous equation (where the input is zero), we find a condition for ‘s’. In this case, s must be -2. This means A * e^(-2t) is a valid solution to the homogeneous equation for any constant A. This A is the arbitrary constant that will be determined by our auxiliary or initial conditions. The -2t in the exponent indicates an exponentially decaying component, often related to the system’s stability or its natural frequency."
  },
  {
    "objectID": "ss_24.html#example-2.14-total-solution-and-initial-rest",
    "href": "ss_24.html#example-2.14-total-solution-and-initial-rest",
    "title": "Signals and Systems",
    "section": "Example 2.14: Total Solution and Initial Rest",
    "text": "Example 2.14: Total Solution and Initial Rest\n\nCombine particular and homogeneous solutions to get the general solution for \\(t&gt;0\\): \\[\ny(t)=A e^{-2 t}+\\frac{K}{5} e^{3 t}, \\quad t&gt;0 \\tag{2.106}\n\\]\nApply the condition of initial rest:\n\nSince the input \\(x(t)=K e^{3 t} u(t)\\) implies \\(x(t)=0\\) for \\(t&lt;0\\), a causal LTI system will have \\(y(t)=0\\) for \\(t&lt;0\\).\nThis means the initial condition is \\(y(0)=0\\).\n\nSolve for \\(A\\) using \\(y(0)=0\\) in the general solution: \\[\n0 = A e^{-2(0)} + \\frac{K}{5} e^{3(0)}  \\implies  0 = A + \\frac{K}{5}  \\implies  A = -\\frac{K}{5}\n\\]\nSubstitute \\(A\\) back into the general solution to obtain the final, complete response: \\[\ny(t)=\\frac{K}{5} e^{3 t} - \\frac{K}{5} e^{-2 t}, \\quad t&gt;0\n\\]\nCombining with \\(y(t)=0\\) for \\(t&lt;0\\): \\[\ny(t)=\\frac{K}{5}\\left[e^{3 t}-e^{-2 t}\\right] u(t) \\tag{2.108}\n\\]\n\n\nNow that we have both the particular and homogeneous solutions, we combine them to form the general solution. Notice that it still has an unknown constant A. This is where our auxiliary condition of “initial rest” comes into play. Since our input x(t) effectively starts at t=0 (due to the u(t)), the condition of initial rest implies that the output y(t) must also be zero before t=0. Thus, we can set y(0) = 0. Plugging this into our general solution allows us to solve for A, making the solution unique. In this example, A turns out to be -K/5. With A determined, we have the complete and unique output y(t) for the given input and initial rest condition. This final form includes the unit step u(t) to correctly represent the causality, meaning the response starts at t=0."
  },
  {
    "objectID": "ss_24.html#example-2.14-output-signal-plot",
    "href": "ss_24.html#example-2.14-output-signal-plot",
    "title": "Signals and Systems",
    "section": "Example 2.14: Output Signal Plot",
    "text": "Example 2.14: Output Signal Plot\nAdjust the input amplitude K and observe the output response. \n\nviewof K_slider = Inputs.range([0, 10], {\n  label: html`&lt;span style=\"font-size: 0.8em;\"&gt;Amplitude K&lt;/span&gt;`,\n  step: 1,\n  value: 5,\n});\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s visualize this. Here, you can change the amplitude ‘K’ of the input signal using the slider. Observe how the input signal x(t) starts at t=0 and grows exponentially. The output y(t) also starts at t=0, due to initial rest. It initially grows because of the e^(3t) term from the input, but also incorporates the decaying e^(-2t) term from the system’s natural response. Notice how if you double K, the output also doubles, confirming the linearity of the system under initial rest conditions. This interactive plot helps us gain an intuitive understanding of how the system processes the input and how the different components of the solution manifest over time."
  },
  {
    "objectID": "ss_24.html#general-nth-order-lccde",
    "href": "ss_24.html#general-nth-order-lccde",
    "title": "Signals and Systems",
    "section": "General Nth-Order LCCDE",
    "text": "General Nth-Order LCCDE\n\nA general \\(N\\)th-order linear constant-coefficient differential equation is given by: \\[\n\\sum_{k=0}^{N} a_{k} \\frac{d^{k} y(t)}{d t^{k}}=\\sum_{k=0}^{M} b_{k} \\frac{d^{k} x(t)}{d t^{k}} \\tag{2.109}\n\\]\n\n\\(N\\) is the order of the system (highest derivative of \\(y(t)\\)).\nIf \\(N=0\\), then \\(y(t)=\\frac{1}{a_{0}} \\sum_{k=0}^{M} b_{k} \\frac{d^{k} x(t)}{d t^{k}}\\), which is an explicit function of \\(x(t)\\) and its derivatives (no auxiliary conditions needed).\n\nSolution approach: Still a sum of particular and homogeneous solutions.\n\nHomogeneous equation: \\(\\sum_{k=0}^{N} a_{k} \\frac{d^{k} y(t)}{d t^{k}}=0\\).\n\nInitial Rest Condition: For \\(x(t)=0\\) for \\(t \\leq t_0\\), the output \\(y(t)=0\\) for \\(t \\leq t_0\\). This requires the initial conditions: \\[\ny\\left(t_{0}\\right)=\\frac{d y\\left(t_{0}\\right)}{d t}=\\ldots=\\frac{d^{N-1} y\\left(t_{0}\\right)}{d t^{N-1}}=0 \\tag{2.112}\n\\]\n\n\nThe concepts we just covered for a first-order differential equation extend directly to more complex, higher-order systems. Equation 2.109 is the general form for an Nth-order LCCDE. The ‘N’ here refers to the highest derivative of the output y(t). If N is zero, the equation simplifies, and y(t) becomes an explicit function of the input and its derivatives; in this highly simplified case, no auxiliary conditions are strictly needed. However, for N greater than or equal to 1, the output is implicitly defined, and we still follow the same strategy: finding particular and homogeneous solutions. For LTI-C systems, the initial rest condition applies, but for an Nth-order system, it means that a total of N initial conditions (the output and its first N-1 derivatives) must all be zero at the starting point t0 where the input becomes non-zero. This provides the N constants needed to uniquely determine the full solution. We will later introduce more advanced tools like Laplace Transforms to greatly simplify the solution of these higher-order equations."
  },
  {
    "objectID": "ss_24.html#linear-constant-coefficient-difference-equations-lccdes",
    "href": "ss_24.html#linear-constant-coefficient-difference-equations-lccdes",
    "title": "Signals and Systems",
    "section": "Linear Constant-Coefficient Difference Equations (LCCDEs)",
    "text": "Linear Constant-Coefficient Difference Equations (LCCDEs)\n\nThe discrete-time counterpart to continuous-time LCCDEs is the \\(N\\)th-order linear constant-coefficient difference equation: \\[\n\\sum_{k=0}^{N} a_{k} y[n-k]=\\sum_{k=0}^{M} b_{k} x[n-k] \\tag{2.113}\n\\]\nSolution approach: Analogous to differential equations – sum of a particular solution and a homogeneous solution.\n\nHomogeneous equation: \\(\\sum_{k=0}^{N} a_{k} y[n-k]=0\\).\n\nCondition of Initial Rest: If \\(x[n]=0\\) for \\(n&lt;n_0\\), then \\(y[n]=0\\) for \\(n&lt;n_0\\).\n\nUnder the condition of initial rest, the system described by the difference equation is LTI and causal.\n\n\n\nJust as differential equations describe continuous-time systems, difference equations are fundamental for discrete-time systems. Equation 2.113 is the general form. The solution methodology mirrors the continuous-time case: find a particular solution responding to the input, and a homogeneous solution representing the system’s natural behavior. And crucially, just as with differential equations, difference equations require auxiliary conditions for a unique solution. Again, for LTI and causal systems, we adopt the “initial rest” condition. This states that if the input is zero before a certain time n0, then the output must also be zero before n0. This ensures causality and preserves the LTI properties."
  },
  {
    "objectID": "ss_24.html#recursive-vs.-nonrecursive-systems",
    "href": "ss_24.html#recursive-vs.-nonrecursive-systems",
    "title": "Signals and Systems",
    "section": "Recursive vs. Nonrecursive Systems",
    "text": "Recursive vs. Nonrecursive Systems\nRecursive Equation (IIR)\n\nIf \\(N \\ge 1\\) (output depends on past outputs), the equation can be rearranged to directly compute \\(y[n]\\): \\[\ny[n]=\\frac{1}{a_{0}}\\left\\{\\sum_{k=0}^{M} b_{k} x[n-k]-\\sum_{k=1}^{N} a_{k} y[n-k]\\right\\} \\tag{2.115}\n\\]\nOutput \\(y[n]\\) depends on both current/past inputs and past outputs.\nRequires auxiliary conditions (e.g., \\(y[n_0-1], \\ldots, y[n_0-N]\\)) to start the recursion.\nOften leads to Infinite Impulse Response (IIR) systems.\n\nNonrecursive Equation (FIR)\n\nIn the special case when \\(N = 0\\): \\[\ny[n]=\\sum_{k=0}^{M}\\left(\\frac{b_{k}}{a_{0}}\\right) x[n-k] \\tag{2.116}\n\\]\nOutput \\(y[n]\\) depends only on present and past inputs.\nNo auxiliary conditions are directly needed, as output is explicit.\nAlways a Finite Impulse Response (FIR) system.\n\nIts impulse response \\(h[n]\\) has finite duration: \\[\nh[n]= \\begin{cases}\\frac{b_{n}}{a_{0}}, & 0 \\leq n \\leq M \\\\ 0, & \\text { otherwise }\\end{cases} \\tag{2.117}\n\\]\n\n\n\nA significant distinction in discrete-time systems is between recursive and nonrecursive equations. If the current output y[n] depends on past outputs (i.e., N is greater than or equal to 1), the equation is recursive. This forms a feedback loop where past results influence current calculations. Because of this feedback, initial conditions are essential to kickstart the process, and such systems typically have an impulse response that goes on indefinitely, hence “Infinite Impulse Response” or IIR.\nIn contrast, if N is zero, the current output y[n] depends only on current and past inputs. This is a nonrecursive equation. There’s no feedback from the output back into the system. As a result, no auxiliary conditions are needed, and the impulse response is always finite in duration, making these “Finite Impulse Response” or FIR systems. FIR systems are simpler to analyze and design in certain contexts."
  },
  {
    "objectID": "ss_24.html#solving-first-order-lccde-example-2.15",
    "href": "ss_24.html#solving-first-order-lccde-example-2.15",
    "title": "Signals and Systems",
    "section": "Solving First-Order LCCDE: Example 2.15",
    "text": "Solving First-Order LCCDE: Example 2.15\nConsider the difference equation: \\(y[n]-\\frac{1}{2} y[n-1]=x[n]\\)\nRearranging for recursive computation: \\(y[n]=x[n]+\\frac{1}{2} y[n-1]\\)\nLet the input be an impulse: \\(x[n]=K \\delta[n]\\)\n\nApply initial rest condition:\n\nSince \\(x[n]=0\\) for \\(n&lt;-1\\), initial rest implies \\(y[n]=0\\) for \\(n&lt;-1\\).\nTherefore, \\(y[-1]=0\\).\n\nIterative Solution for \\(n \\ge 0\\):\n\n\\(y[0] = x[0] + \\frac{1}{2} y[-1] = K \\delta[0] + \\frac{1}{2}(0) = K\\)\n\\(y[1] = x[1] + \\frac{1}{2} y[0] = K \\delta[1] + \\frac{1}{2} K = 0 + \\frac{1}{2} K = \\frac{1}{2} K\\)\n\\(y[2] = x[2] + \\frac{1}{2} y[1] = K \\delta[2] + \\frac{1}{2} \\left(\\frac{1}{2} K\\right) = 0 + \\left(\\frac{1}{2}\\right)^2 K = \\left(\\frac{1}{2}\\right)^2 K\\)\n\\(\\vdots\\)\nIn general, for \\(n \\ge 0\\): \\(y[n] = \\left(\\frac{1}{2}\\right)^n K\\).\n\nImpulse Response (\\(K=1\\)): \\[\nh[n]=\\left(\\frac{1}{2}\\right)^{n} u[n] \\tag{2.125}\n\\] This is an Infinite Impulse Response (IIR) system.\n\n\nLet’s see a discrete-time example. We have a first-order difference equation, rearranged into its recursive form to show that \\(y[n]\\) depends on \\(y[n-1]\\). We apply an impulse input, \\(K \\delta[n]\\). The initial rest condition means that before n=0, y[n] is zero, so y[-1] is zero. From there, we can iteratively calculate y[n] for n=0, 1, 2, ....\nAt n=0, y[0] is x[0] plus half of y[-1], which gives K. At n=1, y[1] is x[1] plus half of y[0], which gives (1/2)K. This pattern continues, leading to \\(y[n] = (1/2)^n K\\) for n greater than or equal to zero. If K=1, this directly gives us the impulse response h[n] = (1/2)^n u[n]. Since this response extends indefinitely, it confirms that this is an IIR system, characteristic of recursive difference equations."
  },
  {
    "objectID": "ss_24.html#example-2.15-impulse-response-of-an-iir-system",
    "href": "ss_24.html#example-2.15-impulse-response-of-an-iir-system",
    "title": "Signals and Systems",
    "section": "Example 2.15: Impulse Response of an IIR System",
    "text": "Example 2.15: Impulse Response of an IIR System\nObserve the decaying impulse response for different coefficients. \n\nviewof alpha_slider = Inputs.range([-0.9, 0.9], {\n  label: html`&lt;span style=\"font-size: 0.8em;\"&gt;Coefficient 'a'&lt;/span&gt;`,\n  step: 0.1,\n  value: 0.5,\n});\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere’s an interactive visualization of the impulse response we just derived. The difference equation is y[n] = x[n] + a*y[n-1]. The impulse response is h[n] = a^n u[n]. You can adjust the coefficient ‘a’ using the slider. Notice how the shape of the impulse response changes: - When a is between 0 and 1 (like 0.5 in our example), the response decays exponentially, indicating a stable system. - When a is negative (e.g., -0.5), it oscillates while decaying. - If the absolute value of a is greater than or equal to 1, the response will not decay, indicating an unstable or marginally stable system.\nThis helps visualize why these are called Infinite Impulse Response (IIR) systems—the response theoretically continues indefinitely, though it can decay quickly depending on the value of ‘a’."
  },
  {
    "objectID": "ss_24.html#block-diagram-representations",
    "href": "ss_24.html#block-diagram-representations",
    "title": "Signals and Systems",
    "section": "Block Diagram Representations",
    "text": "Block Diagram Representations\n\nRepresenting systems using block diagrams offers several advantages:\n\nProvides a pictorial representation for better intuitive understanding of system structure.\nUseful for simulation (analog or digital computers).\nGuides hardware implementation for physical systems.\n\nBasic Operations for Discrete-Time Systems:\n\nAdder: Sums multiple input signals.\nMultiplier: Scales a signal by a constant coefficient.\nUnit Delay: Outputs the input from the previous time step. This is the memory element.\n\n\n\nMoving beyond mathematical equations, block diagrams offer a powerful and intuitive way to visualize and implement systems. They break down a complex system into an interconnection of simple, fundamental operations. This not only enhances our understanding of how signals flow and are processed within the system but also provides a direct blueprint for how these systems could be simulated on computers or even built in hardware. For discrete-time systems, we use three basic building blocks: adders to combine signals, multipliers to scale them by coefficients, and crucially, a unit delay element, which serves as the system’s memory, holding onto a value from the previous time step."
  },
  {
    "objectID": "ss_24.html#discrete-time-first-order-system-block-diagram",
    "href": "ss_24.html#discrete-time-first-order-system-block-diagram",
    "title": "Signals and Systems",
    "section": "Discrete-Time First-Order System Block Diagram",
    "text": "Discrete-Time First-Order System Block Diagram\nSystem equation: \\(y[n]+a y[n-1]=b x[n]\\)\nRearranged for direct computation: \\(y[n] = -a y[n-1] + b x[n]\\)\n\n\n\n\n\ngraph LR\n    x_n(\"x[n]\") --&gt; mult_b{b}\n    mult_b --&gt; sum1(\"$$+$$\")\n    delay(\"$$z^{-1}$$\") -- y[n-1] --&gt; mult_a{-a}\n    mult_a --&gt; sum1\n    sum1 -- y[n] --&gt; delay\n    sum1 -- y[n] --&gt; y_n(\"y[n]\")\n\n    subgraph Operations\n        sum1\n        mult_b\n        mult_a\n        delay\n    end\n\n\n\n\n\n\n\nThe delay element (\\(z^{-1}\\)) represents the system’s memory. Its initial state corresponds to the auxiliary condition \\(y[-1]\\).\nThis diagram clearly shows feedback, characteristic of recursive (IIR) systems.\n\n\nLet’s construct a block diagram for our first-order discrete-time difference equation: y[n] = -a*y[n-1] + b*x[n]. The term b*x[n] is created by multiplying the input x[n] by b. The term -a*y[n-1] is generated from the output y[n]. The output y[n] goes into a delay element, which stores y[n-1]. This y[n-1] is then multiplied by -a. Finally, these two terms (b*x[n] and -a*y[n-1]) are summed to produce the current output y[n]. Notice the feedback loop: the output y[n] is fed back through the delay and multiplier to influence future outputs. The delay element is explicitly where the system’s “memory” resides, and its initial stored value y[-1] is the necessary auxiliary condition. This feedback structure is a hallmark of recursive systems."
  },
  {
    "objectID": "ss_24.html#basic-elements-of-block-diagram",
    "href": "ss_24.html#basic-elements-of-block-diagram",
    "title": "Signals and Systems",
    "section": "Basic Elements of Block Diagram",
    "text": "Basic Elements of Block Diagram\nFor equation \\(y[n] = -a*y[n-1] + b*x[n]\\) : (a) an adder; (b) multiplication by a coefficient; (c) a unit delay; (d) overall representation."
  },
  {
    "objectID": "ss_24.html#basic-operations-for-continuous-time-systems",
    "href": "ss_24.html#basic-operations-for-continuous-time-systems",
    "title": "Signals and Systems",
    "section": "Basic Operations for Continuous-Time Systems",
    "text": "Basic Operations for Continuous-Time Systems\n\nSimilar basic elements found in continuous-time block diagrams:\n\nAdder: Sums multiple input signals.\nMultiplier: Scales a signal by a constant coefficient.\n\nCrucially, for continuous-time, instead of a differentiator (which is difficult to implement and sensitive to noise), we use an integrator.\n\nIntegrator: \\(\\int_{-\\infty}^{t} (\\cdot) d\\tau\\). This is the memory storage element for continuous-time systems.\n\n\n\nFor continuous-time systems, the fundamental building blocks for block diagrams are similar: adders and multipliers. However, a key difference emerges when we deal with derivatives. While analytically we define systems with derivatives, practically, differentiators are very hard to build and are notoriously sensitive to noise amplification. Think about trying to build a circuit that perfectly differentiates a signal – any tiny bit of noise would be greatly exaggerated. So, for practical implementation and stable representations, we prefer to work with integrators instead. An integrator performs the inverse operation of differentiation, accumulating the input over time, and it naturally acts as the memory element in continuous-time systems, much like a capacitor storing charge."
  },
  {
    "objectID": "ss_24.html#continuous-time-first-order-system-block-diagram",
    "href": "ss_24.html#continuous-time-first-order-system-block-diagram",
    "title": "Signals and Systems",
    "section": "Continuous-Time First-Order System Block Diagram",
    "text": "Continuous-Time First-Order System Block Diagram\nSystem equation: \\(\\frac{d y(t)}{d t}+a y(t)=b x(t)\\)\nRearranged for integration: \\(\\frac{d y(t)}{d t}=b x(t)-a y(t)\\)\nIntegrate from \\(-\\infty\\) to \\(t\\): \\(y(t)=\\int_{-\\varkappa}^{t}[b x(\\tau)-a y(\\tau)] d \\tau\\)\n\n\n\n\n\ngraph LR\n    x_t(\"x(t)\") --&gt; mult_b{b}\n    mult_b --&gt; sum1(\"$$+$$\")\n    y_t(\"y(t)\") --&gt; mult_a{-a}\n    mult_a --&gt; sum1\n    sum1 --&gt; intg(\"∫dτ\")\n    intg --&gt; y_t\n\n\n\n\n\n\n\nThe integrator is the memory element, storing the accumulated signal.\nThe value \\(y(t_0)\\) represents the initial condition stored by the integrator.\nThis representation is the basis for analog computer simulations.\n\n\nLet’s visualize the first-order continuous-time LCCDE. Instead of expressing y(t) directly, we rearrange the equation to isolate the derivative of y(t). So, dy(t)/dt = b*x(t) - a*y(t). If we integrate both sides from negative infinity to t, the left side becomes y(t), and the right side becomes the integral of b*x(tau) - a*y(tau).\nIn the block diagram: x(t) is multiplied by b. y(t) is multiplied by -a. These two signals are added together. The sum is then fed into an integrator. The output of the integrator is y(t). Again, we see a feedback loop where y(t) is fed back to influence its own derivative. The integrator is the system’s memory, storing its past values. This type of block diagram directly forms the basis of historical analog computers and remains a fundamental way to understand continuous-time system implementation."
  },
  {
    "objectID": "ss_24.html#basic-elements-of-block-diagram-1",
    "href": "ss_24.html#basic-elements-of-block-diagram-1",
    "title": "Signals and Systems",
    "section": "Basic Elements of Block Diagram",
    "text": "Basic Elements of Block Diagram\n\nan adder; (b) multiplication by a coefficient; (c) a differentiator; (d) overall representation using differentiator; (d) overall representation using integrator"
  },
  {
    "objectID": "ss_24.html#continous-system-in-differential-equations",
    "href": "ss_24.html#continous-system-in-differential-equations",
    "title": "Signals and Systems",
    "section": "Continous System in Differential Equations",
    "text": "Continous System in Differential Equations\nMass–Spring System (Oscillator)\nA mass attached to a spring (ignoring friction first).\n\nHooke’s law: restoring force \\(F = -kx\\)\nNewton’s second law: \\(F = m \\dfrac{d^2x}{dt^2}\\)\n\nSo:\n\\[\nm \\frac{d^2x}{dt^2} + kx = 0\n\\]\nThis second-order ODE governs the oscillation. Its solution is sinusoidal:\n\\[\nx(t) = A \\cos(\\omega t) + B \\sin(\\omega t), \\quad \\omega = \\sqrt{\\tfrac{k}{m}}\n\\]"
  },
  {
    "objectID": "ss_24.html#continous-system-in-differential-equations-1",
    "href": "ss_24.html#continous-system-in-differential-equations-1",
    "title": "Signals and Systems",
    "section": "Continous System in Differential Equations",
    "text": "Continous System in Differential Equations\nRC Circuit (Charging a Capacitor)\nA resistor \\(R\\) and capacitor \\(C\\) in series with a voltage source \\(V\\). Kirchhoff’s law:\n\\[\nV = V_R + V_C = Ri(t) + \\frac{q(t)}{C}\n\\]\nSince \\(i(t) = \\dfrac{dq}{dt}\\):\n\\[\nR \\frac{dq}{dt} + \\frac{q}{C} = V\n\\]\nThis first-order ODE models how charge (and voltage across capacitor) evolves. The solution is exponential:\n\\[\nq(t) = CV \\left(1 - e^{-t/RC}\\right)\n\\]\nPendulum (Nonlinear System)\nFor a pendulum of length \\(L\\), angle \\(\\theta(t)\\):\n\\[\n\\frac{d^2 \\theta}{dt^2} + \\frac{g}{L}\\sin(\\theta) = 0\n\\]\nThis is a nonlinear ODE (due to \\(\\sin\\theta\\))."
  },
  {
    "objectID": "ss_24.html#discrete-system-in-difference-equations",
    "href": "ss_24.html#discrete-system-in-difference-equations",
    "title": "Signals and Systems",
    "section": "Discrete System in Difference Equations",
    "text": "Discrete System in Difference Equations\nDigital RC Circuit (Discrete Approximation)\nIf you sample the continuous RC circuit at time steps of length \\(\\Delta t\\), the capacitor voltage \\(v[n]\\) satisfies a first-order difference equation:\n\\[\nv[n+1] = \\left(1 - \\frac{\\Delta t}{RC}\\right) v[n] + \\frac{\\Delta t}{RC} V\n\\]\nThis models how the voltage changes step by step in a digital simulation."
  },
  {
    "objectID": "ss_24.html#spring-mass-system-with-numerical-integration",
    "href": "ss_24.html#spring-mass-system-with-numerical-integration",
    "title": "Signals and Systems",
    "section": "Spring-Mass System with Numerical Integration",
    "text": "Spring-Mass System with Numerical Integration\nDiscretizing Newton’s second law for a spring–mass:\n\\[\nm \\frac{d^2x}{dt^2} = -kx\n\\]\nUsing finite differences (\\(x_{n+1} - 2x_n + x_{n-1}\\)/\\(\\Delta t^2\\)):\n\\[\nx_{n+1} = 2x_n - x_{n-1} - \\frac{k}{m} \\Delta t^2 \\, x_n\n\\]\nThis is a second-order difference equation that simulates oscillations step by step.\nControl Systems (Z-Domain Models)\nDigital controllers (like in robotics or motor drives) are governed by difference equations. Example: A discrete-time first-order system:\n\\[\ny[n+1] = a y[n] + b u[n]\n\\]\nwhere \\(y[n]\\) is system output and \\(u[n]\\) is input at time step \\(n\\)."
  },
  {
    "objectID": "ss_24.html#conclusion",
    "href": "ss_24.html#conclusion",
    "title": "Signals and Systems",
    "section": "Conclusion",
    "text": "Conclusion\n\nLCCDEs are fundamental for describing causal LTI systems in both continuous and discrete domains.\nSolving involves finding a particular solution (forced response) and a homogeneous solution (natural response).\nAuxiliary conditions, particularly the condition of initial rest, are crucial for unique, causal, and LTI solutions.\nDiscrete-time systems are categorized as Recursive (IIR) or Nonrecursive (FIR) based on output dependence.\nBlock diagrams (using adders, multipliers, delays/integrators) provide valuable visual understanding and aid implementation.\nAhead: We will develop more powerful frequency-domain tools (e.g., Laplace and Z-transforms) to simplify solving these equations and further analyze complex system properties.\n\n\nTo summarize, differential and difference equations are the bedrock for modeling LTI causal systems. We’ve learned that solving them involves combining a particular solution, driven by the input, with a homogeneous solution, representing the system’s natural behavior. Initial rest is the key auxiliary condition for ensuring the crucial properties of causality and LTI. We also explored the distinction between FIR and IIR systems in discrete time and saw how block diagrams offer a powerful visual and practical representation. In the coming chapters, we’ll build on this foundation by introducing frequency-domain transforms that will provide even more efficient and insightful ways to analyze these complex and fascinating systems. Thank you."
  },
  {
    "objectID": "ss_22.html#continuous-time-lti-systems-the-convolution-integral",
    "href": "ss_22.html#continuous-time-lti-systems-the-convolution-integral",
    "title": "Signal and Systems",
    "section": "CONTINUOUS-TIME LTI SYSTEMS: THE CONVOLUTION INTEGRAL",
    "text": "CONTINUOUS-TIME LTI SYSTEMS: THE CONVOLUTION INTEGRAL\nECE Undergraduate Course\nImron Rosyadi\n\nWe’ve seen how the convolution sum governs discrete-time LTI systems. Now, we’ll develop the parallel concept for continuous-time systems. Our goal is the same: to find a way to express any continuous-time signal in terms of impulses, which will then let us find a system’s output for any input, just by knowing its impulse response."
  },
  {
    "objectID": "ss_22.html#representing-continuous-signals-with-impulses",
    "href": "ss_22.html#representing-continuous-signals-with-impulses",
    "title": "Signal and Systems",
    "section": "Representing Continuous Signals with Impulses",
    "text": "Representing Continuous Signals with Impulses\nWe start by approximating a continuous signal \\(x(t)\\) with a “staircase” of narrow pulses.\nEach pulse has a width \\(\\Delta\\) and height \\(x(k\\Delta)\\).\nThe approximation, \\(\\hat{x}(t)\\), is a sum of scaled and shifted rectangular pulses:\n\\[\n\\hat{x}(t) = \\sum_{k=-\\infty}^{\\infty} x(k\\Delta) \\delta_{\\Delta}(t-k\\Delta)\\Delta\n\\]\nwhere \\(\\delta_{\\Delta}(t)\\) is a rectangular pulse of width \\(\\Delta\\) and height \\(1/\\Delta\\).\n\n\n\n\n\n\n\nLook at the plot. The smooth blue line is our original signal, \\(x(t)\\). The orange bars represent the staircase approximation, \\(\\hat{x}(t)\\). Each bar is a narrow pulse. As you can imagine, if we make the width \\(\\Delta\\) of these pulses smaller and smaller, the orange staircase will become a better and better approximation of the blue curve."
  },
  {
    "objectID": "ss_22.html#from-summation-to-integration",
    "href": "ss_22.html#from-summation-to-integration",
    "title": "Signal and Systems",
    "section": "From Summation to Integration",
    "text": "From Summation to Integration\nAs we shrink the pulse width, \\(\\Delta \\rightarrow 0\\):\n\nThe staircase approximation \\(\\hat{x}(t)\\) becomes the signal \\(x(t)\\).\nThe narrow pulse \\(\\delta_{\\Delta}(t)\\) becomes the ideal impulse \\(\\delta(t)\\).\nThe summation becomes an integral.\n\n\\[\n\\lim_{\\Delta \\to 0} \\sum_{k=-\\infty}^{\\infty} x(k\\Delta) \\delta_{\\Delta}(t-k\\Delta)\\Delta \\quad \\longrightarrow \\quad \\int_{-\\infty}^{\\infty} x(\\tau) \\delta(t-\\tau) d\\tau\n\\]\nThis gives us the sifting property for continuous-time signals:\n\\[\nx(t) = \\int_{-\\infty}^{\\infty} x(\\tau) \\delta(t-\\tau) d\\tau\n\\]\n\nThis limiting process is the heart of the transition from discrete to continuous. The sum, which works with discrete points, transforms into an integral, which works over a continuum. The result is the continuous-time sifting property. Just like its discrete counterpart, it says we can represent any signal \\(x(t)\\) as a “continuous sum” (an integral) of weighted, shifted impulses. The weight for the impulse at time \\(\\tau\\) is the value of the signal at that time, \\(x(\\tau)\\)."
  },
  {
    "objectID": "ss_22.html#the-convolution-integral",
    "href": "ss_22.html#the-convolution-integral",
    "title": "Signal and Systems",
    "section": "The Convolution Integral",
    "text": "The Convolution Integral\nBy applying linearity and time-invariance, we arrive at the system output \\(y(t)\\):\n\nInput: \\(x(t) = \\int x(\\tau) \\delta(t-\\tau) d\\tau\\) (An integral of weighted impulses)\nLinearity: The output is the integral of the responses to those weighted impulses.\nTime-Invariance: The response to a shifted impulse \\(\\delta(t-\\tau)\\) is a shifted impulse response \\(h(t-\\tau)\\).\n\nCombining these gives the Convolution Integral:\n\\[\ny(t) = \\int_{-\\infty}^{+\\infty} x(\\tau) h(t-\\tau) d\\tau\n\\]\nThis is denoted as \\(y(t) = x(t) * h(t)\\). Once again, an LTI system is completely characterized by its impulse response \\(h(t)\\).\n\nThe logic is exactly the same as for the discrete-time case. Linearity means we can consider the response to each infinitesimal slice of the input, \\(x(\\tau)d\\tau\\), and then integrate (sum) them all up. Time-invariance means the response to a slice at time \\(\\tau\\) is just a shifted and scaled version of the impulse response, specifically \\(x(\\tau)h(t-\\tau)d\\tau\\). Integrating over all \\(\\tau\\) gives us the total output at time \\(t\\). The convolution integral is the cornerstone of continuous-time LTI system analysis."
  },
  {
    "objectID": "ss_22.html#the-flip-and-slide-method-continuous",
    "href": "ss_22.html#the-flip-and-slide-method-continuous",
    "title": "Signal and Systems",
    "section": "The “Flip-and-Slide” Method (Continuous)",
    "text": "The “Flip-and-Slide” Method (Continuous)\nWe evaluate \\(y(t) = \\int x(\\tau)h(t-\\tau)d\\tau\\) for each output time t.\nProcedure for a fixed t:\n\nPlot vs. \\(\\tau\\): Graph the input \\(x(\\tau)\\) and impulse response \\(h(\\tau)\\).\nFlip: Time-reverse \\(h(\\tau)\\) to get \\(h(-\\tau)\\).\nSlide: Shift \\(h(-\\tau)\\) by \\(t\\) to get \\(h(t-\\tau)\\).\nMultiply: Form the product signal \\(x(\\tau)h(t-\\tau)\\).\nIntegrate: Compute the total area under the product signal. This area is the value of \\(y(t)\\).\n\nRepeat for all t to find the entire output signal \\(y(t)\\).\n\nThe graphical method is also completely analogous. For any specific moment t where we want to find the output, we “flip” the impulse response and “slide” it into position. Then we multiply it by the input signal and calculate the area of the resulting shape. That area is our output value, \\(y(t)\\). We then slide to the next value of t and repeat."
  },
  {
    "objectID": "ss_22.html#example-rc-circuit-integrator",
    "href": "ss_22.html#example-rc-circuit-integrator",
    "title": "Signal and Systems",
    "section": "Example: RC Circuit (Integrator)",
    "text": "Example: RC Circuit (Integrator)\nLet’s find the response of a simple integrator to an exponential input.\nProblem\n\nInput: \\(x(t) = e^{-at}u(t)\\), for \\(a &gt; 0\\).\nImpulse Response: \\(h(t) = u(t)\\) (This is an ideal integrator).\n\nAnalysis\n\nFor \\(t &lt; 0\\), there’s no overlap, so \\(y(t) = 0\\).\nFor \\(t \\ge 0\\), the overlap is for \\(0 &lt; \\tau &lt; t\\). \\[\ny(t) = \\int_{0}^{t} e^{-a\\tau} d\\tau = \\frac{1}{a}(1 - e^{-at})\n\\]\n\nResult: \\(y(t) = \\frac{1}{a}(1 - e^{-at})u(t)\\). This is the classic charging curve of an RC circuit."
  },
  {
    "objectID": "ss_22.html#example-rc-circuit-integrator-1",
    "href": "ss_22.html#example-rc-circuit-integrator-1",
    "title": "Signal and Systems",
    "section": "Example: RC Circuit (Integrator)",
    "text": "Example: RC Circuit (Integrator)\n\n\n\n\n\n\n\nThis is the continuous-time version of the accumulator example we saw earlier. The impulse response \\(h(t)=u(t)\\) corresponds to an integrator. When we feed an exponential input into an integrator, we get the output shown on the right. Engineers will recognize this immediately as the voltage across a capacitor in a series RC circuit when a DC voltage is applied. The convolution integral mathematically derives this well-known physical behavior."
  },
  {
    "objectID": "ss_22.html#interactive-demo-convolving-two-pulses",
    "href": "ss_22.html#interactive-demo-convolving-two-pulses",
    "title": "Signal and Systems",
    "section": "Interactive Demo: Convolving Two Pulses",
    "text": "Interactive Demo: Convolving Two Pulses\n\n\\(x(t)\\) is a rectangle from \\(t=0\\) to \\(t=1\\).\n\\(h(t)\\) is a ramp from \\(t=0\\) to \\(t=2\\).\n\nUse the slider for t to see the “flip-and-slide” method in action.\n\nviewof t = Inputs.range([-0.5, 3.5], {label: \"t\", step: 0.1, value: 1.5});\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s see this in action. The top plot shows the fixed input \\(x(\\tau)\\) and the flipped, sliding impulse response \\(h(t-\\tau)\\). As you move the slider, you slide the red curve. The middle plot shows their product, and the green shaded region is the area we need to calculate. The bottom plot shows the full output signal \\(y(t)\\) built up from these areas. The cyan triangle shows the output value for the current t, which is precisely the green area in the plot above it. Watch how the shape of the overlapping area changes, creating the different segments of the output signal."
  },
  {
    "objectID": "ss_22.html#example-one-sided-exponential",
    "href": "ss_22.html#example-one-sided-exponential",
    "title": "Signal and Systems",
    "section": "Example: One-Sided Exponential",
    "text": "Example: One-Sided Exponential\nA left-sided exponential convolved with a shifted step function.\nProblem\n\n\\(x(t) = e^{2t}u(-t)\\) (left-sided)\n\\(h(t) = u(t-3)\\) (right-sided)\n\nAnalysis\n\nCase 1: \\(t-3 \\le 0\\) (i.e., \\(t \\le 3\\)) Overlap is for \\(\\tau &lt; t-3\\). \\(y(t) = \\int_{-\\infty}^{t-3} e^{2\\tau}d\\tau = \\frac{1}{2} e^{2(t-3)}\\)\nCase 2: \\(t-3 &gt; 0\\) (i.e., \\(t &gt; 3\\)) Overlap is for \\(\\tau &lt; 0\\). \\(y(t) = \\int_{-\\infty}^{0} e^{2\\tau} d\\tau = \\frac{1}{2}\\)"
  },
  {
    "objectID": "ss_22.html#example-one-sided-exponential-1",
    "href": "ss_22.html#example-one-sided-exponential-1",
    "title": "Signal and Systems",
    "section": "Example: One-Sided Exponential",
    "text": "Example: One-Sided Exponential\n\n\n\n\n\n\n\nThis example is interesting because the input signal is non-zero only for negative time. The impulse response is a step that starts at t=3. When we convolve them, we analyze the overlap in two cases. When \\(t\\) is less than 3, the overlap region depends on \\(t\\), resulting in an exponential rise. But once \\(t\\) moves past 3, the region of overlap becomes fixed (from minus infinity to zero), so the integral gives a constant value of 1/2."
  },
  {
    "objectID": "ss_22.html#summary",
    "href": "ss_22.html#summary",
    "title": "Signal and Systems",
    "section": "Summary",
    "text": "Summary\n\nSignal Representation: Any continuous signal \\(x(t)\\) can be represented by the sifting integral: \\(x(t) = \\int x(\\tau)\\delta(t-\\tau)d\\tau\\).\nLTI System Response: The output \\(y(t)\\) of a continuous-time LTI system is the input \\(x(t)\\) convolved with the system’s impulse response \\(h(t)\\).\nThe Convolution Integral: The core operation for continuous-time LTI systems is: \\[ y(t) = x(t) * h(t) = \\int_{-\\infty}^{\\infty} x(\\tau)h(t-\\tau) d\\tau \\]\nCalculation: The “flip-and-slide” method provides a graphical way to compute the convolution integral by finding the area under the product of the input and the flipped, shifted impulse response.\nKey Parallel: The theory and methods for continuous-time systems directly mirror those we learned for discrete-time systems, with sums replaced by integrals.\n\n\nTo summarize, we’ve built a complete framework for analyzing continuous-time LTI systems that is perfectly analogous to the discrete-time framework. We represent signals using impulses, which leads to the convolution integral. This integral, calculated via the “flip-and-slide” method, gives us the system’s output for any input, provided we know its unique “fingerprint”—the impulse response \\(h(t)\\)."
  },
  {
    "objectID": "ss_16.html#introduction-to-system-properties",
    "href": "ss_16.html#introduction-to-system-properties",
    "title": "Signal and Systems",
    "section": "Introduction to System Properties",
    "text": "Introduction to System Properties\n\nUnderstanding System Behavior: Basic system properties help categorize and analyze how systems behave.\nThese properties are crucial for:\n\nSimplifying system analysis and design.\nPredicting system responses to various inputs.\nDeveloping theoretical frameworks for signals and systems.\n\n\n\nWelcome everyone to this session on Basic System Properties. In Signals and Systems, understanding how systems react to signals is fundamental. But to truly understand them, we first need to characterize them based on certain intrinsic properties. These properties act like fundamental rules that govern a system’s behavior, allowing us to predict, design, and even troubleshoot complex systems with greater ease. They’ll be central to all our subsequent discussions in this course."
  },
  {
    "objectID": "ss_16.html#systems-with-and-without-memory",
    "href": "ss_16.html#systems-with-and-without-memory",
    "title": "Signal and Systems",
    "section": "1. Systems with and without Memory",
    "text": "1. Systems with and without Memory\nA system is memoryless if its output at any given time depends only on the input at that same time.\nMemoryless System Examples:\n\nDiscrete-Time: \\[\ny[n]=\\left(2 x[n]-x^{2}[n]\\right)^{2} \\quad \\text{(1.90)}\n\\]\nContinuous-Time (Resistor): \\[\ny(t)=R x(t) \\quad \\text{(1.91)}\n\\]"
  },
  {
    "objectID": "ss_16.html#systems-with-and-without-memory-1",
    "href": "ss_16.html#systems-with-and-without-memory-1",
    "title": "Signal and Systems",
    "section": "1. Systems with and without Memory",
    "text": "1. Systems with and without Memory\nSystems with Memory: Output depends on past or future input values.\n\nDiscrete-Time Accumulator (Summer): \\[\ny[n]=\\sum_{k=-\\infty}^{n} x[k] \\quad \\text{(1.92)}\n\\] This can also be expressed as \\(y[n]=y[n-1]+x[n]\\).\nDiscrete-Time Delay: \\[\ny[n]=x[n-1] \\quad \\text{(1.93)}\n\\]\nContinuous-Time Capacitor: \\[\ny(t)=\\frac{1}{C} \\int_{-\\infty}^{t} x(\\tau) d \\tau \\quad \\text{(1.94)}\n\\]\n\n\nThe first property we’ll discuss is memory. Think of it simply: does the system need to “remember” past (or anticipate future) inputs to produce its current output?\nA memoryless system is like an instantaneous black box. The resistor is a classic example: the voltage across it at time ‘t’ only depends on the current flowing through it at that exact same time ‘t’. It doesn’t care what the current was five seconds ago, or what it will be in the future.\nOn the other hand, systems with memory hold onto information. An accumulator, for instance, needs to sum all past inputs up to the current moment. Similarly, a delay system needs to remember the previous input value to output it at the current time. In physical systems, memory is often associated with energy storage, such as in capacitors (storing charge related to past current) or inductors (storing magnetic energy related to past voltage). Digital systems store information in registers. While we usually think of memory as related to the past, dependency on future values also qualifies as memory, as we’ll see with causality."
  },
  {
    "objectID": "ss_16.html#memory-interactive-demonstration",
    "href": "ss_16.html#memory-interactive-demonstration",
    "title": "Signal and Systems",
    "section": "Memory: Interactive Demonstration",
    "text": "Memory: Interactive Demonstration\nLet’s compare a memoryless system with a system with memory (an accumulator).\nMemoryless System: \\(y[n] = x[n]^2\\)"
  },
  {
    "objectID": "ss_16.html#memory-interactive-demonstration-1",
    "href": "ss_16.html#memory-interactive-demonstration-1",
    "title": "Signal and Systems",
    "section": "Memory: Interactive Demonstration",
    "text": "Memory: Interactive Demonstration\nSystem with Memory (Accumulator): \\(y[n] = \\sum_{k=-\\infty}^{n} x[k]\\)\n\n\n\n\n\n\n\nHere we have two interactive plots to illustrate the concept of memory.\nOn the first, for the memoryless system y[n] = x[n]^2, observe how y[n] at any point n (e.g., n=0) is simply the square of x[n] at that same n. There’s no dependency on x[-1] or x[1]. The output mirrors the input’s shape instantaneously.\nOn the second, we have an accumulator y[n] = sum(x[k]). If the input x[n] is an impulse at n=0 and n=1, the output y[n] at, say, n=2, is the sum of x[-infinity] to x[2]. It “remembers” the past impulses. Notice how the output value at a given n is influenced by all the input values that came before it. This continuous build-up is a clear sign of memory."
  },
  {
    "objectID": "ss_16.html#invertibility-and-inverse-systems",
    "href": "ss_16.html#invertibility-and-inverse-systems",
    "title": "Signal and Systems",
    "section": "2. Invertibility and Inverse Systems",
    "text": "2. Invertibility and Inverse Systems\nA system is invertible if distinct inputs always produce distinct outputs.\n\nIf a system is invertible, an inverse system exists.\nWhen cascaded with the original system, the inverse system yields an output identical to the original input.\n\n\n\n\n\n\ngraph LR\n    A[\"Input x[n]\"] --&gt; S1[\"System S\"]\n    S1 --&gt; B[\"Output y[n]\"]\n    B --&gt; S2[\"Inverse System S⁻¹\"]\n    S2 --&gt; C[\"Output w[n] = x[n]\"]\n\n\n\n\n\n\n(Figure 1.45(a) - Concept of an inverse system)"
  },
  {
    "objectID": "ss_16.html#invertibility-and-inverse-systems-1",
    "href": "ss_16.html#invertibility-and-inverse-systems-1",
    "title": "Signal and Systems",
    "section": "2. Invertibility and Inverse Systems",
    "text": "2. Invertibility and Inverse Systems\nInvertible System Example (Continuous-Time):\n\\[\ny(t)=2 x(t) \\quad \\text{(1.97)}\n\\] Inverse System: \\[\nw(t)=\\frac{1}{2} y(t) \\quad \\text{(1.98)}\n\\]\nInvertible System Example (Discrete-Time Accumulator):\nThe accumulator: \\(y[n]=\\sum_{k=-\\infty}^{n} x[k]\\) Inverse System: \\[\nw[n]=y[n]-y[n-1] \\quad \\text{(1.99)}\n\\]\n\nNext, we consider invertibility. An invertible system is one where you can uniquely determine the input signal if you know the output signal. Imagine a secret code: if you can decode the message perfectly back to its original form, the encoding process was invertible. If multiple different original messages could result in the same encoded message, then it’s not invertible and you can’t uniquely recover the original.\nMathematically, this means that if \\(x_1(t) \\neq x_2(t)\\), then their corresponding outputs \\(y_1(t) \\neq y_2(t)\\). If this condition holds, then an inverse system can be designed. The block diagram shows this concept: the original system followed by its inverse system effectively acts as an identity system, where the final output is just the original input.\nConsider the simple scaling system y(t) = 2x(t). If you know y(t), you can always find x(t) by dividing y(t) by 2. It’s perfectly invertible. The accumulator is also invertible, its inverse is the first difference operator."
  },
  {
    "objectID": "ss_16.html#non-invertible-systems",
    "href": "ss_16.html#non-invertible-systems",
    "title": "Signal and Systems",
    "section": "Non-Invertible Systems",
    "text": "Non-Invertible Systems\nExamples of Non-Invertible Systems:\n\nZero System: \\[\ny[n]=0 \\quad \\text{(1.100)}\n\\] Many different inputs (e.g., \\(x[n]=u[n]\\) or \\(x[n]=\\delta[n]\\)) produce the same zero output.\nSquaring System: \\[\ny(t)=x^{2}(t) \\quad \\text{(1.101)}\n\\] You cannot determine the sign of \\(x(t)\\) from \\(y(t)\\). For example, \\(x(t)=2\\) and \\(x(t)=-2\\) both yield \\(y(t)=4\\).\n\nPractical Application: Encoding Systems * In communications, encoders must be invertible for perfect signal recovery.\n\nNow for non-invertible systems: these are systems where multiple distinct inputs can lead to the same output. If this happens, you can’t uniquely reconstruct the input from the output, as you don’t know which of the multiple input possibilities was the original one.\nThe ‘zero system’ is a trivial but clear example: no matter what you input, the output is always zero. If the output is zero, you have no way of knowing what the original input was. Similarly, for the squaring system \\(y(t) = x^2(t)\\), consider inputs \\(x(t)=2\\) and \\(x(t)=-2\\). Both produce the output \\(y(t)=4\\). If you only see \\(y(t)=4\\), you can’t tell if the input was 2 or -2. Thus, the system is non-invertible.\nThis concept is vital in areas like data compression and encoding. For instance, lossless audio compression aims to be invertible, allowing you to perfectly reconstruct the original audio. Lossy compression, like MP3, is non-invertible because it discards information, making perfect reconstruction impossible, though often perceptually acceptable."
  },
  {
    "objectID": "ss_16.html#invertibility-interactive-demonstration",
    "href": "ss_16.html#invertibility-interactive-demonstration",
    "title": "Signal and Systems",
    "section": "Invertibility: Interactive Demonstration",
    "text": "Invertibility: Interactive Demonstration\nLet’s demonstrate the inverse for an accumulator.\n\n\n\n\n\n\n\nThis interactive plot helps us visualize the invertibility of the accumulator.\n\nOriginal Input x[n]: This is the signal we apply to our system.\nSystem Output y[n] (Accumulator): This shows x[n] after passing through the accumulator, where each y[n] is the running sum of x[k] up to n. Notice how the signal builds up or combines the past values.\nRecovered Signal w[n] (Inverse System): This is the output of the inverse system, which performs a “first difference” operation (\\(w[n] = y[n] - y[n-1]\\)). Observe how this operation effectively “undoes” the accumulation and recovers the original shape of x[n].\n\nThe printout below the graph confirms numerically that the recovered signal w[n] is indeed identical to the original input x[n]. This provides a concrete example of an invertible system and its inverse."
  },
  {
    "objectID": "ss_16.html#causality",
    "href": "ss_16.html#causality",
    "title": "Signal and Systems",
    "section": "3. Causality",
    "text": "3. Causality\nA system is causal if its output at any time depends only on values of the input at the present time and in the past.\n\nOften referred to as nonanticipative.\nIf \\(x_1(t) = x_2(t)\\) for \\(t \\le t_0\\), then \\(y_1(t) = y_2(t)\\) for \\(t \\le t_0\\).\nAll memoryless systems are causal.\n\nCausal System Examples:\n\nRC Circuit: Capacitor voltage responds to present and past source voltage.\nAccumulator: \\(y[n]=\\sum_{k=-\\infty}^{n} x[k]\\)\nDelay: \\(y[n]=x[n-1]\\)"
  },
  {
    "objectID": "ss_16.html#causality-1",
    "href": "ss_16.html#causality-1",
    "title": "Signal and Systems",
    "section": "3. Causality",
    "text": "3. Causality\nNon-Causal System Examples:\n\nFuture-Dependent: \\[\ny[n]=x[n]-x[n+1] \\quad \\text{(1.102)}\n\\] \\[\ny(t)=x(t+1) \\quad \\text{(1.103)}\n\\]\nTime Reversal: \\[\ny[n]=x[-n] \\quad \\text{(1.105)}\n\\] For \\(n &lt; 0\\), e.g., \\(y[-4]=x[4]\\), output depends on future input.\nAveraging System: \\[\ny[n]=\\frac{1}{2 M+1} \\sum_{k=-M}^{+M} x[n-k] \\quad \\text{(1.104)}\n\\] Includes future values like \\(x[n+M]\\).\n\n\nCausality is a crucial property, especially for real-time systems. Simply put, a causal system cannot “look into the future.” Its current output can only be influenced by the current input or inputs that have already occurred (i.e., past inputs). Think of hitting a drum: the sound (output) can only happen after you hit it (input), not before.\nAn RC circuit is causal because the capacitor voltage builds up based on the history of the current, not what current will flow in the future. All memoryless systems are inherently causal because their output only depends on the present input, which by definition means no future dependency.\nHowever, many systems are non-causal. For instance, a system that outputs x[n+1] outputs a future value of the input. This is impossible in real-time, but perfectly fine for processing recorded data, like in audio mastering or image processing, where the entire signal is available. The average system is an example, it takes inputs from a window that spans both past and future values relative to n. Another subtle non-causal system is y[n] = x[-n]. For \\(n &lt; 0\\), say \\(n=-5\\), \\(y[-5]\\) becomes \\(x[5]\\), which is a future value."
  },
  {
    "objectID": "ss_16.html#causality-interactive-demonstration",
    "href": "ss_16.html#causality-interactive-demonstration",
    "title": "Signal and Systems",
    "section": "Causality: Interactive Demonstration",
    "text": "Causality: Interactive Demonstration\nCompare a causal delay with a non-causal advance.\n\n\n\n\n\n\n\nThis interactive plot clearly demonstrates the difference between causal and non-causal systems using a delay and an advance.\n\nOriginal Input x[n]: Our reference signal.\nCausal System: y[n] = x[n-1] (Delay): Observe that the output y[n] is always the value of x[n] from the previous time step. For example, y[0] is x[-1], y[1] is x[0], and so on. The output signal is shifted to the right, appearing after the corresponding input values. This is how physical, real-time systems operate. For any given n, your output only depends on x[n] or x[n-k] where k is a positive integers.\nNon-Causal System: y[n] = x[n+1] (Advance): Notice how the output y[n] is the value of x[n] from the next time step. For example, y[0] is x[1], y[1] is x[2]. The output signal appears shifted to the left, meaning it anticipates future input values. While impossible in real-time, such systems are useful for processing pre-recorded data when all future samples are available."
  },
  {
    "objectID": "ss_16.html#stability",
    "href": "ss_16.html#stability",
    "title": "Signal and Systems",
    "section": "4. Stability",
    "text": "4. Stability\nA system is stable if small (bounded) inputs lead to responses that do not diverge (are also bounded).\n\nBounded-Input, Bounded-Output (BIBO) Stability.\nInformally: A stable system eventually settles down or remains within limits, given a reasonable input.\n\n\n\nStable System Analogy: Pendulum\n Gravity and friction provide restoring/dissipating forces.\n\nUnstable System Analogy: Inverted Pendulum  Gravity increases deviation; small perturbation leads to tipping."
  },
  {
    "objectID": "ss_16.html#stability-1",
    "href": "ss_16.html#stability-1",
    "title": "Signal and Systems",
    "section": "4. Stability",
    "text": "4. Stability\nUnstable System Example: Accumulator for Unit Step Input\nIf \\(x[n]=u[n]\\) (unit step, bounded, equal to 1 for \\(n \\ge 0\\)), the accumulator output is: \\[\ny[n]=\\sum_{k=-\\infty}^{n} u[k]=(n+1) u[n]\n\\] * \\(y[0]=1, y[1]=2, y[2]=3, \\ldots\\) * \\(y[n]\\) grows without bound, so the accumulator is unstable.\n\nStability is a critical property for practical systems. An unstable system, even with a small input, can produce an output that grows infinitely large, potentially leading to system failure or unwanted behavior. Think of a microphone feeding back into a speaker: a small sound can quickly escalate into a loud, ear-splitting screech. That’s instability.\nThe formal definition is BIBO stability: Bounded-Input, Bounded-Output. If you put a signal into the system whose amplitude never exceeds a certain finite maximum (it’s “bounded”), then the output signal must also have an amplitude that never exceeds some finite maximum. If the output grows without limit for any bounded input, the system is unstable.\nThe pendulum analogy is excellent. A normal pendulum is stable: give it a small push, and it oscillates for a while but eventually settles back to its equilibrium point due to damping (friction) and a restoring force (gravity). An inverted pendulum, however, is inherently unstable: a tiny push will cause it to fall over, with its deviation from vertical growing rapidly.\nThe accumulator, as we saw before, is an example of an unstable system. If you feed it a constant input like a unit step, its output just keeps growing indefinitely, accumulating the past values. This unbounded growth from a bounded input directly violates the BIBO stability criterion."
  },
  {
    "objectID": "ss_16.html#stability-interactive-demonstration-accumulator",
    "href": "ss_16.html#stability-interactive-demonstration-accumulator",
    "title": "Signal and Systems",
    "section": "Stability: Interactive Demonstration (Accumulator)",
    "text": "Stability: Interactive Demonstration (Accumulator)\nLet’s observe the accumulator’s response to a bounded input.\n\n\n\n\n\n\n\nThis interactive demonstration uses the accumulator system again, but specifically to show its instability.\n\nBounded Input x[n] = u[n]: The top plot shows a unit step function. This signal is definitely bounded; its maximum value is 1, and its minimum is 0.\nSystem Output (Accumulator): The bottom plot shows the output of the accumulator when u[n] is the input. You can clearly see that y[n] grows linearly with n for n &gt;= 0. For example, y[0]=1, y[1]=2, y[2]=3, and so on.\n\nSince the output y[n] grows indefinitely as n increases, even though the input x[n] remains bounded, the accumulator is an unstable system according to the BIBO definition. This illustrates why the formal definition of stability is vital, as intuitive “slow growth” can still lead to unboundedness over time."
  },
  {
    "objectID": "ss_16.html#examples-of-stability-check",
    "href": "ss_16.html#examples-of-stability-check",
    "title": "Signal and Systems",
    "section": "Examples of Stability Check",
    "text": "Examples of Stability Check\n\n\nSystem \\(S_1\\): \\(y(t) = t x(t)\\) (1.109)\n\nInput: \\(x(t) = 1\\) (A bounded input).\nOutput: \\(y(t) = t \\cdot 1 = t\\).\nAs \\(t \\to \\infty\\), \\(y(t)\\) grows without bound.\nConclusion: System \\(S_1\\) is unstable.\n\n\nSystem \\(S_2\\): \\(y(t) = e^{x(t)}\\) (1.110)\n\nConsider any bounded input: For some \\(B &gt; 0\\), \\(|x(t)| &lt; B\\).\n\nThis means \\(-B &lt; x(t) &lt; B\\).\n\nThen for the output:\n\n\\(e^{-B} &lt; y(t) &lt; e^{B}\\).\n\nThe output \\(y(t)\\) is bounded by \\(e^B\\).\nConclusion: System \\(S_2\\) is stable.\n\n\n\nThese two examples highlight how to formally check for stability.\nFor \\(S_1\\), the strategy for proving instability is to find just one counterexample. A simple constant input, like \\(x(t)=1\\), is bounded. But when passed through \\(y(t)=tx(t)\\), the output becomes \\(y(t)=t\\). This output clearly grows without limit as time progresses. Since we found a bounded input that yields an unbounded output, \\(S_1\\) is unstable.\nFor \\(S_2\\), it’s not immediately obvious to find a counterexample if it’s stable. So, the strategy is to prove it for any bounded input. If an input \\(x(t)\\) is bounded by \\(B\\), meaning its absolute value is always less than \\(B\\), then the exponential function \\(e^{x(t)}\\) will also be bounded. Since \\(e^x\\) is an monotonically increasing function, \\(e^{-B} &lt; e^{x(t)} &lt; e^B\\). This shows that if the input is bounded by \\(B\\), the output is guaranteed to be bounded by \\(e^B\\). Thus, \\(S_2\\) is stable. Always remember to check for all bounded inputs, not just specific ones, when proving stability."
  },
  {
    "objectID": "ss_16.html#time-invariance-interactive-demonstration-time-varying-gain",
    "href": "ss_16.html#time-invariance-interactive-demonstration-time-varying-gain",
    "title": "Signal and Systems",
    "section": "Time Invariance: Interactive Demonstration (Time-Varying Gain)",
    "text": "Time Invariance: Interactive Demonstration (Time-Varying Gain)\nLet’s illustrate that \\(y[n]=nx[n]\\) is time-varying.\n\n\n\n\n\n\n\nThis demonstration visually proves that the system \\(y[n]=nx[n]\\) is not time-invariant.\n\nTop Plot: \\(x_1[n]\\) and \\(y_1[n]\\): We start with an input \\(x_1[n]\\) which is an impulse at \\(n=0\\). The output \\(y_1[n]\\) is calculated as \\(n \\cdot x_1[n]\\), which for \\(n=0\\) is \\(0 \\cdot 1 = 0\\). So, \\(y_1[n]\\) is zero everywhere.\nMiddle Plot: \\(x_2[n]\\) and \\(y_2[n]\\): Now, we shift the input \\(x_1[n]\\) by 2 units to get \\(x_2[n]\\) (an impulse at \\(n=2\\)). The output \\(y_2[n]\\) is then calculated as \\(n \\cdot x_2[n]\\). For \\(n=2\\), this becomes \\(2 \\cdot 1 = 2\\). So, \\(y_2[n]\\) is an impulse of amplitude 2 at \\(n=2\\).\nBottom Plot: Comparison (\\(y_2[n]\\) vs. expected \\(y_1[n-2]\\)): If the system were time-invariant, the output \\(y_2[n]\\) (green stems) should simply be a shifted version of \\(y_1[n]\\) (meaning still all zeros, but shifted, so still all zeros – represented by the dashed blue stems for expected \\(y_1[n-2]\\)). However, y2[n] clearly has a non-zero value at n=2.\n\nSince \\(y_2[n]\\) is not equal to \\(y_1[n-2]\\), the system is time-varying. The gain n changes based on time, so the response to the same input (just shifted) will be different."
  },
  {
    "objectID": "ss_16.html#linearity",
    "href": "ss_16.html#linearity",
    "title": "Signal and Systems",
    "section": "6. Linearity",
    "text": "6. Linearity\nA system is linear if it possesses the property of superposition. This means it satisfies two conditions:\n\nAdditivity: If \\(x_1(t) \\to y_1(t)\\) and \\(x_2(t) \\to y_2(t)\\), then \\(x_1(t)+x_2(t) \\to y_1(t)+y_2(t)\\).\nHomogeneity (Scaling): If \\(x_1(t) \\to y_1(t)\\), then \\(a x_1(t) \\to a y_1(t)\\) for any complex constant \\(a\\).\n\nThese two properties can be combined: * Continuous Time: \\[\n    a x_1(t)+b x_2(t) \\rightarrow a y_1(t)+b y_2(t) \\quad \\text{(1.121)}\n    \\] * Discrete Time: \\[\n    a x_1[n]+b x_2[n] \\rightarrow a y_1[n]+b y_2[n] \\quad \\text{(1.122)}\n    \\]\nImportant Consequence: For linear systems, a zero input \\(x[n]=0\\) for all \\(n\\) must result in a zero output \\(y[n]=0\\) for all \\(n\\).\n\nLinearity is perhaps the most fundamental property in Signals and Systems, forming the basis for many powerful analysis techniques. A linear system obeys the principle of superposition.\nThis principle is broken down into two parts: 1. Additivity: If you have two different inputs, say \\(x_1\\) and \\(x_2\\), and you know their individual outputs, \\(y_1\\) and \\(y_2\\), then if you apply the sum of the inputs (\\(x_1+x_2\\)), the output will simply be the sum of their individual outputs (\\(y_1+y_2\\)). 2. Homogeneity/Scaling: If you scale an input by a factor ‘a’ (e.g., make it twice as strong), the output will also be scaled by the exact same factor ‘a’.\nBoth of these must hold for a system to be linear. If either one fails, the system is nonlinear. The combined statement is very powerful for checking linearity.\nA crucial consequence of linearity is that a linear system with no input should produce no output. This “zero-in/zero-out” check is often the quickest way to spot a nonlinear system. If you apply zero input and get a non-zero output (like a constant offset), the system is definitely not linear. We’ll explore examples now."
  },
  {
    "objectID": "ss_16.html#linearity-examples",
    "href": "ss_16.html#linearity-examples",
    "title": "Signal and Systems",
    "section": "Linearity: Examples",
    "text": "Linearity: Examples\nExample 1: System \\(S_A: y(t) = t x(t)\\) (Example 1.17)\n\nLet \\(x_3(t) = a x_1(t) + b x_2(t)\\).\n\\(y_3(t) = t x_3(t) = t(a x_1(t) + b x_2(t)) = a (t x_1(t)) + b (t x_2(t)) = a y_1(t) + b y_2(t)\\).\nConclusion: System \\(S_A\\) is linear. (It is also time-varying, as seen before!)\n\nExample 2: System \\(S_B: y(t) = x^2(t)\\) (Example 1.18)\n\nLet \\(x_3(t) = a x_1(t) + b x_2(t)\\).\n\\(y_3(t) = (a x_1(t) + b x_2(t))^2 = a^2 x_1^2(t) + b^2 x_2^2(t) + 2ab x_1(t) x_2(t)\\).\nThis is \\(a^2 y_1(t) + b^2 y_2(t) + 2ab x_1(t) x_2(t)\\) which is not \\(a y_1(t) + b y_2(t)\\).\nConclusion: System \\(S_B\\) is non-linear.\n\nExample 3: System \\(S_C: y[n] = \\text{Re}\\{x[n]\\}\\) (Example 1.19)\n\nIs additive, but fails homogeneity for complex scalars.\nIf \\(x_1[n] = r[n]+js[n]\\), then \\(y_1[n] = r[n]\\).\nLet \\(a=j\\). The input \\(x_2[n] = j x_1[n] = -s[n] + j r[n]\\).\n\\(y_2[n] = \\text{Re}\\{x_2[n]\\} = -s[n]\\).\nExpected \\(a y_1[n] = j r[n]\\). Since \\(-s[n] \\ne j r[n]\\), it fails homogeneity.\nConclusion: System \\(S_C\\) is non-linear.\n\n\nLet’s walk through these examples to solidify our understanding of linearity. For each, we apply the superposition test.\n\nSystem \\(S_A: y(t) = t x(t)\\): This is a direct application of the definition. When we substitute the combined input \\(ax_1(t)+bx_2(t)\\) into the system equation, we can factor out \\(a\\) and \\(b\\), showing that the output is indeed the linear combination of individual outputs. So, it’s linear. Note that this system is linear but not time-invariant, as we discussed previously. Systems can have one property without the other.\nSystem \\(S_B: y(t) = x^2(t)\\): Here, the squaring operation \\(x^2(t)\\) immediately suggests non-linearity because of the cross-term \\(2ab x_1(t) x_2(t)\\) that arises from expanding the square of \\((ax_1+bx_2)\\). This cross-term prevents the output from being a simple linear combination of \\(y_1\\) and \\(y_2\\). Therefore, it’s non-linear.\nSystem \\(S_C: y[n] = \\text{Re}\\{x[n]\\}\\): This one is tricky and highlights the importance of considering complex scalars for homogeneity. It is additive, but it fails homogeneity when scaled by a complex constant like \\(j\\). The real part of \\(j \\cdot x_1[n]\\) is not \\(j\\) times the real part of \\(x_1[n]\\). This demonstrates that both additivity AND homogeneity must hold for any complex scalar for a system to be linear."
  },
  {
    "objectID": "ss_16.html#linearity-interactive-demonstration-quadratic-system",
    "href": "ss_16.html#linearity-interactive-demonstration-quadratic-system",
    "title": "Signal and Systems",
    "section": "Linearity: Interactive Demonstration (Quadratic System)",
    "text": "Linearity: Interactive Demonstration (Quadratic System)\nLet’s test \\(y[n] = x^2[n]\\) for linearity.\n\n\n\n\n\n\n\nThis demonstration concretely shows why the system \\(y[n]=x^2[n]\\) is nonlinear. We’re testing the superposition property by comparing two results:\n\nExpected Linear Sum (blue dashed stems): This is what the output should be if the system were linear: \\(a \\cdot y_1[n] + b \\cdot y_2[n]\\). We compute \\(y_1[n]\\) from \\(x_1[n]\\) and \\(y_2[n]\\) from \\(x_2[n]\\) separately, then combine them.\nActual Output (red solid stems): This is the output we get when we first combine the inputs (\\(a \\cdot x_1[n] + b \\cdot x_2[n]\\)) and then pass this combined signal through the system.\n\nObserve: * At n=0: x1[0]=2, x2[0]=0. The combined input is 2*2 + 3*0 = 4. The actual output is 4^2=16. The expected linear sum is 2*(2^2) + 3*(0^2) = 2*4 + 0 = 8. They are different! * At n=1: x1[1]=0, x2[1]=1. The combined input is 2*0 + 3*1 = 3. The actual output is 3^2=9. The expected linear sum is 2*(0^2) + 3*(1^2) = 0 + 3*1 = 3. They are different!\nThe plots clearly show that the red solid stems (actual output) do not match the blue dashed stems (expected linear sum). The numerical checks at the bottom confirm this. This discrepancy, caused by the squaring operation, violates the superposition principle, proving the system is nonlinear."
  },
  {
    "objectID": "ss_16.html#linearity-incrementally-linear-systems",
    "href": "ss_16.html#linearity-incrementally-linear-systems",
    "title": "Signal and Systems",
    "section": "Linearity: Incrementally Linear Systems",
    "text": "Linearity: Incrementally Linear Systems\nExample: System \\(S_D: y[n] = 2x[n] + 3\\) (1.132)\n\nThis system often looks “linear” because it’s a linear equation.\nHowever, it violates the zero-input, zero-output property:\n\nIf \\(x[n]=0\\), then \\(y[n]=2(0)+3=3 \\ne 0\\).\n\nTherefore, System \\(S_D\\) is non-linear.\n\n\n\n\n\n\ngraph TD\n    A[\"Input x[n]\"] --&gt; S_linear[\"Linear System: $$y_L[n]=2x[n]$$\"]\n    S_linear --&gt; add[+]\n    S_const[Constant Input: 3] --&gt; add\n    add --&gt; B[\"Output y[n]\"]\n\n\n\n\n\n\n(Figure 1.48 - Structure of an incrementally linear system)\nThis system is incrementally linear: * The difference between two outputs is a linear function of the difference between their inputs. \\[\ny_1[n]-y_2[n] = (2x_1[n]+3) - (2x_2[n]+3) = 2(x_1[n]-x_2[n]) \\quad \\text{(1.136)}\n\\] * This means it can be viewed as a linear system (\\(2x[n]\\)) with a constant offset (3).\n\nThis final example for linearity, \\(y[n]=2x[n]+3\\), is a very common point of confusion. Many students, seeing a “linear equation”, assume it defines a linear system. However, this is not the case within the context of Signals and Systems.\nThe quickest way to check is the “zero-in, zero-out” property. If \\(x[n]\\) is zero for all \\(n\\), a truly linear system must output zero for all \\(n\\). But for \\(y[n]=2x[n]+3\\), if \\(x[n]=0\\), then \\(y[n]=3\\). Since the output is non-zero, this system is not linear. It fails the homogeneity property (try scaling by zero!).\nHowever, such systems are called “incrementally linear.” This means that while the system itself isn’t linear, changes in its output are linear with respect to changes in its input. The diagram illustrates this: the system can be seen as a linear part (\\(2x[n]\\)) combined with a constant offset (3), which is essentially the system’s “zero-input response.” This distinction is important for understanding more complex systems later."
  },
  {
    "objectID": "ss_16.html#conclusion-summary",
    "href": "ss_16.html#conclusion-summary",
    "title": "Signal and Systems",
    "section": "Conclusion & Summary",
    "text": "Conclusion & Summary\nWe’ve explored six fundamental properties of systems:\n\nMemory / Memoryless: Does output depend only on the current input?\nInvertibility: Can the input be uniquely recovered from the output?\nCausality: Does output depend only on present and past inputs? (Non-anticipative)\nStability: Do bounded inputs lead to bounded outputs? (BIBO)\nTime Invariance: Do a time shift in input cause an identical time shift in output?\nLinearity: Does the system satisfy superposition (additivity & homogeneity)?\n\nThese properties are essential tools for:\n\nClassifying systems.\nSimplifying analysis (especially for linear, time-invariant systems).\nDesigning systems with desired behaviors.\n\nUnderstanding these basics lays the groundwork for advanced topics in Signals and Systems!\n\nTo wrap up, we’ve covered six critical properties that define the behavior of systems in Signals and Systems.\n\nMemory speaks to whether a system “remembers” past inputs or predicts future ones.\nInvertibility is about whether we can uniquely reverse the system’s operation to deduce the original input.\nCausality is about real-time behavior - can the system react only to what has already happened, or does it anticipate?\nStability is crucial for reliability, ensuring that small inputs don’t lead to out-of-control outputs.\nTime Invariance means the system’s behavior is consistent over time, regardless of when an input is applied.\nLinearity is the bedrock for many advanced analysis techniques, allowing us to break down complex inputs into simpler components and superimpose their responses.\n\nThese definitions are not just academic. They directly influence how we design everything from audio filters to control systems, telecommunications networks, and medical imaging devices. Mastering these properties will significantly simplify your journey through the rest of this Signals and Systems course. Thank you!"
  },
  {
    "objectID": "ss_14.html#introduction",
    "href": "ss_14.html#introduction",
    "title": "Signal and Systems",
    "section": "Introduction",
    "text": "Introduction\nThe unit impulse and unit step functions are fundamental signals in Continuous and Discrete Time.\n\nThey serve as building blocks for representing other complex signals.\nCrucial for understanding System Responses (e.g., Impulse Response).\n\n\nToday, we’ll dive into these essential signals. We’ll define them for both discrete and continuous time, explore their relationships, and see how they are used, including their powerful sampling property."
  },
  {
    "objectID": "ss_14.html#the-discrete-time-unit-impulse-sequence-deltan",
    "href": "ss_14.html#the-discrete-time-unit-impulse-sequence-deltan",
    "title": "Signal and Systems",
    "section": "1.4.1 The Discrete-Time Unit Impulse Sequence (\\(\\delta[n]\\))",
    "text": "1.4.1 The Discrete-Time Unit Impulse Sequence (\\(\\delta[n]\\))\nThe discrete-time unit impulse (or unit sample) is defined as:\n\\[\n\\delta[n]= \\begin{cases}0, & n \\neq 0 \\\\ 1, & n=0\\end{cases} \\quad \\text{(Equation 1.63)}\n\\]\nIt represents a single, instantaneous event at \\(n=0\\).\nMathematical Concept\n\nA single point with value 1 at \\(n=0\\).\nAll other points are 0.\nAnalogous to a very short click or tap."
  },
  {
    "objectID": "ss_14.html#the-discrete-time-unit-impulse-sequence-deltan-1",
    "href": "ss_14.html#the-discrete-time-unit-impulse-sequence-deltan-1",
    "title": "Signal and Systems",
    "section": "1.4.1 The Discrete-Time Unit Impulse Sequence (\\(\\delta[n]\\))",
    "text": "1.4.1 The Discrete-Time Unit Impulse Sequence (\\(\\delta[n]\\))\nInteractive Plot\n\n\n\n\n\n\n\nThe discrete-time unit impulse, also known as the unit sample, is a very simple yet powerful signal. As you can see from the definition and the plot, it’s non-zero only at n=0, where its value is 1. Think of it as a single, isolated “click” in time. We often use it to represent instantaneous events or to probe a system’s behavior. We’ll soon see its importance as a building block for other signals."
  },
  {
    "objectID": "ss_14.html#the-discrete-time-unit-step-sequence-un",
    "href": "ss_14.html#the-discrete-time-unit-step-sequence-un",
    "title": "Signal and Systems",
    "section": "1.4.1 The Discrete-Time Unit Step Sequence (\\(u[n]\\))",
    "text": "1.4.1 The Discrete-Time Unit Step Sequence (\\(u[n]\\))\nThe discrete-time unit step is defined by:\n\\[\nu[n]= \\begin{cases}0, & n&lt;0 \\\\ 1, & n \\geq 0\\end{cases} \\quad \\text{(Equation 1.64)}\n\\]\nIt represents a signal that turns on at \\(n=0\\) and stays on.\nEngineering Application\n\nOften used to model a switch being turned on.\nRepresents the initiation of a process."
  },
  {
    "objectID": "ss_14.html#the-discrete-time-unit-step-sequence-un-1",
    "href": "ss_14.html#the-discrete-time-unit-step-sequence-un-1",
    "title": "Signal and Systems",
    "section": "1.4.1 The Discrete-Time Unit Step Sequence (\\(u[n]\\))",
    "text": "1.4.1 The Discrete-Time Unit Step Sequence (\\(u[n]\\))\n\n\n\n\n\n\n\nThe discrete-time unit step is equally important. It’s zero for all negative indices and then jumps to 1 and stays at 1 for all non-negative indices. This signal is often used to represent the moment something is “switched on” or the start of a process. For example, if you apply a constant voltage to a circuit at a specific time, that can be modeled with a step function."
  },
  {
    "objectID": "ss_14.html#relationship-between-deltan-and-un-difference",
    "href": "ss_14.html#relationship-between-deltan-and-un-difference",
    "title": "Signal and Systems",
    "section": "Relationship Between \\(\\delta[n]\\) and \\(u[n]\\) (Difference)",
    "text": "Relationship Between \\(\\delta[n]\\) and \\(u[n]\\) (Difference)\nThe discrete-time unit impulse is the first difference of the discrete-time step:\n\\[\n\\delta[n]=u[n]-u[n-1] \\quad \\text{(Equation 1.65)}\n\\]\nThis means you can generate an impulse from two shifted step functions.\nConcept Illustrated\n\n\n\n\n\ngraph LR\n    A[\"u[n]\"] --&gt; B{Delay by 1};\n    B --&gt; C[\"u[n-1]\"];\n    A --&gt; D{Subtract};\n    C --&gt; D;\n    D --&gt; E[\"$$\\delta[n]$$\"];"
  },
  {
    "objectID": "ss_14.html#relationship-between-deltan-and-un-difference-1",
    "href": "ss_14.html#relationship-between-deltan-and-un-difference-1",
    "title": "Signal and Systems",
    "section": "Relationship Between \\(\\delta[n]\\) and \\(u[n]\\) (Difference)",
    "text": "Relationship Between \\(\\delta[n]\\) and \\(u[n]\\) (Difference)\nInteractive Demonstration\n\n\n\n\n\n\n\nThis is a crucial relationship. If you take the unit step u[n] and subtract a delayed version of itself, u[n-1], you get the unit impulse delta[n]. Think about it: u[n] turns on at n=0. u[n-1] turns on at n=1. So, when you subtract them, the ’1’s cancel out everywhere except at n=0 (where u[n] is 1 and u[n-1] is 0), resulting in a single value of 1. This reinforces how these basic signals are interconnected."
  },
  {
    "objectID": "ss_14.html#relationship-between-deltan-and-un-running-sum",
    "href": "ss_14.html#relationship-between-deltan-and-un-running-sum",
    "title": "Signal and Systems",
    "section": "Relationship Between \\(\\delta[n]\\) and \\(u[n]\\) (Running Sum)",
    "text": "Relationship Between \\(\\delta[n]\\) and \\(u[n]\\) (Running Sum)\nConversely, the discrete-time unit step is the running sum of the unit impulse:\n\\[\nu[n]=\\sum_{m=-\\infty}^{n} \\delta[m] \\quad \\text{(Equation 1.66)}\n\\]\nThis means you can build up a step function by accumulating impulses.\nMathematical Form \\[\nu[n]=\\sum_{k=0}^{\\infty} \\delta[n-k] \\quad \\text{(Equation 1.67)}\n\\] - Or, a superposition of delayed impulses. - Emphasizes accumulation."
  },
  {
    "objectID": "ss_14.html#relationship-between-deltan-and-un-running-sum-1",
    "href": "ss_14.html#relationship-between-deltan-and-un-running-sum-1",
    "title": "Signal and Systems",
    "section": "Relationship Between \\(\\delta[n]\\) and \\(u[n]\\) (Running Sum)",
    "text": "Relationship Between \\(\\delta[n]\\) and \\(u[n]\\) (Running Sum)\nInteractive Demonstration: Running Sum\n\n\n\n\n\n\n\nJust as we can get an impulse by differencing, we can get a step by summing. The unit step u[n] is simply the running sum of the unit impulse. If you add up the values of delta[m] from negative infinity up to n, you’ll see that for any n &lt; 0, the sum is zero because delta[m] is zero. But once n reaches zero or goes positive, the delta[0] component of value 1 gets included, and the sum becomes 1. This also implies that the step function can be seen as an infinite sum of delayed impulses. This concept of summing impulses to form other signals is foundational to convolution, which we’ll cover in Chapter 2."
  },
  {
    "objectID": "ss_14.html#sampling-property-of-deltan",
    "href": "ss_14.html#sampling-property-of-deltan",
    "title": "Signal and Systems",
    "section": "Sampling Property of \\(\\delta[n]\\)",
    "text": "Sampling Property of \\(\\delta[n]\\)\nThe unit impulse can be used to sample the value of a signal at a specific point.\n\\[\nx[n] \\delta[n]=x[0] \\delta[n] \\quad \\text{(Equation 1.68)}\n\\]\nMore generally, for a delayed impulse: \\[\nx[n] \\delta\\left[n-n_{0}\\right]=x\\left[n_{0}\\right] \\delta\\left[n-n_{0}\\right] \\quad \\text{(Equation 1.69)}\n\\]\nThis property is extremely powerful for signal analysis.\nExample: What is \\(x[n]\\delta[n-3]\\) if \\(x[n] = \\cos(\\frac{\\pi n}{4})\\)? \\(x[n]\\delta[n-3] = x[3]\\delta[n-3] = \\cos(\\frac{3\\pi}{4})\\delta[n-3] = -\\frac{\\sqrt{2}}{2}\\delta[n-3]\\)\n\nThis is one of the most important properties of the unit impulse: its sampling property. Because the impulse is only non-zero at one specific point, when you multiply any signal x[n] by delta[n], the result is x[0] at n=0 and zero everywhere else. It “picks out” the value of the signal at that specific time. If the impulse is delayed to n_0, it picks out x[n_0]. This is incredibly useful for isolating specific values of a signal or for simplifying expressions in system analysis, especially when working with convolution."
  },
  {
    "objectID": "ss_14.html#the-continuous-time-unit-step-function-ut",
    "href": "ss_14.html#the-continuous-time-unit-step-function-ut",
    "title": "Signal and Systems",
    "section": "1.4.2 The Continuous-Time Unit Step Function (\\(u(t)\\))",
    "text": "1.4.2 The Continuous-Time Unit Step Function (\\(u(t)\\))\nThe continuous-time unit step function is defined as:\n\\[\nu(t)= \\begin{cases}0, & t&lt;0 \\\\ 1, & t&gt;0\\end{cases} \\quad \\text{(Equation 1.70)}\n\\]\nNote: \\(u(t)\\) is discontinuous at \\(t=0\\).\nPhysical Interpretation\n\nRepresents an abrupt change in a system.\nE.g., turning on a power supply, applying a force."
  },
  {
    "objectID": "ss_14.html#the-continuous-time-unit-step-function-ut-1",
    "href": "ss_14.html#the-continuous-time-unit-step-function-ut-1",
    "title": "Signal and Systems",
    "section": "1.4.2 The Continuous-Time Unit Step Function (\\(u(t)\\))",
    "text": "1.4.2 The Continuous-Time Unit Step Function (\\(u(t)\\))\nInteractive Plot\n\n\n\n\n\n\n\nThe continuous-time unit step u(t) is very similar to its discrete counterpart. It’s zero for negative time and rises to 1 for positive time. The key difference is the discontinuity at t=0. For practical purposes in engineering, we often define u(0) as 0.5 or leave it undefined, but its behavior surrounding that point is what’s important. It models a sharp ‘on’ transition, like closing a switch in an analog circuit."
  },
  {
    "objectID": "ss_14.html#the-continuous-time-unit-impulse-function-deltat",
    "href": "ss_14.html#the-continuous-time-unit-impulse-function-deltat",
    "title": "Signal and Systems",
    "section": "The Continuous-Time Unit Impulse Function (\\(\\delta(t)\\))",
    "text": "The Continuous-Time Unit Impulse Function (\\(\\delta(t)\\))\nThe continuous-time unit impulse is related to the unit step by: \\[\nu(t)=\\int_{-\\infty}^{t} \\delta(\\tau) d \\tau \\quad \\text{(Equation 1.71)}\n\\]\nThis implies: \\[\n\\delta(t)=\\frac{d u(t)}{d t} \\quad \\text{(Equation 1.72)}\n\\]\nChallenge: u(t) is not formally differentiable at \\(t=0\\).\n\nNow, let’s turn to the continuous-time unit impulse, delta(t). It’s fundamentally linked to the unit step. If you integrate the impulse, you get the step. This also means that the impulse can be thought of as the derivative of the unit step. However, u(t) has an abrupt jump at t=0, making it formally non-differentiable in the traditional sense. This is where the concept of the impulse becomes an idealization."
  },
  {
    "objectID": "ss_14.html#continuous-time-unit-impulse-an-idealization",
    "href": "ss_14.html#continuous-time-unit-impulse-an-idealization",
    "title": "Signal and Systems",
    "section": "Continuous-Time Unit Impulse: An Idealization",
    "text": "Continuous-Time Unit Impulse: An Idealization\nTo understand \\(\\delta(t)\\), we approximate \\(u(t)\\) with a smooth function \\(u_{\\Delta}(t)\\).\nApproximate Unit Step (\\(u_{\\Delta}(t)\\))\n\nRises from 0 to 1 over a small interval \\(\\Delta\\).\n\\(\\delta(t)=\\lim _{\\Delta \\rightarrow 0} \\delta_{\\Delta}(t)\\) (Equation 1.74)"
  },
  {
    "objectID": "ss_14.html#continuous-time-unit-impulse-an-idealization-1",
    "href": "ss_14.html#continuous-time-unit-impulse-an-idealization-1",
    "title": "Signal and Systems",
    "section": "Continuous-Time Unit Impulse: An Idealization",
    "text": "Continuous-Time Unit Impulse: An Idealization\nDerivative of Approximation (\\(\\delta_{\\Delta}(t)\\))\n\nA short pulse of duration \\(\\Delta\\).\nCrucially: Its area is always 1.\n\n\n\n\n\n\n\n\nTo properly understand delta(t), we use the concept of an idealization through a limiting process. Imagine an approximate step function, u_delta(t), that smoothly rises from 0 to 1 over a very short duration delta. As delta gets smaller and smaller, u_delta(t) approaches the ideal u(t).\nNow, consider the derivative of this approximate step, delta_delta(t). It’s a pulse of height 1/delta and duration delta, meaning its area is always (1/delta) * delta = 1, regardless of how small delta is. As delta approaches zero, this pulse becomes infinitely tall and infinitesimally narrow, but its area remains unity. This infinitely tall, infinitely narrow pulse with unit area is our continuous-time unit impulse delta(t). This unit area is its defining characteristic."
  },
  {
    "objectID": "ss_14.html#continuous-time-unit-impulse-graphical-representation",
    "href": "ss_14.html#continuous-time-unit-impulse-graphical-representation",
    "title": "Signal and Systems",
    "section": "Continuous-Time Unit Impulse (Graphical Representation)",
    "text": "Continuous-Time Unit Impulse (Graphical Representation)\nThe unit impulse \\(\\delta(t)\\) is graphically represented by an arrow at \\(t=0\\), with its height indicating the area (or strength) of the impulse.\n\n\nUnit Impulse\n\nInfinitely tall, infinitesimally narrow.\nArea = 1.\n\n \n\nScaled Impulse (\\(k\\delta(t)\\))\n\nArea = \\(k\\).\nThe height of the arrow is proportional to \\(k\\).\n\n \n\n\nSince sketching an infinitely narrow, infinitely tall function is impossible, we use a special graphical notation for the unit impulse: an arrow. The number next to the arrow, or its height, represents the area of the impulse, not its amplitude in the traditional sense. A “unit” impulse has an area of 1. A scaled impulse k*delta(t) has an area of k. This area property is crucial because it’s what determines the effect of the impulse on a system, not its theoretical “height.”"
  },
  {
    "objectID": "ss_14.html#relationship-between-deltat-and-ut-running-integral",
    "href": "ss_14.html#relationship-between-deltat-and-ut-running-integral",
    "title": "Signal and Systems",
    "section": "Relationship Between \\(\\delta(t)\\) and \\(u(t)\\) (Running Integral)",
    "text": "Relationship Between \\(\\delta(t)\\) and \\(u(t)\\) (Running Integral)\nThe continuous-time unit step is the running integral of the unit impulse:\n\\[\nu(t)=\\int_{-\\infty}^{t} \\delta(\\tau) d \\tau \\quad \\text{(Equation 1.71)}\n\\]\nThis highlights the accumulation of area.\nEquivalent Form \\[\nu(t)=\\int_{0}^{\\infty} \\delta(t-\\sigma) d \\sigma \\quad \\text{(Equation 1.75)}\n\\]\n\nSuperposition of delayed impulses."
  },
  {
    "objectID": "ss_14.html#relationship-between-deltat-and-ut-running-integral-1",
    "href": "ss_14.html#relationship-between-deltat-and-ut-running-integral-1",
    "title": "Signal and Systems",
    "section": "Relationship Between \\(\\delta(t)\\) and \\(u(t)\\) (Running Integral)",
    "text": "Relationship Between \\(\\delta(t)\\) and \\(u(t)\\) (Running Integral)\nConceptual Illustration (Integral of \\(\\delta(t)\\))\n\n\n\n\n\n\n\nThe integral relationship is the reverse of the derivative. If you integrate the continuous-time unit impulse from negative infinity up to time t, you will trace out the unit step function. The integral is zero until t crosses 0, at which point it captures the unit area of the impulse, and the integral becomes 1. This concept of integrating impulse areas is fundamental to understanding linear time-invariant systems and convolution, showing how systems accumulate the effect of an input over time."
  },
  {
    "objectID": "ss_14.html#sampling-property-of-deltat",
    "href": "ss_14.html#sampling-property-of-deltat",
    "title": "Signal and Systems",
    "section": "Sampling Property of \\(\\delta(t)\\)",
    "text": "Sampling Property of \\(\\delta(t)\\)\nSimilar to discrete-time, the continuous-time impulse has a powerful sampling property:\n\\[\nx(t) \\delta(t)=x(0) \\delta(t) \\quad \\text{(Equation 1.76)}\n\\]\nFor a delayed impulse: \\[\nx(t) \\delta\\left(t-t_{0}\\right)=x\\left(t_{0}\\right) \\delta\\left(t-t_{0}\\right)\n\\]\nThis means multiplying a function by an impulse isolates the function’s value at the impulse’s location."
  },
  {
    "objectID": "ss_14.html#sampling-property-of-deltat-1",
    "href": "ss_14.html#sampling-property-of-deltat-1",
    "title": "Signal and Systems",
    "section": "Sampling Property of \\(\\delta(t)\\)",
    "text": "Sampling Property of \\(\\delta(t)\\)\nDemonstration: x(t) = sin(t) multiplied by $\\delta(t - \\pi/2)$\n\n\n\n\n\n\n\nThe sampling property is just as vital in continuous-time as it is in discrete-time. When you multiply a well-behaved continuous signal x(t) by delta(t), the result is effectively the value of x(t) at t=0, scaled by the impulse itself. This means it’s x(0) multiplied by an impulse of unit area. If the impulse is shifted to t_0, it samples x(t_0). This property allows us to extract specific values from a continuous signal using the impulse function, which is critical in concepts like Fourier transforms and filtering."
  },
  {
    "objectID": "ss_14.html#example-1.7-derivative-of-a-discontinuous-signal",
    "href": "ss_14.html#example-1.7-derivative-of-a-discontinuous-signal",
    "title": "Signal and Systems",
    "section": "Example 1.7: Derivative of a Discontinuous Signal",
    "text": "Example 1.7: Derivative of a Discontinuous Signal\nConsider the signal \\(x(t)\\) (from Figure 1.40a). We want to find its derivative \\(\\dot{x}(t)\\).\nOriginal Signal \\(x(t)\\)\n\nPiecewise constant.\nContains jump discontinuities."
  },
  {
    "objectID": "ss_14.html#example-1.7-derivative-of-a-discontinuous-signal-1",
    "href": "ss_14.html#example-1.7-derivative-of-a-discontinuous-signal-1",
    "title": "Signal and Systems",
    "section": "Example 1.7: Derivative of a Discontinuous Signal",
    "text": "Example 1.7: Derivative of a Discontinuous Signal\nDerivative \\(\\dot{x}(t)\\)\n\nZero where \\(x(t)\\) is constant.\nImpulses at jump discontinuities.\nArea of impulse = size of jump.\n\n\n\n\n\n\n\n\nThis example beautifully demonstrates the power of the impulse function. The derivative of a constant signal is zero. However, when a signal has a ‘jump discontinuity’, its derivative at that point is not conventionally defined. Using the concept of the impulse, the derivative at a jump discontinuity becomes an impulse whose area is equal to the magnitude of the jump. For x(t): - At t=1, x(t) jumps from 0 to 2. This is a jump of +2, so x_dot(t) has an impulse of area +2 at t=1. - At t=2, x(t) jumps from 2 to -1. This is a jump of -3, so x_dot(t) has an impulse of area -3 at t=2. - At t=4, x(t) jumps from -1 to 1. This is a jump of +2, so x_dot(t) has an impulse of area +2 at t=4. Everywhere else, where the signal is flat, the derivative is zero."
  },
  {
    "objectID": "ss_14.html#example-1.7-recovering-xt-from-dotxt",
    "href": "ss_14.html#example-1.7-recovering-xt-from-dotxt",
    "title": "Signal and Systems",
    "section": "Example 1.7: Recovering \\(x(t)\\) from \\(\\dot{x}(t)\\)",
    "text": "Example 1.7: Recovering \\(x(t)\\) from \\(\\dot{x}(t)\\)\nWe can verify the derivative by integrating \\(\\dot{x}(t)\\) to recover \\(x(t)\\). \\[\nx(t)=\\int_{0}^{t} \\dot{x}(\\tau) d \\tau \\quad \\text{(Equation 1.77)}\n\\]\nEach impulse contributes its area to the running sum (integral).\nIntegral Steps\n\nFor \\(t&lt;1\\): \\(\\int_{0}^{t} \\dot{x}(\\tau) d \\tau = 0\\)\nFor \\(1&lt;t&lt;2\\): \\(\\int_{0}^{t} \\dot{x}(\\tau) d \\tau = (\\text{Area at } t=1) = 2\\)\nFor \\(2&lt;t&lt;4\\): \\(\\int_{0}^{t} \\dot{x}(\\tau) d \\tau = (\\text{Area at } t=1) + (\\text{Area at } t=2) = 2 + (-3) = -1\\)\nFor \\(t&gt;4\\): \\(\\int_{0}^{t} \\dot{x}(\\tau) d \\tau = (\\text{Area at } t=1) + (\\text{Area at } t=2) + (\\text{Area at } t=4) = 2 + (-3) + 2 = 1\\)"
  },
  {
    "objectID": "ss_14.html#example-1.7-recovering-xt-from-dotxt-1",
    "href": "ss_14.html#example-1.7-recovering-xt-from-dotxt-1",
    "title": "Signal and Systems",
    "section": "Example 1.7: Recovering \\(x(t)\\) from \\(\\dot{x}(t)\\)",
    "text": "Example 1.7: Recovering \\(x(t)\\) from \\(\\dot{x}(t)\\)\nInteractive Verification (Conceptual)\n\n\n\n\n\n\n\nTo confirm our derivative, we integrate x_dot(t). The integral accumulates the areas of the impulses. - For t less than 1, the integral is 0. - As t crosses 1, the integral captures the area of the impulse at t=1, which is +2. So, x(t) becomes 2. - As t crosses 2, the integral adds the area of the impulse at t=2, which is -3, resulting in 2 - 3 = -1. So, x(t) becomes -1. - Finally, as t crosses 4, the integral adds the area of the impulse at t=4, which is +2, resulting in -1 + 2 = +1. So, x(t) becomes 1. This exactly reconstructs the original signal x(t), demonstrating the complementary nature of differentiation and integration with respect to impulse functions."
  },
  {
    "objectID": "ss_14.html#key-takeaways",
    "href": "ss_14.html#key-takeaways",
    "title": "Signal and Systems",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nUnit Impulse\n\nDiscrete-Time (\\(\\delta[n]\\)): Non-zero only at \\(n=0\\) (value 1).\nContinuous-Time (\\(\\delta(t)\\)): Infinitely tall, infinitesimally narrow pulse with unit area. An idealization.\n\nUnit Step\n\nDiscrete-Time (\\(u[n]\\)): 0 for \\(n&lt;0\\), 1 for \\(n \\ge 0\\).\nContinuous-Time (\\(u(t)\\)): 0 for \\(t&lt;0\\), 1 for \\(t &gt; 0\\).\n\nRelationships\n\n\\(\\delta[n] = u[n] - u[n-1]\\)\n\\(u[n] = \\sum_{m=-\\infty}^{n} \\delta[m]\\)\n\\(\\delta(t) = \\frac{du(t)}{dt}\\) ; \\(u(t) = \\int_{-\\infty}^{t} \\delta(\\tau) d\\tau\\)"
  },
  {
    "objectID": "ss_14.html#key-takeaways-1",
    "href": "ss_14.html#key-takeaways-1",
    "title": "Signal and Systems",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nSampling Property\n\n\\(x[n]\\delta[n-n_0] = x[n_0]\\delta[n-n_0]\\)\n\\(x(t)\\delta(t-t_0) = x(t_0)\\delta(t-t_0)\\)\n\nThese functions are fundamental for signal representation and system analysis.\n\nTo summarize, the unit impulse and unit step functions are the bedrock of signals and systems analysis. They allow us to compactly represent abrupt changes, instantaneous events, and serve as basic building blocks for more complex signals. Their interconnected nature, represented by the difference/sum and derivative/integral relationships, is crucial. But perhaps most importantly, remember their sampling property, which allows us to extract signal values at specific points, a concept vital for future topics. Mastering these basic signals will greatly aid your understanding of system responses and signal processing."
  },
  {
    "objectID": "ss_12.html#introduction-why-transformations",
    "href": "ss_12.html#introduction-why-transformations",
    "title": "Signal and Systems",
    "section": "Introduction: Why Transformations?",
    "text": "Introduction: Why Transformations?\nA central concept in signal and system analysis is the transformation of a signal.\n\nAircraft Control Systems:\n\nPilot actions (signals) are transformed by electrical and mechanical systems.\nResulting in changes to thrust, control surface positions, aircraft velocity, and heading.\n\nHigh-Fidelity Audio Systems:\n\nInput signal (music) is modified.\nTo enhance desirable characteristics, remove noise, or balance components (e.g., treble & bass).\n\n\nThese transformations allow us to introduce basic properties of signals and systems, playing a crucial role in their definition and characterization.\n\nThe core idea is that signals rarely exist in isolation; they are continuously modified, processed, and affected by the systems they pass through. Understanding these transformations, especially simple ones involving the time axis, sets the foundation for more complex system analysis.\nFor instance, in aerospace, a pilot’s joystick movement (a signal) isn’t directly translated into rudder movement. It goes through sensors, analog-to-digital converters, flight control computers (performing complex transformations), digital-to-analog converters, and actuators before physically moving the rudder. Each stage is a transformation."
  },
  {
    "objectID": "ss_12.html#time-shift-continuous-time-signals",
    "href": "ss_12.html#time-shift-continuous-time-signals",
    "title": "Signal and Systems",
    "section": "Time Shift: Continuous-Time Signals",
    "text": "Time Shift: Continuous-Time Signals\nA fundamental transformation where the signal’s shape remains the same, but its position on the time t-axis changes.\n\n\n\nDefinition: \\(x(t-t_0)\\)\n\nDelayed version of \\(x(t)\\) if \\(t_0 &gt; 0\\).\nAdvanced version of \\(x(t)\\) if \\(t_0 &lt; 0\\).\n\n\n\n\n\nApplications: - Radar, sonar, and seismic signal processing: Signals arriving at different receivers at different times due to propagation delays.\n\nThink of \\(x(t)\\) as an original event. If you want some value of \\(x(t)\\) that occurred at time \\(t_{event}\\) to now appear at \\(t_{new}\\) in the transformed signal \\(x(t-t_0)\\), then \\((t_{new} - t_0) = t_{event}\\), which means \\(t_{new} = t_{event} + t_0\\). - If \\(t_0 &gt; 0\\), \\(t_{new}\\) is later than \\(t_{event}\\), hence it’s a delay. - If \\(t_0 &lt; 0\\), \\(t_{new}\\) is earlier than \\(t_{event}\\), hence it’s an advance. The provided figure 1.9 shows \\(t_0 &lt; 0\\), meaning \\(x(t-t_0)\\) is an advanced version of \\(x(t)\\)."
  },
  {
    "objectID": "ss_12.html#time-shift-discrete-time-signals",
    "href": "ss_12.html#time-shift-discrete-time-signals",
    "title": "Signal and Systems",
    "section": "Time Shift: Discrete-Time Signals",
    "text": "Time Shift: Discrete-Time Signals\nAnalogous to continuous-time, but for discrete samples.\n\n\n\nDefinition: \\(x[n-n_0]\\)\n\nThe signal \\(x[n]\\) is shifted by \\(n_0\\) samples.\nIf \\(n_0 &gt; 0\\), it’s a delay.\nIf \\(n_0 &lt; 0\\), it’s an advance.\n\n\n\n\n\nIn this figure, \\(n_0 &gt; 0\\), so \\(x[n-n_0]\\) is a delayed version of \\(x[n]\\). Each point in \\(x[n]\\) occurs later in \\(x[n-n_0]\\).\n\nDiscrete-time shifts are fundamental in digital signal processing, for example, in implementing delay lines, echo effects, or buffering data in communications systems. The variable n represents the sample index, typically integers."
  },
  {
    "objectID": "ss_12.html#time-reversal-reflection",
    "href": "ss_12.html#time-reversal-reflection",
    "title": "Signal and Systems",
    "section": "Time Reversal (Reflection)",
    "text": "Time Reversal (Reflection)\nThis transformation reflects the signal about the origin of the independent variable (\\(t=0\\) or \\(n=0\\)).\nAnalogy: If \\(x(t)\\) is an audio recording, then \\(x(-t)\\) is that recording played backward."
  },
  {
    "objectID": "ss_12.html#time-reversal-reflection-1",
    "href": "ss_12.html#time-reversal-reflection-1",
    "title": "Signal and Systems",
    "section": "Time Reversal (Reflection)",
    "text": "Time Reversal (Reflection)\n\n\nContinuous Time:\n\nDefinition: \\(x(-t)\\)\nObtained by reflection about \\(t=0\\).\n\n\n\n\\(x(t)\\)\n\n\n\n\\(x(-t)\\)\n\n\nDiscrete Time:\n\nDefinition: \\(x[-n]\\)\nObtained by reflection about \\(n=0\\).\n\n\n\n\\(x[n]\\)\n\n\n\n\\(x[-n]\\)\n\n\n\nTime reversal is key for understanding concepts like system causality, stability, and in operations like convolution, where one signal is often reversed. Visually, imagine folding the graph paper along the vertical axis at \\(t=0\\) or \\(n=0\\)."
  },
  {
    "objectID": "ss_12.html#time-scaling",
    "href": "ss_12.html#time-scaling",
    "title": "Signal and Systems",
    "section": "Time Scaling",
    "text": "Time Scaling\nThis transformation compresses or stretches the signal along the time axis.\n\nDefinition: \\(x(\\alpha t)\\) (or \\(x[\\alpha n]\\))\n\nLinearly Compressed if \\(|\\alpha| &gt; 1\\) (e.g., \\(x(2t)\\)). The signal plays faster.\nLinearly Stretched if \\(|\\alpha| &lt; 1\\) (e.g., \\(x(t/2)\\)). The signal plays slower.\nIf \\(\\alpha &lt; 0\\), it also involves a time reversal.\n\n\nAnalogy: Playing an audio recording at twice the speed (\\(x(2t)\\)) or half the speed (\\(x(t/2)\\))."
  },
  {
    "objectID": "ss_12.html#time-scaling-1",
    "href": "ss_12.html#time-scaling-1",
    "title": "Signal and Systems",
    "section": "Time Scaling",
    "text": "Time Scaling\n\n(Top: \\(x(t)\\); Middle: \\(x(2t)\\); Bottom: \\(x(t/2)\\))\n\nFor \\(x(\\alpha t)\\), the value of \\(x(t)\\) at \\(t=t_0\\) will appear in \\(x(\\alpha t)\\) at \\(t = t_0/\\alpha\\). - If \\(|\\alpha| &gt; 1\\), then \\(|t_0/\\alpha| &lt; |t_0|\\), meaning the signal duration is compressed. - If \\(|\\alpha| &lt; 1\\), then \\(|t_0/\\alpha| &gt; |t_0|\\), meaning the signal duration is stretched. This is a critical transformation in frequency analysis (e.g., Fourier Transform), as time scaling in one domain corresponds to frequency scaling in the other."
  },
  {
    "objectID": "ss_12.html#combined-transformations-xalpha-t-beta",
    "href": "ss_12.html#combined-transformations-xalpha-t-beta",
    "title": "Signal and Systems",
    "section": "Combined Transformations: \\(x(\\alpha t + \\beta)\\)",
    "text": "Combined Transformations: \\(x(\\alpha t + \\beta)\\)\nSuch transformations preserve the shape of \\(x(t)\\), but the resulting signal may be:\n\nLinearly stretched if \\(|\\alpha|&lt;1\\)\n\nLinearly compressed if \\(|\\alpha|&gt;1\\)\n\nReversed in time if \\(\\alpha&lt;0\\)\n\nShifted in time if \\(\\beta\\) is nonzero"
  },
  {
    "objectID": "ss_12.html#combined-transformations-xalpha-t-beta-1",
    "href": "ss_12.html#combined-transformations-xalpha-t-beta-1",
    "title": "Signal and Systems",
    "section": "Combined Transformations: \\(x(\\alpha t + \\beta)\\)",
    "text": "Combined Transformations: \\(x(\\alpha t + \\beta)\\)\nSystematic Approach to \\(x(\\alpha t + \\beta)\\):\n\nShift: First, delay or advance \\(x(t)\\) in accordance with the value of \\(\\beta\\). This gives an intermediate signal, e.g., \\(y(t) = x(t+\\beta)\\).\nScale/Reverse: Then, perform time scaling and/or time reversal on the resulting signal \\(y(t)\\) in accordance with the value of \\(\\alpha\\). This means replacing \\(t\\) with \\(\\alpha t\\) in \\(y(t)\\), resulting in \\(y(\\alpha t) = x(\\alpha t + \\beta)\\).\n\n\nIt’s critical to perform these operations in the correct order. The method described (shift first, then scale/reverse) is generally intuitive and widely used. An alternative is to factor out \\(\\alpha\\): \\(x(\\alpha(t + \\beta/\\alpha))\\). This implies a shift by \\(-\\beta/\\alpha\\) after scaling, which can be confusing. Sticking to the text’s method: shift \\(x(t)\\) by \\(\\beta\\) to get \\(x(t+\\beta)\\), then substitute \\(t \\rightarrow \\alpha t\\) to get \\(x(\\alpha t + \\beta)\\)."
  },
  {
    "objectID": "ss_12.html#example-1.1-shift-and-reversal",
    "href": "ss_12.html#example-1.1-shift-and-reversal",
    "title": "Signal and Systems",
    "section": "Example 1.1: Shift and Reversal",
    "text": "Example 1.1: Shift and Reversal\nGiven \\(x(t)\\) in Figure (a), let’s find \\(x(t+1)\\) and \\(x(-t+1)\\).\n\n\na. Original Signal: \\(x(t)\\)\nb. Transformation 1: \\(x(t+1)\\)\n\nThis corresponds to an advance (shift to the left) by one unit along the \\(t\\)-axis.\n\nc. Transformation 2: \\(x(-t+1)\\)\n\nThis signal is the time-reversed version of \\(x(t+1)\\).\nIt is obtained graphically by reflecting \\(x(t+1)\\) about the \\(t\\)-axis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWalk through the points: For \\(x(t+1)\\): - The point at \\(t=0\\) in \\(x(t)\\) moves to \\(t=-1\\) in \\(x(t+1)\\). - The point at \\(t=1\\) in \\(x(t)\\) moves to \\(t=0\\) in \\(x(t+1)\\). - The point at \\(t=2\\) in \\(x(t)\\) moves to \\(t=1\\) in \\(x(t+1)\\). This confirms the shift to the left (advance).\nFor \\(x(-t+1)\\): - Take the signal \\(x(t+1)\\) (from Figure b). - Reflect it about the vertical axis (\\(t=0\\)). - The point at \\(t=-1\\) in \\(x(t+1)\\) moves to \\(t=1\\) in \\(x(-t+1)\\). - The point at \\(t=0\\) in \\(x(t+1)\\) stays at \\(t=0\\) in \\(x(-t+1)\\). - The point at \\(t=1\\) in \\(x(t+1)\\) moves to \\(t=-1\\) in \\(x(-t+1)\\). This precisely matches Figure (c)."
  },
  {
    "objectID": "ss_12.html#example-1.2-time-scaling",
    "href": "ss_12.html#example-1.2-time-scaling",
    "title": "Signal and Systems",
    "section": "Example 1.2: Time Scaling",
    "text": "Example 1.2: Time Scaling\nGiven \\(x(t)\\) in Figure (a), let’s find \\(x(\\frac{3}{2} t)\\).\n\n\na. Original Signal: \\(x(t)\\)\nd. Transformation: \\(x(\\frac{3}{2} t)\\)\n\nThis corresponds to a linear compression of \\(x(t)\\) by a factor of \\(\\frac{2}{3}\\). Since \\(|\\alpha| = \\frac{3}{2} &gt; 1\\).\nThe value of \\(x(t)\\) at \\(t=t_0\\) occurs in \\(x(\\frac{3}{2} t)\\) at \\(t=\\frac{2}{3} t_0\\). E.g., \\(x(1)\\) is found in \\(x(\\frac{3}{2} t)\\) at \\(t=\\frac{2}{3}\\).\nSince \\(x(t)\\) is zero for \\(t&lt;0\\), \\(x(\\frac{3}{2} t)\\) is zero for \\(t&lt;0\\).\nSince \\(x(t)\\) is zero for \\(t&gt;2\\), \\(x(\\frac{3}{2} t)\\) is zero for \\(t &gt; \\frac{4}{3}\\).\n\n\n\n\n\n\n\n\n\n\n\n\nExplain intuitively what “compression by a factor of 2/3” means. It means the signal “finishes” in 2/3 of the original time. - Original signal \\(x(t)\\) starts at \\(t=0\\) and ends at \\(t=2\\). - The transformed signal \\(x(\\frac{3}{2} t)\\) starts when \\(\\frac{3}{2} t = 0 \\Rightarrow t=0\\). - It ends when \\(\\frac{3}{2} t = 2 \\Rightarrow t = 2 \\times \\frac{2}{3} = \\frac{4}{3}\\). The duration of the signal has indeed been compressed from \\(2\\) to \\(4/3\\)."
  },
  {
    "objectID": "ss_12.html#example-1.3-combined-shift-scale",
    "href": "ss_12.html#example-1.3-combined-shift-scale",
    "title": "Signal and Systems",
    "section": "Example 1.3: Combined Shift & Scale",
    "text": "Example 1.3: Combined Shift & Scale\nSuppose we want to determine \\(x(\\frac{3}{2} t+1)\\) for the signal \\(x(t)\\) from Figure (a).\n\nShift based on \\(\\beta=1\\):\n\nFirst, advance (shift to the left) \\(x(t)\\) by 1.\nThis yields the signal \\(x(t+1)\\), shown in Figure (b), which we analyzed in Example 1.1.\n\n\n\n\\(x(t+1)\\)"
  },
  {
    "objectID": "ss_12.html#example-1.3-combined-shift-scale-1",
    "href": "ss_12.html#example-1.3-combined-shift-scale-1",
    "title": "Signal and Systems",
    "section": "Example 1.3: Combined Shift & Scale",
    "text": "Example 1.3: Combined Shift & Scale\n\nScale based on \\(\\alpha=\\frac{3}{2}\\):\n\nNow, take \\(x(t+1)\\) and substitute \\(t \\rightarrow \\frac{3}{2}t\\) to get \\(x(\\frac{3}{2}t+1)\\).\nThis linearly compresses the signal \\(x(t+1)\\) by a factor of \\(\\frac{2}{3}\\).\nThe signal \\(x(t+1)\\) exists for \\(t \\in [-1, 1]\\).\nSo \\(x(\\frac{3}{2}t+1)\\) exists for \\(\\frac{3}{2}t \\in [-1, 1] \\implies t \\in [-\\frac{2}{3}, \\frac{2}{3}]\\).\n\n\n\n\\(x(\\frac{3}{2} t+1)\\)\n\n\n\nReinforce the general rule for \\(x(\\alpha t + \\beta)\\): 1. Apply the shift related to \\(\\beta\\) to the original signal’s time variable. So, \\(x(t) \\rightarrow x(t+\\beta)\\). 2. Apply the scaling/reversal related to \\(\\alpha\\) to the new time variable of the shifted signal. So, \\(x(t+\\beta) \\rightarrow x(\\alpha t + \\beta)\\).\nThis order (shift then scale) is common and easy to visualize. Alternative (factor \\(\\alpha\\) first): \\(x(\\alpha(t+\\beta/\\alpha))\\). This would imply scaling \\(x(t)\\) to \\(x(\\alpha t)\\), then shifting \\(x(\\alpha t)\\) by \\(-\\beta/\\alpha\\). Both methods yield the same result but require careful attention to the order and interpretation of the shift parameter."
  },
  {
    "objectID": "ss_12.html#interactive-demo-signal-transformations",
    "href": "ss_12.html#interactive-demo-signal-transformations",
    "title": "Signal and Systems",
    "section": "Interactive Demo: Signal Transformations",
    "text": "Interactive Demo: Signal Transformations\nExplore how time shifting, scaling, and reversal affect a signal.\nLet’s use a simple triangular pulse signal.\n\n\n\n\n\n\n\nExperiment with the parameters: - beta: Changes the horizontal position (shift). Positive beta means shifting left (advance), negative beta means shifting right (delay). - alpha: - Values greater than 1: Compression. - Values between 0 and 1: Stretching. - Negative values: Reversal combined with compression/stretching. - Try alpha = -1 and beta = 0 to see pure time reversal. - Try alpha = 0.5 and beta = 0 to see stretching. - Try alpha = 2 and beta = 0 to see compression."
  },
  {
    "objectID": "ss_12.html#periodic-signals-continuous-time",
    "href": "ss_12.html#periodic-signals-continuous-time",
    "title": "Signal and Systems",
    "section": "Periodic Signals: Continuous Time",
    "text": "Periodic Signals: Continuous Time\nAn important class of signals that repeat themselves over time.\n\nDefinition: A continuous-time signal \\(x(t)\\) is periodic if there is a positive value of \\(T\\) for which: \\[\nx(t)=x(t+T) \\quad \\text{for all } t \\tag{1.11}\n\\] We say \\(x(t)\\) is periodic with period \\(T\\)."
  },
  {
    "objectID": "ss_12.html#periodic-signals-continuous-time-1",
    "href": "ss_12.html#periodic-signals-continuous-time-1",
    "title": "Signal and Systems",
    "section": "Periodic Signals: Continuous Time",
    "text": "Periodic Signals: Continuous Time\n\nFundamental Period (\\(T_0\\)): The smallest positive value of \\(T\\) for which \\(x(t)=x(t+T)\\) holds.\n\nException: For a constant signal, the fundamental period is undefined, as it is periodic for any \\(T\\).\n\nAperiodic Signal: A signal that is not periodic.\n\nApplications: Natural responses of conserved energy systems (e.g., ideal LC circuits, frictionless mechanical systems) are often periodic.\n\nEmphasize “for all values of \\(t\\)” in the definition. This means the entire signal must repeat. If a signal repeats but has a unique feature (like a discontinuity) that doesn’t repeat, it’s not periodic. This differentiates from signals that might look “locally” periodic."
  },
  {
    "objectID": "ss_12.html#periodic-signals-discrete-time",
    "href": "ss_12.html#periodic-signals-discrete-time",
    "title": "Signal and Systems",
    "section": "Periodic Signals: Discrete Time",
    "text": "Periodic Signals: Discrete Time\nSimilar to continuous-time, but defined for discrete samples.\n\nDefinition: A discrete-time signal \\(x[n]\\) is periodic with period \\(N\\), where \\(N\\) is a positive integer, if: \\[\nx[n]=x[n+N] \\quad \\text{for all } n \\tag{1.12}\n\\] If this holds, \\(x[n]\\) is periodic with period \\(N\\)."
  },
  {
    "objectID": "ss_12.html#periodic-signals-discrete-time-1",
    "href": "ss_12.html#periodic-signals-discrete-time-1",
    "title": "Signal and Systems",
    "section": "Periodic Signals: Discrete Time",
    "text": "Periodic Signals: Discrete Time\n\nFundamental Period (\\(N_0\\)): The smallest positive integer value of \\(N\\) for which \\(x[n]=x[n+N]\\) holds.\n\n\nThe key difference for discrete-time is that the period \\(N\\) must be an integer. This has implications when dealing with discrete-time sinusoids, where their periodicity depends on the ratio of their frequency to \\(2\\pi\\) being a rational number."
  },
  {
    "objectID": "ss_12.html#example-1.4-checking-periodicity",
    "href": "ss_12.html#example-1.4-checking-periodicity",
    "title": "Signal and Systems",
    "section": "Example 1.4: Checking Periodicity",
    "text": "Example 1.4: Checking Periodicity\nConsider the signal given by: \\[\nx(t)=\\left\\{\\begin{array}{ll}\n\\cos (t) & \\text { if } t&lt;0 \\\\\n\\sin (t) & \\text { if } t \\geq 0\n\\end{array}\\right.\n\\]"
  },
  {
    "objectID": "ss_12.html#example-1.4-checking-periodicity-1",
    "href": "ss_12.html#example-1.4-checking-periodicity-1",
    "title": "Signal and Systems",
    "section": "Example 1.4: Checking Periodicity",
    "text": "Example 1.4: Checking Periodicity\nAnalysis:\n\nWe know \\(\\cos(t+2\\pi)=\\cos(t)\\) and \\(\\sin(t+2\\pi)=\\sin(t)\\).\nIndividually, both cosine (for \\(t&lt;0\\)) and sine (for \\(t \\ge 0\\)) parts are periodic with period \\(2\\pi\\).\nHowever, observe the discontinuity at \\(t=0\\).\nFor \\(x(t)\\) to be periodic, every feature in its shape must recur periodically.\nThe discontinuity at \\(t=0\\) does not recur at \\(t=2\\pi, 4\\pi, \\ldots\\) or \\(t=-2\\pi, -4\\pi, \\ldots\\).\n\nConclusion: The signal \\(x(t)\\) is not periodic.\n\nThis example highlights a common mistake. Just because components of a signal are periodic doesn’t mean the composite signal is. The definition \\(x(t)=x(t+T)\\) for all t is strict. The jump at \\(t=0\\) is unique. For \\(x(t)\\) to be periodic with period \\(2\\pi\\), we would need \\(x(0) = x(2\\pi)\\). \\(x(0) = \\sin(0) = 0\\). But for \\(t&lt;0\\), values approaching \\(0\\) from the left are \\(\\cos(t)\\), so \\(\\lim_{t \\to 0^-} x(t) = \\cos(0) = 1\\). And for \\(t \\ge 0\\), values approaching \\(0\\) from the right are \\(\\sin(t)\\), so \\(\\lim_{t \\to 0^+} x(t) = \\sin(0) = 0\\). The mismatch at \\(t=0\\) (specifically, \\(x(0)=0\\) but immediately to its left it’s 1) must repeat if it were periodic. But it clearly doesn’t."
  },
  {
    "objectID": "ss_12.html#interactive-demo-constructing-periodic-signals",
    "href": "ss_12.html#interactive-demo-constructing-periodic-signals",
    "title": "Signal and Systems",
    "section": "Interactive Demo: Constructing Periodic Signals",
    "text": "Interactive Demo: Constructing Periodic Signals\nLet’s visualize how periodicity works for continuous and discrete-time signals.\n\n\n\n\n\n\n\nContinuous-Time Demo: - Observe how changing \\(T_0\\) stretches or compresses the repeating pattern. - The dashed red lines indicate the start of each period.\nDiscrete-Time Demo: - Notice that \\(N_0\\) must be an integer. - See how the sequence repeats after every \\(N_0\\) samples. - If you were to set \\(N_0\\) to a value that is not a multiple of the base signal’s inherent periodicity (e.g., the base_signal has internal period 4), you’ll see how only the first \\(N_0\\) samples are truly repeated. For proper fundamental period, \\(N_0\\) should be the smallest integer for which the specific signal repeats. In the stem plot, the red dashed lines show the declared \\(N_0\\)."
  },
  {
    "objectID": "ss_12.html#even-and-odd-signals",
    "href": "ss_12.html#even-and-odd-signals",
    "title": "Signal and Systems",
    "section": "Even and Odd Signals",
    "text": "Even and Odd Signals\nSignals can exhibit symmetry under time reversal.\n\n\n\nEven Signal: Identical to its time-reversed counterpart.\n\nContinuous Time: \\[\nx(-t)=x(t) \\tag{1.14}\n\\]\nDiscrete Time: \\[\nx[-n]=x[n] \\tag{1.15}\n\\]\n\n\n\n\n\nEven continuous-time signal"
  },
  {
    "objectID": "ss_12.html#even-and-odd-signals-1",
    "href": "ss_12.html#even-and-odd-signals-1",
    "title": "Signal and Systems",
    "section": "Even and Odd Signals",
    "text": "Even and Odd Signals\n\n\n\nOdd Signal: The negative of its time-reversed counterpart.\n\nContinuous Time: \\[\nx(-t)=-x(t) \\tag{1.16}\n\\]\nDiscrete Time: \\[\nx[-n]=-x[n] \\tag{1.17}\n\\]\nProperty: An odd signal must necessarily be \\(0\\) at \\(t=0\\) or \\(n=0\\) (since \\(x(0) = -x(0) \\implies 2x(0) = 0 \\implies x(0)=0\\)).\n\n\n\n\n\nOdd continuous-time signal\n\n\n\nVisualize even signals as being symmetrical around the vertical axis (like a mirror image). Cosine functions are classic examples of even signals. Visualize odd signals as being symmetrical with respect to the origin (rotate 180 degrees). Sine functions are classic examples of odd signals. Emphasize why \\(x(0)\\) must be zero for an odd signal. It’s a key point."
  },
  {
    "objectID": "ss_12.html#even-odd-decomposition",
    "href": "ss_12.html#even-odd-decomposition",
    "title": "Signal and Systems",
    "section": "Even-Odd Decomposition",
    "text": "Even-Odd Decomposition\nAny signal can be uniquely broken down into a sum of an even signal and an odd signal.\nFor a continuous-time signal \\(x(t)\\):\n\nEven Part: \\(\\mathcal{E} v\\{x(t)\\}\\) \\[\n\\mathcal{E} v\\{x(t)\\}=\\frac{1}{2}[x(t)+x(-t)] \\tag{1.18}\n\\]\nOdd Part: \\(\\mathcal{O} d\\{x(t)\\}\\) \\[\n\\mathcal{O} d\\{x(t)\\}=\\frac{1}{2}[x(t)-x(-t)] \\tag{1.19}\n\\]\nDecomposition: \\(x(t) = \\mathcal{E} v\\{x(t)\\} + \\mathcal{O} d\\{x(t)\\}\\)"
  },
  {
    "objectID": "ss_12.html#even-odd-decomposition-1",
    "href": "ss_12.html#even-odd-decomposition-1",
    "title": "Signal and Systems",
    "section": "Even-Odd Decomposition",
    "text": "Even-Odd Decomposition\nThese definitions hold analogously for discrete-time signals \\(x[n]\\).\n\n(Example of discrete-time decomposition)\n\nExplain why these definitions work: - To show \\(\\mathcal{E} v\\{x(t)\\}\\) is even: Replace \\(t\\) with \\(-t\\) in \\(\\mathcal{E} v\\{x(t)\\}\\). You get \\(\\frac{1}{2}[x(-t)+x(t)] = \\mathcal{E} v\\{x(t)\\}\\). - To show \\(\\mathcal{O} d\\{x(t)\\}\\) is odd: Replace \\(t\\) with \\(-t\\) in \\(\\mathcal{O} d\\{x(t)\\}\\). You get \\(\\frac{1}{2}[x(-t)-x(t)] = -\\frac{1}{2}[x(t)-x(-t)] = -\\mathcal{O} d\\{x(t)\\}\\). - To show \\(x(t)\\) is the sum: Add the two equations: \\(\\frac{1}{2}[x(t)+x(-t)] + \\frac{1}{2}[x(t)-x(-t)] = \\frac{1}{2}x(t) + \\frac{1}{2}x(-t) + \\frac{1}{2}x(t) - \\frac{1}{2}x(-t) = x(t)\\).\nThis decomposition is incredibly useful in signal processing, simplifying analysis of complex signals by breaking them into symmetric components. E.g., for Fourier Series, even functions only have cosine terms; odd functions only have sine terms."
  },
  {
    "objectID": "ss_12.html#interactive-demo-even-odd-parts",
    "href": "ss_12.html#interactive-demo-even-odd-parts",
    "title": "Signal and Systems",
    "section": "Interactive Demo: Even & Odd Parts",
    "text": "Interactive Demo: Even & Odd Parts\nVisualize the even and odd decomposition of a signal.\n\n\n\n\n\n\n\nExperiment with Signal Parameters: - Decay Rate: Changes how quickly the exponential component decays. - Offset Strength: Introduces a constant offset, which heavily influences the even part. - Observe that the Even Part is symmetrical about the y-axis, and the Odd Part is symmetrical about the origin (and is zero at \\(t=0\\)). - Mentally (or by adding up the plots), verify that Ev{x(t)} + Od{x(t)} equals the Original Signal x(t)."
  },
  {
    "objectID": "ss_12.html#summary",
    "href": "ss_12.html#summary",
    "title": "Signal and Systems",
    "section": "Summary",
    "text": "Summary\nFundamental transformations of the independent variable (time) for signals.\n\nTime Shift (\\(x(t \\pm t_0)\\) or \\(x[n \\pm n_0]\\)): Delays or advances a signal. Crucial for understanding propagation delays.\nTime Reversal (\\(x(-t)\\) or \\(x[-n]\\)): Reflects a signal about the origin. Important for symmetry concepts.\nTime Scaling (\\(x(\\alpha t)\\) or \\(x[\\alpha n]\\)): Compresses or stretches a signal. Linked to bandwidth in frequency domain.\nCombined Transformations (\\(x(\\alpha t + \\beta)\\)): Requires a systematic order (shift then scale/reverse)."
  },
  {
    "objectID": "ss_12.html#summary-1",
    "href": "ss_12.html#summary-1",
    "title": "Signal and Systems",
    "section": "Summary",
    "text": "Summary\n\nPeriodic Signals (\\(x(t) = x(t+T)\\) or \\(x[n] = x[n+N]\\)): Signals that repeat themselves, characterized by a fundamental period.\nEven and Odd Signals: Signals exhibiting specific symmetry under time reversal, and any signal can be uniquely decomposed into its even and odd parts.\n\n\nReinforce the “why” these transformations matter. They are not just mathematical curiosities. They represent real-world physical changes to signals as they are processed, transmitted, or interact with systems. They also form the basis for later analytical tools, such as the Convolution Integral/Sum and Fourier Transforms."
  },
  {
    "objectID": "rps.html#dosen-pengajar",
    "href": "rps.html#dosen-pengajar",
    "title": "Sinyal dan Sistem",
    "section": "Dosen Pengajar",
    "text": "Dosen Pengajar\n\nImron Rosyadi (kelas A)\nAgung Mubyarto (kelas B-C)\nM. Syaiful Aliim (kelas B-C)\nDinda Wahyu (kelas D)"
  },
  {
    "objectID": "rps.html#jadwal-kelas-google-meet",
    "href": "rps.html#jadwal-kelas-google-meet",
    "title": "Sinyal dan Sistem",
    "section": "Jadwal, Kelas, Google Meet",
    "text": "Jadwal, Kelas, Google Meet\n\nMK Sinyal dan Sistem - Kelas A\nLecturer: Imron Rosyadi\nSchedule: Monday, 09:45 – 12:25\nRoom: GEDUNG TEKNIK C 105\nGoogle Meet: meet.google.com/bat-crca-xua"
  },
  {
    "objectID": "rps.html#link-penting",
    "href": "rps.html#link-penting",
    "title": "Sinyal dan Sistem",
    "section": "Link Penting",
    "text": "Link Penting\n\nCourse Page: imron-course.vercel.app\nPresentation: imron-slide.vercel.app\nDiscussion: discord.gg/6nNTvfmz, expired at 29 Aug 2025\nAttendance: teraversa.unsoed.ac.id\nOnline Learning: eldiru.unsoed.ac.id"
  },
  {
    "objectID": "rps.html#metode-pembelajaran",
    "href": "rps.html#metode-pembelajaran",
    "title": "Sinyal dan Sistem",
    "section": "Metode Pembelajaran",
    "text": "Metode Pembelajaran\n\nEnglish-Indonesia: Salindia presentasi dalam bahasa Inggris, tetapi penyampaian menggunakan bahasa Indonesia.\nLuring-Daring: Pembelajaran dilakukan dengan tatap muka luring, tetapi diselingi secara daring. Cek Discord untuk informasi.\nVisualisasi-Interaktif: Pembelajaran akan menyediakan sumberdaya berupa visualisasi interaktif untuk memudahkan pemahaman. Cek berbagai link yang disediakan.\nCase-based Approach: Pembelajaran akan dikaitkan dengan topik bidang Electrical and Computer Engineering\nDiskusi-Informasi via Discord: Diskusikan materi, ajukan pertanyaan, langsung via Discord (channel #aljabar-linear-1).\nWeb-based Presentation: Salindia presentasi berbentuk laman web, tetapi anda bisa mengekspornya ke PDF.\nReview via Recording: Sesi pembelajaran akan (diusahakan untuk) direkam dan diunggah ke Youtube\nRileks: Pembelajaran dilakukan secara rileks."
  },
  {
    "objectID": "rps.html#referensi",
    "href": "rps.html#referensi",
    "title": "Sinyal dan Sistem",
    "section": "Referensi",
    "text": "Referensi\n\nReferensi Utama: Oppenheim, A. V., Willsky, A. S., & Hamid, S. (1996). Signals and Systems. Prentice Hall.\nReferensi Pendukung: Haykin, S., Van Veen, B. (1999). Signals and Systems. John Wiley & Sons."
  },
  {
    "objectID": "rps.html#capaian-pembelajaran-lulusan-cpl",
    "href": "rps.html#capaian-pembelajaran-lulusan-cpl",
    "title": "Sinyal dan Sistem",
    "section": "Capaian Pembelajaran Lulusan (CPL)",
    "text": "Capaian Pembelajaran Lulusan (CPL)\nCapaian Pembelajaran Lulusan (CPL) yang Dibebankan:\nMenguasai matematika, fisika, kimia dan statistik, teknologi informasi dan rekayasa sebagai landasan penerapan di bidang teknik elektro."
  },
  {
    "objectID": "rps.html#capaian-pembelajaran-mata-kuliah-cpmk-sinyal-dan-sistem",
    "href": "rps.html#capaian-pembelajaran-mata-kuliah-cpmk-sinyal-dan-sistem",
    "title": "Sinyal dan Sistem",
    "section": "Capaian Pembelajaran Mata Kuliah (CPMK) Sinyal dan Sistem",
    "text": "Capaian Pembelajaran Mata Kuliah (CPMK) Sinyal dan Sistem\n\nCPMK 1: Analisis Sinyal dan Sistem Linier Invarian Waktu (LTI) di Domain Waktu\nCPMK 2: Analisis Sinyal dan Sistem di Domain Frekuensi (Waktu-kontinu)\nCPMK 3: Analisis Sinyal dan Sistem di Domain Waktu-Diskret dan Transisinya"
  },
  {
    "objectID": "rps.html#cpmk-1-analisis-sinyal-dan-sistem-linier-invarian-waktu-lti-di-domain-waktu",
    "href": "rps.html#cpmk-1-analisis-sinyal-dan-sistem-linier-invarian-waktu-lti-di-domain-waktu",
    "title": "Sinyal dan Sistem",
    "section": "CPMK 1: Analisis Sinyal dan Sistem Linier Invarian Waktu (LTI) di Domain Waktu",
    "text": "CPMK 1: Analisis Sinyal dan Sistem Linier Invarian Waktu (LTI) di Domain Waktu\nDeskripsi:\nCPMK ini berfokus pada penguasaan konsep dasar sinyal dan sistem serta analisis fundamental sistem LTI di domain waktu.\nRumusan CPMK 1:\nMahasiswa mampu menganalisis sinyal dan sistem dasar serta mengkarakterisasi sistem Linier Invarian Waktu (LTI) baik dalam domain waktu-kontinu maupun waktu-diskret menggunakan operasi konvolusi.\nKeterkaitan dengan CPL:\nCPMK ini mendukung CPL dengan membangun pemahaman fundamental matematika rekayasa (konvolusi, persamaan diferensial/beda) untuk merepresentasikan dan menganalisis sinyal dan sistem fisis sebagai landasan rekayasa di bidang teknik elektro."
  },
  {
    "objectID": "rps.html#cpmk-1-analisis-sinyal-dan-sistem-linier-invarian-waktu-lti-di-domain-waktu-1",
    "href": "rps.html#cpmk-1-analisis-sinyal-dan-sistem-linier-invarian-waktu-lti-di-domain-waktu-1",
    "title": "Sinyal dan Sistem",
    "section": "CPMK 1: Analisis Sinyal dan Sistem Linier Invarian Waktu (LTI) di Domain Waktu",
    "text": "CPMK 1: Analisis Sinyal dan Sistem Linier Invarian Waktu (LTI) di Domain Waktu\nIndikator Kemampuan (Sub-CPMK):\n\nMampu mengklasifikasikan berbagai jenis sinyal (waktu-kontinu/diskret, periodik/aperiodik, energi/daya, deterministik/acak).\nMampu melakukan operasi dasar pada variabel independen sinyal (pergeseran waktu, pembalikan waktu, penskalaan waktu).\nMampu mengklasifikasikan properti sistem (linieritas, invariansi waktu, kausalitas, memori, dan kestabilan).\nMampu menghitung respons sistem LTI terhadap sinyal masukan menggunakan integral konvolusi untuk sistem waktu-kontinu.\nMampu menghitung respons sistem LTI terhadap sinyal masukan menggunakan jumlahan konvolusi untuk sistem waktu-diskret.\nMampu menghubungkan representasi sistem LTI dengan persamaan diferensial (waktu-kontinu) dan persamaan beda (waktu-diskret)."
  },
  {
    "objectID": "rps.html#cpmk-2-analisis-sinyal-dan-sistem-di-domain-frekuensi-waktu-kontinu",
    "href": "rps.html#cpmk-2-analisis-sinyal-dan-sistem-di-domain-frekuensi-waktu-kontinu",
    "title": "Sinyal dan Sistem",
    "section": "CPMK 2: Analisis Sinyal dan Sistem di Domain Frekuensi (Waktu-kontinu)",
    "text": "CPMK 2: Analisis Sinyal dan Sistem di Domain Frekuensi (Waktu-kontinu)\nDeskripsi:\nCPMK ini memfokuskan pada kemampuan analisis sinyal dan sistem LTI dari perspektif domain frekuensi untuk sinyal waktu-kontinu.\nRumusan CPMK 2:\nMahasiswa mampu menerapkan representasi sinyal dalam domain frekuensi menggunakan Deret Fourier dan Transformasi Fourier, serta menganalisis karakteristik sistem LTI menggunakan Transformasi Laplace untuk sinyal waktu-kontinu.\nKeterkaitan dengan CPL:\nCPMK ini memperkuat penguasaan matematika rekayasa (analisis kompleks, integral) untuk analisis frekuensi yang merupakan inti dari banyak aplikasi rekayasa di bidang teknik elektro (misalnya, telekomunikasi, sistem kontrol, dan elektronika)."
  },
  {
    "objectID": "rps.html#cpmk-2-analisis-sinyal-dan-sistem-di-domain-frekuensi-waktu-kontinu-1",
    "href": "rps.html#cpmk-2-analisis-sinyal-dan-sistem-di-domain-frekuensi-waktu-kontinu-1",
    "title": "Sinyal dan Sistem",
    "section": "CPMK 2: Analisis Sinyal dan Sistem di Domain Frekuensi (Waktu-kontinu)",
    "text": "CPMK 2: Analisis Sinyal dan Sistem di Domain Frekuensi (Waktu-kontinu)\nIndikator Kemampuan (Sub-CPMK):\n\nMampu merepresentasikan sinyal periodik waktu-kontinu menggunakan Deret Fourier.\nMampu menganalisis spektrum frekuensi sinyal aperiodik waktu-kontinu menggunakan Transformasi Fourier Waktu-kontinu (CTFT).\nMampu menggunakan properti-properti Transformasi Fourier untuk menyederhanakan analisis sinyal dan sistem.\nMampu menganalisis sistem LTI waktu-kontinu menggunakan respons frekuensi \\(H(j\\omega)\\).\nMampu menerapkan Transformasi Laplace untuk analisis sistem LTI, termasuk mencari solusi persamaan diferensial.\nMampu menganalisis kestabilan dan kausalitas sistem LTI berdasarkan Region of Convergence (ROC) dari Transformasi Laplace."
  },
  {
    "objectID": "rps.html#cpmk-3-analisis-sinyal-dan-sistem-di-domain-waktu-diskret-dan-transisinya",
    "href": "rps.html#cpmk-3-analisis-sinyal-dan-sistem-di-domain-waktu-diskret-dan-transisinya",
    "title": "Sinyal dan Sistem",
    "section": "CPMK 3: Analisis Sinyal dan Sistem di Domain Waktu-Diskret dan Transisinya",
    "text": "CPMK 3: Analisis Sinyal dan Sistem di Domain Waktu-Diskret dan Transisinya\nDeskripsi:\nCPMK ini menjembatani dunia analog dan digital melalui konsep sampling dan memfokuskan pada analisis domain frekuensi dan domain-z untuk sinyal dan sistem waktu-diskret.\nRumusan CPMK 3:\nMahasiswa mampu memahami konsep sampling sebagai jembatan antara sinyal waktu-kontinu dan waktu-diskret, serta menerapkan Transformasi Fourier Waktu-Diskret (DTFT) dan Transformasi-Z untuk menganalisis sinyal dan sistem waktu-diskret.\nKeterkaitan dengan CPL:\nCPMK ini mengintegrasikan pemahaman matematika dan teknologi informasi, yang menjadi landasan penting untuk rekayasa digital modern seperti Pemrosesan Sinyal Digital (DSP), sistem komunikasi digital, dan sistem kendali digital."
  },
  {
    "objectID": "rps.html#cpmk-3-analisis-sinyal-dan-sistem-di-domain-waktu-diskret-dan-transisinya-1",
    "href": "rps.html#cpmk-3-analisis-sinyal-dan-sistem-di-domain-waktu-diskret-dan-transisinya-1",
    "title": "Sinyal dan Sistem",
    "section": "CPMK 3: Analisis Sinyal dan Sistem di Domain Waktu-Diskret dan Transisinya",
    "text": "CPMK 3: Analisis Sinyal dan Sistem di Domain Waktu-Diskret dan Transisinya\nIndikator Kemampuan (Sub-CPMK):\n\nMampu menjelaskan Teorema Sampling Nyquist-Shannon dan implikasinya.\nMampu mengidentifikasi fenomena aliasing dan memahami proses rekonstruksi sinyal.\nMampu menganalisis spektrum frekuensi sinyal waktu-diskret menggunakan Transformasi Fourier Waktu-Diskret (DTFT).\nMampu menerapkan Transformasi-Z untuk menganalisis sistem LTI waktu-diskret.\nMampu menentukan fungsi transfer \\(H(z)\\) dari sistem yang direpresentasikan oleh persamaan beda.\nMampu menganalisis kestabilan dan kausalitas sistem LTI waktu-diskret berdasarkan Region of Convergence (ROC) pada bidang-z."
  },
  {
    "objectID": "rps.html#rencana-pertemuan",
    "href": "rps.html#rencana-pertemuan",
    "title": "Sinyal dan Sistem",
    "section": "Rencana Pertemuan",
    "text": "Rencana Pertemuan\nCPMK 1: Analisis Sinyal dan Sistem Linier Invarian Waktu (LTI) di Domain Waktu\n\nPekan 1. Signals and Systems\nPekan 2. Linear Time-Invariant Systems\nPekan 3. Linear Time-Invariant Systems\nPekan 4. Ujian Kompetensi CPMK 1"
  },
  {
    "objectID": "rps.html#rencana-pertemuan-1",
    "href": "rps.html#rencana-pertemuan-1",
    "title": "Sinyal dan Sistem",
    "section": "Rencana Pertemuan",
    "text": "Rencana Pertemuan\nCPMK 2: Analisis Sinyal dan Sistem di Domain Frekuensi (Waktu-kontinu)\n\nPekan 5. Fourier Series\nPekan 6. Fourier Series\nPekan 7. Continous-Time Fourier Transform\nPekan 8. Continous-Time Fourier Transform\nPekan 9. Laplace Transform\nPekan 10. Laplace Transform\nPekan 11. UTS / Ujian Kompetensi CPMK 2"
  },
  {
    "objectID": "rps.html#rencana-pertemuan-2",
    "href": "rps.html#rencana-pertemuan-2",
    "title": "Sinyal dan Sistem",
    "section": "Rencana Pertemuan",
    "text": "Rencana Pertemuan\nCPMK 3: Analisis Sinyal dan Sistem di Domain Waktu-Diskret dan Transisinya\n\nPekan 12. Sampling\nPekan 13. Discrete-Time Fourier Transform\nPekan 14. Discrete-Time Fourier Transform\nPekan 15. Z-Transform\nPekan 16. Z-Transform\nPekan 17. UAS / Ujian Kompetensi CPMK 3"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Signals and Systems",
    "section": "",
    "text": "This is Signals and System Lecture Notes"
  },
  {
    "objectID": "index.html#week-1",
    "href": "index.html#week-1",
    "title": "Signals and Systems",
    "section": "Week 1",
    "text": "Week 1\n\nRPS\nSignals and Systems 1.1\nSignals and Systems 1.2\nSignals and Systems 1.3\nSignals and Systems 1.4\nSignals and Systems 1.5\nSignals and Systems 1.6"
  },
  {
    "objectID": "index.html#week-2",
    "href": "index.html#week-2",
    "title": "Signals and Systems",
    "section": "Week 2",
    "text": "Week 2\n\nSignals and Systems 2.1\nSignals and Systems 2.2\nSignals and Systems 2.3"
  },
  {
    "objectID": "index.html#week-3",
    "href": "index.html#week-3",
    "title": "Signals and Systems",
    "section": "Week 3",
    "text": "Week 3\n\nSignals and Systems 2.4\nIntroduction to Fourier Analysis\nSignals and Systems 3.2"
  },
  {
    "objectID": "index.html#week-4",
    "href": "index.html#week-4",
    "title": "Signals and Systems",
    "section": "Week 4",
    "text": "Week 4\n\nIntroduction to Fourier Analysis\nBullying"
  },
  {
    "objectID": "index.html#week-5",
    "href": "index.html#week-5",
    "title": "Signals and Systems",
    "section": "Week 5",
    "text": "Week 5\n\nUjian CPMK\nSignals and Systems 3.2\nSignals and Systems 3.3"
  },
  {
    "objectID": "index.html#week-6",
    "href": "index.html#week-6",
    "title": "Signals and Systems",
    "section": "Week 6",
    "text": "Week 6\n\nSignals and Systems 3.4\nSignals and Systems 3.5\nSignals and Systems 3.6\nSignals and Systems 3.7\nSignals and Systems 3.8"
  },
  {
    "objectID": "ss_11.html#what-is-a-signal",
    "href": "ss_11.html#what-is-a-signal",
    "title": "Signal and Systems",
    "section": "What is a Signal?",
    "text": "What is a Signal?\nSignals describe patterns of variation that convey information.\n\n\n\nElectrical Circuits: Voltage/current changes over time. \n\n\n\nAudio/Speech: Acoustic pressure fluctuations. \n\n\n\nSignals are essentially carriers of information. Think of them as dynamic entities where the information is encoded in how a physical quantity changes. We see examples all around us: the humble voltage in a circuit, how a car’s speed changes, the intricate pressure waves our vocal cords produce to form speech, or even the varying brightness levels that make up an image. Each of these variations holds specific information."
  },
  {
    "objectID": "ss_11.html#signal-examples",
    "href": "ss_11.html#signal-examples",
    "title": "Signal and Systems",
    "section": "Signal (Examples)",
    "text": "Signal (Examples)"
  },
  {
    "objectID": "ss_11.html#signal-examples-1",
    "href": "ss_11.html#signal-examples-1",
    "title": "Signal and Systems",
    "section": "Signal (Examples)",
    "text": "Signal (Examples)"
  },
  {
    "objectID": "ss_11.html#mathematical-representation",
    "href": "ss_11.html#mathematical-representation",
    "title": "Signal and Systems",
    "section": "Mathematical Representation",
    "text": "Mathematical Representation\nSignals are mathematically described as functions of one or more independent variables. Typically, we refer to the independent variable as “time.”\n\n\nContinuous-Time (CT) Signals: \\(x(t)\\)\n\nIndependent variable t is continuous.\nDefined for a continuum of values.\n\n\nDiscrete-Time (DT) Signals: \\(x[n]\\)\n\nIndependent variable n is discrete (integers).\nDefined only at discrete points."
  },
  {
    "objectID": "ss_11.html#mathematical-representation-1",
    "href": "ss_11.html#mathematical-representation-1",
    "title": "Signal and Systems",
    "section": "Mathematical Representation",
    "text": "Mathematical Representation\n\n\nContinuous-Time (CT) Signals: \\(x(t)\\)\n\nExamples:\n\nSpeech signal: \\(P(t)\\)\nWind profile vs. height: \\(W(h)\\) \n\n\n\nDiscrete-Time (DT) Signals: \\(x[n]\\)\n\nExamples:\n\nWeekly Dow-Jones Index: \\(D[k]\\)\nDemographic data \n\n\n\n\nMathematically, we represent signals as functions. For this course, we’ll primarily focus on signals with a single independent variable, which we most often call “time,” even if it represents something else like height or depth.\nWe distinguish between two major types: Continuous-Time (CT) signals, denoted \\(x(t)\\), are defined for every possible value of \\(t\\), much like how voltage in a circuit changes smoothly over time.\nIn contrast, Discrete-Time (DT) signals, denoted \\(x[n]\\), are only defined at specific, discrete points, usually integer values of \\(n\\). Think of a stock market index that’s only recorded at the end of each day or week – there’s no data point in between."
  },
  {
    "objectID": "ss_11.html#visualizing-ct-signals",
    "href": "ss_11.html#visualizing-ct-signals",
    "title": "Signal and Systems",
    "section": "Visualizing CT Signals",
    "text": "Visualizing CT Signals\nContinuous-Time Signal Example: \\(x(t) = \\cos(2\\pi t)\\)"
  },
  {
    "objectID": "ss_11.html#visualizing-dt-signals",
    "href": "ss_11.html#visualizing-dt-signals",
    "title": "Signal and Systems",
    "section": "Visualizing DT Signals",
    "text": "Visualizing DT Signals\nDiscrete-Time Signal Example: \\(x[n] = \\cos(\\frac{\\pi}{4}n)\\)\n\n\n\n\n\n\n\nThe visual representation is key to distinguishing these signal types. As you can see from the interactive plots: For a continuous-time signal, the graph is a smooth, unbroken curve, indicating that the signal has a value at every single point in continuous time. For a discrete-time signal, the graph consists of individual stems, or impulses, at integer points. There’s no signal defined in between these integer values. This emphasizes that discrete-time signals are sequences of values. Feel free to run and interact with these code blocks to see how they behave."
  },
  {
    "objectID": "ss_11.html#origins-of-discrete-time-signals",
    "href": "ss_11.html#origins-of-discrete-time-signals",
    "title": "Signal and Systems",
    "section": "Origins of Discrete-Time Signals",
    "text": "Origins of Discrete-Time Signals\nDiscrete-time signals can arise in two ways:\n\nInherently Discrete Phenomena:\n\nVariables are naturally discrete.\nExamples: Population counts, quarterly economic data, number of defects per batch.\n\nSampling of Continuous-Time Signals:\n\nConverting a continuous signal into a discrete sequence.\nProcess: Measuring the continuous signal’s value at regular intervals.\nImportance: Foundation of modern digital signal processing.\nApplications: Digital audio, image processing (pixels are samples of brightness), digital control systems (autopilots).\n\n\n\nIt’s important to understand that discrete-time signals aren’t always born discrete. Many are created by taking a continuous-time signal and sampling it. This process, often done by Analog-to-Digital Converters (ADCs), is fundamental to modern digital systems. Because digital processors thrive on discrete data, sampling allows us to bring real-world analog signals into the digital domain for processing, analysis, and storage. This is why when you zoom in on a digital image, you eventually see the individual pixels – each pixel is a sample of the original continuous image’s brightness."
  },
  {
    "objectID": "ss_11.html#signal-energy-and-power",
    "href": "ss_11.html#signal-energy-and-power",
    "title": "Signal and Systems",
    "section": "Signal Energy and Power",
    "text": "Signal Energy and Power\nMotivation derives from physical systems (e.g., electrical power \\(p(t) = v(t)i(t)\\) across a resistor).\nWe extend these concepts to any signal. For a general signal \\(x(t)\\) or \\(x[n]\\) , we use \\(|x|^2\\).\n\nTotal Energy over a finite interval:\n\nCT: \\(\\int_{t_1}^{t_2} |x(t)|^2 dt\\)\nDT: \\(\\sum_{n=n_1}^{n_2} |x[n]|^2\\)\n\nAverage Power over a finite interval:\n\nCT: \\(\\frac{1}{t_2-t_1} \\int_{t_1}^{t_2} |x(t)|^2 dt\\)\nDT: \\(\\frac{1}{n_2-n_1+1} \\sum_{n=n_1}^{n_2} |x[n]|^2\\)\n\n\n\nThe concepts of energy and power are borrowed from physics but are generalized in Signals and Systems. Even if a signal doesn’t directly represent physical energy (like velocity, or a sound wave frequency), we still use these terms to characterize its ‘strength’ or ‘intensity’ over time.\nFor signals that can take complex values, which we’ll encounter later, we use the magnitude squared, \\(|x|^2\\), analogous to how physical power is proportional to voltage or current squared. These definitions help us quantify how much ‘content’ a signal has within a specific duration."
  },
  {
    "objectID": "ss_11.html#total-energy-e_infty-for-ct-signals",
    "href": "ss_11.html#total-energy-e_infty-for-ct-signals",
    "title": "Signal and Systems",
    "section": "Total Energy (\\(E_{\\infty}\\)) for CT Signals",
    "text": "Total Energy (\\(E_{\\infty}\\)) for CT Signals\nThe total energy of a continuous-time signal \\(x(t)\\) over an infinite interval is:\n\\[\nE_{\\infty} \\triangleq \\lim _{T \\rightarrow \\infty} \\int_{-T}^{T}|x(t)|^{2} d t=\\int_{-\\infty}^{+\\infty}|x(t)|^{2} d t\n\\]\n\nIf \\(E_{\\infty} &lt; \\infty\\), the signal is a finite-energy signal.\nIf \\(E_{\\infty} = \\infty\\), the signal has infinite energy."
  },
  {
    "objectID": "ss_11.html#total-energy-e_infty-for-ct-signals-1",
    "href": "ss_11.html#total-energy-e_infty-for-ct-signals-1",
    "title": "Signal and Systems",
    "section": "Total Energy (\\(E_{\\infty}\\)) for CT Signals",
    "text": "Total Energy (\\(E_{\\infty}\\)) for CT Signals\nExample: Finite Energy Pulse Signal\nConsider a rectangular pulse \\(x(t) = 1\\) for \\(0 \\leq t \\leq 1\\) and \\(0\\) otherwise.\n\n\n\n\n\n\n\nWhen we analyze signals over their entire duration, from negative to positive infinity, we use the concept of total energy. This is defined by integrating the magnitude squared of the signal over all time. A signal with finite total energy is one where this integral converges to a finite value. Such signals are typically “transient” in nature, meaning they eventually die out and don’t persist indefinitely. The rectangular pulse shown is a classic example: it only exists for a brief period, and its energy is clearly finite."
  },
  {
    "objectID": "ss_11.html#total-energy-e_infty-for-dt-signals",
    "href": "ss_11.html#total-energy-e_infty-for-dt-signals",
    "title": "Signal and Systems",
    "section": "Total Energy (\\(E_{\\infty}\\)) for DT Signals",
    "text": "Total Energy (\\(E_{\\infty}\\)) for DT Signals\nThe total energy of a discrete-time signal \\(x[n]\\) over an infinite interval is:\n\\[\nE_{\\infty} \\triangleq \\lim _{N \\rightarrow \\infty} \\sum_{n=-N}^{+N}|x[n]|^{2}=\\sum_{n=-\\infty}^{+\\infty}|x[n]|^{2}\n\\]\n\nIf \\(E_{\\infty} &lt; \\infty\\), the signal is a finite-energy signal."
  },
  {
    "objectID": "ss_11.html#total-energy-e_infty-for-dt-signals-1",
    "href": "ss_11.html#total-energy-e_infty-for-dt-signals-1",
    "title": "Signal and Systems",
    "section": "Total Energy (\\(E_{\\infty}\\)) for DT Signals",
    "text": "Total Energy (\\(E_{\\infty}\\)) for DT Signals\nExample: Decaying Exponential\nConsider \\(x[n] = (0.5)^n u[n]\\), where \\(u[n]\\) is the unit step function (\\(u[n]=1\\) for \\(n \\ge 0\\), \\(0\\) for \\(n &lt; 0\\)).\n\n\n\n\n\n\n\nFor discrete-time signals, total energy is calculated by summing the squared magnitudes of all signal samples from negative to positive infinity. Like their continuous counterparts, discrete-time signals with finite total energy are those whose sum converges. The decaying exponential is a perfect illustration: as ‘n’ increases, the signal values become smaller and smaller, ensuring that the infinite sum of their squares remains finite. This is also a transient signal."
  },
  {
    "objectID": "ss_11.html#average-power-p_infty-for-ct-signals",
    "href": "ss_11.html#average-power-p_infty-for-ct-signals",
    "title": "Signal and Systems",
    "section": "Average Power (\\(P_{\\infty}\\)) for CT Signals",
    "text": "Average Power (\\(P_{\\infty}\\)) for CT Signals\nThe time-averaged power over an infinite interval for a continuous-time signal \\(x(t)\\) is:\n\\[\nP_{\\infty} \\triangleq \\lim _{T \\rightarrow \\infty} \\frac{1}{2 T} \\int_{-T}^{T}|x(t)|^{2} d t\n\\]\n\nIf \\(E_{\\infty} &lt; \\infty\\), then \\(P_{\\infty} = 0\\).\nIf \\(P_{\\infty} &gt; 0\\) and \\(P_{\\infty} &lt; \\infty\\), the signal is a finite-power signal (implying \\(E_{\\infty} = \\infty\\))."
  },
  {
    "objectID": "ss_11.html#average-power-p_infty-for-ct-signals-1",
    "href": "ss_11.html#average-power-p_infty-for-ct-signals-1",
    "title": "Signal and Systems",
    "section": "Average Power (\\(P_{\\infty}\\)) for CT Signals",
    "text": "Average Power (\\(P_{\\infty}\\)) for CT Signals\nExample: Sinusoidal Signal\nConsider \\(x(t) = A \\cos(\\omega t + \\phi)\\). Its average power is \\(P_{\\infty} = A^2/2\\). Let’s test with \\(x(t) = \\cos(2\\pi t)\\).\n\n\n\n\n\n\n\nAverage power describes how much energy, on average, is contained in a signal over an infinitely long period. It’s often more relevant for signals that persist indefinitely, such as periodic signals. If a signal has finite total energy, its average power must be zero, because you’re distributing that finite energy over an infinite duration.\nHowever, if a signal has a constant “strength” over time, it will have finite average power but infinite total energy. Sinusoidal signals, like the cosine wave demonstrated, are prime examples. Their amplitude oscillates but never dies down, so they continuously carry power, leading to infinite total energy over infinite time, but finite average power."
  },
  {
    "objectID": "ss_11.html#average-power-p_infty-for-dt-signals",
    "href": "ss_11.html#average-power-p_infty-for-dt-signals",
    "title": "Signal and Systems",
    "section": "Average Power (\\(P_{\\infty}\\)) for DT Signals",
    "text": "Average Power (\\(P_{\\infty}\\)) for DT Signals\nThe time-averaged power over an infinite interval for a discrete-time signal \\(x[n]\\) is:\n\\[\nP_{\\infty} \\triangleq \\lim _{N \\rightarrow \\infty} \\frac{1}{2 N+1} \\sum_{n=-N}^{+N}|x[n]|^{2}\n\\]\n\nIf \\(E_{\\infty} &lt; \\infty\\), then \\(P_{\\infty} = 0\\).\nIf \\(P_{\\infty} &gt; 0\\) and \\(P_{\\infty} &lt; \\infty\\), the signal is a finite-power signal (implying \\(E_{\\infty} = \\infty\\))."
  },
  {
    "objectID": "ss_11.html#average-power-p_infty-for-dt-signals-1",
    "href": "ss_11.html#average-power-p_infty-for-dt-signals-1",
    "title": "Signal and Systems",
    "section": "Average Power (\\(P_{\\infty}\\)) for DT Signals",
    "text": "Average Power (\\(P_{\\infty}\\)) for DT Signals\nExample: Constant Signal\nConsider \\(x[n] = 4\\).\n\n\n\n\n\n\n\nThe definition of average power for discrete-time signals is conceptually identical to continuous-time, using summation instead of integration. A crucial example is a constant signal, like \\(x[n]=4\\). While its total energy over infinite time is infinite (as you’re summing a non-zero value infinitely many times), its average power is simply the square of its magnitude – a finite, non-zero value. Similarly, discrete-time sinusoidal signals (like \\(x[n] = \\cos(\\omega n)\\)) also have infinite energy but finite average power."
  },
  {
    "objectID": "ss_11.html#signal-classification-based-on-energy-power",
    "href": "ss_11.html#signal-classification-based-on-energy-power",
    "title": "Signal and Systems",
    "section": "Signal Classification based on Energy & Power",
    "text": "Signal Classification based on Energy & Power\nSignals can be broadly classified into three categories:\n\n\n\n\n\ngraph LR\n    A[Signal x] --&gt; B{Calc. $$E_\\infty$$};\n    B --&gt; C{ $$E_\\infty \\lt \\infty$$ ?};\n    C -- Yes --&gt; D[Finite Energy Signal];\n    D --&gt; E[$$P_\\infty = 0$$];\n    C -- No --&gt; F{Calculate $$P_\\infty$$};\n    F --&gt; G{$$P_\\infty \\lt \\infty$$ ?};\n    G -- Yes --&gt; H[Finite Power Signal];\n    H --&gt; I[$$E_\\infty = \\infty$$];\n    G -- No --&gt; J[Neither Finite Energy nor Power];\n    J --&gt; K[$$E_\\infty = \\infty$$];\n    J --&gt; L[$$P_\\infty = \\infty$$];\n\n    style D fill:#ddf,stroke:#333,stroke-width:2px;\n    style H fill:#dfd,stroke:#333,stroke-width:2px;\n    style J fill:#fdd,stroke:#333,stroke-width:2px;\n\n\n\n\n\n\n\nFinite Energy Signals: \\(E_{\\infty} &lt; \\infty \\implies P_{\\infty} = 0\\). (e.g., pulses, decaying exponentials)\nFinite Power Signals: \\(P_{\\infty} &lt; \\infty\\) and \\(P_{\\infty} &gt; 0 \\implies E_{\\infty} = \\infty\\). (e.g., periodic signals, constant signals)\nNeither Finite Energy nor Finite Power: \\(E_{\\infty} = \\infty\\) AND \\(P_{\\infty} = \\infty\\). (e.g., \\(x(t)=t\\), \\(x[n]=n\\))\n\n\nThis diagram summarizes the classification of signals based on their energy and power properties. It’s a critical distinction in signal analysis. Finite energy signals are often those that are transient and die out, such as the output of an event. Finite power signals, on the other hand, are typically continuous, persistent signals like voltage from a power supply or a recurring musical note. It’s important to understand that a signal cannot simultaneously have finite non-zero energy and finite non-zero power from these definitions over an infinite interval. There are also signals that fall into neither category, growing without bound such that both energy and power are infinite."
  },
  {
    "objectID": "ss_11.html#key-takeaways",
    "href": "ss_11.html#key-takeaways",
    "title": "Signal and Systems",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nSignals convey information through patterns of variation.\nDistinguish between Continuous-Time (\\(x(t)\\)) and Discrete-Time (\\(x[n]\\)) signals.\n\nCT: Defined for a continuum of values.\nDT: Defined only at discrete points (often sampled CT signals).\n\nSignals are characterized by their Total Energy (\\(E_{\\infty}\\)) and Average Power (\\(P_{\\infty}\\)).\n\nUsed to classify signals into finite-energy, finite-power, or neither.\n\n\n\nTo wrap up this introductory session, remember these core concepts: signals are central to how we understand and interact with the world around us. Mastering the distinction between continuous and discrete time, and being able to classify signals based on their energy and power, will provide strong foundations for the more advanced topics we’ll cover, such as convolution, Fourier analysis, and system response. Next time, we’ll delve into basic operations we can perform on signals and introduce some fundamental signal types."
  },
  {
    "objectID": "ss_13.html#introduction-fundamental-building-blocks",
    "href": "ss_13.html#introduction-fundamental-building-blocks",
    "title": "Signal and Systems",
    "section": "Introduction: Fundamental Building Blocks",
    "text": "Introduction: Fundamental Building Blocks\nIn signals and systems, several basic types of signals frequently appear and serve as fundamental building blocks from which more complex signals can be constructed.\n\nImportance:\n\nRepresent a wide range of physical phenomena.\nAre crucial for analyzing the behavior of systems.\nForm the basis for powerful analytical tools (e.g., Fourier Analysis).\n\n\nWe will explore two key types:\n\nExponential Signals\nSinusoidal Signals\n\n\nThese signals are not just mathematical constructs; they have direct physical interpretations. For example, the natural response of an electrical circuit can often be described using exponentials or sinusoids. Understanding them is a prerequisite for understanding more advanced concepts like frequency response and filtering. They are to signals and systems what elementary particles are to physics – the smallest, irreducible components from which everything else is built."
  },
  {
    "objectID": "ss_13.html#continuous-time-real-exponential-signals",
    "href": "ss_13.html#continuous-time-real-exponential-signals",
    "title": "Signal and Systems",
    "section": "Continuous-Time Real Exponential Signals",
    "text": "Continuous-Time Real Exponential Signals\nThe continuous-time complex exponential signal is generally of the form \\(x(t)=C e^{a t}\\), where \\(C\\) and \\(a\\) are complex numbers.\nReal Exponential Case: \\(C, a\\) are real\n\n\n\n\n\\(a&gt;0\\)\n\n\n\n\n\\(a&lt;0\\)"
  },
  {
    "objectID": "ss_13.html#continuous-time-real-exponential-signals-1",
    "href": "ss_13.html#continuous-time-real-exponential-signals-1",
    "title": "Signal and Systems",
    "section": "Continuous-Time Real Exponential Signals",
    "text": "Continuous-Time Real Exponential Signals\n\nForm: \\(x(t)=C e^{a t}\\)\nBehavior types (Figure 1.19):\n\nIf \\(a &gt; 0\\): Growing exponential.\n\nUsed for chain reactions in atomic explosions, chemical reactions, population growth.\n\nIf \\(a &lt; 0\\): Decaying exponential.\n\nUsed for radioactive decay, responses of \\(RC\\) circuits, damped mechanical systems.\n\nIf \\(a = 0\\): Constant signal (\\(x(t)=C\\)).\n\n\n\nReal exponentials describe dynamic processes where a quantity either increases or decreases over time at a rate proportional to its current value. Think of compounding interest (\\(a&gt;0\\)) or the discharge of a capacitor (\\(a&lt;0\\)). These are fundamental to understanding the transient responses of many physical systems such as first-order RC or RL circuits."
  },
  {
    "objectID": "ss_13.html#demo-continuous-time-real-exponential",
    "href": "ss_13.html#demo-continuous-time-real-exponential",
    "title": "Signal and Systems",
    "section": "Demo: Continuous-Time Real Exponential",
    "text": "Demo: Continuous-Time Real Exponential\nExplore how changing parameters affect a real exponential signal.\n\n\n\n\n\n\n\nExperiment by changing values in the code: - a_val_default &gt; 0: Observe the exponential growth. Real-world example: Uncontrolled population growth, chain reactions. - a_val_default &lt; 0: Observe the exponential decay. Real-world example: Discharge of a capacitor in an RC circuit, radioactive decay. - a_val_default = 0: The signal becomes a constant. - C_val_default: Changes the initial amplitude and direction (if negative). Notice how quickly the signal can grow or decay with changes in a_val_default."
  },
  {
    "objectID": "ss_13.html#continuous-time-periodic-complex-exponential-sinusoidal-signals",
    "href": "ss_13.html#continuous-time-periodic-complex-exponential-sinusoidal-signals",
    "title": "Signal and Systems",
    "section": "Continuous-Time Periodic Complex Exponential & Sinusoidal Signals",
    "text": "Continuous-Time Periodic Complex Exponential & Sinusoidal Signals\nPurely Imaginary Exponential: \\(a = j \\omega_0\\)\n\nForm: \\(x(t) = e^{j \\omega_0 t}\\)\nThis signal represents a point rotating on the unit circle in the complex plane.\nPeriodicity: This signal is periodic.\n\nCondition: \\(e^{j \\omega_0 t} = e^{j \\omega_0 (t+T)} \\implies e^{j \\omega_0 T} = 1\\).\nRequires \\(\\omega_0 T\\) to be an integer multiple of \\(2\\pi\\).\nFundamental Period (\\(T_0\\)): Smallest positive \\(T\\) satisfying the condition. \\[\nT_0 = \\frac{2\\pi}{|\\omega_0|} \\quad (\\text{for } \\omega_0 \\ne 0) \\tag{1.24}\n\\]\n\\(\\omega_0\\) is the angular frequency (radians/second). \\(f_0 = \\omega_0 / (2\\pi)\\) is the ordinary frequency (Hertz, Hz)."
  },
  {
    "objectID": "ss_13.html#continuous-time-periodic-complex-exponential-sinusoidal-signals-1",
    "href": "ss_13.html#continuous-time-periodic-complex-exponential-sinusoidal-signals-1",
    "title": "Signal and Systems",
    "section": "Continuous-Time Periodic Complex Exponential & Sinusoidal Signals",
    "text": "Continuous-Time Periodic Complex Exponential & Sinusoidal Signals\nSinusoidal Signals\n\nForm: \\(x(t) = A \\cos(\\omega_0 t + \\phi)\\) (Figure 1.20)\n\n\\(A\\): Amplitude, \\(\\omega_0\\): Angular frequency, \\(\\phi\\): Phase (radians), representing a time shift.\n\nPeriodicity: Same fundamental period \\(T_0 = \\frac{2\\pi}{|\\omega_0|}\\) as the complex exponential.\nApplications: Ideal LC circuits, spring-mass systems, AC power signals.\n\n\n\nPeriodic complex exponentials are often called “rotating phasors” in the complex plane. Their real and imaginary parts are sinusoids. This connection is vital, as it allows us to analyze sinusoidal signals using the simpler algebra of complex exponentials. Stress the relationship between \\(\\omega_0\\) and \\(T_0\\): high frequency means short period, low frequency means long period. Sinusoids are the workhorses of EE, representing steady-state AC signals, waves, and oscillations in many physical systems where energy is conserved."
  },
  {
    "objectID": "ss_13.html#eulers-relation-and-signal-representation",
    "href": "ss_13.html#eulers-relation-and-signal-representation",
    "title": "Signal and Systems",
    "section": "Euler’s Relation and Signal Representation",
    "text": "Euler’s Relation and Signal Representation\nEuler’s Relation: Unites complex exponentials and sinusoids – a fundamental identity in ECE. \\[\ne^{j \\omega_0 t}=\\cos \\omega_0 t+j \\sin \\omega_0 t \\tag{1.26}\n\\]\nRepresenting Sinusoids with Exponentials: We can express a sinusoidal signal in terms of complex exponentials, making analysis easier: \\[\nA \\cos (\\omega_0 t+\\phi)=\\frac{A}{2} e^{j \\phi} e^{j \\omega_0 t}+\\frac{A}{2} e^{-j \\phi} e^{-j \\omega_0 t} \\tag{1.27}\n\\]"
  },
  {
    "objectID": "ss_13.html#eulers-relation-and-signal-representation-1",
    "href": "ss_13.html#eulers-relation-and-signal-representation-1",
    "title": "Signal and Systems",
    "section": "Euler’s Relation and Signal Representation",
    "text": "Euler’s Relation and Signal Representation\nAlternatively (using the real part of a single complex exponential): \\[\nA \\cos (\\omega_0 t+\\phi)=A \\mathcal{R} \\text{e}\\left\\{e^{j\\left(\\omega_0 t+\\phi\\right)}\\right\\} \\tag{1.28}\n\\] Similarly for sine (using the imaginary part): \\[\nA \\sin (\\omega_0 t+\\phi)=A \\mathcal{I} \\text{m}\\left\\{e^{j\\left(\\omega_0 t+\\phi\\right)}\\right\\} \\tag{1.29}\n\\]"
  },
  {
    "objectID": "ss_13.html#eulers-relation-and-signal-representation-2",
    "href": "ss_13.html#eulers-relation-and-signal-representation-2",
    "title": "Signal and Systems",
    "section": "Euler’s Relation and Signal Representation",
    "text": "Euler’s Relation and Signal Representation\nFrequency and Period Relationship (Inverse Proportionality): - Increasing \\(|\\omega_0|\\) means higher oscillation rate, smaller period \\(T_0\\). - Decreasing \\(|\\omega_0|\\) means lower oscillation rate, larger period \\(T_0\\).\n\n\n\nFigure 1.21: \\(\\omega_1 &gt; \\omega_2 &gt; \\omega_3 \\implies T_1 &lt; T_2 &lt; T_3\\).\n\nEuler’s relation is arguably the most powerful identity in ECE. It allows us to convert sinusoidal signals, which are common in real-world systems, into complex exponentials, which are mathematically much easier to manipulate (e.g., differentiation, integration, system analysis). This is a cornerstone of Fourier analysis. Explain how the magnitude and phase of the complex amplitudes in (1.27) relate to \\(A\\) and \\(\\phi\\): \\(\\frac{A}{2}e^{j\\phi}\\) is a complex number whose magnitude is \\(A/2\\) and phase is \\(\\phi\\)."
  },
  {
    "objectID": "ss_13.html#energy-and-power-of-periodic-signals",
    "href": "ss_13.html#energy-and-power-of-periodic-signals",
    "title": "Signal and Systems",
    "section": "Energy and Power of Periodic Signals",
    "text": "Energy and Power of Periodic Signals\n\nPeriodic complex exponentials and sinusoidal signals have infinite total energy but finite average power. This classifies them as power signals.\nExample: \\(x(t) = e^{j \\omega_0 t}\\)\n\nTotal energy integrated over all time is infinite, as the signal never dies out.\nHowever, the average power over one period is easily calculated: \\[\nE_{\\text{period}} = \\int_{0}^{T_0} |e^{j \\omega_0 t}|^2 dt = \\int_{0}^{T_0} 1 \\cdot dt = T_0 \\tag{1.30}\n\\]\nAverage power over one period: \\[\nP_{\\text{period}} = \\frac{1}{T_0} E_{\\text{period}} = 1 \\tag{1.31}\n\\]\nSince the signal repeats identically, the average power over all time is also: \\[\nP_x = \\lim_{T \\rightarrow \\infty} \\frac{1}{2T} \\int_{-T}^{T} |e^{j \\omega_0 t}|^2 dt = 1 \\tag{1.32}\n\\]"
  },
  {
    "objectID": "ss_13.html#energy-and-power-of-periodic-signals-1",
    "href": "ss_13.html#energy-and-power-of-periodic-signals-1",
    "title": "Signal and Systems",
    "section": "Energy and Power of Periodic Signals",
    "text": "Energy and Power of Periodic Signals\nHarmonically Related Complex Exponentials\n\nA set of periodic exponentials, all with a common period \\(T_0\\).\nTheir frequencies \\(\\omega\\) must be integer multiples of a fundamental frequency \\(\\omega_0 = 2\\pi/T_0\\). \\[\n\\phi_k(t) = e^{j k \\omega_0 t}, \\quad k=0, \\pm 1, \\pm 2, \\ldots \\tag{1.36}\n\\]\nEach \\(\\phi_k(t)\\) (for \\(k \\ne 0\\)) has a fundamental period \\(T_0/|k|\\). This means they complete \\(|k|\\) cycles within \\(T_0\\).\nThese are the “harmonics” used in music and form the basis for Fourier Series representation of periodic signals (Chapter 3).\n\n\nThe concept of infinite energy but finite average power is crucial. It differentiates “energy signals” (finite total energy) from “power signals” (infinite total energy but finite average power). Most continuous-time periodic signals are power signals. Emphasize that harmonically related exponentials are the basis for building any complex periodic signal into its fundamental frequency and its overtones, similar to how musical instruments produce and combine different frequencies to create timbre."
  },
  {
    "objectID": "ss_13.html#demo-continuous-time-sinusoidal-signal",
    "href": "ss_13.html#demo-continuous-time-sinusoidal-signal",
    "title": "Signal and Systems",
    "section": "Demo: Continuous-Time Sinusoidal Signal",
    "text": "Demo: Continuous-Time Sinusoidal Signal\nVisualize how amplitude, frequency, and phase affect a sinusoidal signal.\n\n\n\n\n\n\n\nExperiment by changing values in the code: - A_val_default: Changes the peak value of the oscillation. This is the maximum displacement from equilibrium. - omega0_val_default: Controls how rapidly the signal oscillates. Higher \\(\\omega_0\\) means higher frequency, shorter period. Try small values (e.g., 0.5) and large values (e.g., 4.0). Notice the number of cycles within the plotting range. - phi_val_default: Shifts the signal horizontally. Positive \\(\\phi\\) means a left shift (advance in time), negative \\(\\phi\\) means a right shift (delay in time). Observe how the cosine wave changes its starting point at \\(t=0\\). A phase of \\(\\pi/2\\) makes a cosine look like a negative sine."
  },
  {
    "objectID": "ss_13.html#example-1.5-sum-of-complex-exponentials",
    "href": "ss_13.html#example-1.5-sum-of-complex-exponentials",
    "title": "Signal and Systems",
    "section": "Example 1.5: Sum of Complex Exponentials",
    "text": "Example 1.5: Sum of Complex Exponentials\nIt’s useful to express the sum of two complex exponentials as a product of a single complex exponential and a sinusoid. This technique is often used in modulation.\nProblem: Plot the magnitude of the signal \\(x(t)=e^{j 2 t}+e^{j 3 t}\\).\nSolution Steps:\n\nFactor out average frequency: The average frequency of \\(2t\\) and \\(3t\\) is \\((2+3)/2 = 2.5t\\). \\[\nx(t)=e^{j 2.5 t}\\left(e^{-j 0.5 t}+e^{j 0.5 t}\\right) \\tag{1.39}\n\\]\nApply Euler’s relation: Recall \\(e^{jx} + e^{-jx} = 2 \\cos(x)\\). Here, \\(x=0.5t\\). \\[\nx(t)=2 e^{j 2.5 t} \\cos (0.5 t) \\tag{1.40}\n\\]"
  },
  {
    "objectID": "ss_13.html#example-1.5-sum-of-complex-exponentials-1",
    "href": "ss_13.html#example-1.5-sum-of-complex-exponentials-1",
    "title": "Signal and Systems",
    "section": "Example 1.5: Sum of Complex Exponentials",
    "text": "Example 1.5: Sum of Complex Exponentials\n\nFind magnitude: We use the property that \\(|z_1 z_2| = |z_1||z_2|\\). Since \\(e^{j\\theta}\\) represents a complex number on the unit circle, its magnitude is always unity (i.e., \\(|e^{j 2.5 t}| = 1\\)). \\[\n|x(t)|=2|\\cos (0.5 t)| \\tag{1.41}\n\\] This is a full-wave rectified sinusoid (Figure 1.22).\n\n\n\nThis example demonstrates a useful algebraic manipulation that simplifies analysis. The resulting expression clearly shows the envelope \\(2|\\cos(0.5t)|\\) over the underlying high-frequency oscillation (from \\(e^{j 2.5t}\\)). This concept is important in areas like amplitude modulation (AM) where a carrier signal is effectively multiplied by a message signal, producing sidebands around the carrier frequency. The ‘beating’ phenomenon in sound waves is also an example of this."
  },
  {
    "objectID": "ss_13.html#continuous-time-general-complex-exponential-signals",
    "href": "ss_13.html#continuous-time-general-complex-exponential-signals",
    "title": "Signal and Systems",
    "section": "Continuous-Time General Complex Exponential Signals",
    "text": "Continuous-Time General Complex Exponential Signals\nThe most general complex exponential combines both real exponential and periodic complex exponential characteristics.\n\nForm: \\(C e^{a t}\\) where \\(C = |C|e^{j\\theta}\\) (polar form amplitude) and \\(a = r + j\\omega_0\\) (rectangular form exponent). \\[\nC e^{a t}=|C| e^{r t} e^{j\\left(\\omega_0 t+\\theta\\right)} \\tag{1.42}\n\\]\nExpanded Form (real and imaginary parts are damped/growing sinusoids): \\[\nC e^{a t}=|C| e^{r t} \\cos \\left(\\omega_0 t+\\theta\\right)+j|C| e^{r t} \\sin \\left(\\omega_0 t+\\theta\\right) \\tag{1.43}\n\\]\n\n\\(r &gt; 0\\): Growing sinusoid (amplitude expands outward).\n\\(r &lt; 0\\): Decaying sinusoid (damped sinusoid, amplitude shrinks inward)."
  },
  {
    "objectID": "ss_13.html#continuous-time-general-complex-exponential-signals-1",
    "href": "ss_13.html#continuous-time-general-complex-exponential-signals-1",
    "title": "Signal and Systems",
    "section": "Continuous-Time General Complex Exponential Signals",
    "text": "Continuous-Time General Complex Exponential Signals\n\nEnvelope: The term \\(|C|e^{rt}\\) acts as an envelope, shaping the amplitude of the oscillations.\n\n\n\n\n\n\\(r&gt;0\\)\n\n\n\n\n\\(r&lt;0\\)\n\n\nApplications: - Damped sinusoids: Responses of RLC circuits, automotive suspension systems, and mechanical systems with both damping and restoring forces. These indicate energy dissipation with oscillations that decay in time. Modeling transient behavior in control systems.\n\nThis general form is extremely powerful for modeling real-world physical systems behavior, particularly the homogeneous solutions to linear differential equations with constant coefficients. r is the damping factor or growth rate, and \\(\\omega_0\\) is the oscillating frequency. For example, a car’s suspension system responding to a bump (\\(r&lt;0\\)) or the oscillations of a mass-spring-damper system. In electrical circuits, r relates to the resistance and \\(\\omega_0\\) to the inductance and capacitance, governing the natural modes of the system."
  },
  {
    "objectID": "ss_13.html#demo-continuous-time-dampedgrowing-sinusoid",
    "href": "ss_13.html#demo-continuous-time-dampedgrowing-sinusoid",
    "title": "Signal and Systems",
    "section": "Demo: Continuous-Time Damped/Growing Sinusoid",
    "text": "Demo: Continuous-Time Damped/Growing Sinusoid\nVisualize the real part of a general complex exponential \\(C e^{at}\\) as a damped or growing sinusoid.\n\n\n\n\n\n\n\nExperiment by changing values in the code: - r_val_default (decay/growth factor): - &lt; 0: The signal is a damped sinusoid (e.g., ringing in an RLC circuit after a disturbance). Oscillations decrease in amplitude. - = 0: The signal is a pure sinusoid (constant amplitude). - &gt; 0: The signal is a growing sinusoid. Oscillations increase in amplitude (e.g., unstable system). - C_magnitude_default (magnitude): Scales the overall initial amplitude. - omega0_val_default (angular frequency): Changes how fast the signal oscillates. - phi_val_deg_default (phase): Shifts the starting point of the oscillation. Observe how the black dashed lines (the envelope) define the boundaries of the sine wave’s peaks and troughs, visually demonstrating the \\(|C|e^{rt}\\) exponential behavior."
  },
  {
    "objectID": "ss_13.html#discrete-time-complex-exponential-sinusoidal-signals",
    "href": "ss_13.html#discrete-time-complex-exponential-sinusoidal-signals",
    "title": "Signal and Systems",
    "section": "Discrete-Time Complex Exponential & Sinusoidal Signals",
    "text": "Discrete-Time Complex Exponential & Sinusoidal Signals\nReal Exponential Signals\n\nForm: \\(x[n] = C \\alpha^n\\) (where \\(C\\) and \\(\\alpha\\) are real numbers). Sometimes expressed as \\(x[n] = C e^{\\beta n}\\), where \\(\\alpha = e^\\beta\\).\nUnlike continuous time, \\(n\\) is an integer here, so \\(\\alpha^n\\) can be interpreted directly for negative \\(\\alpha\\)."
  },
  {
    "objectID": "ss_13.html#discrete-time-complex-exponential-sinusoidal-signals-1",
    "href": "ss_13.html#discrete-time-complex-exponential-sinusoidal-signals-1",
    "title": "Signal and Systems",
    "section": "Discrete-Time Complex Exponential & Sinusoidal Signals",
    "text": "Discrete-Time Complex Exponential & Sinusoidal Signals\n\nBehavior types (Figure 1.24):\n\n\n\n\n\n\\(|\\alpha|&gt;1\\): Growing exponential (magnitudes increase).\n\n\n\n\\(0&lt;|\\alpha|&lt;1\\): Decaying exponential (magnitudes decrease).\n\n\n\n\n\\(-1 &lt; \\alpha &lt; 0\\): Decaying, but with alternating sign at each step.\n\n\n\n\\(\\alpha &lt; -1\\): Growing, with alternating sign."
  },
  {
    "objectID": "ss_13.html#discrete-time-complex-exponential-sinusoidal-signals-2",
    "href": "ss_13.html#discrete-time-complex-exponential-sinusoidal-signals-2",
    "title": "Signal and Systems",
    "section": "Discrete-Time Complex Exponential & Sinusoidal Signals",
    "text": "Discrete-Time Complex Exponential & Sinusoidal Signals\n\nSpecial Cases:\n\n\\(\\alpha=1 \\implies x[n]=C\\) (constant DC signal).\n\\(\\alpha=-1 \\implies x[n]=C(-1)^n\\) (alternates between \\(C\\) and \\(-C\\), a high-frequency square wave).\n\n\n\nThe discrete-time real exponentials are widely used in finance (compounding interest on a discrete basis) or population dynamics (growth/decay per generation). The alternating sign when \\(\\alpha\\) is negative intros oscillation at the highest possible frequency in discrete time (Nyquist frequency) for a real signal. This is due to the inherent digital nature where a signal can “flip” sign at each sample. E.g., a stock price observed daily."
  },
  {
    "objectID": "ss_13.html#demo-discrete-time-real-exponential",
    "href": "ss_13.html#demo-discrete-time-real-exponential",
    "title": "Signal and Systems",
    "section": "Demo: Discrete-Time Real Exponential",
    "text": "Demo: Discrete-Time Real Exponential\nExplore how changing C and alpha affect a discrete-time real exponential.\n\n\n\n\n\n\n\nExperiment by changing values in the code: - alpha_val_default_dt: - |alpha_val_default_dt| &gt; 1: The signal grows rapidly. - 0 &lt; |alpha_val_default_dt| &lt; 1: The signal decays. - alpha_val_default_dt &gt; 0: The sign of \\(x[n]\\) remains consistent with C_val_default_dt. This is like a smoothly changing quantity. - alpha_val_default_dt &lt; 0: The sign of \\(x[n]\\) alternates with each step in \\(n\\), creating an oscillatory appearance. This is a crucial distinction from continuous-time and represents high-frequency content. - C_val_default_dt: Scales the signal vertically. Check the behavior for special cases like \\(\\alpha = 1\\), \\(\\alpha = -1\\), and \\(\\alpha = 0.5\\), \\(\\alpha = -0.5\\)."
  },
  {
    "objectID": "ss_13.html#discrete-time-sinusoidal-signals",
    "href": "ss_13.html#discrete-time-sinusoidal-signals",
    "title": "Signal and Systems",
    "section": "Discrete-Time Sinusoidal Signals",
    "text": "Discrete-Time Sinusoidal Signals\n\nComplex Exponential Forms (where \\(\\alpha = e^{j\\omega_0}\\) or \\(\\beta = j\\omega_0\\)):\n\n\\(x[n] = e^{j \\omega_0 n}\\)\n\nSinusoidal Form:\n\n\\(x[n] = A \\cos(\\omega_0 n + \\phi)\\)\n\\(\\omega_0\\): discrete-time angular frequency (radians).\n\nEuler’s Relation (same arithmetic form as CT): \\[\ne^{j \\omega_0 n}=\\cos \\omega_0 n+j \\sin \\omega_0 n \\tag{1.48}\n\\] \\[\nA \\cos (\\omega_0 n+\\phi)=\\frac{A}{2} e^{j \\phi} e^{j \\omega_0 n}+\\frac{A}{2} e^{-j \\phi} e^{-j \\omega_0 n} \\tag{1.49}\n\\]"
  },
  {
    "objectID": "ss_13.html#discrete-time-sinusoidal-signals-1",
    "href": "ss_13.html#discrete-time-sinusoidal-signals-1",
    "title": "Signal and Systems",
    "section": "Discrete-Time Sinusoidal Signals",
    "text": "Discrete-Time Sinusoidal Signals\n\nLike continuous-time, these signals have infinite total energy but finite average power (e.g., \\(P_{\\text{avg}}=1\\) for \\(e^{j \\omega_0 n}\\)).\n\n\n\n\nFigure 1.25: Discrete-time sinusoidal signals.\n\nDiscrete-time sinusoids are prevalent in digital signal processing (DSP), such as digital audio synthesis, image processing algorithms, and digital communications systems. Their properties are slightly different from CT sinusoids, especially concerning periodicity and frequency uniqueness, which we will discuss next. Visually, they are simply samples taken from an underlying continuous-time sinusoid."
  },
  {
    "objectID": "ss_13.html#discrete-time-general-complex-exponential-signals",
    "href": "ss_13.html#discrete-time-general-complex-exponential-signals",
    "title": "Signal and Systems",
    "section": "Discrete-Time General Complex Exponential Signals",
    "text": "Discrete-Time General Complex Exponential Signals\nIf \\(C = |C|e^{j\\theta}\\) (amplitude) and \\(\\alpha = |\\alpha|e^{j\\omega_0}\\) (rate of change + frequency): \\[\nC \\alpha^n = |C| |\\alpha|^n \\cos (\\omega_0 n+\\theta)+j|C| |\\alpha|^n \\sin (\\omega_0 n+\\theta) \\tag{1.50}\n\\]\n\n\\(|\\alpha|=1\\): Purely sinusoidal samples (constant amplitude oscillations).\n\\(|\\alpha|&lt;1\\): Decaying sinusoidal samples (damped oscillations, e.g., digital filter responses).\n\\(|\\alpha|&gt;1\\): Growing sinusoidal samples (unstable system responses)."
  },
  {
    "objectID": "ss_13.html#discrete-time-general-complex-exponential-signals-1",
    "href": "ss_13.html#discrete-time-general-complex-exponential-signals-1",
    "title": "Signal and Systems",
    "section": "Discrete-Time General Complex Exponential Signals",
    "text": "Discrete-Time General Complex Exponential Signals\n\nFigure 1.26: (a) Growing discrete-time sinusoidal signals; (b) decaying discrete-time sinusoid.\n\nSimilar to continuous time, these complex exponentials serve as general solutions to discrete-time linear difference equations with constant coefficients, which are mathematical models for many discrete-time systems (e.g., digital filters). The \\(|\\alpha|\\) term determines the envelope’s growth or decay, and \\(\\omega_0\\) determines the oscillation frequency. Understanding these is key to stability analysis in digital filters and control systems."
  },
  {
    "objectID": "ss_13.html#periodicity-frequency-differences-discrete-time-vs.-continuous-time",
    "href": "ss_13.html#periodicity-frequency-differences-discrete-time-vs.-continuous-time",
    "title": "Signal and Systems",
    "section": "Periodicity & Frequency Differences (Discrete-Time vs. Continuous-Time)",
    "text": "Periodicity & Frequency Differences (Discrete-Time vs. Continuous-Time)\nA key distinction between discrete-time and continuous-time complex exponentials that impacts frequency analysis.\n\n\nContinuous-Time \\(e^{j \\omega_0 t}\\):\n\nDistinct Frequencies: All signals \\(e^{j \\omega_0 t}\\) are distinct for distinct values of \\(\\omega_0\\). An infinite number of unique frequencies.\n\n\nDiscrete-Time \\(e^{j \\omega_0 n}\\):\n\nNon-Distinct Frequencies (Aliasing): Frequencies separated by multiples of \\(2\\pi\\) are identical. \\[\ne^{j(\\omega_0+2\\pi)n} = e^{j\\omega_0 n} \\tag{1.51}\n\\]\n\nOnly need to consider \\(\\omega_0\\) in an interval of length \\(2\\pi\\) (e.g., \\([0, 2\\pi)\\) or \\((-\\pi, \\pi]\\)). All frequencies outside this interval are “aliases” of frequencies within it.\n\n\n\n\nThis slide is critical. The “folding” or “aliasing” of frequencies in discrete-time is a very important concept with practical implications, directly related to the Nyquist-Shannon sampling theorem. Explain that a high frequency like \\(\\omega_0 = 1.9\\pi\\) looks very similar to a low frequency like \\(\\omega_0 = -0.1\\pi\\) (which is effectively equivalent to \\(0.1\\pi\\) in terms of oscillation rate but reflected). The fastest oscillation is always at \\(\\omega_0 = \\pi\\). This difference arises because sampling can cause higher frequencies to appear as lower frequencies."
  },
  {
    "objectID": "ss_13.html#periodicity-frequency-differences-discrete-time-vs.-continuous-time-1",
    "href": "ss_13.html#periodicity-frequency-differences-discrete-time-vs.-continuous-time-1",
    "title": "Signal and Systems",
    "section": "Periodicity & Frequency Differences (Discrete-Time vs. Continuous-Time)",
    "text": "Periodicity & Frequency Differences (Discrete-Time vs. Continuous-Time)\nA key distinction between discrete-time and continuous-time complex exponentials that impacts frequency analysis.\n\n\nContinuous-Time \\(e^{j \\omega_0 t}\\):\n\nPeriodicity: Periodic for any value of \\(\\omega_0\\) (except \\(\\omega_0=0\\) where period is undefined but the signal is constant).\n\n\\(T_0 = 2\\pi/|\\omega_0|\\) (for \\(\\omega_0 \\ne 0\\)).\n\n\n\nDiscrete-Time \\(e^{j \\omega_0 n}\\):\n\nConditional Periodicity: Periodic only if \\(\\omega_0 / (2\\pi)\\) is a rational number.\n\n\\(\\omega_0 N = 2\\pi m \\implies \\frac{\\omega_0}{2\\pi} = \\frac{m}{N}\\) (for coprime integers \\(m, N&gt;0\\)).\nFundamental Period \\(N = m(2\\pi/\\omega_0)\\) (if \\(m,N\\) are coprime).\nIf \\(\\omega_0 / (2\\pi)\\) is irrational (e.g., \\(\\omega_0 = 1\\)), the signal is aperiodic.\n\\(T_0 = 2\\pi/|\\omega_0|\\) (for \\(\\omega_0 \\ne 0\\))."
  },
  {
    "objectID": "ss_13.html#periodicity-frequency-differences-discrete-time-vs.-continuous-time-2",
    "href": "ss_13.html#periodicity-frequency-differences-discrete-time-vs.-continuous-time-2",
    "title": "Signal and Systems",
    "section": "Periodicity & Frequency Differences (Discrete-Time vs. Continuous-Time)",
    "text": "Periodicity & Frequency Differences (Discrete-Time vs. Continuous-Time)\nA key distinction between discrete-time and continuous-time complex exponentials that impacts frequency analysis.\n\n\nContinuous-Time \\(e^{j \\omega_0 t}\\):\n\nFrequency Rate: Increasing \\(|\\omega_0|\\) always increases the rate of oscillation. No upper bound on frequency.\n\n\nDiscrete-Time \\(e^{j \\omega_0 n}\\):\n\nFrequency Rate: Rate of oscillation increases as \\(\\omega_0\\) goes from \\(0\\) to \\(\\pi\\), then decreases from \\(\\pi\\) to \\(2\\pi\\) (or \\(-\\pi\\)).\n\n\\(\\omega_0 = 0, 2\\pi, \\ldots\\) are “DC” or slowest.\n\\(\\omega_0 = \\pm \\pi, \\pm 3\\pi, \\ldots\\) are “fastest” (\\(e^{j \\pi n} = (-1)^n\\), alternating sample by sample)."
  },
  {
    "objectID": "ss_13.html#demo-discrete-time-frequency-behavior",
    "href": "ss_13.html#demo-discrete-time-frequency-behavior",
    "title": "Signal and Systems",
    "section": "Demo: Discrete-Time Frequency Behavior",
    "text": "Demo: Discrete-Time Frequency Behavior\nObserve unique frequency behavior in discrete-time sinusoids due to the sampling process.\n\n\n\n\n\n\n\nExperiment by changing omega0_val_default_dt_freq (angular frequency) in the code: - omega0_val_default_dt_freq near 0 (e.g., 0.1 or 0.2): Very slow oscillation, almost a constant (DC). - Increase omega0_val_default_dt_freq towards π (approximately 3.14): The oscillation rate increases. The samples become more spread out relative to the number of cycles within a range. - omega0_val_default_dt_freq = π: The signal becomes \\(cos(\\pi n) = (-1)^n\\), which is the fastest possible oscillation in discrete time (alternating between 1 and -1 at each sample). This is the Nyquist frequency. - Increase omega0_val_default_dt_freq from π towards 2π (approximately 6.28): The oscillation rate decreases again (it starts to “fold back”). For example, ω_0 = 1.9π will look identical to a signal with frequency -0.1π (which is effectively seen as 0.1π). - omega0_val_default_dt_freq = 2π (or 0): The signal becomes \\(cos(2\\pi n) = \\cos(0) = 1\\), which is a constant DC signal. This demonstrates how frequencies greater than \\(\\pi\\) are effectively “aliased” to lower frequencies within the fundamental range \\([0, 2\\pi)\\)."
  },
  {
    "objectID": "ss_13.html#example-1.6-fundamental-period-of-sum-of-dt-exponentials",
    "href": "ss_13.html#example-1.6-fundamental-period-of-sum-of-dt-exponentials",
    "title": "Signal and Systems",
    "section": "Example 1.6: Fundamental Period of Sum of DT Exponentials",
    "text": "Example 1.6: Fundamental Period of Sum of DT Exponentials\nProblem: Determine the fundamental period of the discrete-time signal \\(x[n]=e^{j(2 \\pi / 3) n}+e^{j(3 \\pi / 4) n}\\).\nStep-by-Step Solution:\n\nAnalyze the first term: \\(x_1[n] = e^{j(2 \\pi / 3) n}\\)\n\nFor \\(x_1[n]\\) to be periodic with period \\(N_1\\), we need \\(e^{j(2\\pi/3)(n+N_1)} = e^{j(2\\pi/3)n}\\).\nThis implies \\(e^{j(2\\pi/3)N_1} = 1\\).\nSo, \\((2\\pi/3)N_1\\) must be an integer multiple of \\(2\\pi\\).\n\\((2\\pi/3)N_1 = 2\\pi k_1 \\Rightarrow N_1/3 = k_1\\).\nThe smallest positive integer \\(N_1\\) occurs when \\(k_1=1\\), so \\(N_1=\\mathbf{3}\\)."
  },
  {
    "objectID": "ss_13.html#example-1.6-fundamental-period-of-sum-of-dt-exponentials-1",
    "href": "ss_13.html#example-1.6-fundamental-period-of-sum-of-dt-exponentials-1",
    "title": "Signal and Systems",
    "section": "Example 1.6: Fundamental Period of Sum of DT Exponentials",
    "text": "Example 1.6: Fundamental Period of Sum of DT Exponentials\n\nAnalyze the second term: \\(x_2[n] = e^{j(3 \\pi / 4) n}\\)\n\nSimilarly, we need \\(e^{j(3\\pi/4)N_2} = 1\\).\nSo, \\((3\\pi/4)N_2 = 2\\pi k_2 \\Rightarrow (3/4)N_2 = 2 k_2\\).\n\\(N_2 = (8/3) k_2\\). For \\(N_2\\) to be an integer, \\(k_2\\) must be a multiple of 3.\nThe smallest positive integer \\(N_2\\) occurs when \\(k_2=3\\), so \\(N_2 = (8/3) \\times 3 = \\mathbf{8}\\).\n\nFind the overall fundamental period: For the entire signal \\(x[n]\\) to repeat, both \\(x_1[n]\\) and \\(x_2[n]\\) must complete an integer number of their respective fundamental periods simultaneously.\n\nThis means the overall period of \\(x[n]\\) must be a common multiple of \\(N_1=3\\) and \\(N_2=8\\).\nThe fundamental period is the Least Common Multiple (LCM) of \\(N_1\\) and \\(N_2\\).\nLCM(3, 8) = \\(\\mathbf{24}\\).\n\n\nConclusion: The fundamental period of \\(x[n]\\) is \\(\\mathbf{24}\\).\n\nThis is a standard problem type when dealing with discrete-time periodic signals. Emphasize that for a sum of periodic discrete-time signals, the overall signal is periodic if and only if all individual signals are periodic. Its fundamental period is the LCM of their individual fundamental periods, assuming there isn’t a cancellation effect (e.g., if one signal is the negative of another after some shift). This understanding is crucial for designing periodic sequences in digital systems."
  },
  {
    "objectID": "ss_13.html#harmonically-related-discrete-time-complex-exponentials",
    "href": "ss_13.html#harmonically-related-discrete-time-complex-exponentials",
    "title": "Signal and Systems",
    "section": "Harmonically Related Discrete-Time Complex Exponentials",
    "text": "Harmonically Related Discrete-Time Complex Exponentials\n\nSet of periodic exponentials with a common period \\(N\\): \\[\n\\phi_k[n]=e^{j k(2 \\pi / N) n}, \\quad k=0, \\pm 1, \\pm 2, \\ldots \\tag{1.60}\n\\]\nCrucial Difference from CT (due to frequency aliasing): Unlike continuous time, these are not all distinct. \\[\n\\phi_{k+N}[n] = e^{j(k+N)(2\\pi/N)n} = e^{j k(2\\pi/N)n} e^{j 2\\pi n} = \\phi_k[n] \\tag{1.61}\n\\] Since \\(e^{j 2\\pi n} = (e^{j 2\\pi})^n = 1^n = 1\\).\nTherefore, there are only N distinct periodic exponentials in this set: \\[\n\\phi_0[n], \\phi_1[n], \\ldots, \\phi_{N-1}[n] \\tag{1.62}\n\\] Any other \\(\\phi_k[n]\\) is identical to one of these (e.g., \\(\\phi_N[n]=\\phi_0[n]\\), \\(\\phi_{-1}[n]=\\phi_{N-1}[n]\\)).\n\n\nThis property is fundamental to the Discrete Fourier Transform (DFT) and Discrete Fourier Series (DFS). It means that when analyzing discrete-time signals over a finite period \\(N\\), there are only a finite number (\\(N\\)) of unique frequencies or harmonics you can extract. This is a direct consequence of the sampling process inherent in discrete-time systems. It implies that information about frequencies beyond the Nyquist frequency is lost or “folded” down."
  },
  {
    "objectID": "ss_13.html#comparison-summary-ej-omega_0-t-vs.-ej-omega_0-n",
    "href": "ss_13.html#comparison-summary-ej-omega_0-t-vs.-ej-omega_0-n",
    "title": "Signal and Systems",
    "section": "Comparison Summary: \\(e^{j \\omega_0 t}\\) vs. \\(e^{j \\omega_0 n}\\)",
    "text": "Comparison Summary: \\(e^{j \\omega_0 t}\\) vs. \\(e^{j \\omega_0 n}\\)\n\n\n\n\n\n\n\n\nFeature\nContinuous-Time (\\(e^{j \\omega_0 t}\\))\nDiscrete-Time (\\(e^{j \\omega_0 n}\\))\n\n\n\n\nDistinctness of Freqs\nDistinct signals for distinct values of \\(\\omega_0\\). Infinite unique frequencies.\nIdentical signals for values of \\(\\omega_0\\) separated by multiples of \\(2\\pi\\). Only \\(2\\pi\\) range of unique frequencies.\n\n\nPeriodicity\nPeriodic for any choice of \\(\\omega_0\\) (except \\(\\omega_0=0\\)).\nPeriodic only if \\(\\omega_0 = 2\\pi m / N\\) for some positive integers \\(N\\) and \\(m\\).\n\n\nFundamental Frequency\n\\(\\omega_0\\) (for \\(\\omega_0 \\ne 0\\))\n\\(\\omega_0 / m\\) (for \\(\\omega_0 \\ne 0\\), assuming \\(m,N\\) are coprime).\n\n\nFundamental Period\n\\(\\omega_0=0\\): undefined\\(\\omega_0 \\ne 0: \\frac{2\\pi}{|\\omega_0|}\\)\n\\(\\omega_0=0\\): undefined\\(\\omega_0 \\ne 0: N = m\\left(\\frac{2\\pi}{\\omega_0}\\right)\\) (if \\(m,N\\) coprime)\n\n\nHighest Oscillation Rate\nAs \\(\\omega_0 \\rightarrow \\pm \\infty\\)\nAt \\(\\omega_0 = \\pi\\) (or odd multiples of \\(\\pi\\))\n\n\nLowest Oscillation Rate\nAt \\(\\omega_0 = 0\\)\nAt \\(\\omega_0 = 0\\) (or even multiples of \\(2\\pi\\))\n\n\n\n\nReview this table thoroughly. It synthesizes the most important differences and highlights why Discrete-Time Signal Processing requires careful consideration of frequency ranges and periodicity, topics that are less complex in continuous time. This table is essentially a cheat sheet for avoiding common misconceptions when transitioning from continuous to discrete domain, especially concerning frequency."
  },
  {
    "objectID": "ss_13.html#conclusion",
    "href": "ss_13.html#conclusion",
    "title": "Signal and Systems",
    "section": "Conclusion",
    "text": "Conclusion\nWe’ve introduced fundamental continuous-time and discrete-time signals:\n\nExponential Signals:\n\nReal exponentials: excellent models for growth or decay phenomena.\nComplex exponentials: fundamental “building blocks” related to rotation, basis for all sinusoids.\nGeneral complex exponentials: model damped or growing oscillatory responses, common in LTI systems.\n\nSinusoidal Signals:\n\nRepresent pure, stable oscillations, deeply connected to complex exponentials via Euler’s relation.\nUbiquitous in nature and engineering (AC circuits, mechanical vibrations, wave propagation)."
  },
  {
    "objectID": "ss_13.html#conclusion-1",
    "href": "ss_13.html#conclusion-1",
    "title": "Signal and Systems",
    "section": "Conclusion",
    "text": "Conclusion\nKey Takeaways on Differences:\n\nContinuous-time signals have distinct frequencies across the entire real number line.\nDiscrete-time signals have frequencies that repeat every \\(2\\pi\\) (aliasing).\nDiscrete-time signals are periodic only if their frequency is a rational multiple of \\(2\\pi\\).\n\nThese simple signals are profound building blocks for understanding linearity, time-invariance, and the powerful frequency domain analysis of signals and systems, which we will explore in subsequent chapters.\n\nReiterate the “building blocks” idea and how these signals will appear repeatedly throughout the course, especially when analyzing systems and using Fourier techniques. Emphasize the unique characteristics of discrete-time signals as they will be central in DSP courses. Understanding these basic signals at an intuitive level is paramount for mastering the rest of the Signals and Systems curriculum."
  },
  {
    "objectID": "ss_15.html#what-is-a-system",
    "href": "ss_15.html#what-is-a-system",
    "title": "Signal and Systems",
    "section": "What is a System?",
    "text": "What is a System?\nA system is a process by which input signals are transformed to produce output signals.\n\n\nConceptual Model:\n\n\n\n\n\ngraph LR\n    A[Input Signal] --&gt; B(System);\n    B --&gt; C[Output Signal];\n\n\n\n\n\n\nExamples:\n\nHi-Fi System: Raw audio \\(\\rightarrow\\) Amplified & Equalized sound\nRC Circuit: Input voltage \\(\\rightarrow\\) Capacitor voltage\nAutomobile: Force applied \\(\\rightarrow\\) Vehicle velocity\nImage Enhancement: Raw image \\(\\rightarrow\\) Improved contrast image\n\n\nKey Idea:\nSystems provide a mathematical framework to model how physical phenomena respond to external stimuli.\nThey allow us to predict, control, and design complex engineering applications.\n\n\nIn the broadest sense, a system is simply anything that takes an input and produces an output. Think of it as a black box where something goes in, is processed, and something else comes out. In electrical and computer engineering, these inputs and outputs are typically signals, and the “processing” is defined by mathematical relationships. We analyze these relationships to understand how a system behaves."
  },
  {
    "objectID": "ss_15.html#systems-examples",
    "href": "ss_15.html#systems-examples",
    "title": "Signal and Systems",
    "section": "Systems (Examples)",
    "text": "Systems (Examples)"
  },
  {
    "objectID": "ss_15.html#systems-examples-1",
    "href": "ss_15.html#systems-examples-1",
    "title": "Signal and Systems",
    "section": "Systems (Examples)",
    "text": "Systems (Examples)"
  },
  {
    "objectID": "ss_15.html#continuous-time-ct-vs.-discrete-time-dt-systems",
    "href": "ss_15.html#continuous-time-ct-vs.-discrete-time-dt-systems",
    "title": "Signal and Systems",
    "section": "Continuous-Time (CT) vs. Discrete-Time (DT) Systems",
    "text": "Continuous-Time (CT) vs. Discrete-Time (DT) Systems\nSystems are classified based on the nature of their input and output signals.\n\n\nContinuous-Time Systems\n\nInput \\(x(t)\\) and output \\(y(t)\\) are continuous functions of time.\nRepresented by: \\(x(t) \\rightarrow y(t)\\)\n\nReal-world examples:\n\nAnalog filters\nMechanical systems\nElectrical circuits (e.g., op-amp circuits)\n\n\nDiscrete-Time Systems\n\nInput \\(x[n]\\) and output \\(y[n]\\) are sequences (defined at discrete instants).\nRepresented by: \\(x[n] \\rightarrow y[n]\\)\n\nReal-world examples:\n\nDigital signal processors (DSPs)\nComputer algorithms\nFinancial modeling (e.g., monthly balances)\n\n\n\nThe fundamental distinction lies in how time is handled. Continuous-time systems operate on signals that are defined for all values of time, much like a continuous waveform. Discrete-time systems, on the other hand, operate on signals that are sampled or defined only at specific, discrete points in time. We will study both types in parallel throughout this course, but in Chapter 7, we’ll see how they are connected through the concept of sampling. Many real-world systems, especially in modern engineering, involve both, for example, processing an analog signal using a digital computer."
  },
  {
    "objectID": "ss_15.html#simple-examples-of-systems-continuous-time-12",
    "href": "ss_15.html#simple-examples-of-systems-continuous-time-12",
    "title": "Signal and Systems",
    "section": "Simple Examples of Systems: Continuous-Time (1/2)",
    "text": "Simple Examples of Systems: Continuous-Time (1/2)\nMany diverse physical systems can share the same mathematical description.\nExample 1.8: RC Circuit Voltage\nSystem: An RC circuit where \\(v_s(t)\\) is the input voltage and \\(v_c(t)\\) is the output voltage across the capacitor.\nInput-Output Relationship (Differential Equation): \\[\n\\frac{d v_{c}(t)}{d t}+\\frac{1}{R C} v_{c}(t)=\\frac{1}{R C} v_{s}(t) \\quad \\text{(Equation 1.82)}\n\\]\n\nLet’s look at some basic examples. Consider a simple RC circuit. The input is the source voltage vs(t) and the output is the capacitor voltage vc(t). Using Kirchhoff’s laws and the constitutive relations for resistors and capacitors, we can derive a first-order differential equation that describes how the output voltage vc(t) changes in response to the input voltage vs(t). This equation captures the dynamics of the circuit."
  },
  {
    "objectID": "ss_15.html#simple-examples-of-systems-continuous-time-22",
    "href": "ss_15.html#simple-examples-of-systems-continuous-time-22",
    "title": "Signal and Systems",
    "section": "Simple Examples of Systems: Continuous-Time (2/2)",
    "text": "Simple Examples of Systems: Continuous-Time (2/2)\nExample 1.9: Automobile Velocity\nSystem: An automobile where \\(f(t)\\) is the input force and \\(v(t)\\) is the output velocity. (Assumes mass \\(m\\) and friction \\(\\rho v\\)).\nInput-Output Relationship (Differential Equation): \\[\n\\frac{d v(t)}{d t}+\\frac{\\rho}{m} v(t)=\\frac{1}{m} f(t) \\quad \\text{(Equation 1.84)}\n\\]"
  },
  {
    "objectID": "ss_15.html#simple-examples-of-systems-continuous-time-22-1",
    "href": "ss_15.html#simple-examples-of-systems-continuous-time-22-1",
    "title": "Signal and Systems",
    "section": "Simple Examples of Systems: Continuous-Time (2/2)",
    "text": "Simple Examples of Systems: Continuous-Time (2/2)\nComparison: Both Example 1.8 (RC circuit) and 1.9 (automobile) are described by the same general first-order linear differential equation:\n\\[\n\\frac{d y(t)}{d t}+a y(t)=b x(t) \\quad \\text{(Equation 1.85)}\n\\]\n\n\\(y(t)\\): output signal, \\(x(t)\\): input signal.\n\\(a\\), \\(b\\): constants derived from system parameters.\n\n\nNow, let’s consider a completely different physical system: an automobile. If we model the applied force f(t) as the input and the vehicle’s velocity v(t) as the output, and account for mass and a linear approximation for frictional resistance, Newton’s second law leads us to another first-order differential equation.\nWhat’s striking is that this equation has the exact same mathematical form as the RC circuit equation. This demonstrates a powerful concept in systems analysis: seemingly different physical systems can be described by the same mathematical models. Developing tools to analyze a general class of systems, like first-order linear differential equations, allows us to apply those tools across a wide variety of engineering disciplines."
  },
  {
    "objectID": "ss_15.html#simple-examples-of-systems-discrete-time-12",
    "href": "ss_15.html#simple-examples-of-systems-discrete-time-12",
    "title": "Signal and Systems",
    "section": "Simple Examples of Systems: Discrete-Time (1/2)",
    "text": "Simple Examples of Systems: Discrete-Time (1/2)\nExample 1.10: Bank Account Balance\nSystem: A bank account where \\(x[n]\\) is the net deposit in month \\(n\\), and \\(y[n]\\) is the balance at the end of month \\(n\\). (Assumes 1% interest per month).\nInput-Output Relationship (Difference Equation): \\[\ny[n] = 1.01 y[n-1] + x[n] \\quad \\text{(Equation 1.86)}\n\\] \\[\ny[n] - 1.01 y[n-1] = x[n] \\quad \\text{(Equation 1.87)}\n\\]"
  },
  {
    "objectID": "ss_15.html#simple-examples-of-systems-discrete-time-12-1",
    "href": "ss_15.html#simple-examples-of-systems-discrete-time-12-1",
    "title": "Signal and Systems",
    "section": "Simple Examples of Systems: Discrete-Time (1/2)",
    "text": "Simple Examples of Systems: Discrete-Time (1/2)\nExample 1.10: Bank Account Balance\nInteractive Simulation: Assume initial balance \\(y[-1]=1000\\) and a monthly deposit \\(x[n]=100\\) for \\(n \\ge 0\\). The system calculates balance \\(y[n]\\) based on previous month’s balance and current deposit.\n\n\n\n\n\n\n\nDiscrete-time systems are modeled using difference equations. A classic example is tracking a bank account balance. Here, the current month’s balance, y[n], depends on the previous month’s balance, y[n-1], augmented by interest, and the current month’s net deposit, x[n]. This difference equation captures the temporal evolution of the balance. The interactive plot shows how the balance grows over time with regular deposits and compounding interest."
  },
  {
    "objectID": "ss_15.html#simple-examples-of-systems-discrete-time-22",
    "href": "ss_15.html#simple-examples-of-systems-discrete-time-22",
    "title": "Signal and Systems",
    "section": "Simple Examples of Systems: Discrete-Time (2/2)",
    "text": "Simple Examples of Systems: Discrete-Time (2/2)\nExample 1.11: Digital Simulation of Automobile Model\nSystem: A discrete-time approximation of the continuous-time automobile model from Example 1.9. (Approximates \\(\\frac{dv(t)}{dt}\\) with backward difference \\(\\frac{v(n\\Delta)-v((n-1)\\Delta)}{\\Delta}\\)).\nInput-Output Relationship (Difference Equation): \\[\nv[n]-\\frac{m}{(m+\\rho \\Delta)} v[n-1]=\\frac{\\Delta}{(m+\\rho \\Delta)} f[n] \\quad \\text{(Equation 1.88)}\n\\]\n\n\\(v[n]\\): sampled velocity, \\(f[n]\\): sampled force.\n\\(\\Delta\\): time step."
  },
  {
    "objectID": "ss_15.html#simple-examples-of-systems-discrete-time-22-1",
    "href": "ss_15.html#simple-examples-of-systems-discrete-time-22-1",
    "title": "Signal and Systems",
    "section": "Simple Examples of Systems: Discrete-Time (2/2)",
    "text": "Simple Examples of Systems: Discrete-Time (2/2)\nComparison: Both Example 1.10 (bank account) and 1.11 (digital simulation) are described by the same general first-order linear difference equation:\n\\[\ny[n]+a y[n-1]=b x[n] \\quad \\text{(Equation 1.89)}\n\\]\n\n\\(y[n]\\): output sequence, \\(x[n]\\): input sequence.\n\\(a\\), \\(b\\): constants derived from system parameters or approximation schemes.\n\n\nJust as with continuous-time systems, discrete-time systems from different applications can share the same mathematical form. Here, we see a digital simulation of the automobile model, where the continuous-time derivative is approximated by a discrete-time difference. This gives us a first-order linear difference equation. Comparing it to the bank account example, they both fit the same general form. This highlights the power of abstraction in signals and systems: by understanding general forms of equations, we can analyze countless real-world systems, whether they are inherently discrete or are continuous systems approximated for digital processing."
  },
  {
    "objectID": "ss_15.html#interconnections-of-systems",
    "href": "ss_15.html#interconnections-of-systems",
    "title": "Signal and Systems",
    "section": "1.5.2 Interconnections of Systems",
    "text": "1.5.2 Interconnections of Systems\nComplex systems are often built by interconnecting simpler subsystems.\n\nBenefit: Analyze complex systems by understanding their components and their connections.\nApplication: Design and synthesize new systems from basic building blocks."
  },
  {
    "objectID": "ss_15.html#interconnections-of-systems-1",
    "href": "ss_15.html#interconnections-of-systems-1",
    "title": "Signal and Systems",
    "section": "1.5.2 Interconnections of Systems",
    "text": "1.5.2 Interconnections of Systems\n\n\nReal-World Application: Audio System\n\n\n\n\n\ngraph TD\n    A[Radio Receiver] --&gt; B(Amplifier);\n    B --&gt; C[Speakers];\n\n\n\n\n\n\nAn audio system cascaded for playback.\n\nReal-World Application: Digitally Controlled Aircraft\n\n\n\n\n\ngraph TD\n    Pilot[Pilot Commands] --&gt; Autopilot(Digital Autopilot);\n    Sensors --&gt; Autopilot;\n    Autopilot --&gt; Actuators(Aircraft Actuators);\n    Actuators --&gt; Aircraft(Aircraft);\n    Aircraft --&gt; Sensors[\"Sensors (Velocity)\"];\n    Aircraft --&gt; PilotsDisplay(Pilot Display);\n    Autopilot --Desired--&gt; Aircraft;\n    Aircraft --Actual--&gt; Autopilot;\n\n    style Autopilot fill:#f9f\n    style Aircraft fill:#cf9\n    style Sensors fill:#9fc\n    style Actuators fill:#c9f\n\n\n\n\n\n\nA feedback system structure.\n\n\nMost real-world engineering systems aren’t just one simple block. They are complex structures made by connecting several simpler components. Understanding how systems are interconnected is crucial for both analysis and design. By breaking down a large system into its component parts, we can apply our knowledge of the simpler subsystems and understand the overall behavior. This modular approach is fundamental in system engineering."
  },
  {
    "objectID": "ss_15.html#basic-interconnections-series-cascade",
    "href": "ss_15.html#basic-interconnections-series-cascade",
    "title": "Signal and Systems",
    "section": "Basic Interconnections: Series (Cascade)",
    "text": "Basic Interconnections: Series (Cascade)\nIn a series or cascade interconnection, the output of one system becomes the input to the next.\n\n\n\n\n\ngraph LR\n    input(Input) --&gt; Sys1(System 1);\n    Sys1 --&gt; Sys2(System 2);\n    Sys2 --&gt; output(Output);\n\n    subgraph Overall System\n        Sys1\n        Sys2\n    end\n\n\n\n\n\n\n\nDescription: Input signal is processed by System 1, then its output is processed by System 2.\nExample: A radio receiver (System 1) connected to an amplifier (System 2).\nNotation: If System 1 output = \\(y_1\\) and System 2 output = \\(y_2\\), then \\(y_1 = \\text{Sys1}(\\text{input})\\) and \\(y_2 = \\text{Sys2}(y_1)\\).\n\n\nThe series or cascade interconnection is the most straightforward. Imagine a chain: the output of the first link feeds into the second, and so on. In block diagrams, this is represented by arrows connecting blocks sequentially. An example is a signal flowing from your phone to an amplifier, then to speakers. Each component performs a specific function in sequence."
  },
  {
    "objectID": "ss_15.html#basic-interconnections-parallel",
    "href": "ss_15.html#basic-interconnections-parallel",
    "title": "Signal and Systems",
    "section": "Basic Interconnections: Parallel",
    "text": "Basic Interconnections: Parallel\nIn a parallel interconnection, the same input signal is applied to multiple systems, and their outputs are combined (typically summed).\n\n\n\n\n\ngraph LR\n    input(Input) --&gt; Sys1(System 1);\n    input --&gt; Sys2(System 2);\n    Sys1 --&gt; Sum;\n    Sys2 --&gt; Sum;\n    Sum(($\\oplus$)) --&gt; output(Output);\n\n    subgraph Overall System\n        Sys1\n        Sys2\n        Sum\n    end\n\n\n\n\n\n\n\nDescription: Input simultaneously feeds System 1 and System 2. Their individual outputs are added together to form the overall output.\nExample: Multiple microphones (inputs) feeding into a mixing console (summing outputs) then to a single amplifier.\n\n\nIn a parallel interconnection, the input signal is duplicated and sent to two or more systems simultaneously. Their individual outputs are then combined, usually by addition, to form the final output. Think of it like a sound system with multiple microphones all feeding into a single mixer. Each microphone is a “system” producing an output, and the mixer sums these outputs."
  },
  {
    "objectID": "ss_15.html#basic-interconnections-feedback",
    "href": "ss_15.html#basic-interconnections-feedback",
    "title": "Signal and Systems",
    "section": "Basic Interconnections: Feedback",
    "text": "Basic Interconnections: Feedback\nIn a feedback interconnection, the output of a system (or subsystem) is fed back and used to influence its own input.\n\n\n\n\n\ngraph LR\n    input(External Input) --&gt; Summer;\n    System2_out(Output of S2) --&gt; Summer;\n    Summer(($\\oplus$)) --&gt; System1(System 1);\n    System1 --&gt; System2(System 2);\n    System2 --&gt; output(Output);\n    System2_out -- Feedback --&gt; Summer;\n\n    subgraph Overall System\n        Summer\n        System1\n        System2\n    end\n\n\n\n\n\n\n\nDescription: The output of System 2 is added (or subtracted) from the external input, which then becomes the effective input to System 1.\nExamples:\n\nCruise Control: Senses vehicle speed, adjusts engine to maintain desired speed.\nThermostat: Senses room temperature, turns heating/cooling on/off to reach set temperature.\nOperational Amplifiers: Many op-amp circuits use feedback for stability and gain control.\n\n\n\nFeedback is arguably the most powerful and common type of system interconnection. In a feedback system, part of the output is “fed back” to the input, influencing the system’s future behavior. This creates a closed loop. A common household example is a thermostat. It senses the room temperature (output), compares it to the desired temperature (input), and then adjusts the heating or cooling (actuator) to correct any difference. Feedback systems are essential for control, stability, and achieving precise performance in dynamic environments."
  },
  {
    "objectID": "ss_15.html#key-takeaways",
    "href": "ss_15.html#key-takeaways",
    "title": "Signal and Systems",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nSystem Definition\n\nTransforms input signals into output signals.\nCan be continuous-time (\\(x(t) \\rightarrow y(t)\\)) or discrete-time (\\(x[n] \\rightarrow y[n]\\)).\n\nMathematical Models\n\nContinuous-time systems often described by differential equations.\nDiscrete-time systems often described by difference equations.\nMany diverse physical systems share the same mathematical forms (e.g., first-order linear differential/difference equations)."
  },
  {
    "objectID": "ss_15.html#key-takeaways-1",
    "href": "ss_15.html#key-takeaways-1",
    "title": "Signal and Systems",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nSystem Interconnections\n\nComplex systems can be understood and built from simpler subsystems.\nSeries (Cascade): Output of one system feeds the input of another.\nParallel: Same input to multiple systems; outputs are combined.\nFeedback: Output of a system/subsystem is routed back to influence its input, crucial for control and stability.\n\nUnderstanding these concepts is foundational for further study in Signals and Systems.\n\nTo wrap up our discussion on systems: Remember that a system is a fundamental concept where an input signal is transformed into an output signal. We differentiate between continuous-time systems, described by differential equations, and discrete-time systems, described by difference equations. A crucial takeaway is that the mathematical description of a system is often consistent across very different physical phenomena, providing a powerful framework for analysis. Finally, understanding how systems are interconnected—through series, parallel, or feedback arrangements—is key to analyzing complex real-world engineering systems and designing sophisticated solutions. These foundational concepts will be built upon throughout the course as we delve deeper into system properties and analysis techniques."
  },
  {
    "objectID": "ss_21.html#discrete-time-lti-systems-the-convolution-sum",
    "href": "ss_21.html#discrete-time-lti-systems-the-convolution-sum",
    "title": "Signal and Systems",
    "section": "DISCRETE-TIME LTI SYSTEMS: THE CONVOLUTION SUM",
    "text": "DISCRETE-TIME LTI SYSTEMS: THE CONVOLUTION SUM\nECE Undergraduate Course\nImron Rosyadi\n\nWelcome everyone. Today, we’re diving into one of the most fundamental concepts in Signals and Systems: The Convolution Sum. This operation is the key to understanding how Linear Time-Invariant, or LTI, systems work. By the end of this lecture, you’ll be able to represent any discrete-time signal in a new way and use that representation to find the output of any LTI system for any given input."
  },
  {
    "objectID": "ss_21.html#moment-of-silence",
    "href": "ss_21.html#moment-of-silence",
    "title": "Signal and Systems",
    "section": "Moment of Silence",
    "text": "Moment of Silence"
  },
  {
    "objectID": "ss_21.html#what-is-a-system",
    "href": "ss_21.html#what-is-a-system",
    "title": "Signal and Systems",
    "section": "What is a System?",
    "text": "What is a System?\nA system is a process by which input signals are transformed to produce output signals.\n\n\n\n\n\ngraph LR\n    A[Input Signal] --&gt; B(System);\n    B --&gt; C[Output Signal];"
  },
  {
    "objectID": "ss_21.html#signal-decomposition-with-impulses",
    "href": "ss_21.html#signal-decomposition-with-impulses",
    "title": "Signal and Systems",
    "section": "Signal Decomposition with Impulses",
    "text": "Signal Decomposition with Impulses\nAny discrete-time signal \\(x[n]\\) can be represented as a sum of scaled and shifted unit impulses.\nThink of it as breaking down a signal into its most basic building blocks.\nThe “Sifting Property”\nThis decomposition is also known as the sifting property of the unit impulse:\n\\[\nx[n] = \\sum_{k=-\\infty}^{\\infty} x[k] \\delta[n-k]\n\\]\nFor any given \\(n\\), the summation “sifts” through all values of \\(x[k]\\) and picks out only the one where \\(k=n\\)."
  },
  {
    "objectID": "ss_21.html#signal-decomposition-with-impulses-1",
    "href": "ss_21.html#signal-decomposition-with-impulses-1",
    "title": "Signal and Systems",
    "section": "Signal Decomposition with Impulses",
    "text": "Signal Decomposition with Impulses\n\n\n\n\n\ngraph TD\n    subgraph Decomposing x[n]\n        XN(\"x[n]\") --&gt; IMP1(\"x[-1]δ[n+1]\")\n        XN --&gt; IMP0(\"x[0]δ[n]\")\n        XN --&gt; IMP2(\"x[1]δ[n-1]\")\n        XN --&gt; ETC(...)\n    end\n    subgraph Reconstructing x[n]\n        IMP1 --&gt; SUM(\"(&Sigma;)\")\n        IMP0 --&gt; SUM\n        IMP2 --&gt; SUM\n        ETC --&gt; SUM\n        SUM --&gt; XN_OUT(\"x[n]\")\n    end\n\n\n\n\n\n\n\nThe core idea here is incredibly powerful. We take a complex signal, \\(x[n]\\), and express it as a linear combination of the simplest possible signal: the unit impulse, \\(\\delta[n]\\). Each term in the sum, \\(x[k]\\delta[n-k]\\), isolates a single point from the original signal. The weight of each impulse is simply the value of the signal at that point in time. This representation is the foundation for everything that follows."
  },
  {
    "objectID": "ss_21.html#visualizing-decomposition",
    "href": "ss_21.html#visualizing-decomposition",
    "title": "Signal and Systems",
    "section": "Visualizing Decomposition",
    "text": "Visualizing Decomposition\nLet’s see this in action. The signal \\(x[n]\\) is built by summing its individual impulse components.\n\n\n\n\n\n\n\nHere you can see the process graphically, just like in Figure 2.1 from the textbook. The top-left plot shows our original signal, \\(x[n]\\). The next four plots each show a single, scaled impulse. For example, the plot titled \\(x[0]\\delta[n-0]\\) is a signal that’s zero everywhere except at \\(n=0\\), where its value is \\(x[0]=2.0\\). When we add all these simple impulse components together, as shown in the bottom-right plot, we perfectly reconstruct the original signal."
  },
  {
    "objectID": "ss_21.html#lti-systems-the-convolution-sum",
    "href": "ss_21.html#lti-systems-the-convolution-sum",
    "title": "Signal and Systems",
    "section": "LTI Systems & The Convolution Sum",
    "text": "LTI Systems & The Convolution Sum\nHow does an LTI system respond to an input \\(x[n]\\)?\n\nLinearity: The response to a sum of inputs is the sum of the individual responses.\n\nInput: \\(x[n] = \\sum_k x[k] \\delta[n-k]\\)\nOutput: \\(y[n] = \\sum_k x[k] \\cdot \\{\\text{Response to } \\delta[n-k]\\}\\)\n\nTime-Invariance: A shift in the input causes the same shift in the output.\n\nResponse to \\(\\delta[n]\\) is the impulse response, \\(h[n]\\).\nResponse to \\(\\delta[n-k]\\) is just a shifted impulse response, \\(h[n-k]\\).\n\n\nCombining these gives the Convolution Sum:\n\\[\ny[n] = \\sum_{k=-\\infty}^{\\infty} x[k] h[n-k]\n\\]\nWe denote this operation with an asterisk: \\(y[n] = x[n] * h[n]\\).\n\nThis is the central result. Because the system is linear, we can find the output by summing the responses to all the simple impulse components of the input. And because the system is time-invariant, the response to a shifted impulse \\(\\delta[n-k]\\) is just a shifted version of the response to a non-shifted impulse \\(\\delta[n]\\). We call the response to \\(\\delta[n]\\) the “impulse response” and denote it \\(h[n]\\). By putting these two properties together, we arrive at the convolution sum. It tells us that the output \\(y[n]\\) is a weighted sum of shifted versions of the impulse response. The weights are the values of the input signal, \\(x[k]\\)."
  },
  {
    "objectID": "ss_21.html#the-flip-and-slide-method",
    "href": "ss_21.html#the-flip-and-slide-method",
    "title": "Signal and Systems",
    "section": "The “Flip-and-Slide” Method",
    "text": "The “Flip-and-Slide” Method\nThe convolution sum \\(y[n]=\\sum_{k=-\\infty}^{\\infty} x[k] h[n-k]\\) can be computed graphically for each output sample n.\nProcedure for a fixed n:\n\nPlot signals vs. k: Plot the input \\(x[k]\\) and the impulse response \\(h[k]\\).\nFlip: Time-reverse \\(h[k]\\) to get \\(h[-k]\\).\nSlide: Shift \\(h[-k]\\) by \\(n\\) to get \\(h[n-k]\\).\n\nShift right for \\(n &gt; 0\\); left for \\(n &lt; 0\\).\n\nMultiply: Point-wise multiply the sequences \\(x[k]\\) and \\(h[n-k]\\).\nSum: Sum all the values of the product sequence. The result is \\(y[n]\\).\n\nRepeat for all values of n to find the entire output signal \\(y[n]\\).\n\nWhile the formula looks abstract, there’s a very mechanical, graphical way to compute it, which we call the “flip-and-slide” method. The key is to think of n as a fixed value for now. We are trying to compute a single output point, y[n]. The expression involves a sum over the dummy variable k. So, we plot both signals, x and h, as functions of k. The tricky part is the term \\(h[n-k]\\). As a function of k, it’s a flipped and shifted version of the original impulse response. Once you have \\(x[k]\\) and \\(h[n-k]\\) plotted, you just multiply them point by point and add up all the results. That sum gives you the single value \\(y[n]\\). Then you change n and do it all over again."
  },
  {
    "objectID": "ss_21.html#interactive-demo-flip-and-slide",
    "href": "ss_21.html#interactive-demo-flip-and-slide",
    "title": "Signal and Systems",
    "section": "Interactive Demo: Flip-and-Slide",
    "text": "Interactive Demo: Flip-and-Slide\n\n\\(x[n] = 0.5\\delta[n] + 2\\delta[n-1]\\)\n\\(h[n] = u[n] - u[n-3]\\)\n\nUse the slider to change the value of n and observe the convolution process.\n\nviewof n = Inputs.range([-4, 12], {value: 2, step: 1, label: \"Time shift (n)\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive demo brings the flip-and-slide method to life. The top plot shows the stationary input signal \\(x[k]\\) in blue and the flipped and shifted impulse response, \\(h[n-k]\\), in red. As you move the slider for n, you can see the red signal slide. The middle plot shows the point-wise product. The bottom plot shows the final output signal \\(y[n]\\). The cyan triangle shows the value of \\(y[n]\\) for the current n, which is calculated by summing the green stems in the middle plot."
  },
  {
    "objectID": "ss_21.html#example-accumulator-system",
    "href": "ss_21.html#example-accumulator-system",
    "title": "Signal and Systems",
    "section": "Example: Accumulator System",
    "text": "Example: Accumulator System\nLet’s convolve an exponential signal with a unit step. This models a system called an accumulator.\nProblem\n\nInput: \\(x[n] = \\alpha^n u[n]\\), for \\(0 &lt; \\alpha &lt; 1\\).\nImpulse Response: \\(h[n] = u[n]\\).\n\nAnalysis\n\nFor \\(n &lt; 0\\), there’s no overlap between \\(x[k]\\) and \\(h[n-k]\\). So, \\(y[n] = 0\\).\nFor \\(n \\ge 0\\), the overlap is for \\(0 \\le k \\le n\\). \\[\ny[n] = \\sum_{k=0}^{n} \\alpha^k = \\frac{1 - \\alpha^{n+1}}{1 - \\alpha}\n\\] Result: \\(y[n] = \\left(\\frac{1 - \\alpha^{n+1}}{1 - \\alpha}\\right) u[n]\\)."
  },
  {
    "objectID": "ss_21.html#example-accumulator-system-1",
    "href": "ss_21.html#example-accumulator-system-1",
    "title": "Signal and Systems",
    "section": "Example: Accumulator System",
    "text": "Example: Accumulator System\n\n\n\n\n\n\n\nThis is a classic example. The impulse response, \\(h[n]=u[n]\\), defines an accumulator. We feed in a decaying exponential signal. As seen in the derivation, the output is zero for \\(n&lt;0\\) because there is no overlap. For \\(n \\ge 0\\), the sum is a finite geometric series. The plot on the right shows the output starting at \\(y[0]=1\\) and rising to a final value of \\(1/(1-\\alpha)\\)."
  },
  {
    "objectID": "ss_21.html#example-convolving-two-pulses",
    "href": "ss_21.html#example-convolving-two-pulses",
    "title": "Signal and Systems",
    "section": "Example: Convolving Two Pulses",
    "text": "Example: Convolving Two Pulses\nLet’s convolve two finite-length pulses. The output shape and length depend on the inputs.\n\n\\(x[n] = 1\\) for \\(0 \\le n \\le 4\\).\n\\(h[n] = \\alpha^n\\) for \\(0 \\le n \\le 6\\) (with \\(\\alpha &gt; 1\\)).\n\nThe convolution \\(y[n]\\) is non-zero for \\(0 \\le n \\le 10\\), with a trapezoidal shape due to changing overlap.\n\n\n\n\n\n\n\nHere we convolve two pulses of finite length. The key to solving this analytically is to break it down into five regions for n based on the overlap: no overlap, partial increasing overlap, full overlap, partial decreasing overlap, and no overlap again. The result has a trapezoidal shape and its length is \\(N_x + N_h - 1 = 5 + 7 - 1 = 11\\) samples. This length property is general for the convolution of two finite-length sequences."
  },
  {
    "objectID": "ss_21.html#application-digital-audio-reverb",
    "href": "ss_21.html#application-digital-audio-reverb",
    "title": "Signal and Systems",
    "section": "Application: Digital Audio Reverb",
    "text": "Application: Digital Audio Reverb\nConvolution is used in audio engineering to create effects like reverberation (reverb).\n\nInput Signal \\(x[n]\\): A “dry” audio signal (e.g., a single clap).\nImpulse Response \\(h[n]\\): The “room response.” This is what you would record if you made a perfect impulse (like a starter pistol shot) in a concert hall. It captures all the echoes.\nOutput Signal \\(y[n]\\): The “wet” audio signal, with reverb. \\[y[n] = x[n] * h[n]\\]\n\nBy convolving any dry sound with the impulse response of a space, we can make it sound like it was recorded there!"
  },
  {
    "objectID": "ss_21.html#application-digital-audio-reverb-1",
    "href": "ss_21.html#application-digital-audio-reverb-1",
    "title": "Signal and Systems",
    "section": "Application: Digital Audio Reverb",
    "text": "Application: Digital Audio Reverb\n\n\n\n\n\ngraph TD\n    A[\"Dry Audio&lt;br&gt;x[n]\"] --&gt; C{\"Convolution&lt;br&gt;y[n] = x[n]*h[n]\"};\n    B[\"Room Impulse Response&lt;br&gt;h[n]\"] --&gt; C;\n    C --&gt; D[\"Audio with Reverb&lt;br&gt;y[n]\"];\n\n\n\n\n\n\n\nLet’s talk about a fun, real-world application: creating artificial reverb for music and movies. Imagine you record a singer in a perfectly “dead” room with no echoes. This is your dry input signal, \\(x[n]\\). Now, you go to a large cathedral and pop a balloon. The sound you record—a series of echoes that die out—is the impulse response, \\(h[n]\\), of the cathedral. If you convolve the singer’s dry vocal track with the cathedral’s impulse response, the output will sound exactly as if the singer was performing there! This is the principle behind most digital reverb plugins."
  },
  {
    "objectID": "ss_21.html#summary",
    "href": "ss_21.html#summary",
    "title": "Signal and Systems",
    "section": "Summary",
    "text": "Summary\n\nSignal Decomposition: Any discrete signal \\(x[n]\\) can be written as a sum of scaled, shifted impulses: \\(x[n] = \\sum_k x[k]\\delta[n-k]\\).\nLTI System Response: The output \\(y[n]\\) of an LTI system is the input \\(x[n]\\) convolved with the system’s impulse response \\(h[n]\\).\nThe Convolution Sum: This fundamental operation is defined as: \\[ y[n] = x[n] * h[n] = \\sum_{k=-\\infty}^{\\infty} x[k]h[n-k] \\]\nCalculation: We can compute this using the graphical “flip-and-slide” method.\nKey Insight: The impulse response \\(h[n]\\) is a complete characterization of an LTI system. If you know \\(h[n]\\), you know how the system will react to any input.\n\n\nLet’s recap. We started with the idea that any signal can be broken down into impulses. This allowed us to derive the convolution sum, which is the mathematical tool for finding the output of any LTI system. We learned the practical “flip-and-slide” method for computing it. And most importantly, we established that the impulse response, \\(h[n]\\), is the ultimate fingerprint of an LTI system. It tells you everything you need to know about its behavior."
  },
  {
    "objectID": "ss_23.html#properties-of-linear-time-invariant-systems",
    "href": "ss_23.html#properties-of-linear-time-invariant-systems",
    "title": "Signal and Systems",
    "section": "PROPERTIES OF LINEAR TIME-INVARIANT SYSTEMS",
    "text": "PROPERTIES OF LINEAR TIME-INVARIANT SYSTEMS\nECE Undergraduate Course\nImron Rosyadi\n\nIn our last sessions, we derived the convolution sum and integral. These are powerful tools because they show that an LTI system is completely defined by its impulse response. Today, we’ll explore the profound implications of this fact by examining the key properties of LTI systems, such as commutativity, associativity, stability, and causality, all through the lens of the impulse response."
  },
  {
    "objectID": "ss_23.html#the-power-of-the-impulse-response",
    "href": "ss_23.html#the-power-of-the-impulse-response",
    "title": "Signal and Systems",
    "section": "The Power of the Impulse Response",
    "text": "The Power of the Impulse Response\nThe convolution representation shows that an LTI system is completely characterized by its impulse response, \\(h[n]\\) or \\(h(t)\\).\n\nDiscrete-Time: \\(y[n] = x[n] * h[n] = \\sum_{k=-\\infty}^{\\infty} x[k]h[n-k]\\)\nContinuous-Time: \\(y(t) = x(t) * h(t) = \\int_{-\\infty}^{\\infty} x(\\tau)h(t-\\tau)d\\tau\\)\n\nThis is a unique feature of LTI systems. For non-linear systems, the impulse response is not a complete characterization.\nFor example, an LTI system with \\(h[n] = \\delta[n] + \\delta[n-1]\\) is uniquely defined as \\(y[n] = x[n] + x[n-1]\\).\nBut non-linear systems like \\(y[n] = (x[n]+x[n-1])^2\\) or \\(y[n]=\\max(x[n], x[n-1])\\) have the same impulse response, yet behave differently for other inputs.\n\nIt’s crucial to grasp this distinction. For an LTI system, if I give you its impulse response, I’ve told you everything. You can predict its output for any input. This is not true for a non-linear system. Multiple different non-linear systems can share the same impulse response. This is why the properties we’re about to discuss are so special to the LTI world."
  },
  {
    "objectID": "ss_23.html#the-commutative-property",
    "href": "ss_23.html#the-commutative-property",
    "title": "Signal and Systems",
    "section": "The Commutative Property",
    "text": "The Commutative Property\nConvolution is a commutative operation. The order doesn’t matter.\n\\[\nx[n] * h[n] = h[n] * x[n]\n\\] \\[\nx(t) * h(t) = h(t) * x(t)\n\\]\nThis means we can swap the roles of the input and the impulse response."
  },
  {
    "objectID": "ss_23.html#the-commutative-property-1",
    "href": "ss_23.html#the-commutative-property-1",
    "title": "Signal and Systems",
    "section": "The Commutative Property",
    "text": "The Commutative Property\n\n\n\n\n\ngraph TD\n    subgraph \"Standard View\"\n        A[Input: x] --&gt; S1[System: h]\n        S1 --&gt; O1[Output: y]\n    end\n\n\n\n\n\n\n\n\n\n\n\ngraph TD\n    subgraph \"Equivalent View\"\n        B[Input: h] --&gt; S2[System: x]\n        S2 --&gt; O2[Output: y]\n    end\n\n\n\n\n\n\nWhy is this useful? Sometimes flipping and sliding one signal is much easier than the other. Commutativity lets us choose the easier path.\n\nThe commutative property is more than just a mathematical curiosity. It has a practical benefit. When you’re faced with a convolution problem, you have a choice: you can either flip and slide ‘h’ or you can flip and slide ‘x’. Often, one of the signals is much simpler (like a rectangle or an impulse), making the “flip and slide” operation on that signal significantly easier. Commutativity gives you the freedom to choose the simpler calculation."
  },
  {
    "objectID": "ss_23.html#the-distributive-property",
    "href": "ss_23.html#the-distributive-property",
    "title": "Signal and Systems",
    "section": "The Distributive Property",
    "text": "The Distributive Property\nConvolution distributes over addition.\n\\[\nx(t) * [h_1(t) + h_2(t)] = x(t) * h_1(t) + x(t) * h_2(t)\n\\]\nThis means a parallel combination of LTI systems is equivalent to a single system whose impulse response is the sum of the individual impulse responses."
  },
  {
    "objectID": "ss_23.html#the-distributive-property-1",
    "href": "ss_23.html#the-distributive-property-1",
    "title": "Signal and Systems",
    "section": "The Distributive Property",
    "text": "The Distributive Property\n\n\n\n\n\ngraph TD\n    subgraph \"Parallel Systems\"\n        X(x) --&gt; H1(h1);\n        X --&gt; H2(h2);\n        H1 --&gt; A(\"(+)\");\n        H2 --&gt; A;\n        A --&gt; Y(y);\n    end\n\n    subgraph \"is equivalent to\"\n       X2(x) --&gt; H_EQ(h1 + h2);\n       H_EQ --&gt; Y2(y);\n    end\n\n\n\n\n\n\nThis property can simplify complex convolutions by breaking them into simpler parts.\n\nThe distributive property provides the rule for handling parallel systems. If you have an input that feeds into two different LTI systems and their outputs are added together, you can replace that entire parallel structure with a single LTI system. The impulse response of this new equivalent system is simply the sum of the two original impulse responses. We can also use this property in reverse to break down a complicated convolution problem into several simpler ones."
  },
  {
    "objectID": "ss_23.html#distributive-property-in-action",
    "href": "ss_23.html#distributive-property-in-action",
    "title": "Signal and Systems",
    "section": "Distributive Property in Action",
    "text": "Distributive Property in Action\nLet’s convolve \\(x[n] = (\\frac{1}{2})^n u[n] + 2^n u[-n]\\) with the accumulator \\(h[n]=u[n]\\).\nWe can break \\(x[n]\\) into two parts: a right-sided part \\(x_1[n]\\) and a left-sided part \\(x_2[n]\\).\n\\(y[n] = (x_1[n] + x_2[n]) * h[n] = (x_1[n]*h[n]) + (x_2[n]*h[n])\\)\n\n\n\n\n\n\n\nHere’s a concrete example of using the distributive property. The input signal \\(x[n]\\) is “two-sided,” which makes direct convolution tedious. By splitting \\(x[n]\\) into a right-sided exponential \\(x_1\\) and a left-sided exponential \\(x_2\\), we create two simpler problems. We already solved these two separate convolutions in previous examples! The final result is just the sum of those two simpler results, as shown in the green plot on the right."
  },
  {
    "objectID": "ss_23.html#the-associative-property",
    "href": "ss_23.html#the-associative-property",
    "title": "Signal and Systems",
    "section": "The Associative Property",
    "text": "The Associative Property\nConvolution is also an associative operation.\n\\[\n[x(t) * h_1(t)] * h_2(t) = x(t) * [h_1(t) * h_2(t)]\n\\]\nThis means for a cascade (series) of LTI systems, the overall impulse response is the convolution of the individual impulse responses.\n\n\n\n\n\ngraph LR\n    subgraph \"Cascade of Systems\"\n        X(x) --&gt; H1(h1) --&gt; W(w) --&gt; H2(h2) --&gt; Y(y);\n    end\n\n    subgraph \"is equivalent to\"\n        X2(x) --&gt; H_EQ(h1 * h2) --&gt; Y2(y);\n    end\n\n\n\n\n\n\n\nAssociativity gives us the rule for systems in series. When the output of one LTI system becomes the input to another, this is a cascade. The entire cascade can be replaced by a single equivalent LTI system. The impulse response of this new system is found by convolving the impulse responses of the individual systems in the cascade."
  },
  {
    "objectID": "ss_23.html#order-doesnt-matter-for-lti-systems",
    "href": "ss_23.html#order-doesnt-matter-for-lti-systems",
    "title": "Signal and Systems",
    "section": "Order Doesn’t Matter… for LTI systems!",
    "text": "Order Doesn’t Matter… for LTI systems!\nCombining the associative and commutative properties leads to a powerful conclusion:\nThe order of LTI systems in a cascade can be interchanged without changing the overall system response.\n\\(h_1 * h_2 = h_2 * h_1\\)\nThis is a very special characteristic of LTI systems. It does NOT hold for non-linear systems.\n\n\nLTI Systems\n\n\n\n\n\ngraph TD\n    X1(x) --&gt; H1(h1) --&gt; H2(h2) --&gt; Y1(y)\n    Y1 -- yields same y --&gt; Y2\n    X2(x) --&gt; H2b(h2) --&gt; H1b(h1) --&gt; Y2(y)\n\n\n\n\n\n\n\nNon-Linear Counterexample\n\nLet System 1 be squaring\nLet System 2 be multiply by 2\n\n\\(x \\rightarrow [\\text{Square}] \\rightarrow \\times 2 \\rightarrow y = 2x^2\\)\n\\(x \\rightarrow \\times 2 \\rightarrow [\\text{Square}] \\rightarrow y = (2x)^2 = 4x^2\\)\nThe results are different! (\\(2x^2 \\ne 4x^2\\))\n\n\nThis is one of the most important results. Because convolution is both associative and commutative, we can swap the order of LTI systems in a series connection and the final output remains exactly the same. The counterexample on the right shows how quickly this breaks down for non-linear systems. Simply changing the order of a squaring operation and a gain of 2 completely changes the final result. This flexibility is a privilege we only enjoy with LTI systems."
  },
  {
    "objectID": "ss_23.html#system-properties-via-impulse-response",
    "href": "ss_23.html#system-properties-via-impulse-response",
    "title": "Signal and Systems",
    "section": "System Properties via Impulse Response",
    "text": "System Properties via Impulse Response\nWe can determine key system properties directly from \\(h\\).\n\n\n\n\n\n\n\nProperty\nCondition on Impulse Response \\(h\\)\n\n\n\n\nMemoryless\n\\(h[n]=K\\delta[n]\\) or \\(h(t)=K\\delta(t)\\). Non-zero only at the origin.\n\n\nCausal\n\\(h[n]=0\\) for \\(n&lt;0\\) or \\(h(t)=0\\) for \\(t&lt;0\\). Response can’t precede the impulse.\n\n\nStable (BIBO)\nImpulse response must be absolutely summable/integrable. \\(\\sum \\|h[k]\\| &lt; \\infty\\) or \\(\\int \\|h(\\tau)\\|d\\tau &lt; \\infty\\).\n\n\nInvertible\nAn inverse system \\(h_{inv}\\) exists such that \\(h * h_{inv} = \\delta\\).\n\n\n\n\nThis summary table is your cheat sheet. It connects the high-level system properties we’ve discussed—memory, causality, stability, and invertibility—to concrete, testable conditions on the impulse response. By simply inspecting \\(h\\), we can immediately determine these fundamental characteristics of an LTI system."
  },
  {
    "objectID": "ss_23.html#invertibility-of-lti-systems",
    "href": "ss_23.html#invertibility-of-lti-systems",
    "title": "Signal and Systems",
    "section": "Invertibility of LTI Systems",
    "text": "Invertibility of LTI Systems\nAn LTI system is invertible if we can find an inverse system, \\(h_{inv}\\), that perfectly undoes its effect.\nIn a cascade, they form an identity system.\n\n\n\n\n\ngraph LR\n    X(\"x(t)\") --&gt; H(\"h(t)\") --&gt; W(\"w(t)\") --&gt; H_INV(\"h_inv(t)\") --&gt; Y(\"y(t) = x(t)\");\n\n    subgraph \"is equivalent to\"\n        X2(\"x(t)\") --&gt; ID(\"δ(t)\") --&gt; Y2(\"y(t)=x(t)\");\n    end\n\n\n\n\n\n\nThe condition for invertibility is:\n\\[\nh(t) * h_{inv}(t) = \\delta(t) \\quad \\text{or} \\quad h[n] * h_{inv}[n] = \\delta[n]\n\\]\n\nThe concept of an inverse is intuitive: it’s a system that gets you back your original input. For LTI systems, this concept has a precise mathematical form. The inverse system, when convolved with the original system, must produce a unit impulse. The unit impulse acts as the “identity element” for convolution, just like the number 1 is the identity for multiplication."
  },
  {
    "objectID": "ss_23.html#example-inverting-the-accumulator",
    "href": "ss_23.html#example-inverting-the-accumulator",
    "title": "Signal and Systems",
    "section": "Example: Inverting the Accumulator",
    "text": "Example: Inverting the Accumulator\n\nSystem: The discrete-time accumulator. Its impulse response is the unit step, \\(h[n] = u[n]\\).\n\n\\(y[n] = \\sum_{k=-\\infty}^{n} x[k]\\)\n\nInverse System: The first-difference system.\n\n\\(w[n] = y[n] - y[n-1]\\)\nIts impulse response is \\(h_{inv}[n] = \\delta[n] - \\delta[n-1]\\).\n\n\nLet’s verify: \\(h[n] * h_{inv}[n] = u[n] * (\\delta[n] - \\delta[n-1]) = u[n] - u[n-1] = \\delta[n]\\) ✔️\n\n\n\n\n\n\n\nThe accumulator and the first-difference system are a perfect pair of inverse systems. The accumulator sums up values over time. The first-difference looks at the change from one sample to the next. It makes intuitive sense that differencing undoes summation. The plot confirms this mathematically: convolving the impulse response of the accumulator (a step function) with the impulse response of the first-difference system gives us a single unit impulse."
  },
  {
    "objectID": "ss_23.html#causality-and-stability",
    "href": "ss_23.html#causality-and-stability",
    "title": "Signal and Systems",
    "section": "Causality and Stability",
    "text": "Causality and Stability\n\nCausality: An LTI system is causal if its output at time n only depends on inputs up to time n (present and past).\n\nCondition: \\(h[n] = 0\\) for \\(n&lt;0\\) or \\(h(t)=0\\) for \\(t&lt;0\\).\nIntuition: The system can’t react to an impulse before it happens.\n\nStability (BIBO): A system is stable if every bounded input produces a bounded output.\n\nCondition: The impulse response must be absolutely summable/integrable.\n\\(\\sum_{k=-\\infty}^{\\infty} |h[k]| &lt; \\infty\\) or \\(\\int_{-\\infty}^{\\infty} |h(\\tau)| d\\tau &lt; \\infty\\)\nIntuition: The system’s “memory” or “echoes” must eventually die out.\n\n\nExample: The accumulator, \\(h[n]=u[n]\\), is causal but unstable because \\(\\sum_{n=0}^\\infty |u[n]| = \\infty\\).\n\nCausality is a simple check: is the impulse response zero for all negative time? If yes, it’s causal. Stability is a bit more involved. The condition means that the total “energy” of the impulse response, ignoring sign, must be finite. If the impulse response doesn’t decay to zero fast enough, like the unit step, a bounded input (like a constant value of 1) can cause the output to grow infinitely."
  },
  {
    "objectID": "ss_23.html#the-unit-step-response",
    "href": "ss_23.html#the-unit-step-response",
    "title": "Signal and Systems",
    "section": "The Unit Step Response",
    "text": "The Unit Step Response\nBesides the impulse response \\(h(t)\\), the unit step response \\(s(t)\\) is also used to characterize an LTI system. It’s the output when the input is a unit step, \\(u(t)\\).\nThe two are directly related:\n\n\nDiscrete-Time\nThe step response is the running sum of the impulse response. \\[s[n] = \\sum_{k=-\\infty}^{n} h[k]\\] The impulse response is the first difference of the step response. \\[h[n] = s[n] - s[n-1]\\]\n\nContinuous-Time\nThe step response is the running integral of the impulse response. \\[s(t) = \\int_{-\\infty}^{t} h(\\tau)d\\tau\\] The impulse response is the derivative of the step response. \\[h(t) = \\frac{ds(t)}{dt}\\]\n\nKnowing either \\(h(t)\\) or \\(s(t)\\) allows you to fully describe the LTI system.\n\nThe impulse response is the fundamental theoretical tool, but in a real-world lab, creating a perfect impulse is impossible. Creating a good step input (like flipping a switch) is much easier. The step response is therefore a common practical way to measure and characterize a system. These equations show that the impulse response and step response are two sides of the same coin. They contain the same information about the system, just presented in different ways."
  },
  {
    "objectID": "ss_32.html#introduction-why-basic-signals",
    "href": "ss_32.html#introduction-why-basic-signals",
    "title": "Signals and Systems",
    "section": "1. Introduction: Why Basic Signals?",
    "text": "1. Introduction: Why Basic Signals?\nIn Signals and Systems, we seek “basic signals” with two crucial properties:\n\nBroad Applicability: They can construct a wide range of useful signals.\nSimple Response: The system’s response to them is simple enough for convenient analysis.\n\nComplex exponentials (\\(e^{st}\\) and \\(z^n\\)) fit these criteria perfectly.\n\nTo effectively analyze and design systems, we often look for fundamental building blocks. Imagine trying to understand complex machinery without knowing about simple gears or levers. In signals, we want signals that, when fed into an LTI system, behave predictably and simply. This predictability allows us to analyze the system’s overall behavior for any input that can be decomposed into these basic signals. The complex exponential is precisely this kind of fundamental building block for LTI systems. It simplifies the inherently complex operation of convolution into something much more manageable."
  },
  {
    "objectID": "ss_32.html#eigentheory-for-lti-systems",
    "href": "ss_32.html#eigentheory-for-lti-systems",
    "title": "Signals and Systems",
    "section": "2. Eigentheory for LTI Systems",
    "text": "2. Eigentheory for LTI Systems\nA signal for which the system output is a (possibly complex) constant times the input is called an eigenfunction of the system.\nFor a system \\(\\mathcal{H}\\), if: \\[\n\\mathcal{H}\\{x(t)\\} = y(t) = \\lambda x(t)\n\\]\nthen \\(x(t)\\) is an eigenfunction of the system, and \\(\\lambda\\) is the corresponding eigenvalue.\n\n\nContinuous-Time\nInput: \\(x(t) = e^{st}\\)\nOutput: \\(y(t) = H(s) e^{st}\\)\nHere, \\(e^{st}\\) is the eigenfunction, and \\(H(s)\\) is the corresponding eigenvalue.\n\nDiscrete-Time\nInput: \\(x[n] = z^n\\)\nOutput: \\(y[n] = H(z) z^n\\)\nHere, \\(z^n\\) is the eigenfunction, and \\(H(z)\\) is the corresponding eigenvalue.\n\n\nThe term “eigen” comes from German, meaning “own” or “characteristic”. So, an eigenfunction is a characteristic function of the system. It means that when you apply this specific type of signal to an LTI system, its fundamental form doesn’t change; only its amplitude and phase might be modified. This property is incredibly powerful for simplifying system analysis, as we only need to understand how the system scales these specific basic signals, rather than performing complex convolutions for every possible input."
  },
  {
    "objectID": "ss_32.html#derivation-continuous-time-case",
    "href": "ss_32.html#derivation-continuous-time-case",
    "title": "Signals and Systems",
    "section": "3. Derivation: Continuous-Time Case",
    "text": "3. Derivation: Continuous-Time Case\nLet an LTI system have impulse response \\(h(t)\\).\nInput: \\(x(t) = e^{st}\\).\nThe output \\(y(t)\\) is given by the convolution integral: \\[\ny(t) = \\int_{-\\infty}^{+\\infty} h(\\tau) x(t-\\tau) d \\tau\n\\]\nSubstitute \\(x(t-\\tau) = e^{s(t-\\tau)}\\): \\[\ny(t) = \\int_{-\\infty}^{+\\infty} h(\\tau) e^{s(t-\\tau)} d \\tau\n\\] \\[\ny(t) = \\int_{-\\infty}^{+\\infty} h(\\tau) e^{st} e^{-s\\tau} d \\tau\n\\]"
  },
  {
    "objectID": "ss_32.html#derivation-continuous-time-case-cont.",
    "href": "ss_32.html#derivation-continuous-time-case-cont.",
    "title": "Signals and Systems",
    "section": "3. Derivation: Continuous-Time Case (cont.)",
    "text": "3. Derivation: Continuous-Time Case (cont.)\nSince \\(e^{st}\\) is independent of \\(\\tau\\), we can pull it out of the integral: \\[\ny(t) = e^{st} \\int_{-\\infty}^{+\\infty} h(\\tau) e^{-s\\tau} d \\tau\n\\]\nThus, the output is: \\[\ny(t) = H(s) e^{st}\n\\] where the eigenvalue \\(H(s)\\) is: \\[\nH(s) = \\int_{-\\infty}^{+\\infty} h(\\tau) e^{-s\\tau} d \\tau\n\\]\n\nThis derivation explicitly illustrates how the convolution integral simplifies when the input is a complex exponential. The integral term, \\(\\int_{-\\infty}^{+\\infty} h(\\tau) e^{-s\\tau} d \\tau\\), is a fundamental transformation of the system’s impulse response. For particular values of ‘s’, this expression corresponds to the Laplace Transform of \\(h(t)\\), or more specifically, the Fourier Transform if \\(s=j\\omega\\). This \\(H(s)\\) completely characterizes how the system behaves for each specific complex exponential frequency, providing a direct link between the system’s time-domain properties and its response in the complex frequency domain."
  },
  {
    "objectID": "ss_32.html#derivation-discrete-time-case",
    "href": "ss_32.html#derivation-discrete-time-case",
    "title": "Signals and Systems",
    "section": "4. Derivation: Discrete-Time Case",
    "text": "4. Derivation: Discrete-Time Case\nSimilarly, for a discrete-time LTI system with impulse response \\(h[n]\\).\nInput: \\(x[n] = z^n\\).\nThe output \\(y[n]\\) is given by the convolution sum: \\[\ny[n] = \\sum_{k=-\\infty}^{+\\infty} h[k] x[n-k]\n\\]\nSubstitute \\(x[n-k] = z^{n-k}\\): \\[\ny[n] = \\sum_{k=-\\infty}^{+\\infty} h[k] z^{n-k}\n\\] \\[\ny[n] = \\sum_{k=-\\infty}^{+\\infty} h[k] z^n z^{-k}\n\\]"
  },
  {
    "objectID": "ss_32.html#derivation-discrete-time-case-cont.",
    "href": "ss_32.html#derivation-discrete-time-case-cont.",
    "title": "Signals and Systems",
    "section": "4. Derivation: Discrete-Time Case (cont.)",
    "text": "4. Derivation: Discrete-Time Case (cont.)\nSince \\(z^n\\) is independent of \\(k\\), we can pull it out of the sum: \\[\ny[n] = z^n \\sum_{k=-\\infty}^{+\\infty} h[k] z^{-k}\n\\]\nThus, the output is: \\[\ny[n] = H(z) z^n\n\\] where the eigenvalue \\(H(z)\\) is: \\[\nH(z) = \\sum_{k=-\\infty}^{+\\infty} h[k] z^{-k}\n\\]\n\nThe discrete-time derivation strongly parallels the continuous-time one, demonstrating the consistent mathematical framework for LTI systems in both domains. The sum, \\(\\sum_{k=-\\infty}^{+\\infty} h[k] z^{-k}\\), represents the Z-Transform of \\(h[n]\\). This again highlights how the eigenvalue function, \\(H(z)\\), is a transform of the system’s impulse response, providing a direct link between the system’s internal characteristics (its impulse response) and its external behavior when excited by discrete-time complex exponentials. This parallel makes the concepts transferable and reinforces the universality of eigenfunctions for LTI analysis."
  },
  {
    "objectID": "ss_32.html#power-of-superposition",
    "href": "ss_32.html#power-of-superposition",
    "title": "Signals and Systems",
    "section": "5. Power of Superposition",
    "text": "5. Power of Superposition\nThe eigenfunction property, combined with the linearity of LTI systems (superposition), significantly simplifies analysis.\nConsider an input composed of a sum of complex exponentials: \\[\nx(t) = a_1 e^{s_1 t} + a_2 e^{s_2 t} + a_3 e^{s_3 t}\n\\]\nBy superposition, the response to the sum is the sum of individual responses:\n\n\\(a_1 e^{s_1 t} \\longrightarrow a_1 H(s_1) e^{s_1 t}\\)\n\\(a_2 e^{s_2 t} \\longrightarrow a_2 H(s_2) e^{s_2 t}\\)\n\\(a_3 e^{s_3 t} \\longrightarrow a_3 H(s_3) e^{s_3 t}\\)\n\nSumming these up, the total output \\(y(t)\\) is: \\[\ny(t) = a_1 H(s_1) e^{s_1 t} + a_2 H(s_2) e^{s_2 t} + a_3 H(s_3) e^{s_3 t}\n\\]\n\nThis is the core insight that underpins Fourier analysis. If we can represent any complex-looking signal as a sum (or integral) of these simple complex exponentials, then finding the system’s output just means multiplying each component’s amplitude coefficient by the corresponding eigenvalue. We completely avoid the computationally intensive process of performing a full convolution for the entire input signal. This drastically simplifies the problem of finding system responses and provides a powerful alternative to time-domain analysis."
  },
  {
    "objectID": "ss_32.html#general-form-continuous-time",
    "href": "ss_32.html#general-form-continuous-time",
    "title": "Signals and Systems",
    "section": "6. General Form: Continuous-Time",
    "text": "6. General Form: Continuous-Time\nIf the input to a continuous-time LTI system is a linear combination of complex exponentials:\n\n\nInput Signal \\[\nx(t) = \\sum_{k} a_k e^{s_k t}\n\\]\n\nOutput Signal \\[\ny(t) = \\sum_{k} a_k H(s_k) e^{s_k t}\n\\]\n\nThe output is simply a linear combination of the same complex exponentials, but each scaled by its corresponding eigenvalue \\(H(s_k)\\).\n\nThis equation is truly fundamental in Signals and Systems. It means that the LTI system preserves the frequency components of the input complex exponentials. It doesn’t generate new frequencies; it only changes their amplitudes and phases. This property is why LTI systems are often called “linear filters.” They selectively pass or attenuate different frequency components based on the value of \\(H(s_k)\\) at that specific complex frequency \\(s_k\\)."
  },
  {
    "objectID": "ss_32.html#general-form-discrete-time",
    "href": "ss_32.html#general-form-discrete-time",
    "title": "Signals and Systems",
    "section": "7. General Form: Discrete-Time",
    "text": "7. General Form: Discrete-Time\nSimilarly, for a discrete-time LTI system:\n\n\nInput Signal \\[\nx[n] = \\sum_{k} a_k z_k^n\n\\]\n\nOutput Signal \\[\ny[n] = \\sum_{k} a_k H(z_k) z_k^n\n\\]\n\nThe output is also a linear combination of the same complex exponential sequences, each scaled by its corresponding eigenvalue \\(H(z_k)\\).\n\nThe strong parallelism between continuous-time and discrete-time systems in this context is both elegant and important. It demonstrates that the underlying principles of how LTI systems interact with complex exponentials are consistent across both domains, whether the signals are continuous or sampled. This makes the concepts widely applicable and reinforces the universality of eigenfunctions for LTI analysis, simplifying the transition between analog and digital signal processing."
  },
  {
    "objectID": "ss_32.html#restriction-for-fourier-analysis",
    "href": "ss_32.html#restriction-for-fourier-analysis",
    "title": "Signals and Systems",
    "section": "8. Restriction for Fourier Analysis",
    "text": "8. Restriction for Fourier Analysis\nWhile \\(s\\) and \\(z\\) can be arbitrary complex numbers, Fourier analysis focuses on specific values:\n\n\nContinuous-Time We restrict \\(s\\) to purely imaginary values: \\[\ns = j\\omega\n\\] This focuses on complex exponentials of the form \\(e^{j\\omega t}\\).\n\nDiscrete-Time We restrict \\(z\\) to values of unit magnitude: \\[\nz = e^{j\\omega}\n\\] This focuses on complex exponentials of the form \\(e^{j\\omega n}\\).\n\n\nThe restriction to \\(s=j\\omega\\) and \\(z=e^{j\\omega}\\) is profoundly important. These specific forms represent sinusoidal signals, which are ubiquitous in electrical and computer engineering (e.g., AC circuits, oscillations, modulation). \\(e^{j\\omega t}\\) can be decomposed into \\(\\cos(\\omega t)\\) and \\(\\sin(\\omega t)\\) using Euler’s formula. By focusing on these values, we are effectively analyzing the system’s frequency response. The values of \\(H(j\\omega)\\) and \\(H(e^{j\\omega})\\) directly tell us how the system passes or attenuates different frequencies, analogous to how an audio equalizer or a radio filter works."
  },
  {
    "objectID": "ss_32.html#example-3.1-time-shift-system",
    "href": "ss_32.html#example-3.1-time-shift-system",
    "title": "Signals and Systems",
    "section": "9. Example 3.1: Time Shift System",
    "text": "9. Example 3.1: Time Shift System\nConsider an LTI system where the output \\(y(t)\\) is a time-shifted version of the input \\(x(t)\\): \\[\ny(t)=x(t-3).\n\\] This system represents a pure time delay of 3 units.\nPart 1: Response to a single complex exponential input\nInput: \\(x(t) = e^{j2t}\\).\nTo find the output, we directly substitute \\(x(t)\\) into the system equation: \\[\ny(t) = e^{j2(t-3)} = e^{-j6} e^{j2t}.\n\\]\nComparing \\(y(t) = e^{-j6} e^{j2t}\\) with the eigenfunction form \\(y(t) = H(s) e^{st}\\), we can identify: The eigenvalue \\(H(j2) = e^{-j6}\\).\n\nThis simple example clearly illustrates the eigenfunction property in action. The output is still a complex exponential of the exact same frequency (\\(2\\) rad/sec), but its amplitude has been modified by the complex constant \\(e^{-j6}\\). This complex constant is crucial: its magnitude (which is 1 here, meaning no gain or attenuation) and its phase (\\(-6\\) radians, indicating a lag) reveal how the system affects this specific frequency component. It’s a phase shift corresponding to the time delay at that particular frequency."
  },
  {
    "objectID": "ss_32.html#example-3.1-impulse-response-verification",
    "href": "ss_32.html#example-3.1-impulse-response-verification",
    "title": "Signals and Systems",
    "section": "10. Example 3.1: Impulse Response Verification",
    "text": "10. Example 3.1: Impulse Response Verification\nLet’s verify the eigenvalue \\(H(s)\\) by first determining the system’s impulse response \\(h(t)\\).\nFor a system defined by \\(y(t)=x(t-3)\\), the impulse response is a delayed impulse function: \\(h(t) = \\delta(t-3)\\).\nNow, we use the formula for \\(H(s)\\) derived earlier: \\[\nH(s) = \\int_{-\\infty}^{+\\infty} h(\\tau) e^{-s\\tau} d \\tau\n\\] Substitute \\(h(\\tau) = \\delta(\\tau-3)\\): \\[\nH(s) = \\int_{-\\infty}^{+\\infty} \\delta(\\tau-3) e^{-s\\tau} d \\tau\n\\]"
  },
  {
    "objectID": "ss_32.html#example-3.1-impulse-response-verification-cont.",
    "href": "ss_32.html#example-3.1-impulse-response-verification-cont.",
    "title": "Signals and Systems",
    "section": "10. Example 3.1: Impulse Response Verification (cont.)",
    "text": "10. Example 3.1: Impulse Response Verification (cont.)\nDue to the sifting property of the Dirac delta function, this integral evaluates to: \\[\nH(s) = e^{-s(3)} = e^{-3s}\n\\]\nFinally, we evaluate \\(H(s)\\) at \\(s=j2\\) (the frequency of our input \\(x(t)=e^{j2t}\\)): \\[\nH(j2) = e^{-j3(2)} = e^{-j6}\n\\] This result precisely matches the eigenvalue we found by direct substitution on the previous slide!\n\nThis verification step is important because it shows the consistency of the eigenfunction framework. Whether we calculate the eigenvalue by observing the output directly or by transforming the system’s impulse response, we get the same result. The use of the Dirac delta function here significantly simplifies the integral, demonstrating its powerful sifting property. This consistency reinforces the fundamental relationship between a system’s time-domain characteristic (its impulse response) and its frequency-domain characteristic (its eigenvalue function)."
  },
  {
    "objectID": "ss_32.html#example-3.1-superposition-with-cosines",
    "href": "ss_32.html#example-3.1-superposition-with-cosines",
    "title": "Signals and Systems",
    "section": "11. Example 3.1: Superposition with Cosines",
    "text": "11. Example 3.1: Superposition with Cosines\nConsider a more complex input signal composed of a sum of two cosine waves: \\[\nx(t) = \\cos(4t) + \\cos(7t)\n\\]\nDirect approach: From \\(y(t)=x(t-3)\\), the output is simply: \\[\ny(t) = \\cos(4(t-3)) + \\cos(7(t-3))\n\\]\nUsing Eigenfunctions & Superposition: First, we express \\(x(t)\\) using Euler’s formula to decompose it into complex exponentials: \\[\nx(t) = \\frac{1}{2}e^{j4t} + \\frac{1}{2}e^{-j4t} + \\frac{1}{2}e^{j7t} + \\frac{1}{2}e^{-j7t}\n\\]\nWe already found the system’s eigenvalue \\(H(j\\omega)\\) for a time delay of 3: \\(H(j\\omega) = e^{-j3\\omega}\\).\n\nThis example transitions from a single exponential to a sum of real sinusoids. This is where the true power of the eigenfunction approach, combined with linearity, becomes apparent. By using Euler’s formula, we transform the real-valued input into a sum of complex exponentials. Each of these complex exponential components is an eigenfunction of the LTI system, allowing us to find their individual responses simply by scaling. This decomposition is the critical first step in applying Fourier analysis to real-world signals."
  },
  {
    "objectID": "ss_32.html#example-3.1-superposition-cont.",
    "href": "ss_32.html#example-3.1-superposition-cont.",
    "title": "Signals and Systems",
    "section": "12. Example 3.1: Superposition (Cont.)",
    "text": "12. Example 3.1: Superposition (Cont.)\nNow, we apply the eigenfunction property to each complex exponential component of \\(x(t)\\):\n\nFor \\(\\frac{1}{2}e^{j4t}\\): The eigenvalue is \\(H(j4) = e^{-j3(4)} = e^{-j12}\\).\nOutput component: \\(\\frac{1}{2} H(j4) e^{j4t} = \\frac{1}{2} e^{-j12} e^{j4t} = \\frac{1}{2} e^{j(4t-12)} = \\frac{1}{2} e^{j4(t-3)}\\)\nFor \\(\\frac{1}{2}e^{-j4t}\\): The eigenvalue is \\(H(-j4) = e^{-j3(-4)} = e^{j12}\\).\nOutput component: \\(\\frac{1}{2} H(-j4) e^{-j4t} = \\frac{1}{2} e^{j12} e^{-j4t} = \\frac{1}{2} e^{-j(4t-12)} = \\frac{1}{2} e^{-j4(t-3)}\\)\nFor \\(\\frac{1}{2}e^{j7t}\\): The eigenvalue is \\(H(j7) = e^{-j3(7)} = e^{-j21}\\).\nOutput component: \\(\\frac{1}{2} H(j7) e^{j7t} = \\frac{1}{2} e^{-j21} e^{j7t} = \\frac{1}{2} e^{j(7t-21)} = \\frac{1}{2} e^{j7(t-3)}\\)\nFor \\(\\frac{1}{2}e^{-j7t}\\): The eigenvalue is \\(H(-j7) = e^{-j3(-7)} = e^{j21}\\).\nOutput component: \\(\\frac{1}{2} H(-j7) e^{-j7t} = \\frac{1}{2} e^{j21} e^{-j7t} = \\frac{1}{2} e^{-j(7t-21)} = \\frac{1}{2} e^{-j7(t-3)}\\)"
  },
  {
    "objectID": "ss_32.html#example-3.1-superposition-cont.-1",
    "href": "ss_32.html#example-3.1-superposition-cont.-1",
    "title": "Signals and Systems",
    "section": "12. Example 3.1: Superposition (Cont.)",
    "text": "12. Example 3.1: Superposition (Cont.)\nSumming these individual outputs, we get the total output \\(y(t)\\): \\[\ny(t) = \\frac{1}{2} e^{j4(t-3)} + \\frac{1}{2} e^{-j4(t-3)} + \\frac{1}{2} e^{j7(t-3)} + \\frac{1}{2} e^{-j7(t-3)}\n\\] Applying Euler’s formula in reverse (recall \\(\\cos(\\theta) = \\frac{e^{j\\theta} + e^{-j\\theta}}{2}\\)): \\[\ny(t) = \\cos(4(t-3)) + \\cos(7(t-3))\n\\] This result precisely matches the direct calculation, demonstrating the power of the eigenfunction approach!\n\nThis detailed breakdown confirms that the eigenfunction approach, coupled with the principle of superposition, correctly and systematically determines the system’s output for complex inputs. While direct substitution was trivial for this particular simple time-delay system, the eigenfunction method is universally applicable and becomes absolutely indispensable for more complex LTI systems where direct convolution or time-domain manipulation is cumbersome or intractable. This systematically transforms complex time-domain problems into simpler frequency-domain multiplications, paving the way for advanced frequency-domain analysis and filter design."
  },
  {
    "objectID": "ss_32.html#conclusion-next-steps",
    "href": "ss_32.html#conclusion-next-steps",
    "title": "Signals and Systems",
    "section": "14. Conclusion & Next Steps",
    "text": "14. Conclusion & Next Steps\nKey Takeaways:\n\nComplex exponentials (\\(e^{st}\\), \\(z^n\\)) are the eigenfunctions of LTI systems.\nLTI systems respond to these signals by simply scaling them by an eigenvalue (\\(H(s)\\), \\(H(z)\\)).\nThis property, combined with superposition, simplifies the analysis of complex signals through LTI systems from convolution to simple multiplication.\n\nWhat’s Next?\nThis lays the essential foundation for Fourier Analysis:\n\nFourier Series: A powerful tool for representing periodic signals as sums of complex exponentials.\nFourier Transform: Extending this representation to aperiodic signals, enabling universal frequency-domain analysis.\n\n\nUnderstanding eigenfunctions is the critical bridge that connects time-domain convolution with the powerful frequency-domain analysis techniques. It’s the “why” behind transforms like the Fourier, Laplace, and Z-transforms. These transforms don’t just calculate; they inherently leverage this eigenfunction property to transform the complex operation of convolution into simpler multiplication operations in the frequency domain. This shift makes system analysis and design immensely more intuitive and manageable, forming the backbone for nearly all aspects of electrical and computer engineering, from communication systems and audio processing to control theory and image processing. This is a concept that truly permeates the entire field."
  },
  {
    "objectID": "ss_34.html#convergence-of-the-fourier-series",
    "href": "ss_34.html#convergence-of-the-fourier-series",
    "title": "Signals and Systems",
    "section": "3.4 Convergence of the Fourier Series",
    "text": "3.4 Convergence of the Fourier Series\n\nWelcome to this lecture on the convergence of the Fourier Series. Today, we’ll explore the conditions under which a periodic signal can be accurately represented by its Fourier series, and delve into some interesting phenomena that arise when approximating discontinuous signals. This is a critical topic for understanding the practical applications and limitations of Fourier analysis in ECE."
  },
  {
    "objectID": "ss_34.html#approximating-signals-euler-lagrange-and-fourier",
    "href": "ss_34.html#approximating-signals-euler-lagrange-and-fourier",
    "title": "Signals and Systems",
    "section": "Approximating Signals: Euler, Lagrange, and Fourier",
    "text": "Approximating Signals: Euler, Lagrange, and Fourier\n\nEarly mathematicians like Euler and Lagrange struggled with Fourier series for discontinuous signals.\n\nFourier, however, maintained that even discontinuous signals like square waves could be represented.\n\nWe approximate a periodic signal \\(x(t)\\) using a finite sum of harmonically related complex exponentials: \\[\nx_{N}(t)=\\sum_{k=-N}^{N} a_{k} e^{j k \\omega_{0} t}\n\\]\nThe approximation error is defined as: \\[\ne_{N}(t)=x(t)-x_{N}(t)\n\\]\n\n\nHistorically, the idea of representing a discontinuous function as a sum of continuous sinusoids was quite controversial. Euler and Lagrange, pillars of classical analysis, found this problematic. Fourier, with his more applied perspective, pushed the boundaries, asserting the validity of such representations for a much wider class of signals. This finite sum, \\(x_N(t)\\), is our practical tool for approximation, and understanding its behavior as N grows is key to convergence."
  },
  {
    "objectID": "ss_34.html#minimizing-approximation-error",
    "href": "ss_34.html#minimizing-approximation-error",
    "title": "Signals and Systems",
    "section": "Minimizing Approximation Error",
    "text": "Minimizing Approximation Error\n\nTo quantify the approximation’s quality, we use the energy in the error over one period: \\[\nE_{N}=\\int_{T}\\left|e_{N}(t)\\right|^{2} d t\n\\]\nKey Result: The coefficients \\(a_k\\) that minimize this energy \\(E_N\\) are precisely the Fourier series coefficients: \\[\na_{k}=\\frac{1}{T} \\int_{T} x(t) e^{-j k \\omega_{0} t} d t\n\\]\n\n\n\n\n\n\n\nTip\n\n\nThis means that truncating the Fourier series provides the “best” approximation in a least-squares sense for a finite number of terms.\n\n\n\n\nMinimizing the energy of the error is a common and powerful criterion in signal processing, often leading to optimal solutions. The fact that the Fourier series coefficients naturally emerge as the minimizers here is a fundamental and elegant result, strengthening the importance of the Fourier series. As we increase N, adding more terms, the energy in the error, E_N, will decrease. For a convergent series, E_N will approach zero as N approaches infinity."
  },
  {
    "objectID": "ss_34.html#when-does-a-fourier-series-converge",
    "href": "ss_34.html#when-does-a-fourier-series-converge",
    "title": "Signals and Systems",
    "section": "When Does a Fourier Series Converge?",
    "text": "When Does a Fourier Series Converge?\n\nNot all periodic signals have a valid Fourier series representation.\n\nThe integral for \\(a_k\\) might diverge, or the infinite series for \\(x(t)\\) might not converge to \\(x(t)\\).\n\nFortunately, most practical ECE signals do have Fourier series representations.\nWe’ll discuss two main classes of conditions guaranteeing convergence:\n\nFinite Energy over a Period (Square Integrability)\nDirichlet Conditions (Point-wise Convergence)\n\n\n\nWhile Fourier’s claim that any periodic signal could be represented was a bit ambitious, his intuition was largely correct for engineering applications. Pathological signals that don’t converge are rarely encountered in practice. Understanding these conditions helps us appreciate the robustness of Fourier analysis and its applicability to a vast range of real-world signals, including those with discontinuities."
  },
  {
    "objectID": "ss_34.html#condition-1-finite-energy-square-integrability",
    "href": "ss_34.html#condition-1-finite-energy-square-integrability",
    "title": "Signals and Systems",
    "section": "Condition 1: Finite Energy (Square Integrability)",
    "text": "Condition 1: Finite Energy (Square Integrability)\n\nA periodic signal \\(x(t)\\) has a Fourier series representation if it has finite energy over a single period: \\[ \\int_{T}|x(t)|^{2} d t&lt;\\infty \\]\nGuarantees:\n\nCoefficients \\(a_k\\) are finite.\nThe energy in the approximation error \\(E_N\\) converges to 0 as \\(N \\rightarrow \\infty\\). \\[ \\int_{T}|e(t)|^{2} d t=0 \\quad \\text{where} \\quad e(t)=x(t)-\\sum_{k=-\\infty}^{+\\infty} a_{k} e^{j k \\omega_{0} t} \\]\n\n\n\n\n\n\n\n\nImportant\n\n\nThis means \\(x(t)\\) and its infinite Fourier series representation are indistinguishable from an energy perspective, even if they differ at isolated points. Physical systems respond to signal energy.\n\n\n\n\nThis condition is perhaps the most fundamental for engineers because it directly relates to the physical reality of signals. If the difference between a signal and its Fourier series has zero energy, then any practical system responding to energy will treat them as identical. This is a very strong form of convergence for engineering purposes. Most signals we encounter, like square waves, triangle waves, and even pulse trains, satisfy this condition."
  },
  {
    "objectID": "ss_34.html#condition-2-dirichlet-conditions",
    "href": "ss_34.html#condition-2-dirichlet-conditions",
    "title": "Signals and Systems",
    "section": "Condition 2: Dirichlet Conditions",
    "text": "Condition 2: Dirichlet Conditions\n\nA set of conditions, developed by P. L. Dirichlet, that guarantee point-wise convergence.\nThis means \\(x(t)\\) equals its Fourier series representation except at isolated discontinuities.\nAt discontinuities, the series converges to the average of the values on either side.\n\n\nWhile finite energy convergence is great for overall system response, sometimes we need to know that the series converges to the exact value of the signal at every point. This is where the Dirichlet conditions come in. They are more restrictive but provide a stronger guarantee about the specific values of the signal. Let’s look at each of these conditions."
  },
  {
    "objectID": "ss_34.html#dirichlet-condition-1-absolute-integrability",
    "href": "ss_34.html#dirichlet-condition-1-absolute-integrability",
    "title": "Signals and Systems",
    "section": "Dirichlet Condition 1: Absolute Integrability",
    "text": "Dirichlet Condition 1: Absolute Integrability\n\nOver any period, \\(x(t)\\) must be absolutely integrable: \\[ \\int_{T}|x(t)| d t&lt;\\infty \\]\nEnsures: Each coefficient \\(a_k\\) will be finite. \\[ \\left|a_{k}\\right| \\leq \\frac{1}{T} \\int_{T}|x(t)| d t \\]\nViolation Example: \\(x(t) = 1/t\\) for \\(0 &lt; t \\leq 1\\), periodic with \\(T=1\\).\n\nThe integral \\(\\int_0^1 (1/t) dt\\) diverges (see Figure 3.8a in textbook).\n\n\n\nThis condition prevents signals from having “infinite area” over a period, which would make the Fourier coefficients infinite. The example \\(x(t)=1/t\\) blows up at \\(t=0\\), making its absolute integral infinite. Such signals are generally not physically realizable or encountered in typical ECE contexts."
  },
  {
    "objectID": "ss_34.html#dirichlet-condition-2-bounded-variation",
    "href": "ss_34.html#dirichlet-condition-2-bounded-variation",
    "title": "Signals and Systems",
    "section": "Dirichlet Condition 2: Bounded Variation",
    "text": "Dirichlet Condition 2: Bounded Variation\n\nIn any finite interval of time, \\(x(t)\\) must have a finite number of maxima and minima during any single period.\nViolation Example: \\(x(t) = \\sin(2\\pi/t)\\) for \\(0 &lt; t \\leq 1\\), periodic with \\(T=1\\).\n\nThis function has an infinite number of oscillations (maxima and minima) as \\(t \\rightarrow 0\\) (see Figure 3.8b in textbook).\n\n\n\nThis condition essentially means the signal can’t oscillate infinitely fast or have an infinite number of “wiggles” within a finite time. The example given, \\(\\sin(2\\pi/t)\\), illustrates this perfectly. As t approaches zero, the argument of the sine function goes to infinity, causing infinitely many oscillations. Again, this is a highly pathological case not typically seen in practical signals."
  },
  {
    "objectID": "ss_34.html#dirichlet-condition-3-finite-discontinuities",
    "href": "ss_34.html#dirichlet-condition-3-finite-discontinuities",
    "title": "Signals and Systems",
    "section": "Dirichlet Condition 3: Finite Discontinuities",
    "text": "Dirichlet Condition 3: Finite Discontinuities\n\nIn any finite interval of time, there are only a finite number of discontinuities.\nFurthermore, each of these discontinuities must be finite.\nViolation Example: A signal with an infinite number of increasingly smaller sections, each introducing a discontinuity (see Figure 3.8c in textbook).\n\n\n\n\n\n\n\nNote\n\n\nMost physically generated or processed signals in ECE satisfy all three Dirichlet conditions.\n\n\n\n\nThis condition is about the “smoothness” of the signal in terms of its jumps. A signal cannot have infinitely many sudden jumps within a finite period. The example describes a signal that gets progressively more complex with an infinite number of steps, violating this condition. In engineering, we generally deal with signals that have a finite number of step changes or impulses within any given time frame."
  },
  {
    "objectID": "ss_34.html#practical-implications-of-dirichlet-conditions",
    "href": "ss_34.html#practical-implications-of-dirichlet-conditions",
    "title": "Signals and Systems",
    "section": "Practical Implications of Dirichlet Conditions",
    "text": "Practical Implications of Dirichlet Conditions\n\n\nFor Signals Satisfying Dirichlet Conditions: - Fourier series converges and equals \\(x(t)\\) everywhere except at isolated discontinuities. - At discontinuities, the series converges to the average value of the signal on either side.\n\nWhy it matters for ECE: - Signals differing only at isolated points have identical integrals. - They behave identically under convolution. - Therefore, they are considered equivalent for LTI system analysis.\n\n\nThis is a crucial takeaway for our field. Even with discontinuities, the Fourier series representation is incredibly robust. The differences are so localized that they don’t affect integral properties, which are fundamental to how LTI systems process signals (e.g., convolution involves integration). From a system’s perspective, these signals are effectively the same."
  },
  {
    "objectID": "ss_34.html#the-gibbs-phenomenon-an-introduction",
    "href": "ss_34.html#the-gibbs-phenomenon-an-introduction",
    "title": "Signals and Systems",
    "section": "The Gibbs Phenomenon: An Introduction",
    "text": "The Gibbs Phenomenon: An Introduction\n\nDiscovered by Michelson (1898) and explained by Gibbs (1899).\nIt describes the peculiar behavior of the Fourier series approximation near discontinuities.\nObservation: When approximating a discontinuous signal (like a square wave) with a finite Fourier series, ripples and overshoot occur at the discontinuities.\n\n\nMichelson, a physicist, built a harmonic analyzer and was puzzled by the results when feeding it a square wave. He thought his machine was broken! He contacted Josiah Gibbs, a prominent mathematical physicist, who then provided the full explanation. This phenomenon highlights a key characteristic of approximating discontinuous functions with smooth sinusoids."
  },
  {
    "objectID": "ss_34.html#visualizing-the-gibbs-phenomenon",
    "href": "ss_34.html#visualizing-the-gibbs-phenomenon",
    "title": "Signals and Systems",
    "section": "Visualizing the Gibbs Phenomenon",
    "text": "Visualizing the Gibbs Phenomenon\n\nConsider a symmetric square wave.\nWe’ll observe its finite Fourier series approximation, \\(x_N(t)\\), as \\(N\\) increases.\nNotice the overshoot and ripples near the edges of the square wave.\n\n\nviewof N = Inputs.range([1, 100], {step: 2, value: 5, label: \"Number of terms (N)\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere we have an interactive plot. You can adjust the number of terms, N, using the slider. As you increase N, you’ll see the approximation get closer to the square wave. However, pay close attention to the regions around the discontinuities, where the signal jumps. You’ll notice persistent ripples and an overshoot, regardless of how large N becomes. This is the essence of the Gibbs phenomenon. This visualization directly relates to Figure 3.9 in your textbook."
  },
  {
    "objectID": "ss_34.html#understanding-the-gibbs-phenomenon",
    "href": "ss_34.html#understanding-the-gibbs-phenomenon",
    "title": "Signals and Systems",
    "section": "Understanding the Gibbs Phenomenon",
    "text": "Understanding the Gibbs Phenomenon\n\nOvershoot: For a discontinuity of unity height, the peak amplitude of the ripple is approximately 1.09 (an overshoot of ~9%).\n\nThis overshoot does not decrease with increasing \\(N\\).\n\nRipples: As \\(N\\) increases, the ripples become compressed closer to the discontinuity.\n\nThe width of the ripples decreases, but their peak amplitude remains constant.\n\nConvergence at a point: For any fixed \\(t_1\\), \\(x_N(t_1)\\) will converge to \\(x(t_1)\\) as \\(N \\rightarrow \\infty\\) (or to the average at a discontinuity).\n\nHowever, the closer \\(t_1\\) is to a discontinuity, the larger \\(N\\) must be for the error to be acceptably small.\n\n\n\nIt’s crucial to distinguish between point-wise convergence and the behavior of the peak overshoot. While the Fourier series does converge to the correct value at any given point (including the midpoint of a discontinuity), the maximum error, the overshoot, persists in the vicinity of the discontinuity. This means that even with millions of terms, you’ll still see that 9% overshoot, just squeezed into a smaller and smaller region around the jump. This is why it’s a “phenomenon.”"
  },
  {
    "objectID": "ss_34.html#practical-significance-of-gibbs-phenomenon",
    "href": "ss_34.html#practical-significance-of-gibbs-phenomenon",
    "title": "Signals and Systems",
    "section": "Practical Significance of Gibbs Phenomenon",
    "text": "Practical Significance of Gibbs Phenomenon\n\nIn real-world applications, if a truncated Fourier series \\(x_N(t)\\) is used to approximate a discontinuous signal:\n\nExpect high-frequency ripples and overshoot near discontinuities.\n\nEngineering Consideration:\n\nChoose a sufficiently large \\(N\\) so that the total energy in these ripples becomes insignificant.\nWhile the peak overshoot remains, the energy contained within the shrinking ripple region diminishes.\n\n\n\n\n\n\n\n\nCaution\n\n\nFor applications sensitive to peak values (e.g., driving an amplifier to saturation), the Gibbs phenomenon can be a critical design consideration.\n\n\n\n\nThe Gibbs phenomenon is not a failure of Fourier series, but rather a characteristic of approximating discontinuous signals with continuous functions. It means that if your system is sensitive to instantaneous peak values, you need to be aware that a Fourier series approximation will likely exceed the original signal’s amplitude at the edges. However, for most applications that depend on average power or integrated energy, the effect becomes negligible with enough terms, as the ripples become very narrow."
  },
  {
    "objectID": "ss_34.html#conclusion-convergence-and-practicality",
    "href": "ss_34.html#conclusion-convergence-and-practicality",
    "title": "Signals and Systems",
    "section": "Conclusion: Convergence and Practicality",
    "text": "Conclusion: Convergence and Practicality\n\nMost practical signals in ECE have Fourier series representations.\n\nGuaranteed by finite energy or Dirichlet conditions.\n\nFourier series provides the best least-squares approximation for a finite number of terms.\nThe Gibbs Phenomenon highlights a limitation for discontinuous signals:\n\nPersistent overshoot and ripples near discontinuities, even as \\(N \\rightarrow \\infty\\).\nRipples compress, but peak amplitude remains constant (for a given discontinuity height).\n\nDespite Gibbs, Fourier series remains an indispensable tool for analyzing and synthesizing signals in ECE.\n\n\nTo wrap up, the Fourier series is an incredibly powerful tool in signals and systems. It allows us to decompose complex periodic signals into simpler, harmonically related components. While convergence is generally guaranteed for signals we encounter in practice, understanding the nuances like the Gibbs phenomenon is key to applying these tools effectively and interpreting their results correctly in engineering contexts. This knowledge helps us design better filters, understand system responses, and accurately model signal behavior."
  },
  {
    "objectID": "ss_36.html#introduction-discrete-time-periodic-signals",
    "href": "ss_36.html#introduction-discrete-time-periodic-signals",
    "title": "Signals and Systems",
    "section": "Introduction: Discrete-Time Periodic Signals",
    "text": "Introduction: Discrete-Time Periodic Signals\nDiscrete-time (DT) signals \\(x[n]\\) are periodic with period \\(N\\) if \\(x[n] = x[n+N]\\). The fundamental period \\(N\\) is the smallest positive integer for which this holds. The fundamental frequency is \\(\\omega_0 = 2\\pi/N\\).\n\n\n\n\n\n\nNote\n\n\nKey Difference from Continuous-Time (CT): The Fourier series representation for discrete-time signals is a finite series, unlike the infinite series required for continuous-time signals. This simplifies convergence issues significantly.\n\n\n\n\nToday, we’re diving into the discrete-time Fourier series. Recall from Chapter 1 that a discrete-time signal is periodic if it repeats itself after a certain number of samples, denoted by \\(N\\). The fundamental frequency \\(\\omega_0\\) is directly related to this period \\(N\\). A crucial point to remember from the outset is that the DTFS is a finite sum, which has profound implications, especially regarding convergence, as we’ll see later."
  },
  {
    "objectID": "ss_36.html#harmonically-related-complex-exponentials",
    "href": "ss_36.html#harmonically-related-complex-exponentials",
    "title": "Signals and Systems",
    "section": "Harmonically Related Complex Exponentials",
    "text": "Harmonically Related Complex Exponentials\nThe set of all discrete-time complex exponential signals periodic with period \\(N\\) is given by:\n\\[ \\phi_k[n] = e^{j k \\omega_0 n} = e^{j k(2\\pi/N)n}, \\quad k = 0, \\pm 1, \\pm 2, \\ldots \\]\nThere are only \\(N\\) distinct signals in this set. This is because discrete-time complex exponentials differing by a multiple of \\(2\\pi\\) in frequency are identical.\n\\[ \\phi_k[n] = \\phi_{k+rN}[n] \\]\nThis means \\(e^{j k(2\\pi/N)n}\\) and \\(e^{j (k+N)(2\\pi/N)n}\\) are the same sequence.\n\nJust like in continuous time, we build our periodic signals from harmonically related complex exponentials. However, discrete-time is special. While \\(k\\) can theoretically go from negative to positive infinity, only \\(N\\) of these complex exponentials are unique. After \\(N\\) terms, the sequence of complex exponentials repeats itself. This is a direct consequence of the periodic nature of discrete-time exponentials. Let’s see an example."
  },
  {
    "objectID": "ss_36.html#demonstrating-distinct-exponentials",
    "href": "ss_36.html#demonstrating-distinct-exponentials",
    "title": "Signals and Systems",
    "section": "Demonstrating Distinct Exponentials",
    "text": "Demonstrating Distinct Exponentials\nLet’s visualize how \\(\\phi_k[n]\\) repeats for \\(k\\) and \\(k+N\\). Consider \\(N=5\\). We expect \\(\\phi_0[n] = \\phi_5[n]\\), \\(\\phi_1[n] = \\phi_6[n]\\), etc.\n\n\n\n\n\n\nThe plots show the real and imaginary parts of \\(\\phi_k[n]\\) for \\(k=1\\) and \\(k=1+N\\) (with \\(N=5\\)). Observe that the two plots are identical, confirming that \\(\\phi_k[n] = \\phi_{k+N}[n]\\).\n\n\n\n\n\n\nTip\n\n\nInteractive Element: Feel free to change \\(N_{val}\\) and \\(k\\) (by modifying the get_phi_k calls) in the Python code to explore this property further! For example, try \\(k=0\\) and \\(k=N_{val}\\).\n\n\n\n\nHere, we use Python to plot two complex exponentials. The top plot shows \\(\\phi_1[n]\\) for \\(N=5\\). The bottom plot shows \\(\\phi_{1+5}[n]\\), which is \\(\\phi_6[n]\\). As you can clearly see, both plots produce the exact same sequence of real and imaginary values. This visually demonstrates the periodicity of the discrete-time complex exponentials in the index \\(k\\). Try changing the value of N_val or the k value in the get_phi_k calls in the code block to see how this relationship holds. For instance, set k_val=0 and observe k_val=N_val for any N_val."
  },
  {
    "objectID": "ss_36.html#discrete-time-fourier-series-dtfs-synthesis",
    "href": "ss_36.html#discrete-time-fourier-series-dtfs-synthesis",
    "title": "Signals and Systems",
    "section": "Discrete-Time Fourier Series (DTFS) Synthesis",
    "text": "Discrete-Time Fourier Series (DTFS) Synthesis\nA periodic discrete-time signal \\(x[n]\\) can be represented as a linear combination of these harmonically related complex exponentials.\n\\[ x[n] = \\sum_{k=\\langle N\\rangle} a_k \\phi_k[n] = \\sum_{k=\\langle N\\rangle} a_k e^{j k \\omega_0 n} = \\sum_{k=\\langle N\\rangle} a_k e^{j k(2\\pi/N)n} \\]\nThe notation \\(\\sum_{k=\\langle N\\rangle}\\) indicates summation over any set of \\(N\\) successive integers for \\(k\\). Common choices include \\(k=0, 1, \\ldots, N-1\\) or \\(k=- (N-1)/2, \\ldots, (N-1)/2\\) (for odd \\(N\\)).\nThe coefficients \\(a_k\\) are called the Fourier series coefficients.\n\nNow that we understand the building blocks, we can construct any periodic discrete-time signal using a sum of these exponentials. This equation is the synthesis equation. Notice the summation is over \\(N\\) terms. This is because, as we just saw, there are only \\(N\\) distinct complex exponentials. The specific range for \\(k\\) doesn’t change the set of exponentials used, only which \\(N\\) coefficients we’re solving for."
  },
  {
    "objectID": "ss_36.html#determination-of-dtfs-coefficients-12",
    "href": "ss_36.html#determination-of-dtfs-coefficients-12",
    "title": "Signals and Systems",
    "section": "Determination of DTFS Coefficients (1/2)",
    "text": "Determination of DTFS Coefficients (1/2)\nGiven \\(x[n]\\) periodic with fundamental period \\(N\\), we want to find \\(a_k\\). We can solve a system of \\(N\\) linear equations, but a more direct method exists.\nThe key identity (similar to orthogonality in CT):\n\\[ \\sum_{n=\\langle N\\rangle} e^{j k(2\\pi/N)n} = \\begin{cases} N, & k = 0, \\pm N, \\pm 2N, \\ldots \\\\ 0, & \\text{otherwise} \\end{cases} \\]\nThis identity is crucial for isolating each \\(a_k\\).\n\nHow do we find these \\(a_k\\) coefficients? Similar to continuous time, we use an orthogonality property. This specific identity states that if you sum a complex exponential over one period, the result is zero unless the exponential is a constant (i.e., its frequency is a multiple of \\(2\\pi\\)). When it’s a constant, the sum is simply \\(N\\) times the constant. Let’s verify this interactively."
  },
  {
    "objectID": "ss_36.html#interactive-identity-verification",
    "href": "ss_36.html#interactive-identity-verification",
    "title": "Signals and Systems",
    "section": "Interactive Identity Verification",
    "text": "Interactive Identity Verification\nLet’s verify the summation identity: \\(\\sum_{n=\\langle N\\rangle} e^{j k(2\\pi/N)n}\\).\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nExperiment:\nChange N_period and k_val in the code above and run it. Observe the sum. What happens when k_val is a multiple of N_period? What happens otherwise?\n\n\n\n\nThis interactive code block allows you to directly test the identity. When k_val is 0, the exponential is \\(e^0 = 1\\), and summing \\(N\\) ones gives \\(N\\). When k_val is a multiple of \\(N\\), for example \\(k=N\\), the exponential is \\(e^{jN(2\\pi/N)n} = e^{j2\\pi n} = 1\\) for all integer \\(n\\), so the sum is again \\(N\\). For any other \\(k\\), the terms \\(e^{j k(2\\pi/N)n}\\) will trace a full circle (or multiple full circles) in the complex plane, summing to zero over one period. This is a fundamental property."
  },
  {
    "objectID": "ss_36.html#determination-of-dtfs-coefficients-22",
    "href": "ss_36.html#determination-of-dtfs-coefficients-22",
    "title": "Signals and Systems",
    "section": "Determination of DTFS Coefficients (2/2)",
    "text": "Determination of DTFS Coefficients (2/2)\nTo derive \\(a_k\\):\n\nMultiply the synthesis equation by \\(e^{-j r(2\\pi/N)n}\\).\nSum over one period \\(n=\\langle N\\rangle\\).\n\n\\[ \\sum_{n=\\langle N\\rangle} x[n] e^{-j r(2\\pi/N)n} = \\sum_{n=\\langle N\\rangle} \\sum_{k=\\langle N\\rangle} a_k e^{j(k-r)(2\\pi/N)n} \\]\nInterchanging summation order and applying the identity:\n\\[ \\sum_{n=\\langle N\\rangle} x[n] e^{-j r(2\\pi/N)n} = \\sum_{k=\\langle N\\rangle} a_k \\left( \\sum_{n=\\langle N\\rangle} e^{j(k-r)(2\\pi/N)n} \\right) \\]\nThe inner sum is \\(N\\) if \\(k-r\\) is a multiple of \\(N\\) (i.e., \\(k=r\\) within the \\(\\langle N \\rangle\\) range), and \\(0\\) otherwise. This simplifies to \\(N a_r\\), leading to:\n\\[ a_k = \\frac{1}{N} \\sum_{n=\\langle N\\rangle} x[n] e^{-j k(2\\pi/N)n} \\]\nThis is the Discrete-Time Fourier Series Analysis Equation.\n\nWith that identity established, the derivation of the analysis equation is straightforward. We multiply the synthesis equation by a specific complex exponential, \\(e^{-j r(2\\pi/N)n}\\), and sum over one period. This process effectively “filters out” all terms except for the one corresponding to \\(a_r\\). The result is a direct formula to compute each \\(a_k\\) from the signal \\(x[n]\\)."
  },
  {
    "objectID": "ss_36.html#dtfs-synthesis-and-analysis-pair",
    "href": "ss_36.html#dtfs-synthesis-and-analysis-pair",
    "title": "Signals and Systems",
    "section": "DTFS Synthesis and Analysis Pair",
    "text": "DTFS Synthesis and Analysis Pair\nThese two equations form the Discrete-Time Fourier Series pair:\n\n\nSynthesis Equation:\n(How to build \\(x[n]\\) from coefficients \\(a_k\\))\n\\[ x[n] = \\sum_{k=\\langle N\\rangle} a_k e^{j k \\omega_0 n} = \\sum_{k=\\langle N\\rangle} a_k e^{j k(2\\pi/N)n} \\]\n\nAnalysis Equation:\n(How to find coefficients \\(a_k\\) from \\(x[n]\\))\n\\[ a_k = \\frac{1}{N} \\sum_{n=\\langle N\\rangle} x[n] e^{-j k \\omega_0 n} = \\frac{1}{N} \\sum_{n=\\langle N\\rangle} x[n] e^{-j k(2\\pi/N)n} \\]\n\n\n\n\n\n\n\nImportant\n\n\nThe Fourier series coefficients \\(a_k\\) are periodic with period \\(N\\): \\(a_k = a_{k+N}\\). This is a direct consequence of the periodicity of \\(\\phi_k[n]\\).\n\n\n\n\nHere we have the complete Discrete-Time Fourier Series pair. The synthesis equation tells us how to construct the signal from its frequency components, and the analysis equation tells us how to decompose the signal into those components. Crucially, remember the coefficients \\(a_k\\) themselves are periodic with period \\(N\\). This means if you calculate \\(a_0, a_1, \\ldots, a_{N-1}\\), you automatically know \\(a_N, a_{N+1}\\), etc., because they simply repeat."
  },
  {
    "objectID": "ss_36.html#example-3.10-discrete-time-sine-wave",
    "href": "ss_36.html#example-3.10-discrete-time-sine-wave",
    "title": "Signals and Systems",
    "section": "Example 3.10: Discrete-Time Sine Wave",
    "text": "Example 3.10: Discrete-Time Sine Wave\nConsider the signal \\(x[n] = \\sin(\\omega_0 n)\\). This signal is periodic only if \\(2\\pi/\\omega_0\\) is an integer (or ratio of integers). If \\(\\omega_0 = 2\\pi/N\\), then \\(x[n]\\) is periodic with fundamental period \\(N\\).\nWe can expand \\(x[n]\\) using Euler’s formula:\n\\[ x[n] = \\frac{1}{2j} e^{j(2\\pi/N)n} - \\frac{1}{2j} e^{-j(2\\pi/N)n} \\]\nBy comparing this with the synthesis equation \\(x[n] = \\sum_{k=\\langle N\\rangle} a_k e^{j k(2\\pi/N)n}\\), we can identify the coefficients:\n\\[ a_1 = \\frac{1}{2j}, \\quad a_{-1} = -\\frac{1}{2j} \\]\nAll other \\(a_k\\) (for \\(k\\) within one period) are zero.\n\nLet’s apply our new tools to an example. A discrete-time sine wave is only periodic under specific conditions related to its frequency. If \\(\\omega_0\\) is a multiple of \\(2\\pi/N\\), it’s periodic. By expressing the sine wave as a sum of two complex exponentials, we can directly compare it to the Fourier series synthesis equation and pick out the coefficients by inspection. This is a very common technique when the signal is already in the form of complex exponentials."
  },
  {
    "objectID": "ss_36.html#example-3.10-sine-wave-dtfs-visualization",
    "href": "ss_36.html#example-3.10-sine-wave-dtfs-visualization",
    "title": "Signals and Systems",
    "section": "Example 3.10: Sine Wave DTFS Visualization",
    "text": "Example 3.10: Sine Wave DTFS Visualization\nIf \\(\\omega_0 = (2\\pi M)/N\\) (where \\(M, N\\) are coprime), \\(x[n]\\) has fundamental period \\(N\\).\nThen, \\(a_M = \\frac{1}{2j}\\), \\(a_{-M} = -\\frac{1}{2j}\\), and other \\(a_k=0\\) within one period.\n\nN_val = viewof n_slider\nM_val = viewof m_slider\n\nn_slider = html`&lt;div style=\"display:flex; align-items:center;\"&gt;\n                  &lt;label style=\"margin-right:10px;\"&gt;N:&lt;/label&gt;\n                  &lt;input type=\"range\" min=\"5\" max=\"20\" value=\"5\" step=\"1\"&gt;\n                  &lt;span style=\"margin-left:10px;\"&gt;${N_val}&lt;/span&gt;\n                &lt;/div&gt;`\nm_slider = html`&lt;div style=\"display:flex; align-items:center;\"&gt;\n                  &lt;label style=\"margin-right:10px;\"&gt;M:&lt;/label&gt;\n                  &lt;input type=\"range\" min=\"1\" max=\"4\" value=\"1\" step=\"1\"&gt;\n                  &lt;span style=\"margin-left:10px;\"&gt;${M_val}&lt;/span&gt;\n                &lt;/div&gt;`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive plot visualizes the Fourier series coefficients for a discrete-time sine wave. You can adjust \\(N\\) (the period) and \\(M\\) (the multiplier for the fundamental frequency) using the sliders. Observe how only two coefficients are non-zero within any given period of \\(N\\). When \\(M=1\\), you see coefficients at \\(k=1\\) and \\(k=N-1\\) (which is equivalent to \\(k=-1\\)). When \\(M\\) changes, the locations of these non-zero coefficients shift to \\(k=M\\) and \\(k=N-M\\). Notice the magnitudes are always 0.5, and the phases are \\(\\pm 90^\\circ\\) (or \\(\\pm \\pi/2\\) radians), which corresponds to \\(\\pm j/2\\). This clearly illustrates the spectral content of a discrete-time sine wave."
  },
  {
    "objectID": "ss_36.html#example-3.11-more-complex-signal",
    "href": "ss_36.html#example-3.11-more-complex-signal",
    "title": "Signals and Systems",
    "section": "Example 3.11: More Complex Signal",
    "text": "Example 3.11: More Complex Signal\nConsider \\(x[n]=1+\\sin\\left(\\frac{2\\pi}{N}\\right)n+3\\cos\\left(\\frac{2\\pi}{N}\\right)n+\\cos\\left(\\frac{4\\pi}{N}n+\\frac{\\pi}{2}\\right)\\). This signal is periodic with period \\(N\\). We expand each term into complex exponentials:\n\\(1 \\implies a_0 = 1\\)\n\\(\\sin\\left(\\frac{2\\pi}{N}\\right)n \\implies a_1 = \\frac{1}{2j}, a_{-1} = -\\frac{1}{2j}\\)\n\\(3\\cos\\left(\\frac{2\\pi}{N}\\right)n \\implies a_1 = \\frac{3}{2}, a_{-1} = \\frac{3}{2}\\)\n\\(\\cos\\left(\\frac{4\\pi}{N}n+\\frac{\\pi}{2}\\right) \\implies \\frac{1}{2}e^{j\\pi/2}e^{j2(2\\pi/N)n} + \\frac{1}{2}e^{-j\\pi/2}e^{-j2(2\\pi/N)n}\\)\n\\(\\implies a_2 = \\frac{1}{2}e^{j\\pi/2} = \\frac{1}{2}j, a_{-2} = \\frac{1}{2}e^{-j\\pi/2} = -\\frac{1}{2}j\\)\nCombining terms for each \\(k\\): \\(a_0 = 1\\)\n\\(a_1 = \\frac{1}{2j} + \\frac{3}{2} = \\frac{3}{2} - \\frac{1}{2}j\\)\n\\(a_{-1} = -\\frac{1}{2j} + \\frac{3}{2} = \\frac{3}{2} + \\frac{1}{2}j\\)\n\\(a_2 = \\frac{1}{2}j\\)\n\\(a_{-2} = -\\frac{1}{2}j\\)\nAll other \\(a_k=0\\) within the period.\n\n\n\n\n\n\nTip\n\n\nProperty for Real Signals: For a real signal \\(x[n]\\), the Fourier coefficients exhibit conjugate symmetry: \\(a_{-k} = a_k^*\\). Verify this for the calculated coefficients!\n\n\n\n\nThis example shows how to find Fourier coefficients for a more complex signal by breaking it down into its constituent complex exponentials. Each sinusoidal component contributes two complex exponential terms, and the constant term contributes to \\(a_0\\). We then sum the coefficients for each harmonic index \\(k\\). Notice that \\(a_{-1}\\) is the complex conjugate of \\(a_1\\), and \\(a_{-2}\\) is the complex conjugate of \\(a_2\\). This is a general property for real-valued signals, known as conjugate symmetry."
  },
  {
    "objectID": "ss_36.html#example-3.11-coefficients-visualization",
    "href": "ss_36.html#example-3.11-coefficients-visualization",
    "title": "Signals and Systems",
    "section": "Example 3.11: Coefficients Visualization",
    "text": "Example 3.11: Coefficients Visualization\nVisualizing the real, imaginary, magnitude, and phase of the coefficients for \\(N=10\\).\n\n\n\n\n\n\n\nHere we see a visual representation of the coefficients calculated in the previous slide. The top-left plot shows the real parts, and the bottom-left shows the imaginary parts. Notice the symmetry: Real parts are even (\\(Re\\{a_k\\} = Re\\{a_{-k}\\}\\)), and imaginary parts are odd (\\(Im\\{a_k\\} = -Im\\{a_{-k}\\}\\)). On the right, we have the magnitude and phase. Magnitudes are even (\\(|a_k| = |a_{-k}|\\)), and phases are odd (\\(Arg\\{a_k\\} = -Arg\\{a_{-k}\\}\\)). This visual confirms the conjugate symmetry property for real signals: \\(a_{-k} = a_k^*\\)."
  },
  {
    "objectID": "ss_36.html#example-3.12-discrete-time-periodic-square-wave",
    "href": "ss_36.html#example-3.12-discrete-time-periodic-square-wave",
    "title": "Signals and Systems",
    "section": "Example 3.12: Discrete-Time Periodic Square Wave",
    "text": "Example 3.12: Discrete-Time Periodic Square Wave\nConsider a discrete-time periodic square wave \\(x[n]\\) with period \\(N\\). It is defined as \\(x[n]=1\\) for \\(-N_1 \\leq n \\leq N_1\\), and \\(x[n]=0\\) otherwise within one period.\nThe analysis equation is: \\[ a_k = \\frac{1}{N} \\sum_{n=\\langle N\\rangle} x[n] e^{-j k(2\\pi/N)n} \\]\nChoosing the summation range from \\(-N_1\\) to \\(N_1\\):\n\\[ a_k = \\frac{1}{N} \\sum_{n=-N_1}^{N_1} e^{-j k(2\\pi/N)n} \\]\nThis sum is a geometric series.\n\nLet’s analyze a common signal: the discrete-time periodic square wave. This signal is 1 for a certain duration around \\(n=0\\) and 0 otherwise within its period. To find its Fourier coefficients, we plug its definition into the analysis equation. The summation simplifies because \\(x[n]\\) is only non-zero for a specific range. The sum then becomes a geometric series, which we can solve using a known formula."
  },
  {
    "objectID": "ss_36.html#example-3.12-square-wave-coefficients",
    "href": "ss_36.html#example-3.12-square-wave-coefficients",
    "title": "Signals and Systems",
    "section": "Example 3.12: Square Wave Coefficients",
    "text": "Example 3.12: Square Wave Coefficients\nApplying the geometric series sum formula and simplifying, we get:\n\\[ a_k = \\frac{1}{N} \\frac{\\sin\\left[2\\pi k(N_1+1/2)/N\\right]}{\\sin(\\pi k/N)}, \\quad k \\neq 0, \\pm N, \\pm 2N, \\ldots \\]\nAnd for \\(k=0, \\pm N, \\pm 2N, \\ldots\\):\n\\[ a_k = \\frac{2N_1+1}{N} \\]\nThis formula resembles the continuous-time sinc function, but uses \\(\\sin(\\cdot)/\\sin(\\cdot)\\) due to discrete nature.\n\nAfter some algebraic manipulation, which involves using the geometric series sum formula and Euler’s identity, we arrive at this closed-form expression for the Fourier coefficients. It looks similar to the sinc function we encountered in continuous-time, but it’s a ratio of sines. This is typical for discrete-time Fourier transforms and series, as the discrete nature often leads to these periodic sinc-like functions. Note the special case for \\(k=0\\) (and its multiples), where the denominator would be zero if not handled separately. At \\(k=0\\), \\(a_0\\) represents the average value of the signal over one period, which is simply the number of non-zero samples (\\(2N_1+1\\)) divided by the total period (\\(N\\))."
  },
  {
    "objectID": "ss_36.html#example-3.12-coefficients-visualization",
    "href": "ss_36.html#example-3.12-coefficients-visualization",
    "title": "Signals and Systems",
    "section": "Example 3.12: Coefficients Visualization",
    "text": "Example 3.12: Coefficients Visualization\nLet’s visualize the coefficients for \\(2N_1+1=5\\) and varying \\(N\\).\n\nN_val_sq = viewof n_sq_slider\nN1_val_sq = viewof n1_sq_slider\n\nn_sq_slider = html`&lt;div style=\"display:flex; align-items:center;\"&gt;\n                  &lt;label style=\"margin-right:10px;\"&gt;N (Period):&lt;/label&gt;\n                  &lt;input type=\"range\" min=\"10\" max=\"40\" value=\"10\" step=\"5\"&gt;\n                  &lt;span style=\"margin-left:10px;\"&gt;${N_val_sq}&lt;/span&gt;\n                &lt;/div&gt;`\nn1_sq_slider = html`&lt;div style=\"display:flex; align-items:center;\"&gt;\n                  &lt;label style=\"margin-right:10px;\"&gt;N1 (Pulse Half-Width):&lt;/label&gt;\n                  &lt;input type=\"range\" min=\"1\" max=\"5\" value=\"2\" step=\"1\"&gt; &lt;!-- 2N1+1 = 5 when N1=2 --&gt;\n                  &lt;span style=\"margin-left:10px;\"&gt;${N1_val_sq}&lt;/span&gt;\n                &lt;/div&gt;`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive plot lets you see how the Fourier coefficients of a discrete-time square wave change with the period \\(N\\) and the pulse width \\(N_1\\). The shape of the coefficients resembles a sampled sinc function. As \\(N\\) increases (making the signal sparser in time), the coefficients become more closely spaced in frequency. The main lobe width is inversely proportional to the pulse width. Experiment with the sliders to see these effects. For instance, notice how the \\(a_k\\) values are scaled by \\(1/N\\)."
  },
  {
    "objectID": "ss_36.html#convergence-of-discrete-time-fourier-series",
    "href": "ss_36.html#convergence-of-discrete-time-fourier-series",
    "title": "Signals and Systems",
    "section": "Convergence of Discrete-Time Fourier Series",
    "text": "Convergence of Discrete-Time Fourier Series\nUnlike continuous-time Fourier series, discrete-time Fourier series have no convergence issues. The DTFS is a finite sum of \\(N\\) terms. A discrete-time periodic sequence \\(x[n]\\) is completely specified by its \\(N\\) values over one period. The DTFS analysis equation transforms these \\(N\\) values into \\(N\\) Fourier coefficients. The DTFS synthesis equation perfectly reconstructs the original \\(N\\) values from these \\(N\\) coefficients.\n\n\n\n\n\n\nImportant\n\n\nThere is no Gibbs phenomenon in discrete-time Fourier series. The partial sum (if it includes all \\(N\\) distinct terms) will exactly equal \\(x[n]\\).\n\n\n\n\nThis is a critical distinction between continuous-time and discrete-time Fourier series. Because a discrete-time signal is defined by a finite number of samples within a period, its Fourier series also has a finite number of terms. This means that if you sum up all \\(N\\) terms of the DTFS, you will exactly reconstruct the original signal. There are no approximation errors, no ripples at discontinuities, and therefore, no Gibbs phenomenon. This makes DTFS mathematically much simpler in terms of convergence."
  },
  {
    "objectID": "ss_36.html#dtfs-reconstruction-no-gibbs-phenomenon",
    "href": "ss_36.html#dtfs-reconstruction-no-gibbs-phenomenon",
    "title": "Signals and Systems",
    "section": "DTFS Reconstruction: No Gibbs Phenomenon",
    "text": "DTFS Reconstruction: No Gibbs Phenomenon\nLet’s reconstruct the square wave using partial sums. \\(\\hat{x}[n] = \\sum_{k=-M}^{M} a_k e^{j k(2\\pi/N)n}\\) (for odd \\(N\\), \\(M=(N-1)/2\\) includes all terms).\n\nN_rec = viewof n_rec_slider\nN1_rec = viewof n1_rec_slider\nM_rec = viewof m_rec_slider\n\nn_rec_slider = html`&lt;div style=\"display:flex; align-items:center;\"&gt;\n                  &lt;label style=\"margin-right:10px;\"&gt;N (Period, odd):&lt;/label&gt;\n                  &lt;input type=\"range\" min=\"5\" max=\"15\" value=\"9\" step=\"2\"&gt;\n                  &lt;span style=\"margin-left:10px;\"&gt;${N_rec}&lt;/span&gt;\n                &lt;/div&gt;`\nn1_rec_slider = html`&lt;div style=\"display:flex; align-items:center;\"&gt;\n                  &lt;label style=\"margin-right:10px;\"&gt;N1 (Pulse Half-Width):&lt;/label&gt;\n                  &lt;input type=\"range\" min=\"1\" max=\"3\" value=\"2\" step=\"1\"&gt; &lt;!-- 2N1+1 should be &lt;= N --&gt;\n                  &lt;span style=\"margin-left:10px;\"&gt;${N1_rec}&lt;/span&gt;\n                &lt;/div&gt;`\nm_rec_slider = html`&lt;div style=\"display:flex; align-items:center;\"&gt;\n                  &lt;label style=\"margin-right:10px;\"&gt;M (Summation Terms):&lt;/label&gt;\n                  &lt;input type=\"range\" min=\"0\" max=\"4\" value=\"0\" step=\"1\"&gt; &lt;!-- Max M for N=9, N1=2 is M=(9-1)/2 = 4 --&gt;\n                  &lt;span style=\"margin-left:10px;\"&gt;${M_rec}&lt;/span&gt;\n                &lt;/div&gt;`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive demonstration highlights the key difference in convergence. We plot the original square wave and its reconstruction \\(\\hat{x}[n]\\) using a partial sum of \\(2M+1\\) terms. Adjust the M slider. Notice that as M increases, the reconstructed signal gets closer to the original. When M reaches its maximum value, which corresponds to including all \\(N\\) unique terms (e.g., \\(M=(N-1)/2\\) for odd \\(N\\)), the reconstructed signal perfectly matches the original signal. There are no overshoots or undershoots, no Gibbs phenomenon, because the series is finite and exact. This is a powerful advantage of the discrete-time Fourier series."
  },
  {
    "objectID": "ss_36.html#summary-and-key-takeaways",
    "href": "ss_36.html#summary-and-key-takeaways",
    "title": "Signals and Systems",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\nDiscrete-Time Fourier Series (DTFS):\n\nRepresents periodic discrete-time signals \\(x[n]\\) with period \\(N\\).\nUses a finite sum of \\(N\\) harmonically related complex exponentials.\n\nKey Equations:\n\nSynthesis: \\(x[n] = \\sum_{k=\\langle N\\rangle} a_k e^{j k(2\\pi/N)n}\\)\nAnalysis: \\(a_k = \\frac{1}{N} \\sum_{n=\\langle N\\rangle} x[n] e^{-j k(2\\pi/N)n}\\)\n\nCrucial Differences from Continuous-Time (CTFS):\n\nOnly \\(N\\) distinct complex exponentials \\(\\phi_k[n]\\).\nFourier coefficients \\(a_k\\) are periodic with period \\(N\\) (\\(a_k = a_{k+N}\\)).\nNo convergence issues or Gibbs phenomenon. The finite sum perfectly reconstructs \\(x[n]\\).\n\n\nTo summarize, the Discrete-Time Fourier Series is a powerful tool for analyzing periodic discrete-time signals. Its finite nature is its most distinguishing feature, leading to exact reconstruction and avoiding the convergence complexities of its continuous-time counterpart. Understanding these differences is crucial for working with sampled signals and discrete systems. Thank you."
  }
]