[
  {
    "objectID": "rps.html#dosen-pengajar",
    "href": "rps.html#dosen-pengajar",
    "title": "Aljabar Linear",
    "section": "Dosen Pengajar",
    "text": "Dosen Pengajar\n\nImron Rosyadi (kelas A)\nAisya Nur Aulia Yusuf (kelas B)\nNorma Amalia (kelas C)\nYogi Ramadhani (kelas D)"
  },
  {
    "objectID": "rps.html#jadwal-kelas-google-meet",
    "href": "rps.html#jadwal-kelas-google-meet",
    "title": "Aljabar Linear",
    "section": "Jadwal, Kelas, Google Meet",
    "text": "Jadwal, Kelas, Google Meet\n\nMK Aljabar Linear - Kelas A\nLecturer: Imron Rosyadi\nSchedule: Monday, 13:00 – 15:40\nRoom: GEDUNG TEKNIK E 203\nGoogle Meet: meet.google.com/bqm-jnqt-yrw"
  },
  {
    "objectID": "rps.html#tautan-penting",
    "href": "rps.html#tautan-penting",
    "title": "Aljabar Linear",
    "section": "Tautan Penting",
    "text": "Tautan Penting\n\nCourse Page: imron-course.vercel.app\nPresentation: imron-slide.vercel.app\nDiscussion: discord.gg/6nNTvfmz, expired at 29 Aug 2025\nAttendance: teraversa.unsoed.ac.id\nOnline Learning: eldiru.unsoed.ac.id"
  },
  {
    "objectID": "rps.html#metode-pembelajaran",
    "href": "rps.html#metode-pembelajaran",
    "title": "Aljabar Linear",
    "section": "Metode Pembelajaran",
    "text": "Metode Pembelajaran\n\nEnglish-Indonesia: Salindia presentasi dalam bahasa Inggris, tetapi penyampaian menggunakan bahasa Indonesia.\nLuring-Daring: Pembelajaran dilakukan dengan tatap muka luring, tetapi diselingi secara daring. Cek Discord untuk informasi.\nVisualisasi-Interaktif: Pembelajaran akan menyediakan sumberdaya berupa visualisasi interaktif untuk memudahkan pemahaman. Cek berbagai link yang disediakan.\nCase-based Approach: Pembelajaran akan dikaitkan dengan topik bidang Electrical and Computer Engineering\nDiskusi-Informasi via Discord: Diskusikan materi, ajukan pertanyaan, langsung via Discord (channel #aljabar-linear-1).\nWeb-based Presentation: Salindia presentasi berbentuk laman web, tetapi anda bisa mengekspornya ke PDF.\nReview via Recording: Sesi pembelajaran akan (diusahakan untuk) direkam dan diunggah ke Youtube\nRileks: Pembelajaran dilakukan secara rileks."
  },
  {
    "objectID": "rps.html#buku-referensi",
    "href": "rps.html#buku-referensi",
    "title": "Aljabar Linear",
    "section": "Buku Referensi",
    "text": "Buku Referensi\n\nReferensi Utama: Anton, H., Rorres, C. (2013). Elementary Linear Algebra: Applications Version. John Wiley & Sons.\nReferensi Pendukung: Lay, D. C., Lay, S. R., McDonald, J. (2016). Linear Algebra and Its Applications. Pearson."
  },
  {
    "objectID": "rps.html#referensi-interaktif",
    "href": "rps.html#referensi-interaktif",
    "title": "Aljabar Linear",
    "section": "Referensi Interaktif",
    "text": "Referensi Interaktif\nInteractive Linear Algebra Courses\n\nTU Delft - Linear algebra\nUBC Newest- Interactive Linear Algebra\nImmersive Linear Algebra\n\nInteractive Tools\n\nOnline Octave\nCalculator - GeoGebra\n\nVideo Courses\n\nGilbert Strang: Linear Algebra"
  },
  {
    "objectID": "rps.html#capaian-pembelajaran-lulusan-cpl",
    "href": "rps.html#capaian-pembelajaran-lulusan-cpl",
    "title": "Aljabar Linear",
    "section": "Capaian Pembelajaran Lulusan (CPL)",
    "text": "Capaian Pembelajaran Lulusan (CPL)\nCapaian Pembelajaran Lulusan (CPL) yang Dibebankan:\nMenguasai matematika, fisika, kimia dan statistik, teknologi informasi dan rekayasa sebagai landasan penerapan di bidang teknik elektro."
  },
  {
    "objectID": "rps.html#capaian-pembelajaran-mata-kuliah-cpmk-aljabar-linear-1",
    "href": "rps.html#capaian-pembelajaran-mata-kuliah-cpmk-aljabar-linear-1",
    "title": "Aljabar Linear",
    "section": "Capaian Pembelajaran Mata Kuliah (CPMK) Aljabar Linear 1",
    "text": "Capaian Pembelajaran Mata Kuliah (CPMK) Aljabar Linear 1\n\nCPMK-1: Mahasiswa mampu menguasai penyelesaian sistem persamaan linear, melakukan operasi matriks dan menentukan determinan matriks, dan menerapkannya di bidang teknik elektro.\nCPMK-2: Mahasiswa mampu menguasai konsep vektor, operasi vektor, dan ruang vector dan menerapkannya di bidang teknik elektro.\nCPMK-3: Mahasiswa mampu menguasai konsep eigenvalue, eigenvector, transformasi linear, dan menerapkan aljabar linear di bidang teknik elektro."
  },
  {
    "objectID": "rps.html#rencana-pertemuan",
    "href": "rps.html#rencana-pertemuan",
    "title": "Aljabar Linear",
    "section": "Rencana Pertemuan",
    "text": "Rencana Pertemuan\nCPMK-1: Mahasiswa mampu menguasai penyelesaian sistem persamaan linear, melakukan operasi matriks dan menentukan determinan matriks, dan menerapkannya di bidang teknik elektro.\n\nPekan 1. Sistem Persamaan Linear dan Eliminasi Gauss-Jordan\nPekan 2. Matriks dan Operasi Matriks\nPekan 3. Matriks dan Operasi Matriks (Determinan)\nPekan 4. Aplikasi Sistem Persamaan Linear dan Matriks\nPekan 5. Ujian Kompetensi CPMK 1 & Project-based Learning"
  },
  {
    "objectID": "rps.html#rencana-pertemuan-1",
    "href": "rps.html#rencana-pertemuan-1",
    "title": "Aljabar Linear",
    "section": "Rencana Pertemuan",
    "text": "Rencana Pertemuan\nCPMK-2: Mahasiswa mampu menguasai konsep vektor, operasi vektor, dan ruang vector dan menerapkannya di bidang teknik elektro.\n\nPekan 6. Vektor di Ruang Euclidian\nPekan 7. Vektor di Ruang Euclidian\nPekan 8. Ruang Vektor Umum (Ruang Vektor, Sub Ruang)\nPekan 9. Ruang Vektor Umum (Independensi Linear, Basis, Dimensi)\nPekan 10. Ruang Vektor Umum (Rank, Nullity)\nPekan 11. Ujian Kompetensi CPMK 2 (UTS)"
  },
  {
    "objectID": "rps.html#rencana-pertemuan-2",
    "href": "rps.html#rencana-pertemuan-2",
    "title": "Aljabar Linear",
    "section": "Rencana Pertemuan",
    "text": "Rencana Pertemuan\nCPMK 3: Mahasiswa mampu menguasai konsep transformasi matriks, eigenvalue, eigenvector, dan menerapkan aljabar linear di bidang teknik elektro.\n\nPekan 12. Transformasi Matriks\nPekan 13. Eigenvalue dan Eigenvector\nPekan 14. Aplikasi Aljabar Linear\nPekan 15. Aplikasi Aljabar Linear\nPekan 16. Ujian Kompetensi CPMK 2 (UAS) & Laporan Project-based Learning"
  },
  {
    "objectID": "la_prompt/linear_algebra_prompt.html",
    "href": "la_prompt/linear_algebra_prompt.html",
    "title": "Linear Algebra",
    "section": "",
    "text": "Task: Create an interactive Quarto-based presentation using reveal.js for an undergraduate Linear Algebra course in the Electrical and Computer Engineering (ECE) program. Follow the slide structure, formatting rules, and course-specific requirements below."
  },
  {
    "objectID": "la_prompt/linear_algebra_prompt.html#slide-structure",
    "href": "la_prompt/linear_algebra_prompt.html#slide-structure",
    "title": "Linear Algebra",
    "section": "1. Slide Structure",
    "text": "1. Slide Structure\n\nSlide Separator: ---\nTitle Slide: Begin with a first-level heading (#) and/or second-level heading (##).\nStandard Slides: Start with a second-level heading (##) followed by content."
  },
  {
    "objectID": "la_prompt/linear_algebra_prompt.html#content-formatting",
    "href": "la_prompt/linear_algebra_prompt.html#content-formatting",
    "title": "Linear Algebra",
    "section": "2. Content & Formatting",
    "text": "2. Content & Formatting\n\nDiagrams: use Mermaid.js as following\n\n\\`\\`\\`\\{mermaid\\}\nmermaid code here\n… \n\\`\\`\\`\\\n\nCode Blocks: use Python and Pyodide as following\n\n\\`\\`\\`\\{pyodide\\}\n#| max-lines: 10\npython code here\n… \n\\`\\`\\`\\\n\nMath (LaTeX):\n\nInline: $E=mc^2$\nBlock:\n$$\nE=mc^2\n$$\n\nHTML Entities: Use &entity_name; (e.g., &ne; for ≠).\nWeb Content: Embed via &lt;iframe&gt;."
  },
  {
    "objectID": "la_prompt/linear_algebra_prompt.html#multi-column-layout",
    "href": "la_prompt/linear_algebra_prompt.html#multi-column-layout",
    "title": "Linear Algebra",
    "section": "3. Multi-Column Layout",
    "text": "3. Multi-Column Layout\n:::: {.columns}\n::: {.column width=\"40%\"}\n**Left Column**  \n- Item L1  \n- Item L2  \n:::\n::: {.column width=\"60%\"}\n**Right Column**  \n- Item R1  \n- Item R2  \n- Item R3  \n:::\n::::"
  },
  {
    "objectID": "la_prompt/linear_algebra_prompt.html#speaker-notes",
    "href": "la_prompt/linear_algebra_prompt.html#speaker-notes",
    "title": "Linear Algebra",
    "section": "4. Speaker Notes",
    "text": "4. Speaker Notes\nSlide content  \n\n- Point 1  \n- Point 2  \n\n::: {.notes}\nSpeaker notes here.\n:::"
  },
  {
    "objectID": "la_prompt/linear_algebra_prompt.html#course-specific-requirements",
    "href": "la_prompt/linear_algebra_prompt.html#course-specific-requirements",
    "title": "Linear Algebra",
    "section": "5. Course-Specific Requirements",
    "text": "5. Course-Specific Requirements\n\nContent Source: Use provided text and linked images.\nAcademic Context: Ensure explanations, examples, and terminology match Electrical and Computer Engineering standards.\nInteractivity:\n\nInclude Python-based interactive elements (e.g., plots, vector and matrix operations, transformations).\nEmbed charts, plots, and simulations directly in slides.\n\nEnhancement: Add real-world engineering applications, analogies, and problem-solving examples.\nClarity & Engagement: Maintain a clear, logical structure and engaging visuals.\nConciseness: Keep slide text concise; expand explanations in speaker notes."
  },
  {
    "objectID": "la_prompt/linear_algebra_prompt.html#presentation-yaml",
    "href": "la_prompt/linear_algebra_prompt.html#presentation-yaml",
    "title": "Linear Algebra",
    "section": "6. Presentation YAML",
    "text": "6. Presentation YAML\nUse this YAML as the document header. Replace {Lecture Title} with the specific lecture subtitle.\n---\ntitle: \"Linear Algebra\"\nsubtitle: \"{Lecture Title}\"\nauthor: \"Imron Rosyadi\"\nauthor: \"Imron Rosyadi\"\nformat:\n  live-revealjs:\n    logo: \"qrjs_assets/unsoed_logo.png\"\n    footer: \"[irosyadi-2025](https://imron-slide.vercel.app)\"\n    slide-number: true\n    chalkboard: true\n    scrollable: true\n    controls: true\n    progress: true\n    preview-links: false\n    transition: fade\n    incremental: false\n    smaller: false\n    theme: [default, qrjs_assets/ir_style.scss]\nfilters:\n  - pyodide\n---\n\nGoal: Deliver an interactive, educational slide deck that combines theoretical depth with practical demonstrations, making Linear Algebra concepts clear, engaging, and relevant to undergraduate ECE students."
  },
  {
    "objectID": "la-octave.html#why-octave-for-ece-linear-algebra",
    "href": "la-octave.html#why-octave-for-ece-linear-algebra",
    "title": "Linear Algebra",
    "section": "Why Octave for ECE Linear Algebra?",
    "text": "Why Octave for ECE Linear Algebra?\nOctave is a powerful, open-source numerical computation environment. It’s highly compatible with MATLAB, making it an excellent tool for ECE.\nKey Benefits\n\nFree & Open-Source: Accessible to everyone.\nMatrix-Oriented: Designed for vector and matrix operations, core to Linear Algebra.\nPrototyping: Quickly test algorithms and theories.\nVisualization: Generate plots and graphs to understand data.\nIndustry Relevance: Similar syntax to MATLAB, widely used in engineering.\n\n\nOctave provides a robust platform for numerical analysis directly applicable to many ECE fields like signal processing, control systems, and circuit analysis. Its matrix-centric nature simplifies complex computations."
  },
  {
    "objectID": "la-octave.html#getting-started-installation-basics",
    "href": "la-octave.html#getting-started-installation-basics",
    "title": "Linear Algebra",
    "section": "Getting Started: Installation & Basics",
    "text": "Getting Started: Installation & Basics\nInstallation Guide\n\nDownload: Visit GNU Octave.\nInstall: Follow platform-specific instructions (Windows, macOS, Linux).\nLaunch: Open the Octave GUI or command-line interface.\n\nOnline Octave\nVisit octave-online.net\nCommand Line Interface (CLI) Basics\nOctave operates like a powerful calculator."
  },
  {
    "objectID": "la-octave.html#getting-started-installation-basics-1",
    "href": "la-octave.html#getting-started-installation-basics-1",
    "title": "Linear Algebra",
    "section": "Getting Started: Installation & Basics",
    "text": "Getting Started: Installation & Basics\n# Basic Arithmetic\n2 + 3\n5 * 4.5\npi / 2\n\n# Variable Assignment\nx = 10;  # Semicolon suppresses output\ny = x + 5\nz = 'Hello Octave'\n\n# Predefined Constants\npi\ne   # Euler's number\ninf # Infinity\nnan # Not-a-Number\n\nEncourage students to try these commands as you speak. Emphasize the semicolon to suppress output, which is crucial for larger computations. Also mention clc to clear the command window and clear to clear variables from memory. who and whos can list variables."
  },
  {
    "objectID": "la-octave.html#working-with-vectors-in-octave",
    "href": "la-octave.html#working-with-vectors-in-octave",
    "title": "Linear Algebra",
    "section": "Working with Vectors in Octave",
    "text": "Working with Vectors in Octave\nVectors are fundamental building blocks in Linear Algebra.\nCreating Vectors\n\nRow Vector: v_row = [1 2 3]\nColumn Vector: v_col = [4; 5; 6]\nRange: 1:5 (produces [1 2 3 4 5])\nlinspace: linspace(0, 10, 5) (5 points from 0 to 10)\n\nVector Operations\n\nAddition: v1 + v2\nScalar Multiplication: 3 * v1\nDot Product: dot(v1, v2) or v1 * v2' (if v1 is row, v2 is row)\nCross Product: cross(v1, v2) (for 3D vectors)\nMagnitude (Norm): norm(v)"
  },
  {
    "objectID": "la-octave.html#working-with-vectors-in-octave-1",
    "href": "la-octave.html#working-with-vectors-in-octave-1",
    "title": "Linear Algebra",
    "section": "Working with Vectors in Octave",
    "text": "Working with Vectors in Octave\nExample: Vector Addition\n\\[\n\\mathbf{u} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}, \\quad \\mathbf{v} = \\begin{bmatrix} -1 \\\\ 3 \\end{bmatrix} \\\\\n\\mathbf{u} + \\mathbf{v} = \\begin{bmatrix} 2 + (-1) \\\\ 1 + 3 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix}\n\\]\nu = [2; 1];\nv = [-1; 3];\nsum_vec = u + v\n\nEmphasize that Octave handles vector operations intuitively. The interactive plot, although using Python, demonstrates the geometric interpretation of vector addition, which is identical whether calculated in Octave or Python. Explain the difference between * (matrix multiplication) and .* (element-wise multiplication) for later. For vectors, * behaves as a dot product if dimensions align correctly."
  },
  {
    "objectID": "la-octave.html#mastering-matrices-in-octave",
    "href": "la-octave.html#mastering-matrices-in-octave",
    "title": "Linear Algebra",
    "section": "Mastering Matrices in Octave",
    "text": "Mastering Matrices in Octave\nMatrices are central to solving systems of equations, transformations, and more.\nCreating Matrices\n\nRows separated by ;, elements by space/comma. A = [1 2; 3 4]\nSpecial Matrices:\n\nzeros(2,3): 2x3 matrix of zeros.\nones(3): 3x3 matrix of ones.\neye(4): 4x4 identity matrix.\ndiag([1 2 3]): Diagonal matrix.\nrand(2,2): Random matrix.\n\n\nAccessing Elements\n\nA(row, col): A(1,2) (element at row 1, col 2)\nA(1,:): First row.\nA(:,2): Second column."
  },
  {
    "objectID": "la-octave.html#mastering-matrices-in-octave-1",
    "href": "la-octave.html#mastering-matrices-in-octave-1",
    "title": "Linear Algebra",
    "section": "Mastering Matrices in Octave",
    "text": "Mastering Matrices in Octave\nMatrix Operations\n\nAddition/Subtraction: A + B, A - B (must be same size)\nScalar Multiplication: 5 * A\nMatrix Multiplication: A * B (inner dimensions must match)\nElement-wise Multiplication: A .* B (must be same size)\nTranspose: A'\nInverse: inv(A)\nDeterminant: det(A)\n\nA = [2 1; 1 3];\nB = [4 0; -1 2];\n\nC_mult = A * B\nD_inv = inv(A)\nE_det = det(A)\n\nEmphasize the difference between * (matrix multiplication) and .* (element-wise multiplication). This is a common source of error for beginners. Discuss why the inverse and determinant are important (e.g., for solving systems, checking singularity). Mention that inv(A) can be computationally expensive for large matrices, and often direct solution methods are preferred."
  },
  {
    "objectID": "la-octave.html#solving-linear-systems-ax-b",
    "href": "la-octave.html#solving-linear-systems-ax-b",
    "title": "Linear Algebra",
    "section": "Solving Linear Systems: Ax = b",
    "text": "Solving Linear Systems: Ax = b\nOne of the most common applications of linear algebra in ECE.\n\\[\n\\mathbf{A}\\mathbf{x} = \\mathbf{b}\n\\]\nWhere:\n\n\\(\\mathbf{A}\\) is the coefficient matrix.\n\\(\\mathbf{x}\\) is the unknown vector.\n\\(\\mathbf{b}\\) is the constant vector.\n\nSolving in Octave using Left Division Octave uses the backslash operator (\\) for efficient and numerically stable solutions.\n# Example: \n# 2x + y = 5\n# x - y = 1\n\nA = [2 1; 1 -1];\nb = [5; 1];\n\nx = A \\ b  # Solves Ax = b for x\n\nExplain that A \\ b is equivalent to inv(A) * b but is computationally more efficient and numerically stable, especially for large or ill-conditioned matrices. Provide a simple ECE context: Kirchhoff’s laws for circuit analysis often lead to systems of linear equations."
  },
  {
    "objectID": "la-octave.html#scripting-in-octave-.m-files",
    "href": "la-octave.html#scripting-in-octave-.m-files",
    "title": "Linear Algebra",
    "section": "Scripting in Octave: .m Files",
    "text": "Scripting in Octave: .m Files\nFor more complex tasks, write scripts or functions in .m files.\nCreating a Script\n\nOpen Octave Editor (File -&gt; New -&gt; Script).\nSave as my_script.m.\nRun from CLI: my_script (no .m needed).\n\nExample: Vector Magnitude Script\n# my_magnitude.m\n# Calculates the magnitude of a 3D vector\n\nclc;        # Clear command window\nclear;      # Clear workspace variables\n\n% Define a vector\nv = [3; 4; 0]; \n\n% Calculate magnitude\nmag_v = norm(v);\n\n% Display result\nfprintf('The magnitude of v is: %.2f\\n', mag_v);"
  },
  {
    "objectID": "la-octave.html#scripting-in-octave-.m-files-1",
    "href": "la-octave.html#scripting-in-octave-.m-files-1",
    "title": "Linear Algebra",
    "section": "Scripting in Octave: .m Files",
    "text": "Scripting in Octave: .m Files\nFunctions in Octave Functions allow reusable code blocks.\n# my_area_triangle.m\nfunction area = my_area_triangle(base, height)\n  % MY_AREA_TRIANGLE Calculates the area of a triangle.\n  %   area = MY_AREA_TRIANGLE(base, height)\n  %   Example: my_area_triangle(10, 5)\n\n  area = 0.5 * base * height;\nendfunction\nUsing the Function\n# In CLI or another script:\nresult = my_area_triangle(7, 3) \n\nExplain why scripting is important: automation, reproducibility, and handling larger problems. Discuss comments (# or %) and the fprintf function for formatted output. Highlight the structure of a function file: function output = func_name(input) ... endfunction. Mention that function names should match the filename for Octave to find them automatically."
  },
  {
    "objectID": "la-octave.html#conclusion-further-resources",
    "href": "la-octave.html#conclusion-further-resources",
    "title": "Linear Algebra",
    "section": "Conclusion & Further Resources",
    "text": "Conclusion & Further Resources\nKey Takeaways\n\nOctave is a powerful, free tool for Linear Algebra.\nIt simplifies vector and matrix operations.\nEssential for solving systems and understanding transformations.\nScripting enhances efficiency and reproducibility.\n\nNext Steps\n\nPractice: The best way to learn is by doing.\nOctave Documentation: doc command or online manual.\nLinear Algebra Textbooks: Apply concepts with Octave.\nECE Applications: Explore how these concepts are used in your specific field of interest."
  },
  {
    "objectID": "la-octave.html#thank-you",
    "href": "la-octave.html#thank-you",
    "title": "Linear Algebra",
    "section": "Thank You!",
    "text": "Thank You!\nHappy Computing!\n\nEncourage questions and provide contact information if applicable. Reinforce the practical utility of Octave in their ECE curriculum and future careers."
  },
  {
    "objectID": "la-32.html#introduction",
    "href": "la-32.html#introduction",
    "title": "Linear Algebra",
    "section": "Introduction",
    "text": "Introduction\nIn this section, we explore concepts of length and distance as they apply to vectors. We’ll start with familiar 2D and 3D spaces, then generalize these ideas algebraically to \\(R^n\\).\n\nConnecting geometric intuition from 2D and 3D with the abstract world of \\(n\\)-space is a key theme in linear algebra. These concepts are fundamental for understanding vector spaces, optimization, and many signal processing techniques in ECE. For example, signal power relates to vector norm, and signal similarity relates to the dot product."
  },
  {
    "objectID": "la-32.html#norm-of-a-vector",
    "href": "la-32.html#norm-of-a-vector",
    "title": "Linear Algebra",
    "section": "Norm of a Vector",
    "text": "Norm of a Vector\nThe length of a vector \\(\\mathbf{v}\\) is called its norm, denoted by \\(\\| \\mathbf{v}\\|\\). It’s also referred to as the length or magnitude of \\(\\mathbf{v}\\).\nIn \\(R^2\\) (Pythagorean Theorem): For \\(\\mathbf{v} = (v_1, v_2)\\), \\[\n\\| \\mathbf{v}\\| = \\sqrt{v_1^2 + v_2^2} \\tag{1}\n\\]\nIn \\(R^3\\) (Pythagorean Theorem twice): For \\(\\mathbf{v} = (v_1, v_2, v_3)\\), \\[\n\\| \\mathbf{v}\\| = \\sqrt{v_1^2 + v_2^2 + v_3^2} \\tag{2}\n\\]"
  },
  {
    "objectID": "la-32.html#norm-of-a-vector-1",
    "href": "la-32.html#norm-of-a-vector-1",
    "title": "Linear Algebra",
    "section": "Norm of a Vector",
    "text": "Norm of a Vector\n\nFigure 3.2.1: Geometric interpretation of vector norm in \\(R^2\\) and \\(R^3\\).\nThe norm is essentially the distance from the origin to the vector’s terminal point. It’s a direct generalization of the Pythagorean theorem. In ECE, the norm of a signal vector can represent its energy or power."
  },
  {
    "objectID": "la-32.html#norm-in-rn-definition-1",
    "href": "la-32.html#norm-in-rn-definition-1",
    "title": "Linear Algebra",
    "section": "Norm in \\(R^n\\) (Definition 1)",
    "text": "Norm in \\(R^n\\) (Definition 1)\nFor a vector \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_n)\\) in \\(R^n\\), its norm is defined as:\n\\[\n\\| \\mathbf{v}\\| = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2} \\tag{3}\n\\]\nEXAMPLE 1: Calculating Norms\n\nNorm of \\(\\mathbf{v} = (-3, 2, 1)\\) in \\(R^3\\): \\[\n\\| \\mathbf{v}\\| = \\sqrt{(-3)^2 + 2^2 + 1^2} = \\sqrt{9 + 4 + 1} = \\sqrt{14}\n\\]\nNorm of \\(\\mathbf{v} = (2, -1, 3, -5)\\) in \\(R^4\\): \\[\n\\| \\mathbf{v}\\| = \\sqrt{2^2 + (-1)^2 + 3^2 + (-5)^2} = \\sqrt{4 + 1 + 9 + 25} = \\sqrt{39}\n\\]\n\n\nThis definition extends the Pythagorean theorem to any number of dimensions. It’s a scalar value, always non-negative. The examples show how straightforward the calculation is, even for higher dimensions."
  },
  {
    "objectID": "la-32.html#interactive-norm-calculator",
    "href": "la-32.html#interactive-norm-calculator",
    "title": "Linear Algebra",
    "section": "Interactive Norm Calculator",
    "text": "Interactive Norm Calculator\nCalculate the norm for a vector in \\(R^n\\). Enter components as a comma-separated list.\n\n\n\n\n\n\n\nThis interactive tool allows you to compute the norm for any vector in \\(R^n\\). Just modify the vector_str variable in the code block and click run. This helps reinforce the calculation and shows how NumPy handles it efficiently."
  },
  {
    "objectID": "la-32.html#properties-of-the-norm-theorem-3.2.1",
    "href": "la-32.html#properties-of-the-norm-theorem-3.2.1",
    "title": "Linear Algebra",
    "section": "Properties of the Norm (Theorem 3.2.1)",
    "text": "Properties of the Norm (Theorem 3.2.1)\nFor a vector \\(\\mathbf{v}\\) in \\(R^n\\) and any scalar \\(k\\):\n\n\\(\\| \\mathbf{v}\\| \\geq 0\\) (Norm is non-negative)\n\\(\\| \\mathbf{v}\\| = 0\\) if and only if \\(\\mathbf{v} = \\mathbf{0}\\) (Only the zero vector has zero norm)\n\\(\\| k\\mathbf{v}\\| = |k| \\| \\mathbf{v}\\|\\) (Scaling a vector scales its norm by the absolute value of the scalar)\n\nProof (c): If \\(\\mathbf{v} = (v_1, \\ldots, v_n)\\), then \\(k\\mathbf{v} = (kv_1, \\ldots, kv_n)\\). \\[\n\\begin{aligned}\n\\| k\\mathbf{v}\\| &= \\sqrt{(kv_1)^2 + (kv_2)^2 + \\cdots + (kv_n)^2} \\\\\n&= \\sqrt{k^2 v_1^2 + k^2 v_2^2 + \\cdots + k^2 v_n^2} \\\\\n&= \\sqrt{k^2(v_1^2 + v_2^2 + \\cdots + v_n^2)} \\\\\n&= \\sqrt{k^2} \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2} \\\\\n&= |k| \\| \\mathbf{v}\\|\n\\end{aligned}\n\\]\n\nThese properties are intuitive extensions of what we know about length. Property (c) is particularly important for understanding how scaling affects a vector’s magnitude. It means that if you double a vector, its length doubles; if you negate a vector, its length remains the same."
  },
  {
    "objectID": "la-32.html#unit-vectors",
    "href": "la-32.html#unit-vectors",
    "title": "Linear Algebra",
    "section": "Unit Vectors",
    "text": "Unit Vectors\nA vector with a norm of 1 is called a unit vector. Unit vectors are useful for specifying direction without concern for length.\nTo obtain a unit vector \\(\\mathbf{u}\\) that has the same direction as a nonzero vector \\(\\mathbf{v}\\), we normalize \\(\\mathbf{v}\\): \\[\n\\mathbf{u} = \\frac{1}{\\| \\mathbf{v}\\|} \\mathbf{v} \\tag{4}\n\\]\n\n\n\n\n\n\nWarning\n\n\nNotation Alert: You might also see this written as \\(\\mathbf{u} = \\frac{\\mathbf{v}}{\\| \\mathbf{v}\\|}\\). This is just a compact way of writing the scalar multiplication, not actual vector division.\n\n\n\nWe can confirm \\(\\| \\mathbf{u}\\| = 1\\) using Theorem 3.2.1(c): \\[\n\\| \\mathbf{u}\\| = \\left\\| \\frac{1}{\\| \\mathbf{v}\\|} \\mathbf{v} \\right\\| = \\left| \\frac{1}{\\| \\mathbf{v}\\|} \\right| \\| \\mathbf{v}\\| = \\frac{1}{\\| \\mathbf{v}\\|} \\| \\mathbf{v}\\| = 1\n\\]\n\nUnit vectors are crucial in many ECE applications, such as defining directions for antennas, forces, or normalizing data to prevent features with larger magnitudes from dominating analyses. Normalizing a vector makes it a unit vector while preserving its direction."
  },
  {
    "objectID": "la-32.html#example-2-normalizing-a-vector",
    "href": "la-32.html#example-2-normalizing-a-vector",
    "title": "Linear Algebra",
    "section": "EXAMPLE 2: Normalizing a Vector",
    "text": "EXAMPLE 2: Normalizing a Vector\nFind the unit vector \\(\\mathbf{u}\\) that has the same direction as \\(\\mathbf{v} = (2, 2, -1)\\).\nSolution: First, find the norm of \\(\\mathbf{v}\\): \\[\n\\| \\mathbf{v}\\| = \\sqrt{2^2 + 2^2 + (-1)^2} = \\sqrt{4 + 4 + 1} = \\sqrt{9} = 3\n\\] Now, normalize \\(\\mathbf{v}\\): \\[\n\\mathbf{u} = \\frac{1}{3} (2, 2, -1) = \\left(\\frac{2}{3}, \\frac{2}{3}, -\\frac{1}{3}\\right)\n\\] You can verify that \\(\\| \\mathbf{u}\\| = \\sqrt{(2/3)^2 + (2/3)^2 + (-1/3)^2} = \\sqrt{4/9 + 4/9 + 1/9} = \\sqrt{9/9} = 1\\).\n\nThis example clearly shows the two steps: calculate the norm, then divide each component by the norm. The check confirms that the resulting vector indeed has a length of 1."
  },
  {
    "objectID": "la-32.html#interactive-vector-normalization",
    "href": "la-32.html#interactive-vector-normalization",
    "title": "Linear Algebra",
    "section": "Interactive Vector Normalization",
    "text": "Interactive Vector Normalization\nEnter a vector in \\(R^n\\) and normalize it.\n\n\n\n\n\n\n\nUse this code block to practice normalizing vectors. Input different vectors, including those with fractions or higher dimensions. Observe that the output u always has a norm of 1 (or very close due to floating-point precision)."
  },
  {
    "objectID": "la-32.html#the-standard-unit-vectors",
    "href": "la-32.html#the-standard-unit-vectors",
    "title": "Linear Algebra",
    "section": "The Standard Unit Vectors",
    "text": "The Standard Unit Vectors\nThese are unit vectors along the positive coordinate axes.\nIn \\(R^2\\): \\(\\mathbf{i} = (1,0)\\) and \\(\\mathbf{j} = (0,1)\\)\nIn \\(R^3\\): \\(\\mathbf{i} = (1,0,0)\\), \\(\\mathbf{j} = (0,1,0)\\), and \\(\\mathbf{k} = (0,0,1)\\)\nAny vector \\(\\mathbf{v}\\) can be expressed as a linear combination of these:\n\n\\(R^2\\): \\(\\mathbf{v} = (v_1, v_2) = v_1\\mathbf{i} + v_2\\mathbf{j}\\) (Eq. 5)\n\\(R^3\\): \\(\\mathbf{v} = (v_1, v_2, v_3) = v_1\\mathbf{i} + v_2\\mathbf{j} + v_3\\mathbf{k}\\) (Eq. 6)\n\n\nStandard unit vectors form a fundamental basis for \\(R^2\\) and \\(R^3\\). They are mutually orthogonal and have unit length. This representation is very common in physics and engineering for breaking down forces or fields into their orthogonal components."
  },
  {
    "objectID": "la-32.html#standard-unit-vectors-in-rn",
    "href": "la-32.html#standard-unit-vectors-in-rn",
    "title": "Linear Algebra",
    "section": "Standard Unit Vectors in \\(R^n\\)",
    "text": "Standard Unit Vectors in \\(R^n\\)\nThe concept extends to \\(R^n\\):\n\\[\n\\mathbf{e}_1 = (1,0,0,\\ldots,0), \\quad \\mathbf{e}_2 = (0,1,0,\\ldots,0), \\quad \\ldots, \\quad \\mathbf{e}_n = (0,0,0,\\ldots,1) \\tag{7}\n\\] Any vector \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_n)\\) in \\(R^n\\) can be expressed as: \\[\n\\mathbf{v} = v_1\\mathbf{e}_1 + v_2\\mathbf{e}_2 + \\dots + v_n\\mathbf{e}_n \\tag{8}\n\\]\nEXAMPLE 3: Linear Combinations of Standard Unit Vectors\n\n\\((2, -3, 4) = 2\\mathbf{i} - 3\\mathbf{j} + 4\\mathbf{k}\\)\n\\((7, 3, -4, 5) = 7\\mathbf{e}_1 + 3\\mathbf{e}_2 - 4\\mathbf{e}_3 + 5\\mathbf{e}_4\\)\n\n\nThe standard unit vectors \\(\\{\\mathbf{e}_1, \\ldots, \\mathbf{e}_n\\}\\) form the standard basis for \\(R^n\\). They are incredibly useful because they simplify many calculations and provide a clear way to represent any vector. In signal processing, these can represent basis functions or orthogonal components of a signal."
  },
  {
    "objectID": "la-32.html#distance-in-rn",
    "href": "la-32.html#distance-in-rn",
    "title": "Linear Algebra",
    "section": "Distance in \\(R^n\\)",
    "text": "Distance in \\(R^n\\)\nThe distance between two points \\(P_1\\) and \\(P_2\\) is the length of the vector \\(\\overrightarrow{P_1P_2}\\).\nIn \\(R^2\\): For \\(P_1(x_1,y_1)\\) and \\(P_2(x_2,y_2)\\), \\[\nd = \\| \\overrightarrow{P_1P_2}\\| = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2} \\tag{9}\n\\] In \\(R^3\\): For \\(P_1(x_1,y_1,z_1)\\) and \\(P_2(x_2,y_2,z_2)\\), \\[\nd = \\| \\overrightarrow{P_1P_2}\\| = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2 + (z_2 - z_1)^2} \\tag{10}\n\\]\n\nFigure 3.2.3: Distance between two points in \\(R^2\\).\nThe distance formula is a direct application of the norm of the difference vector between two points. This is a fundamental concept in geometry and has vast applications in ECE, such as calculating the distance between sensor readings, or the error between an actual and desired output."
  },
  {
    "objectID": "la-32.html#distance-in-rn-definition-2",
    "href": "la-32.html#distance-in-rn-definition-2",
    "title": "Linear Algebra",
    "section": "Distance in \\(R^n\\) (Definition 2)",
    "text": "Distance in \\(R^n\\) (Definition 2)\nFor points \\(\\mathbf{u} = (u_1, \\ldots, u_n)\\) and \\(\\mathbf{v} = (v_1, \\ldots, v_n)\\) in \\(R^n\\), the distance \\(d(\\mathbf{u},\\mathbf{v})\\) is:\n\\[\nd(\\mathbf{u},\\mathbf{v}) = \\| \\mathbf{u} - \\mathbf{v}\\| = \\sqrt{(u_1 - v_1)^2 + (u_2 - v_2)^2 + \\cdots + (u_n - v_n)^2} \\tag{11}\n\\]\nEXAMPLE 4: Calculating Distance in \\(R^n\\)\nIf \\(\\mathbf{u} = (1, 3, -2, 7)\\) and \\(\\mathbf{v} = (0, 7, 2, 2)\\), then the distance is: \\[\n\\begin{aligned}\nd(\\mathbf{u},\\mathbf{v}) &= \\sqrt{(1 - 0)^2 + (3 - 7)^2 + (-2 - 2)^2 + (7 - 2)^2} \\\\\n&= \\sqrt{1^2 + (-4)^2 + (-4)^2 + 5^2} \\\\\n&= \\sqrt{1 + 16 + 16 + 25} = \\sqrt{58}\n\\end{aligned}\n\\]\n\nThis formal definition shows that distance in \\(R^n\\) is simply the norm of the vector representing the displacement from one point to another. The example illustrates the calculation for points in \\(R^4\\)."
  },
  {
    "objectID": "la-32.html#interactive-distance-calculator",
    "href": "la-32.html#interactive-distance-calculator",
    "title": "Linear Algebra",
    "section": "Interactive Distance Calculator",
    "text": "Interactive Distance Calculator\nCalculate the distance between two points in \\(R^n\\). Enter components as comma-separated lists.\n\n\n\n\n\n\n\nThis interactive tool lets you compute the distance between any two points in \\(R^n\\). Ensure both vectors have the same number of components. This is useful for understanding error metrics or similarity measures in high-dimensional data, common in machine learning and data science."
  },
  {
    "objectID": "la-32.html#dot-product-geometric-definition",
    "href": "la-32.html#dot-product-geometric-definition",
    "title": "Linear Algebra",
    "section": "Dot Product: Geometric Definition",
    "text": "Dot Product: Geometric Definition\nTo define the “angle” between vectors, we use the dot product.\nFor nonzero vectors \\(\\mathbf{u}, \\mathbf{v}\\) in \\(R^2\\) or \\(R^3\\) (with initial points coinciding), the angle \\(\\theta\\) (\\(0 \\leq \\theta \\leq \\pi\\)) between them is shown below:\n\nFigure 3.2.4: Angle between two vectors.Definition 3 (Geometric Dot Product): \\[\n\\mathbf{u}\\cdot \\mathbf{v} = \\| \\mathbf{u}\\| \\| \\mathbf{v}\\| \\cos \\theta \\tag{12}\n\\] If \\(\\mathbf{u} = \\mathbf{0}\\) or \\(\\mathbf{v} = \\mathbf{0}\\), then \\(\\mathbf{u}\\cdot \\mathbf{v} = 0\\).\n\nThe dot product is a scalar value that captures the relationship between two vectors in terms of their magnitudes and the angle between them. It’s a foundational concept in physics (e.g., work done by a force) and ECE (e.g., correlation between signals)."
  },
  {
    "objectID": "la-32.html#angle-from-dot-product",
    "href": "la-32.html#angle-from-dot-product",
    "title": "Linear Algebra",
    "section": "Angle from Dot Product",
    "text": "Angle from Dot Product\nFrom the geometric definition, we can find the angle: \\[\n\\cos \\theta = \\frac{\\mathbf{u}\\cdot\\mathbf{v}}{\\| \\mathbf{u}\\| \\| \\mathbf{v}\\|} \\tag{13}\n\\] Since \\(0 \\leq \\theta \\leq \\pi\\):\n\n\\(\\mathbf{u}\\cdot \\mathbf{v} &gt; 0 \\implies \\theta\\) is acute (\\(0 &lt; \\theta &lt; \\pi/2\\))\n\\(\\mathbf{u}\\cdot \\mathbf{v} &lt; 0 \\implies \\theta\\) is obtuse (\\(\\pi/2 &lt; \\theta &lt; \\pi\\))\n\\(\\mathbf{u}\\cdot \\mathbf{v} = 0 \\implies \\theta = \\pi/2\\) (orthogonal vectors)\n\n\nFigure 3.2.5: Angle types based on dot product sign.\nThe sign of the dot product tells us a lot about the orientation of the vectors. Orthogonality (vectors at 90 degrees) is especially important in ECE for concepts like independent signals or perpendicular fields. If two signals have a dot product of zero, they are uncorrelated."
  },
  {
    "objectID": "la-32.html#example-5-dot-product-geometric",
    "href": "la-32.html#example-5-dot-product-geometric",
    "title": "Linear Algebra",
    "section": "EXAMPLE 5: Dot Product (Geometric)",
    "text": "EXAMPLE 5: Dot Product (Geometric)\nFind the dot product of the vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) shown in Figure 3.2.5.\nThe lengths are:\n\\(\\| \\mathbf{u}\\| = 1\\)\n\\(\\| \\mathbf{v}\\| = \\sqrt{2^2 + 2^2} = \\sqrt{8} = 2\\sqrt{2}\\)\nThe angle between them is \\(\\theta = 45^\\circ\\), so \\(\\cos(45^\\circ) = 1/\\sqrt{2}\\).\nThus, \\[\n\\mathbf{u}\\cdot \\mathbf{v} = \\| \\mathbf{u}\\| \\| \\mathbf{v}\\| \\cos \\theta = (1)(2\\sqrt{2})(1/\\sqrt{2}) = 2\n\\]\n\nThis example uses the geometric definition directly. It’s a good way to visualize what the dot product means before we move to the component-wise calculation."
  },
  {
    "objectID": "la-32.html#component-form-of-the-dot-product",
    "href": "la-32.html#component-form-of-the-dot-product",
    "title": "Linear Algebra",
    "section": "Component Form of the Dot Product",
    "text": "Component Form of the Dot Product\nFor computational purposes, we need a formula in terms of components. Using the Law of Cosines for the triangle formed by \\(\\mathbf{u}\\), \\(\\mathbf{v}\\), and \\(\\mathbf{v}-\\mathbf{u}\\):\n\nFigure 3.2.6: Derivation of component form of dot product.This leads to:\n\nIn \\(R^2\\): \\(\\mathbf{u}\\cdot \\mathbf{v} = u_1v_1 + u_2v_2\\) (Eq. 16)\nIn \\(R^3\\): \\(\\mathbf{u}\\cdot \\mathbf{v} = u_1v_1 + u_2v_2 + u_3v_3\\) (Eq. 15)\n\n\nThe derivation, though not fully shown here, is an important connection between geometry and algebra. It demonstrates that the component-wise definition is consistent with the geometric one."
  },
  {
    "objectID": "la-32.html#dot-product-in-rn-definition-4",
    "href": "la-32.html#dot-product-in-rn-definition-4",
    "title": "Linear Algebra",
    "section": "Dot Product in \\(R^n\\) (Definition 4)",
    "text": "Dot Product in \\(R^n\\) (Definition 4)\nFor vectors \\(\\mathbf{u} = (u_1, \\ldots, u_n)\\) and \\(\\mathbf{v} = (v_1, \\ldots, v_n)\\) in \\(R^n\\), the dot product is:\n\\[\n\\mathbf{u}\\cdot \\mathbf{v} = u_1v_1 + u_2v_2 + \\dots + u_nv_n \\tag{17}\n\\] In words: Multiply corresponding components and add the products.\nEXAMPLE 6: Calculating Dot Products Using Components\n\nFor \\(\\mathbf{u} = (0,0,1)\\) and \\(\\mathbf{v} = (0,2,2)\\) (from Example 5): \\[\n\\mathbf{u}\\cdot \\mathbf{v} = (0)(0) + (0)(2) + (1)(2) = 2\n\\] (This agrees with the geometric calculation.)\nFor \\(\\mathbf{u} = (-1,3,5,7)\\) and \\(\\mathbf{v} = (-3,-4,1,0)\\) in \\(R^4\\): \\[\n\\mathbf{u}\\cdot \\mathbf{v} = (-1)(-3) + (3)(-4) + (5)(1) + (7)(0) = 3 - 12 + 5 + 0 = -4\n\\]\n\n\nThis component-wise definition is the most practical for computation. It’s simple, efficient, and extends perfectly to \\(n\\)-dimensions. In ECE, this is how inner products are typically computed in digital signal processing, control systems, and machine learning algorithms."
  },
  {
    "objectID": "la-32.html#interactive-dot-product-calculator",
    "href": "la-32.html#interactive-dot-product-calculator",
    "title": "Linear Algebra",
    "section": "Interactive Dot Product Calculator",
    "text": "Interactive Dot Product Calculator\nCompute the dot product of two vectors in \\(R^n\\). Enter components as comma-separated lists.\n\n\n\n\n\n\n\nThis interactive Python block computes the dot product and the angle between two vectors. Experiment with different vectors to see how the dot product and angle change. Pay attention to when the dot product is zero, and what that implies about the angle."
  },
  {
    "objectID": "la-32.html#example-7-a-geometry-problem-solved-using-dot-product",
    "href": "la-32.html#example-7-a-geometry-problem-solved-using-dot-product",
    "title": "Linear Algebra",
    "section": "EXAMPLE 7: A Geometry Problem Solved Using Dot Product",
    "text": "EXAMPLE 7: A Geometry Problem Solved Using Dot Product\nFind the angle between a diagonal of a cube and one of its edges.\nLet the cube have edge length \\(k\\). Place one vertex at the origin \\((0,0,0)\\).\n\nAn edge vector: \\(\\mathbf{u}_1 = (k,0,0)\\)\nA diagonal vector (from origin to opposite corner): \\(\\mathbf{d} = (k,k,k)\\)\n\nUsing the formula \\(\\cos \\theta = \\frac{\\mathbf{u}\\cdot\\mathbf{v}}{\\| \\mathbf{u}\\| \\| \\mathbf{v}\\|}\\): \\[\n\\cos \\theta = \\frac{\\mathbf{u}_1\\cdot\\mathbf{d}}{\\| \\mathbf{u}_1\\| \\| \\mathbf{d}\\|} = \\frac{(k)(k) + (0)(k) + (0)(k)}{\\sqrt{k^2}\\sqrt{k^2+k^2+k^2}} = \\frac{k^2}{k\\sqrt{3k^2}} = \\frac{k^2}{k(k\\sqrt{3})} = \\frac{1}{\\sqrt{3}}\n\\] \\[\n\\theta = \\cos^{-1}\\left(\\frac{1}{\\sqrt{3}}\\right) \\approx 54.74^{\\circ}\n\\]\n\n\n\n\n\n\nNote\n\n\nThe angle \\(\\theta\\) does not depend on \\(k\\). This is expected because scaling a cube doesn’t change its internal angles."
  },
  {
    "objectID": "la-32.html#example-7-cont.",
    "href": "la-32.html#example-7-cont.",
    "title": "Linear Algebra",
    "section": "EXAMPLE 7 (cont.)",
    "text": "EXAMPLE 7 (cont.)\n\nFigure 3.2.7: Diagonal of a cube and one of its edges.\nThis is a classic problem demonstrating the power of vectors and dot products in geometry. It’s a good example of how abstract linear algebra tools can solve concrete visual problems. This type of angular analysis is relevant in applications like antenna design or robotic arm kinematics."
  },
  {
    "objectID": "la-32.html#algebraic-properties-of-the-dot-product",
    "href": "la-32.html#algebraic-properties-of-the-dot-product",
    "title": "Linear Algebra",
    "section": "Algebraic Properties of the Dot Product",
    "text": "Algebraic Properties of the Dot Product\nRelationship between Norm and Dot Product: \\[\n\\mathbf{v}\\cdot \\mathbf{v} = v_1^2 + \\dots + v_n^2 = \\| \\mathbf{v}\\|^2 \\tag{18}\n\\] So, \\(\\| \\mathbf{v}\\| = \\sqrt{\\mathbf{v}\\cdot \\mathbf{v}}\\) (Eq. 19)\nTheorem 3.2.2: For \\(\\mathbf{u}, \\mathbf{v}, \\mathbf{w}\\) in \\(R^n\\), scalar \\(k\\):\n\n\\(\\mathbf{u}\\cdot \\mathbf{v} = \\mathbf{v}\\cdot \\mathbf{u}\\) (Symmetry)\n\\(\\mathbf{u}\\cdot (\\mathbf{v} + \\mathbf{w}) = \\mathbf{u}\\cdot \\mathbf{v} + \\mathbf{u}\\cdot \\mathbf{w}\\) (Distributive)\n\\(k(\\mathbf{u}\\cdot \\mathbf{v}) = (k\\mathbf{u})\\cdot \\mathbf{v} = \\mathbf{u}\\cdot (k\\mathbf{v})\\) (Homogeneity)\n\\(\\mathbf{v}\\cdot \\mathbf{v} \\geq 0\\) and \\(\\mathbf{v}\\cdot \\mathbf{v} = 0\\) iff \\(\\mathbf{v} = \\mathbf{0}\\)\n\nTheorem 3.2.3 (Additional Properties):\n\n\\(\\mathbf{0}\\cdot \\mathbf{v} = \\mathbf{v}\\cdot \\mathbf{0} = 0\\)\n\\((\\mathbf{u} + \\mathbf{v})\\cdot \\mathbf{w} = \\mathbf{u}\\cdot \\mathbf{w} + \\mathbf{v}\\cdot \\mathbf{w}\\)\n\\(\\mathbf{u}\\cdot (\\mathbf{v} - \\mathbf{w}) = \\mathbf{u}\\cdot \\mathbf{v} - \\mathbf{u}\\cdot \\mathbf{w}\\)\n\\((\\mathbf{u} - \\mathbf{v})\\cdot \\mathbf{w} = \\mathbf{u}\\cdot \\mathbf{w} - \\mathbf{v}\\cdot \\mathbf{w}\\)\n\n\nThese theorems establish the algebraic rules for working with dot products. They are crucial for simplifying expressions and proving other theorems without always resorting to component-wise calculations. They are analogous to the properties of multiplication of real numbers."
  },
  {
    "objectID": "la-32.html#example-8-calculating-with-dot-products",
    "href": "la-32.html#example-8-calculating-with-dot-products",
    "title": "Linear Algebra",
    "section": "EXAMPLE 8: Calculating with Dot Products",
    "text": "EXAMPLE 8: Calculating with Dot Products\nSimplify \\((\\mathbf{u} - 2\\mathbf{v})\\cdot (3\\mathbf{u} + 4\\mathbf{v})\\).\nUsing distributive and homogeneity properties: \\[\n\\begin{aligned}\n(\\mathbf{u} - 2\\mathbf{v})\\cdot (3\\mathbf{u} + 4\\mathbf{v}) &= \\mathbf{u}\\cdot (3\\mathbf{u} + 4\\mathbf{v}) - 2\\mathbf{v}\\cdot (3\\mathbf{u} + 4\\mathbf{v}) \\\\\n&= (3\\mathbf{u}\\cdot \\mathbf{u} + 4\\mathbf{u}\\cdot \\mathbf{v}) - (6\\mathbf{v}\\cdot \\mathbf{u} + 8\\mathbf{v}\\cdot \\mathbf{v}) \\\\\n&= 3(\\mathbf{u}\\cdot \\mathbf{u}) + 4(\\mathbf{u}\\cdot \\mathbf{v}) - 6(\\mathbf{u}\\cdot \\mathbf{v}) - 8(\\mathbf{v}\\cdot \\mathbf{v}) & \\text{[Symmetry: }\\mathbf{v}\\cdot \\mathbf{u} = \\mathbf{u}\\cdot \\mathbf{v}] \\\\\n&= 3\\| \\mathbf{u}\\|^2 - 2(\\mathbf{u}\\cdot \\mathbf{v}) - 8\\| \\mathbf{v}\\|^2 & \\text{[Since }\\mathbf{x}\\cdot \\mathbf{x} = \\| \\mathbf{x}\\|^2]\n\\end{aligned}\n\\]\n\nThis example shows how to algebraically expand and simplify expressions involving dot products. It’s a good exercise in applying the properties from Theorems 3.2.2 and 3.2.3. This skill is vital for derivations in various ECE fields, such as optimizing controller gains or analyzing signal processing filter responses."
  },
  {
    "objectID": "la-32.html#cauchy-schwarz-inequality",
    "href": "la-32.html#cauchy-schwarz-inequality",
    "title": "Linear Algebra",
    "section": "Cauchy-Schwarz Inequality",
    "text": "Cauchy-Schwarz Inequality\nTo define the angle in \\(R^n\\) using \\(\\cos \\theta = \\frac{\\mathbf{u}\\cdot\\mathbf{v}}{\\| \\mathbf{u}\\| \\| \\mathbf{v}\\|}\\), we need to ensure the argument of \\(\\cos^{-1}\\) is between -1 and 1. This is guaranteed by the Cauchy-Schwarz Inequality:\nTheorem 3.2.4 (Cauchy-Schwarz Inequality): If \\(\\mathbf{u} = (u_1, \\ldots, u_n)\\) and \\(\\mathbf{v} = (v_1, \\ldots, v_n)\\) are vectors in \\(R^n\\), then: \\[\n|\\mathbf{u}\\cdot \\mathbf{v}|\\leq \\| \\mathbf{u}\\| \\| \\mathbf{v}\\| \\tag{22}\n\\] Or in component form: \\[\n|u_1v_1 + \\dots + u_nv_n| \\leq (u_1^2 + \\dots + u_n^2)^{1/2}(v_1^2 + \\dots + v_n^2)^{1/2} \\tag{23}\n\\]\nImplication for Angle: Dividing by \\(\\| \\mathbf{u}\\| \\| \\mathbf{v}\\|\\) (for nonzero vectors) gives: \\[\n\\left|\\frac{\\mathbf{u}\\cdot\\mathbf{v}}{\\| \\mathbf{u}\\| \\| \\mathbf{v}\\|}\\right|\\leq 1 \\quad \\implies \\quad -1 \\leq \\frac{\\mathbf{u}\\cdot\\mathbf{v}}{\\| \\mathbf{u}\\| \\| \\mathbf{v}\\|} \\leq 1\n\\] This confirms that the angle formula is well-defined for all nonzero vectors in \\(R^n\\).\n\nThe Cauchy-Schwarz inequality is one of the most important inequalities in mathematics. It has profound implications in many fields, including signal processing, information theory, and quantum mechanics in ECE. It essentially states that the dot product of two vectors cannot exceed the product of their lengths. Equality holds if and only if the vectors are collinear (one is a scalar multiple of the other)."
  },
  {
    "objectID": "la-32.html#triangle-inequality-for-vectors-and-distances",
    "href": "la-32.html#triangle-inequality-for-vectors-and-distances",
    "title": "Linear Algebra",
    "section": "Triangle Inequality for Vectors and Distances",
    "text": "Triangle Inequality for Vectors and Distances\nThese generalize familiar geometric results.\nTheorem 3.2.5: For vectors \\(\\mathbf{u},\\mathbf{v},\\mathbf{w}\\) in \\(R^n\\):\n\n\\(\\| \\mathbf{u} + \\mathbf{v}\\| \\leq \\| \\mathbf{u}\\| + \\| \\mathbf{v}\\|\\) (Triangle inequality for vectors)\n\\(d(\\mathbf{u},\\mathbf{v})\\leq d(\\mathbf{u},\\mathbf{w}) + d(\\mathbf{w},\\mathbf{v})\\) (Triangle inequality for distances)\n\n\nFigure 3.2.8: Triangle inequality for vectors."
  },
  {
    "objectID": "la-32.html#triangle-inequality-for-vectors-and-distances-1",
    "href": "la-32.html#triangle-inequality-for-vectors-and-distances-1",
    "title": "Linear Algebra",
    "section": "Triangle Inequality for Vectors and Distances",
    "text": "Triangle Inequality for Vectors and Distances\nProof (a): Starting with \\(\\| \\mathbf{u} + \\mathbf{v}\\|^2 = (\\mathbf{u} + \\mathbf{v})\\cdot (\\mathbf{u} + \\mathbf{v}) = \\| \\mathbf{u}\\|^2 + 2(\\mathbf{u}\\cdot \\mathbf{v}) + \\| \\mathbf{v}\\|^2\\). Using \\(\\mathbf{u}\\cdot \\mathbf{v} \\leq |\\mathbf{u}\\cdot \\mathbf{v}|\\) and Cauchy-Schwarz \\(|\\mathbf{u}\\cdot \\mathbf{v}| \\leq \\| \\mathbf{u}\\| \\| \\mathbf{v}\\|\\): \\[\n\\| \\mathbf{u} + \\mathbf{v}\\|^2 \\leq \\| \\mathbf{u}\\|^2 + 2\\| \\mathbf{u}\\| \\| \\mathbf{v}\\| + \\| \\mathbf{v}\\|^2 = (\\| \\mathbf{u}\\| + \\| \\mathbf{v}\\|)^2\n\\] Taking the square root (both sides non-negative) gives the result.\n\nThe triangle inequality is a very intuitive geometric concept: the shortest path between two points is a straight line. It’s fundamental in metric spaces and has applications in network routing, error correction codes, and signal reconstruction in ECE."
  },
  {
    "objectID": "la-32.html#parallelogram-equation",
    "href": "la-32.html#parallelogram-equation",
    "title": "Linear Algebra",
    "section": "Parallelogram Equation",
    "text": "Parallelogram Equation\nThis theorem generalizes the property that the sum of the squares of the diagonals of a parallelogram equals the sum of the squares of its four sides.\nTheorem 3.2.6 (Parallelogram Equation): If \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are vectors in \\(R^n\\), then: \\[\n\\| \\mathbf{u} + \\mathbf{v}\\|^2 + \\| \\mathbf{u} - \\mathbf{v}\\|^2 = 2\\left(\\| \\mathbf{u}\\|^2 + \\| \\mathbf{v}\\|^2\\right) \\tag{24}\n\\]\n\nFigure 3.2.10: Parallelogram formed by \\(\\mathbf{u}\\), \\(\\mathbf{v}\\), \\(\\mathbf{u}+\\mathbf{v}\\), and \\(\\mathbf{u}-\\mathbf{v}\\).Theorem 3.2.7 (Dot Product from Norms): For vectors \\(\\mathbf{u}, \\mathbf{v}\\) in \\(R^n\\): \\[\n\\mathbf{u}\\cdot \\mathbf{v} = \\frac{1}{4}\\| \\mathbf{u} + \\mathbf{v}\\|^2 - \\frac{1}{4}\\| \\mathbf{u} - \\mathbf{v}\\|^2 \\tag{25}\n\\] This formula expresses the dot product solely in terms of norms.\n\nThe parallelogram equation provides an identity relating sums and differences of vectors to their norms. Theorem 3.2.7 is particularly interesting as it defines the dot product purely from the concept of length, reinforcing the deep connections between these fundamental vector operations. This is important in abstract vector spaces where an inner product (dot product generalization) might be defined through a norm."
  },
  {
    "objectID": "la-32.html#dot-products-as-matrix-multiplication",
    "href": "la-32.html#dot-products-as-matrix-multiplication",
    "title": "Linear Algebra",
    "section": "Dot Products as Matrix Multiplication",
    "text": "Dot Products as Matrix Multiplication\nThe dot product can be expressed using matrix notation, depending on whether vectors are row or column matrices.\n\n\n\n\n\n\n\n\n\nForm\nDot Product\n\n\n\n\n\\(\\mathbf{u}\\) column, \\(\\mathbf{v}\\) column\n\\(\\mathbf{u}^T\\mathbf{v} = \\mathbf{v}^T\\mathbf{u}\\)\n\n\n\\(\\mathbf{u}\\) row, \\(\\mathbf{v}\\) column\n\\(\\mathbf{u}\\mathbf{v} = \\mathbf{v}^T\\mathbf{u}^T\\)\n\n\n\\(\\mathbf{u}\\) column, \\(\\mathbf{v}\\) row\n\\(\\mathbf{v}\\mathbf{u} = \\mathbf{u}^T\\mathbf{v}^T\\)\n\n\n\\(\\mathbf{u}\\) row, \\(\\mathbf{v}\\) row\n\\(\\mathbf{u}\\mathbf{v}^T = \\mathbf{v}\\mathbf{u}^T\\)\n\n\n\n(Partial Table 1 from text)\n\nKey Identities: For an \\(n \\times n\\) matrix \\(A\\) and \\(n \\times 1\\) column vectors \\(\\mathbf{u}, \\mathbf{v}\\): \\[\nA\\mathbf{u}\\cdot \\mathbf{v} = \\mathbf{u}\\cdot A^T\\mathbf{v} \\tag{26}\n\\] \\[\n\\mathbf{u}\\cdot A\\mathbf{v} = A^T\\mathbf{u}\\cdot \\mathbf{v} \\tag{27}\n\\] These link matrix multiplication with the transpose.\n\n\nThis section highlights the close relationship between dot products and matrix operations. The identities involving the transpose are extremely important in proofs and algorithms in linear algebra, especially in optimization, least squares, and signal processing. For instance, in machine learning, the gradient of a loss function often involves transposes and dot products."
  },
  {
    "objectID": "la-32.html#example-9-verifying-amathbfucdot-mathbfv-mathbfucdot-atmathbfv",
    "href": "la-32.html#example-9-verifying-amathbfucdot-mathbfv-mathbfucdot-atmathbfv",
    "title": "Linear Algebra",
    "section": "EXAMPLE 9: Verifying \\(A\\mathbf{u}\\cdot \\mathbf{v} = \\mathbf{u}\\cdot A^T\\mathbf{v}\\)",
    "text": "EXAMPLE 9: Verifying \\(A\\mathbf{u}\\cdot \\mathbf{v} = \\mathbf{u}\\cdot A^T\\mathbf{v}\\)\nGiven: \\[\nA={\\left[\\begin{array}{l l l}{1}&{-2}&{3}\\\\ {2}&{4}&{1}\\\\ {-1}&{0}&{1}\\end{array}\\right]}, \\quad \\mathbf{u}={\\left[\\begin{array}{l}{-1}\\\\ {2}\\\\ {4}\\end{array}\\right]}, \\quad \\mathbf{v}={\\left[\\begin{array}{l}{-2}\\\\ {0}\\\\ {5}\\end{array}\\right]}\n\\] Calculate \\(A\\mathbf{u}\\cdot \\mathbf{v}\\) and \\(\\mathbf{u}\\cdot A^T\\mathbf{v}\\).\n\\(A\\mathbf{u}\\) calculation: \\[\nA\\mathbf{u}={\\left[\\begin{array}{l}{1(-1) - 2(2) + 3(4)}\\\\ {2(-1) + 4(2) + 1(4)}\\\\ {-1(-1) + 0(2) + 1(4)}\\end{array}\\right]}={\\left[\\begin{array}{l}{7}\\\\ {10}\\\\ {5}\\end{array}\\right]}\n\\] Then \\(A\\mathbf{u}\\cdot \\mathbf{v} = (7)(-2) + (10)(0) + (5)(5) = -14 + 0 + 25 = 11\\)."
  },
  {
    "objectID": "la-32.html#example-9-cont.",
    "href": "la-32.html#example-9-cont.",
    "title": "Linear Algebra",
    "section": "EXAMPLE 9: (cont.)",
    "text": "EXAMPLE 9: (cont.)\n\\(A^T\\mathbf{v}\\) calculation: \\[\nA^T={\\left[\\begin{array}{l l l}{1}&{2}&{-1}\\\\ {-2}&{4}&{0}\\\\ {3}&{1}&{1}\\end{array}\\right]} \\implies A^T\\mathbf{v}={\\left[\\begin{array}{l}{1(-2) + 2(0) - 1(5)}\\\\ {-2(-2) + 4(0) + 0(5)}\\\\ {3(-2) + 1(0) + 1(5)}\\end{array}\\right]}={\\left[\\begin{array}{l}{-7}\\\\ {4}\\\\ {-1}\\end{array}\\right]}\n\\] Then \\(\\mathbf{u}\\cdot A^T\\mathbf{v} = (-1)(-7) + (2)(4) + (4)(-1) = 7 + 8 - 4 = 11\\). Thus, \\(A\\mathbf{u}\\cdot \\mathbf{v} = \\mathbf{u}\\cdot A^T\\mathbf{v}\\) is verified.\n\nThis example numerically confirms the important identity \\(A\\mathbf{u}\\cdot \\mathbf{v} = \\mathbf{u}\\cdot A^T\\mathbf{v}\\). Understanding this relationship is key for advanced topics like singular value decomposition and eigenvalue problems."
  },
  {
    "objectID": "la-32.html#interactive-identity-verification",
    "href": "la-32.html#interactive-identity-verification",
    "title": "Linear Algebra",
    "section": "Interactive Identity Verification",
    "text": "Interactive Identity Verification\nVerify \\(A\\mathbf{u}\\cdot \\mathbf{v} = \\mathbf{u}\\cdot A^T\\mathbf{v}\\) with custom matrices and vectors.\n\n\n\n\n\n\n\nThis interactive code block allows you to experiment with different matrices and vectors to confirm the identity \\(A\\mathbf{u}\\cdot \\mathbf{v} = \\mathbf{u}\\cdot A^T\\mathbf{v}\\). Modify A, u, and v to see how the calculations change. This practical demonstration reinforces the theoretical identity and its computational aspects using NumPy."
  },
  {
    "objectID": "la-32.html#application-isbn-check-digits",
    "href": "la-32.html#application-isbn-check-digits",
    "title": "Linear Algebra",
    "section": "Application: ISBN Check Digits",
    "text": "Application: ISBN Check Digits\nThe International Standard Book Number (ISBN) uses a check digit based on the dot product.\n\nAn older 10-digit ISBN (e.g., 0-471-15307-9) consists of 9 data digits and a 10th check digit.\nLet \\(\\mathbf{b} = (b_1, b_2, \\ldots, b_9)\\) be the first nine digits.\nLet \\(\\mathbf{a} = (1, 2, 3, 4, 5, 6, 7, 8, 9)\\).\nThe check digit \\(c\\) is the remainder when \\(\\mathbf{a}\\cdot \\mathbf{b}\\) is divided by 11.\n\nIf \\(c=10\\), it’s written as ‘X’.\n\n\nExample: ISBN 0-471-15307-9\n\\(\\mathbf{b} = (0, 4, 7, 1, 1, 5, 3, 0, 7)\\)\n\\(\\mathbf{a}\\cdot \\mathbf{b} = (1)(0) + (2)(4) + (3)(7) + (4)(1) + (5)(1) + (6)(5) + (7)(3) + (8)(0) + (9)(7)\\)\n\\(= 0 + 8 + 21 + 4 + 5 + 30 + 21 + 0 + 63 = 152\\)\n\\(152 \\div 11 = 13\\) with a remainder of \\(9\\).\nSo, \\(c=9\\).\n\nThis is a real-world application of the dot product for error detection. This algorithm ensures that single-digit errors or transpositions of adjacent digits can be detected. This concept of error detection and correction is vital in digital communication and data storage in ECE."
  },
  {
    "objectID": "la-32.html#interactive-isbn-check-digit-calculator",
    "href": "la-32.html#interactive-isbn-check-digit-calculator",
    "title": "Linear Algebra",
    "section": "Interactive ISBN Check Digit Calculator",
    "text": "Interactive ISBN Check Digit Calculator\nEnter the first 9 digits of an ISBN to calculate its check digit.\n\n\n\n\n\n\n\nUse this interactive tool to calculate ISBN check digits. You can input any 9-digit sequence and see the resulting check digit. This helps solidify the understanding of how the dot product is used in a very practical error-checking scheme. It’s a simple yet effective application of linear algebra in daily life."
  },
  {
    "objectID": "la-32.html#summary-and-key-takeaways",
    "href": "la-32.html#summary-and-key-takeaways",
    "title": "Linear Algebra",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\n\nNorm: Measures the length or magnitude of a vector. Derived from Pythagorean theorem, generalized to \\(R^n\\).\nUnit Vectors: Vectors with norm 1, used to specify direction. Obtained by normalizing a vector.\nDistance: The norm of the difference between two vectors (points) in \\(R^n\\).\nDot Product: A scalar product that measures the “alignment” of two vectors.\n\nGeometric: \\(\\| \\mathbf{u}\\| \\| \\mathbf{v}\\| \\cos \\theta\\)\nAlgebraic: \\(\\sum u_iv_i\\)\nIndicates angle (acute, obtuse, orthogonal).\n\nCauchy-Schwarz Inequality: Fundamental for defining angles in \\(R^n\\).\nKey Identities: Triangle Inequality, Parallelogram Equation, and matrix multiplication identities.\nApplications: Error detection (ISBN), geometric analysis (cube angles), signal processing (energy, correlation).\n\n\n\n\n\n\n\nTip\n\n\nPractical Importance for ECE:\n\nSignal Processing: Norm for signal energy, dot product for correlation.\nControl Systems: State vectors, error distances.\nMachine Learning: Distance metrics, similarity measures, feature scaling.\n\n\n\n\n\nThis section has laid the groundwork for understanding vector magnitudes, proximity, and relationships. These concepts are not just theoretical; they are the bedrock of quantitative analysis in electrical and computer engineering. Ensure you’re comfortable with both the geometric and algebraic interpretations of norms, distances, and dot products. Any questions before we move on?"
  },
  {
    "objectID": "la-23.html#properties-of-determinants-cramers-rule",
    "href": "la-23.html#properties-of-determinants-cramers-rule",
    "title": "Linear Algebra",
    "section": "2.3 Properties of Determinants; Cramer’s Rule",
    "text": "2.3 Properties of Determinants; Cramer’s Rule\nImron Rosyadi"
  },
  {
    "objectID": "la-23.html#introduction-objectives",
    "href": "la-23.html#introduction-objectives",
    "title": "Linear Algebra",
    "section": "Introduction & Objectives",
    "text": "Introduction & Objectives\nToday we’ll explore fundamental properties of determinants that are essential for deeper matrix analysis and, crucially, for practical tools like finding matrix inverses and solving linear systems.\nLearning Objectives\n\nUnderstand how scalars, matrix sums, and products affect determinants.\nDefine the adjoint of a matrix and use it to find \\(A^{-1}\\).\nLearn Cramer’s Rule for solving \\(A\\mathbf{x} = \\mathbf{b}\\).\nConsolidate the major theorems linking invertibility, determinants, and system solutions.\n\nECE Relevance\nThese topics are fundamental for advanced signal processing, control system design, robotics, and circuit analysis, where efficient computation of inverses and system solutions is paramount.\n\nIn ECE, we frequently encounter large systems of linear equations or matrices representing system dynamics. Efficiently solving these or determining matrix properties is not just academic; it directly impacts the performance, stability, and feasibility of engineered systems. For example, knowing if a determinant is zero for a control system matrix quickly tells us if the system is uncontrollable or unobservable."
  },
  {
    "objectID": "la-23.html#basic-properties-scalar-multiplication-and-sums",
    "href": "la-23.html#basic-properties-scalar-multiplication-and-sums",
    "title": "Linear Algebra",
    "section": "Basic Properties: Scalar Multiplication and Sums",
    "text": "Basic Properties: Scalar Multiplication and Sums\nLet \\(A\\) and \\(B\\) be \\(n \\times n\\) matrices and \\(k\\) be any scalar.\nScalar Multiplication of a Matrix\nScaling an entire matrix \\(A\\) by \\(k\\) affects its determinant by \\(k^n\\). \\[\n\\operatorname *{det}(k A) = k^{n}\\operatorname *{det}(A)\n\\] Reason: Each of the \\(n\\) rows of \\(kA\\) has a common factor of \\(k\\). Since a common factor from any individual row can be factored out of the determinant, factoring \\(k\\) from each of the \\(n\\) rows results in \\(n\\) factors of \\(k\\).\nExample for \\(n=3\\): \\[\n\\left| \\begin{array}{lll}k a_{11} & k a_{12} & k a_{13} \\\\ k a_{21} & k a_{22} & k a_{23} \\\\ k a_{31} & k a_{32} & k a_{33} \\end{array} \\right| = k^{3}\\left| \\begin{array}{lll}a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{array} \\right|\n\\]"
  },
  {
    "objectID": "la-23.html#basic-properties-scalar-multiplication-and-sums-1",
    "href": "la-23.html#basic-properties-scalar-multiplication-and-sums-1",
    "title": "Linear Algebra",
    "section": "Basic Properties: Scalar Multiplication and Sums",
    "text": "Basic Properties: Scalar Multiplication and Sums\nDeterminant of a Sum of Matrices\nUnlike many matrix operations, there is no simple relationship between \\(\\operatorname{det}(A+B)\\), \\(\\operatorname{det}(A)\\), and \\(\\operatorname{det}(B)\\). In general, \\(\\operatorname{det}(A+B) \\neq \\operatorname{det}(A) + \\operatorname{det}(B)\\).\nExample 1: \\(\\operatorname *{det}(A + B) \\neq \\operatorname *{det}(A) + \\operatorname *{det}(B)\\)\n\\[\nA = \\left[ \\begin{array}{ll}1 & 2 \\\\ 2 & 5 \\end{array} \\right], \\quad B = \\left[ \\begin{array}{ll}3 & 1 \\\\ 1 & 3 \\end{array} \\right]\n\\] \\(\\operatorname *{det}(A) = (1)(5) - (2)(2) = 1\\) \\(\\operatorname *{det}(B) = (3)(3) - (1)(1) = 8\\) \\(A + B = \\left[ \\begin{array}{ll}4 & 3 \\\\ 3 & 8 \\end{array} \\right]\\) \\(\\operatorname *{det}(A+B) = (4)(8) - (3)(3) = 32 - 9 = 23\\) Here, \\(1 + 8 \\neq 23\\).\n\nThe scalar multiplication property for determinants is highly useful. For instance, in scaling system parameters (e.g., gains in a control system or resistor values in a circuit), this helps understand how the overall system scaling impacts the determinant, which is linked to system stability and invertibility. The non-additivity of determinants is a common point of confusion; always emphasize that determinants are not linear functions like that."
  },
  {
    "objectID": "la-23.html#sums-of-determinants-with-one-different-rowcolumn",
    "href": "la-23.html#sums-of-determinants-with-one-different-rowcolumn",
    "title": "Linear Algebra",
    "section": "Sums of Determinants with One Different Row/Column",
    "text": "Sums of Determinants with One Different Row/Column\nWhile \\(\\operatorname{det}(A+B) \\neq \\operatorname{det}(A) + \\operatorname{det}(B)\\) is true in general, there’s a specific case where a useful addition property holds.\nTheorem 2.3.1: Determinant Sums (One Row/Column Different)\nLet \\(A, B, C\\) be \\(n \\times n\\) matrices that differ only in a single row (say the \\(r\\)-th row). If the \\(r\\)-th row of \\(C\\) is the sum of the \\(r\\)-th rows of \\(A\\) and \\(B\\), then: \\[\n\\operatorname *{det}(C) = \\operatorname *{det}(A) + \\operatorname *{det}(B)\n\\] The same result holds for columns.\nExample: \\(2 \\times 2\\) Illustration\nIf \\(A = \\left[ \\begin{array}{ll}a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{array} \\right]\\) and \\(B = \\left[ \\begin{array}{ll}a_{11} & a_{12} \\\\ b_{21} & b_{22} \\end{array} \\right]\\), then \\(C = \\left[ \\begin{array}{cc}{a_{11}} & {a_{12}}\\\\ {a_{21} + b_{21}} & {a_{22} + b_{22}} \\end{array} \\right]\\).\nCalculating:\n\\(\\operatorname{det}(A) + \\operatorname{det}(B) = (a_{11}a_{22} - a_{12}a_{21}) + (a_{11}b_{22} - a_{12}b_{21})\\)\n\\(= a_{11}(a_{22} + b_{22}) - a_{12}(a_{21} + b_{21})\\)\nThis is exactly \\(\\operatorname{det}(C)\\), expanded along the second row.\n\nThis property is less intuitive but can sometimes simplify calculations if you can decompose a matrix into two simpler ones that differ by only one row or column. It’s a precise condition, not a general one. While less frequently applied directly than other properties in typical ECE problem-solving, it is fundamental in proving other more advanced determinant properties and in some theoretical developments in linear algebra."
  },
  {
    "objectID": "la-23.html#determinant-of-a-matrix-product",
    "href": "la-23.html#determinant-of-a-matrix-product",
    "title": "Linear Algebra",
    "section": "Determinant of a Matrix Product",
    "text": "Determinant of a Matrix Product\nOne of the most elegant and powerful properties of determinants is how they behave with matrix multiplication.\nTheorem 2.3.4: Determinant of a Product\nIf \\(A\\) and \\(B\\) are square matrices of the same size, then: \\[\n\\operatorname *{det}(A B) = \\operatorname *{det}(A)\\operatorname *{det}(B)\n\\]\nProof sketch:\n\nLemma 2.3.2 (Elementary Matrices): First, show for an elementary matrix \\(E\\) and any matrix \\(B\\), \\(\\operatorname{det}(EB) = \\operatorname{det}(E)\\operatorname{det}(B)\\).\n\nThis is shown by considering the three types of EROs and how they affect the determinant (Theorem 2.2.3 and 2.2.4)."
  },
  {
    "objectID": "la-23.html#determinant-of-a-matrix-product-1",
    "href": "la-23.html#determinant-of-a-matrix-product-1",
    "title": "Linear Algebra",
    "section": "Determinant of a Matrix Product",
    "text": "Determinant of a Matrix Product\n\nDeterminant Test for Invertibility (Theorem 2.3.3): A square matrix \\(A\\) is invertible if and only if \\(\\operatorname{det}(A) \\ne 0\\).\n\nThis critical theorem implies that if \\(\\operatorname{det}(A)=0\\), then \\(AB\\) is also not invertible, meaning \\(\\operatorname{det}(AB)=0\\), so \\(0 = 0 \\cdot \\operatorname{det}(B)\\).\n\nGeneral Case: If \\(A\\) is invertible, it can be written as a product of elementary matrices \\(A = E_1 E_2 \\cdots E_r\\). Then, using Lemma 2.3.2 repeatedly:\n\\(\\operatorname{det}(AB) = \\operatorname{det}(E_1 E_2 \\cdots E_r B) = \\operatorname{det}(E_1)\\operatorname{det}(E_2)\\cdots\\operatorname{det}(E_r)\\operatorname{det}(B)\\)\nAnd \\(\\operatorname{det}(A) = \\operatorname{det}(E_1 E_2 \\cdots E_r) = \\operatorname{det}(E_1)\\operatorname{det}(E_2)\\cdots\\operatorname{det}(E_r)\\).\nCombining these gives \\(\\operatorname{det}(AB) = \\operatorname{det}(A)\\operatorname{det}(B)\\).\n\n\nThis theorem is incredibly useful in ECE, especially in transform domains (e.g., Fourier, Laplace) where operations might involve matrix products. For example, if you’re chaining linear transformations (like processing a signal through multiple filters represented by matrices), the overall scaling effect on the “volume” (determinant) is simply the product of the individual scalings. This property is also critical when dealing with system stability where product matrices arise, e.g., in cascaded control systems."
  },
  {
    "objectID": "la-23.html#interactive-example-determinant-of-a-product",
    "href": "la-23.html#interactive-example-determinant-of-a-product",
    "title": "Linear Algebra",
    "section": "Interactive Example: Determinant of a Product",
    "text": "Interactive Example: Determinant of a Product\nLet’s verify \\(\\operatorname *{det}(AB) = \\operatorname *{det}(A)\\operatorname *{det}(B)\\) with an example.\nGiven matrices: \\[\nA={\\left[\\begin{array}{l l}{3}&{1}\\\\ {2}&{1}\\end{array}\\right]},\\quad B={\\left[\\begin{array}{l l}{-1}&{3}\\\\ {5}&{8}\\end{array}\\right]}\n\\]\n\nCalculate \\(\\operatorname *{det}(A)\\) and \\(\\operatorname *{det}(B)\\).\nCalculate \\(AB\\).\nCalculate \\(\\operatorname *{det}(AB)\\).\nCompare results.\n\n\n\n\n\n\n\n\nThe example clearly shows that \\(\\operatorname{det}(A)=1\\), \\(\\operatorname{det}(B)=-23\\), and \\(\\operatorname{det}(AB)=-23\\), confirming the theorem. Play around with different matrices in the interactive code. This hands-on verification strengthens the understanding of this very important determinant property."
  },
  {
    "objectID": "la-23.html#determinant-of-an-inverse",
    "href": "la-23.html#determinant-of-an-inverse",
    "title": "Linear Algebra",
    "section": "Determinant of an Inverse",
    "text": "Determinant of an Inverse\nAnother useful property relates the determinant of an invertible matrix to the determinant of its inverse.\nTheorem 2.3.5: Determinant of Inverse\nIf \\(A\\) is invertible, then: \\[\n\\operatorname *{det}(A^{-1}) = \\frac{1}{\\operatorname*{det}(A)}\n\\]\nProof sketch:\nSince \\(A^{-1}A = I\\), by Theorem 2.3.4, we have \\(\\operatorname{det}(A^{-1}A) = \\operatorname{det}(A^{-1})\\operatorname{det}(A)\\).\nAlso, \\(\\operatorname{det}(I) = 1\\).\nThus, \\(\\operatorname{det}(A^{-1})\\operatorname{det}(A) = 1\\).\nSince \\(A\\) is invertible, \\(\\operatorname{det}(A) \\ne 0\\), so we can divide by \\(\\operatorname{det}(A)\\) to get \\(\\operatorname{det}(A^{-1}) = 1/\\operatorname{det}(A)\\).\n\nThis theorem reinforces the link between invertibility (\\(\\operatorname{det}(A) \\ne 0\\)) and the determinant. Its direct application is when you have to calculate a determinant of an inverse, you don’t actually have to calculate the inverse matrix itself. For system stability analysis or filter design in ECE, understanding the determinant of \\(A^{-1}\\) can indicate how “robust” a system is to perturbations."
  },
  {
    "objectID": "la-23.html#adjoint-of-a-matrix",
    "href": "la-23.html#adjoint-of-a-matrix",
    "title": "Linear Algebra",
    "section": "Adjoint of a Matrix",
    "text": "Adjoint of a Matrix\nThe adjoint of a matrix provides a direct formula for the inverse, extending the \\(2 \\times 2\\) formula.\nDEFINITION 1: Adjoint of \\(A\\), \\(\\operatorname{adj}(A)\\) If \\(A\\) is any \\(n \\times n\\) matrix and \\(C_{ij}\\) is the cofactor of \\(a_{ij}\\), then the matrix of cofactors is: \\[\n\\left[ \\begin{array}{c c c c}{C_{11}} & {C_{12}} & \\dots & {C_{1n}}\\\\ {C_{21}} & {C_{22}} & \\dots & {C_{2n}}\\\\ \\vdots & \\vdots & & \\vdots \\\\ {C_{n1}} & {C_{n2}} & \\dots & {C_{n n}} \\end{array} \\right]\n\\] The adjoint of \\(A\\), denoted \\(\\operatorname{adj}(A)\\), is the transpose of this matrix of cofactors. \\[\n\\operatorname{adj}(A) = [\\text{Matrix of Cofactors}]^T\n\\]"
  },
  {
    "objectID": "la-23.html#adjoint-of-a-matrix-1",
    "href": "la-23.html#adjoint-of-a-matrix-1",
    "title": "Linear Algebra",
    "section": "Adjoint of a Matrix",
    "text": "Adjoint of a Matrix\nImportant Property: Off-Diagonal Cofactor Sums\nIf you multiply the entries in any row (or column) of \\(A\\) by the corresponding cofactors from a different row (or column), the sum is always zero. Example (from text): \\(3C_{21} + 2C_{22} + (-1)C_{23} = 0 \\quad\\) (entries from row 1, cofactors from row 2).\nTheorem 2.3.6: Formula for \\(A^{-1}\\)\nIf \\(A\\) is an invertible matrix, then: \\[\nA^{-1} = \\frac{1}{\\operatorname*{det}(A)} \\operatorname {adj}(A)\n\\]\n\nThe adjoint formula is a foundational result for understanding matrix inverses in a theoretical sense. While numerically, Gaussian elimination is much faster for high-dimensional matrices, the adjoint formula is invaluable for smaller systems or symbolic computations, for example, in deriving transfer functions in control systems or impedance matrices in circuits. The property that off-diagonal cofactor sums are zero is key to the proof and a beautiful aspect of determinant theory."
  },
  {
    "objectID": "la-23.html#example-finding-inverse-using-adjoint",
    "href": "la-23.html#example-finding-inverse-using-adjoint",
    "title": "Linear Algebra",
    "section": "Example: Finding Inverse using Adjoint",
    "text": "Example: Finding Inverse using Adjoint\nLet’s find the inverse of \\(A\\) using the adjoint formula. \\[\nA = \\left[ \\begin{array}{rrr}3 & 2 & -1 \\\\ 1 & 6 & 3 \\\\ 2 & -4 & 0 \\end{array} \\right]\n\\]\nFrom previous calculations (Example 5. in original text): \\(\\operatorname *{det}(A) = 64\\).\nCofactors of \\(A\\): \\(C_{11}=12, \\quad C_{12}=6, \\quad C_{13}=-16\\) \\(C_{21}=4, \\quad C_{22}=2, \\quad C_{23}=16\\) \\(C_{31}=12, \\quad C_{32}=-10, \\quad C_{33}=16\\)\nMatrix of Cofactors: \\[\n\\left[ \\begin{array}{rrr}12 & 6 & -16 \\\\ 4 & 2 & 16 \\\\ 12 & -10 & 16 \\end{array} \\right]\n\\]"
  },
  {
    "objectID": "la-23.html#example-finding-inverse-using-adjoint-1",
    "href": "la-23.html#example-finding-inverse-using-adjoint-1",
    "title": "Linear Algebra",
    "section": "Example: Finding Inverse using Adjoint",
    "text": "Example: Finding Inverse using Adjoint\nAdjoint of \\(A\\) (transpose of matrix of cofactors): \\[\n\\operatorname {adj}(A) = \\left[ \\begin{array}{rrr}12 & 4 & 12 \\\\ 6 & 2 & -10 \\\\ -16 & 16 & 16 \\end{array} \\right]\n\\]\nInverse of \\(A\\): \\[\nA^{-1} = \\frac{1}{\\operatorname*{det}(A)}\\mathrm{adj}(A) = \\frac{1}{64}\\left[ \\begin{array}{rrr}{12} & 4 & {12}\\\\ 6 & {2} & {-10}\\\\ {-16} & {16} & {16} \\end{array} \\right] = \\left[ \\begin{array}{rrr}\\frac{12}{64} & \\frac{4}{64} & \\frac{12}{64}\\\\ \\frac{6}{64} & \\frac{2}{64} & -\\frac{10}{64}\\\\ -\\frac{16}{64} & \\frac{16}{64} & \\frac{16}{64} \\end{array} \\right]\n= \\left[ \\begin{array}{rrr}\\frac{3}{16} & \\frac{1}{16} & \\frac{3}{16}\\\\ \\frac{3}{32} & \\frac{1}{32} & -\\frac{5}{32}\\\\ -\\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} \\end{array} \\right]\n\\]\n\nWalk through the calculation of cofactors and then forming the adjoint carefully, emphasizing the transpose step. Then show the final division by the determinant. This method provides a clear, step-by-step approach for computing the inverse of \\(3 \\times 3\\) matrices, which is essential practice. In ECE, for small systems (like a 3-node circuit or a 3-state control system), this formula is viable."
  },
  {
    "objectID": "la-23.html#interactive-verification-adjoint-inverse",
    "href": "la-23.html#interactive-verification-adjoint-inverse",
    "title": "Linear Algebra",
    "section": "Interactive Verification: Adjoint Inverse",
    "text": "Interactive Verification: Adjoint Inverse\nVerify the inverse calculated using the adjoint method with NumPy."
  },
  {
    "objectID": "la-23.html#cramers-rule-for-solving-linear-systems",
    "href": "la-23.html#cramers-rule-for-solving-linear-systems",
    "title": "Linear Algebra",
    "section": "Cramer’s Rule for Solving Linear Systems",
    "text": "Cramer’s Rule for Solving Linear Systems\nCramer’s Rule offers a general formula for the solution of \\(A\\mathbf{x} = \\mathbf{b}\\) when \\(A\\) is invertible. While powerful conceptually, it’s often slower than Gaussian elimination for larger systems.\nTheorem 2.3.7: Cramer’s Rule\nIf \\(A\\mathbf{x} = \\mathbf{b}\\) is a system of \\(n\\) linear equations in \\(n\\) unknowns such that \\(\\operatorname *{det}(A) \\neq 0\\), then the system has a unique solution given by: \\[\nx_{1} = \\frac{\\operatorname*{det}(A_{1})}{\\operatorname*{det}(A)}, x_{2} = \\frac{\\operatorname*{det}(A_{2})}{\\operatorname*{det}(A)}, \\ldots , x_{n} = \\frac{\\operatorname*{det}(A_{n})}{\\operatorname*{det}(A)}\n\\] where \\(A_{j}\\) is the matrix obtained by replacing the \\(j\\)-th column of \\(A\\) with the column vector \\(\\mathbf{b}\\).\nProof sketch:\nThe proof uses the formula \\(\\mathbf{x} = A^{-1}\\mathbf{b} = \\frac{1}{\\operatorname{det}(A)}\\operatorname{adj}(A)\\mathbf{b}\\).\nExpanding this matrix multiplication term by term reveals that the \\(j\\)-th component \\(x_j\\) is precisely \\(\\frac{b_1 C_{1j} + b_2 C_{2j} + \\dots + b_n C_{nj}}{\\operatorname{det}(A)}\\).\nThe numerator is precisely the cofactor expansion of \\(\\operatorname{det}(A_j)\\) along its \\(j\\)-th column.\n\nCramer’s Rule is a classic result with conceptual importance. In ECE, it’s particularly useful for small systems (like 2x2 or 3x3) where you might need an analytical solution or when parameters are involved, allowing calculation of a symbolic rather than just numerical result. In signal processing, for example, if you have a transfer function denominator polynomial, calculating its roots often involves solving systems of equations, and Cramer’s rule can sometimes provide the explicit functions for these roots. For large systems, however, direct numerical solvers are preferable due to computational cost."
  },
  {
    "objectID": "la-23.html#example-using-cramers-rule",
    "href": "la-23.html#example-using-cramers-rule",
    "title": "Linear Algebra",
    "section": "Example: Using Cramer’s Rule",
    "text": "Example: Using Cramer’s Rule\nSolve the following system using Cramer’s Rule: \\[\n\\begin{array}{r l} & {x_{1} + 0x_{2} + 2x_{3} = 6}\\\\ & {-3x_{1} + 4x_{2} + 6x_{3} = 30}\\\\ & {-x_{1} - 2x_{2} + 3x_{3} = 8}\\end{array}\n\\]\nThe coefficient matrix \\(A\\) and the augmented matrices \\(A_1, A_2, A_3\\) are: \\[\nA=\\left[\\begin{array}{crc}{1}&{0}&{2}\\\\ {-3}&{4}&{6}\\\\ {-1}&{-2}&{3}\\end{array}\\right], \\quad A_{1}=\\left[\\begin{array}{crc}{6}&{0}&{2}\\\\ {30}&{4}&{6}\\\\ {8}&{-2}&{3}\\end{array}\\right]\n\\] \\[\nA_{2}=\\left[\\begin{array}{crc}{1}&{6}&{2}\\\\ {-3}&{30}&{6}\\\\ {-1}&{8}&{3}\\end{array}\\right], \\quad A_{3}=\\left[\\begin{array}{crc}{1}&{0}&{6}\\\\ {-3}&{4}&{30}\\\\ {-1}&{-2}&{8}\\end{array}\\right]\n\\]"
  },
  {
    "objectID": "la-23.html#example-using-cramers-rule-1",
    "href": "la-23.html#example-using-cramers-rule-1",
    "title": "Linear Algebra",
    "section": "Example: Using Cramer’s Rule",
    "text": "Example: Using Cramer’s Rule\nFirst, find \\(\\operatorname{det}(A)\\): \\(\\operatorname{det}(A) = 1(12 - (-12)) - 0(\\dots) + 2(6 - (-4)) = 1(24) + 2(10) = 24 + 20 = 44\\).\nNext, calculate determinants of \\(A_1, A_2, A_3\\):\n\\(\\operatorname{det}(A_1) = 6(12 - (-12)) - 0(\\dots) + 2(-60 - 32) = 6(24) + 2(-92) = 144 - 184 = -40\\).\n\\(\\operatorname{det}(A_2) = 1(90 - 48) - 6(-9 - (-6)) + 2(-24 - (-30)) = 1(42) - 6(-3) + 2(6) = 42 + 18 + 12 = 72\\).\n\\(\\operatorname{det}(A_3) = 1(32 - (-60)) - 0(\\dots) + 6(6 - (-4)) = 1(92) + 6(10) = 92 + 60 = 152\\).\nFinally, compute \\(x_1, x_2, x_3\\):\n\\(x_{1} = \\frac{\\operatorname*{det}(A_{1})}{\\operatorname*{det}(A)} = \\frac{-40}{44} = \\frac{-10}{11}\\)\n\\(x_{2} = \\frac{\\operatorname*{det}(A_{2})}{\\operatorname*{det}(A)} = \\frac{72}{44} = \\frac{18}{11}\\)\n\\(x_{3} = \\frac{\\operatorname*{det}(A_{3})}{\\operatorname*{det}(A)} = \\frac{152}{44} = \\frac{38}{11}\\)\n\nThis manual calculation is quite tedious but demonstrates the mechanics of Cramer’s Rule. Emphasize that each step involves calculating a determinant. While it’s great for understanding, for realistic engineering systems, numerical methods like Gauss-Jordan or LU decomposition are orders of magnitude faster."
  },
  {
    "objectID": "la-23.html#interactive-verification-cramers-rule",
    "href": "la-23.html#interactive-verification-cramers-rule",
    "title": "Linear Algebra",
    "section": "Interactive Verification: Cramer’s Rule",
    "text": "Interactive Verification: Cramer’s Rule\nLet’s use Pyodide to verify our solution from the previous example."
  },
  {
    "objectID": "la-23.html#equivalent-statements-the-big-picture",
    "href": "la-23.html#equivalent-statements-the-big-picture",
    "title": "Linear Algebra",
    "section": "Equivalent Statements: The Big Picture",
    "text": "Equivalent Statements: The Big Picture\nThis theorem unifies many core concepts in Linear Algebra that relate to matrix invertibility, system solutions, and rank.\nTheorem 2.3.8: Equivalent Statements\nIf \\(A\\) is an \\(n \\times n\\) matrix, then the following statements are equivalent:\n\n\\(A\\) is invertible.\n\\(A\\mathbf{x} = \\mathbf{0}\\) has only the trivial solution (\\(\\mathbf{x} = \\mathbf{0}\\)).\nThe reduced row echelon form of \\(A\\) is \\(I_{n}\\).\n\\(A\\) can be expressed as a product of elementary matrices.\n\\(A\\mathbf{x} = \\mathbf{b}\\) is consistent for every \\(n \\times 1\\) matrix \\(\\mathbf{b}\\).\n\\(A\\mathbf{x} = \\mathbf{b}\\) has exactly one solution for every \\(n \\times 1\\) matrix \\(\\mathbf{b}\\).\n\\(\\operatorname *{det}(A)\\neq 0\\)."
  },
  {
    "objectID": "la-23.html#equivalent-statements-the-big-picture-1",
    "href": "la-23.html#equivalent-statements-the-big-picture-1",
    "title": "Linear Algebra",
    "section": "Equivalent Statements: The Big Picture",
    "text": "Equivalent Statements: The Big Picture\nECE Significance\nThis theorem is a cornerstone for understanding and diagnosing linear systems in engineering.\n\nSystem Solvability: In circuit analysis or control, if \\(\\det(A) = 0\\), the system might have no unique solution, indicating a design flaw or dependency.\nControl & State Space: An invertible system matrix ensures controllability or observability (depends on specific matrix, e.g., controllability matrix), crucial for designing stable and responsive systems.\nHardware/Software Implications: Non-invertible matrices often lead to numerical instability or errors in computational simulations and algorithms used in embedded systems or scientific computing.\nStability Analysis In control systems, if a system matrix \\(A\\) needs to be inverted (e.g., to find state transitions), a non-zero determinant is essential. The value \\(1/\\operatorname{det}(A)\\) provides insight into how sensitive the inverse operation might be to small changes in \\(A\\). A very small \\(\\operatorname{det}(A)\\) implies a very large \\(\\operatorname{det}(A^{-1})\\), indicating the system might be close to singularity or numerically ill-conditioned, which is a critical concern in signal processing and numerical simulations.\n\n\nEmphasize this theorem as a grand unifying theory up to this point in Linear Algebra. For ECE students, it’s not just a list to memorize; it’s a set of diagnostic tools. For example, if you’re trying to find an inverse and \\(\\operatorname{det}(A)=0\\), you immediately know A is not invertible, which in turn tells you Ax=0 has non-trivial solutions, and its RREF is not the identity."
  },
  {
    "objectID": "la-23.html#summary-and-key-takeaways",
    "href": "la-23.html#summary-and-key-takeaways",
    "title": "Linear Algebra",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\nProperties of Determinants\n\nScalar Multiplication: \\(\\operatorname{det}(kA) = k^n \\operatorname{det}(A)\\).\nSums (Specific Case): \\(\\operatorname{det}(C) = \\operatorname{det}(A) + \\operatorname{det}(B)\\) if only one row/column differs.\nProducts: \\(\\operatorname{det}(AB) = \\operatorname{det}(A)\\operatorname{det}(B)\\).\nInverse: \\(\\operatorname{det}(A^{-1}) = 1/\\operatorname{det}(A)\\).\n\nAdvanced Tools\n\nAdjoint Formula for Inverse: \\(A^{-1} = \\frac{1}{\\operatorname*{det}(A)} \\operatorname {adj}(A)\\).\nCramer’s Rule for Linear Systems: \\(x_j = \\frac{\\operatorname*{det}(A_j)}{\\operatorname*{det}(A)}\\)."
  },
  {
    "objectID": "la-23.html#summary-and-key-takeaways-1",
    "href": "la-23.html#summary-and-key-takeaways-1",
    "title": "Linear Algebra",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\nUnifying Theorem\n\nEquivalent Statements (Theorem 2.3.8): Invertibility, unique solutions, RREF to \\(I_n\\), product of elementary matrices, and non-zero determinant are all interconnected.\n\nImportance for ECE\nThese concepts are vital for:\n\nAnalyzing stability and behavior of linear systems.\nEfficiently solving systems of equations in hardware and software.\nUnderstanding the theoretical underpinnings of advanced numerical algorithms.\n\n\nToday’s lecture has armed you with crucial insights into the behavior of determinants under various matrix operations. These aren’t just abstract mathematical facts; they are foundational principles that will guide your understanding and problem-solving in numerous ECE applications, from signal processing to control theory and beyond. Master these properties, and you’ll gain a deeper appreciation for the logic and power of linear algebra."
  },
  {
    "objectID": "la-21.html#determinants-by-cofactor-expansion",
    "href": "la-21.html#determinants-by-cofactor-expansion",
    "title": "Linear Algebra",
    "section": "2.1 Determinants by Cofactor Expansion",
    "text": "2.1 Determinants by Cofactor Expansion\nImron Rosyadi"
  },
  {
    "objectID": "la-21.html#what-are-determinants",
    "href": "la-21.html#what-are-determinants",
    "title": "Linear Algebra",
    "section": "What are Determinants?",
    "text": "What are Determinants?\nDeterminants are fundamental to Linear Algebra, providing scalar values that encapsulate key properties of a square matrix.\nWhy do we care?\n\nMatrix Invertibility: A square matrix \\(A\\) is invertible if and only if \\(\\operatorname{det}(A) \\ne 0\\).\nSolving Linear Systems: Determinants are crucial in Cramer’s Rule for solving systems of linear equations.\nGeometric Interpretation: Represent the scaling factor of the linear transformation defined by the matrix (area, volume).\nEigenvalues: Used to find eigenvalues, critical for understanding system stability in ECE (e.g., control systems, signal processing).\n\n\nToday, we’ll start with the definition of determinants using cofactor expansion, focusing on how to compute them and why they are important. We’ll build up from simple \\(2 \\times 2\\) matrices to higher orders, connecting these mathematical concepts to real-world engineering problems."
  },
  {
    "objectID": "la-21.html#times-2-matrix-determinants",
    "href": "la-21.html#times-2-matrix-determinants",
    "title": "Linear Algebra",
    "section": "\\(2 \\times 2\\) Matrix Determinants",
    "text": "\\(2 \\times 2\\) Matrix Determinants\nRecall from previous discussions, a \\(2 \\times 2\\) matrix \\(A\\) is invertible if its determinant is non-zero.\n\\[\nA = \\left[ \\begin{array}{ll}a & b \\\\ c & d \\end{array} \\right]\n\\]\nThe determinant of \\(A\\) is a scalar value: \\[\n\\operatorname {det}(A) = a d - b c \\quad \\text{or} \\quad \\left| \\begin{array}{ll}a & b \\\\ c & d \\end{array} \\right| = a d - b c\n\\]\nImportant: \\(\\operatorname{det}(A)\\) is a number, while \\(A\\) is a matrix.\nInverse of a \\(2 \\times 2\\) Matrix\nIf \\(\\operatorname{det}(A) \\ne 0\\), the inverse of \\(A\\) can be expressed as: \\[\nA^{-1} = \\frac{1}{\\operatorname{det}(A)} \\left[ \\begin{array}{rr}d & -b \\\\ -c & a \\end{array} \\right]\n\\]\n\nFor ECE students, knowing if a system matrix is invertible is critical. For example, in circuit analysis, if the system matrix of nodal equations is singular (i.e., its determinant is zero), it implies the circuit has no unique solution or is improperly designed (e.g., a short circuit or open circuit that prevents a unique voltage/current distribution)."
  },
  {
    "objectID": "la-21.html#minors-and-cofactors-building-blocks",
    "href": "la-21.html#minors-and-cofactors-building-blocks",
    "title": "Linear Algebra",
    "section": "Minors and Cofactors: Building Blocks",
    "text": "Minors and Cofactors: Building Blocks\nTo define determinants for larger matrices inductively, we use new terminology: minors and cofactors.\nFirst, let’s use subscripted entries for a general \\(2 \\times 2\\) matrix: \\[\nA = \\left[ \\begin{array}{ll}a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{array} \\right] \\implies \\operatorname *{det}(A) = a_{11}a_{22} - a_{12}a_{21}\n\\]\nBase case for induction: The determinant of a \\(1 \\times 1\\) matrix \\([a_{11}]\\) is \\(\\operatorname *{det}[a_{11}] = a_{11}\\).\nDefinition 1: Minors and Cofactors\nIf \\(A\\) is a square matrix:\n\nThe minor of entry \\(a_{ij}\\), denoted \\(M_{ij}\\), is the determinant of the submatrix that remains after deleting the \\(i\\)-th row and \\(j\\)-th column from \\(A\\).\nThe cofactor of entry \\(a_{ij}\\), denoted \\(C_{ij}\\), is given by \\(C_{ij} = (- 1)^{i + j}M_{ij}\\).\n\n\nThe concept of minors and cofactors allows us to break down a larger determinant calculation into smaller, more manageable determinant calculations. This is similar to how we might analyze a complex system (like a control system) by breaking it into smaller sub-systems. This inductive definition lets us define determinants for any \\(n \\times n\\) matrix."
  },
  {
    "objectID": "la-21.html#visualizing-cofactor-signs-and-examples",
    "href": "la-21.html#visualizing-cofactor-signs-and-examples",
    "title": "Linear Algebra",
    "section": "Visualizing Cofactor Signs and Examples",
    "text": "Visualizing Cofactor Signs and Examples\nThe sign factor \\((-1)^{i+j}\\) follows a “checkerboard” pattern:\n\\[\n\\left[{\\begin{array}{l l l l l l}{+}&{-}&{+}&{-}&{+}&{\\cdots}\\\\ {-}&{+}&{-}&{+}&{-}&{\\cdots}\\\\ {+}&{-}&{+}&{-}&{+}&{\\cdots}\\\\ {-}&{+}&{-}&{+}&{-}&{\\cdots}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}\\end{array}}\\right]\n\\]\nThis means \\(C_{ij} = M_{ij}\\) if \\(i+j\\) is even (positive sign), and \\(C_{ij} = -M_{ij}\\) if \\(i+j\\) is odd (negative sign)."
  },
  {
    "objectID": "la-21.html#visualizing-cofactor-signs-and-examples-1",
    "href": "la-21.html#visualizing-cofactor-signs-and-examples-1",
    "title": "Linear Algebra",
    "section": "Visualizing Cofactor Signs and Examples",
    "text": "Visualizing Cofactor Signs and Examples\nExample: \\(M_{11}\\) and \\(C_{11}\\)\nLet \\[\nA = \\left[ \\begin{array}{ccc}3 & 1 & -4 \\\\ 2 & 5 & 6 \\\\ 1 & 4 & 8 \\end{array} \\right]\n\\] To find \\(M_{11}\\), delete row 1 and column 1: \\[\nM_{11} = \\operatorname{det}\\left| \\begin{array}{cc}5 & 6 \\\\ 4 & 8 \\end{array} \\right| = (5)(8) - (6)(4) = 40 - 24 = 16\n\\] Since \\(1+1=2\\) (even), \\(C_{11} = (-1)^{1+1}M_{11} = M_{11} = 16\\).\nExample: \\(M_{32}\\) and \\(C_{32}\\)\nTo find \\(M_{32}\\), delete row 3 and column 2: \\[\nM_{32}=\\operatorname{det}\\left|\\begin{array}{cc}{3}&{-4}\\\\ {2}&{6}\\end{array}\\right|=(3)(6)-(-4)(2) = 18 - (-8) = 26\n\\] Since \\(3+2=5\\) (odd), \\(C_{32} = (-1)^{3+2}M_{32} = -M_{32} = -26\\).\n\nFor students, it’s often easier and less error-prone to first compute the minor \\(M_{ij}\\), and then apply the sign from the checkerboard pattern rather than explicitly calculating \\((-1)^{i+j}\\) every time. This visual aid helps to quickly determine the correct sign."
  },
  {
    "objectID": "la-21.html#cofactor-expansions-of-a-2-times-2-matrix",
    "href": "la-21.html#cofactor-expansions-of-a-2-times-2-matrix",
    "title": "Linear Algebra",
    "section": "Cofactor Expansions of a \\(2 \\times 2\\) Matrix",
    "text": "Cofactor Expansions of a \\(2 \\times 2\\) Matrix\nLet’s re-examine the \\(2 \\times 2\\) determinant using cofactors. For \\(A = \\left[ \\begin{array}{ll}a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{array} \\right]\\), the checkerboard pattern for signs is \\(\\left[{\\begin{array}{c c}{+}&{-}\\\\ {-}&{+}\\end{array}}\\right]\\).\nThe cofactors are:\n\n\\(C_{11} = M_{11} = a_{22}\\)\n\\(C_{12} = -M_{12} = -a_{21}\\)\n\\(C_{21} = -M_{21} = -a_{12}\\)\n\\(C_{22} = M_{22} = a_{11}\\)\n\nUsing these, \\(\\operatorname{det}(A)\\) can be expressed in terms of cofactors in four ways:\n\nFirst row expansion: \\(\\operatorname{det}(A) = a_{11}C_{11} + a_{12}C_{12}\\)\nSecond row expansion: \\(\\operatorname{det}(A) = a_{21}C_{21} + a_{22}C_{22}\\)\nFirst column expansion: \\(\\operatorname{det}(A) = a_{11}C_{11} + a_{21}C_{21}\\)\nSecond column expansion: \\(\\operatorname{det}(A) = a_{12}C_{12} + a_{22}C_{22}\\)\n\nAll four expansions evaluate to \\(\\operatorname{det}(A) = a_{11}a_{22} - a_{12}a_{21}\\).\n\nThe fact that all four cofactor expansions yield the same determinant is a powerful result, formalized in Theorem 2.1.1 for general \\(n \\times n\\) matrices. This flexibility is key: it means we have a choice of which row or column to expand along, which can be used to simplify calculations significantly, especially for higher-order matrices."
  },
  {
    "objectID": "la-21.html#general-definition-of-a-determinant",
    "href": "la-21.html#general-definition-of-a-determinant",
    "title": "Linear Algebra",
    "section": "General Definition of a Determinant",
    "text": "General Definition of a Determinant\nTheorem 2.1.1 (Uniqueness of Cofactor Expansion) If \\(A\\) is an \\(n \\times n\\) matrix, then regardless of which row or column of \\(A\\) is chosen, the number obtained by multiplying the entries in that row or column by the corresponding cofactors and adding the resulting products is always the same.\nThis allows for the formal definition for general \\(n \\times n\\) matrices:\nDefinition 2: Determinant of an \\(n \\times n\\) Matrix\nThe determinant of an \\(n \\times n\\) matrix \\(A\\) is the number obtained by any of the following cofactor expansions:\n\nCofactor expansion along the \\(j\\)-th column: \\[\n\\operatorname *{det}(A) = a_{1j}C_{1j} + a_{2j}C_{2j} + \\dots +a_{nj}C_{nj}\n\\]\nCofactor expansion along the \\(i\\)-th row: \\[\n\\operatorname *{det}(A) = a_{i1}C_{i1} + a_{i2}C_{i2} + \\dots +a_{in}C_{in}\n\\]\n\n\nThis theorem reinforces the consistency of the determinant value regardless of the expansion path chosen. This property is crucial for making computations efficient. For instance, in analysis of large-scale systems in ECE, we often need to determine matrix properties. While direct computation of determinants by hand is infeasible for these large systems, the underlying principle of cofactor expansion is fundamental to understanding more advanced numerical algorithms."
  },
  {
    "objectID": "la-21.html#example-3-times-3-cofactor-expansion-row",
    "href": "la-21.html#example-3-times-3-cofactor-expansion-row",
    "title": "Linear Algebra",
    "section": "Example: \\(3 \\times 3\\) Cofactor Expansion (Row)",
    "text": "Example: \\(3 \\times 3\\) Cofactor Expansion (Row)\nLet’s find the determinant of \\(A\\) by cofactor expansion along the first row:\n\\[\nA=\\left[{\\begin{array}{r r r}{3}&{1}&{0}\\\\ {-2}&{-4}&{3}\\\\ {5}&{4}&{-2}\\end{array}}\\right]\n\\]\n\\(\\operatorname *{det}(A) = a_{11}C_{11} + a_{12}C_{12} + a_{13}C_{13}\\)\n\\[\n\\begin{aligned}\n\\operatorname *{det}(A) &= 3 \\cdot \\operatorname{det}\\left| \\begin{array}{cc}{-4} & 3\\\\ 4 & {-2} \\end{array} \\right| - 1 \\cdot \\operatorname{det}\\left| \\begin{array}{cc}{-2} & 3\\\\ 5 & {-2} \\end{array} \\right| + 0 \\cdot \\operatorname{det}\\left| \\begin{array}{cc}{-2} & {-4}\\\\ 5 & 4 \\end{array} \\right| \\\\\n&= 3((-4)(-2) - (3)(4)) - 1((-2)(-2) - (3)(5)) + 0 \\cdot (\\text{any value}) \\\\\n&= 3(8 - 12) - 1(4 - 15) + 0 \\\\\n&= 3(-4) - 1(-11) + 0 \\\\\n&= -12 + 11 + 0 \\\\\n&= -1\n\\end{aligned}\n\\]\n\nNotice how the term multiplied by zero (\\(a_{13}C_{13}\\)) conveniently disappears. This is our first practical clue for a smart strategy: always choose rows or columns with the most zeros! This significantly reduces the number of minors you have to compute, making the process much faster—a critical skill for efficiency in engineering calculations."
  },
  {
    "objectID": "la-21.html#smart-choice-for-cofactor-expansion",
    "href": "la-21.html#smart-choice-for-cofactor-expansion",
    "title": "Linear Algebra",
    "section": "Smart Choice for Cofactor Expansion",
    "text": "Smart Choice for Cofactor Expansion\nThe best strategy for cofactor expansion is to expand along a row or column with the most zero entries. This reduces the number of minors you need to compute.\nLet’s use the same matrix and evaluate \\(\\operatorname{det}(A)\\) by cofactor expansion along the first column:\n\\[\nA=\\left[{\\begin{array}{r r r}{3}&{1}&{0}\\\\ {-2}&{-4}&{3}\\\\ {5}&{4}&{-2}\\end{array}}\\right]\n\\]\n\\(\\operatorname *{det}(A) = a_{11}C_{11} + a_{21}C_{21} + a_{31}C_{31}\\)\n\\[\n\\begin{aligned}\n\\operatorname *{det}(A) &= 3 \\cdot \\operatorname{det}\\left| \\begin{array}{cc}{-4} & 3\\\\ 4 & {-2} \\end{array} \\right| - (-2) \\cdot \\operatorname{det}\\left| \\begin{array}{cc}{1} & 0\\\\ 4 & {-2} \\end{array} \\right| + 5 \\cdot \\operatorname{det}\\left| \\begin{array}{cc}{1} & 0\\\\ {-4} & 3 \\end{array} \\right| \\\\\n&= 3((-4)(-2) - (3)(4)) + 2((1)(-2) - (0)(4)) + 5((1)(3) - (0)(-4)) \\\\\n&= 3(8 - 12) + 2(-2 - 0) + 5(3 - 0) \\\\\n&= 3(-4) + 2(-2) + 5(3) \\\\\n&= -12 - 4 + 15 \\\\\n&= -1\n\\end{aligned}\n\\] This agrees with the result from the first-row expansion.\n\nEmphasize this consistency. Whether we expanded along the first row or first column, the determinant is the same. This reinforces Theorem 2.1.1. It also highlights the strategic choice: although this matrix didn’t have many zeros in any single column, the principle applies strongly to larger matrices, where such a choice can save a huge amount of work."
  },
  {
    "objectID": "la-21.html#interactive-example-optimal-expansion",
    "href": "la-21.html#interactive-example-optimal-expansion",
    "title": "Linear Algebra",
    "section": "Interactive Example: Optimal Expansion",
    "text": "Interactive Example: Optimal Expansion\nConsider this \\(4 \\times 4\\) matrix. Which row or column would you choose for expansion to minimize calculations?\n\\[\nA = \\left[ \\begin{array}{cccc}1 & 0 & 0 & -1 \\\\ 3 & 1 & 2 & 2 \\\\ 1 & 0 & -2 & 1 \\\\ 2 & 0 & 0 & 1 \\end{array} \\right]\n\\]\nThe second column is the optimal choice as it contains three zeros: \\(a_{12}=0\\), \\(a_{32}=0\\), \\(a_{42}=0\\). Expanding along the second column: \\[\n\\operatorname *{det}(A) = a_{12}C_{12} + a_{22}C_{22} + a_{32}C_{32} + a_{42}C_{42}\n\\] Since \\(a_{12}, a_{32}, a_{42}\\) are zero, only the \\(a_{22}C_{22}\\) term remains: \\[\n\\operatorname *{det}(A) = 1 \\cdot C_{22} = 1 \\cdot (-1)^{2+2} M_{22} = M_{22}\n\\] Now we need to calculate \\(M_{22}\\), which is the determinant of the \\(3 \\times 3\\) submatrix after deleting row 2 and column 2: \\[\nM_{22} = \\operatorname{det}\\left| \\begin{array}{ccc} 1 & 0 & -1 \\\\ 1 & -2 & 1 \\\\ 2 & 0 & 1 \\end{array} \\right| \\quad (\\text{again, second column has zeros!})\n\\]"
  },
  {
    "objectID": "la-21.html#interactive-example-optimal-expansion-1",
    "href": "la-21.html#interactive-example-optimal-expansion-1",
    "title": "Linear Algebra",
    "section": "Interactive Example: Optimal Expansion",
    "text": "Interactive Example: Optimal Expansion\nCalculating \\(M_{22}\\) by expanding along its second column: \\(M_{22} = 0 \\cdot C_{12}^{(M_{22})} + (-2) \\cdot C_{22}^{(M_{22})} + 0 \\cdot C_{32}^{(M_{22})}\\)\n\\(M_{22} = (-2) \\cdot (-1)^{2+2} \\operatorname{det}\\left| \\begin{array}{cc}{1}&{{-1}}\\\\ {{2}}&{{1}}\\end{array}\\right|\\)\n\\(M_{22} = -2 \\cdot ((1)(1) - (-1)(2))\\)\n\\(M_{22} = -2 \\cdot (1 + 2) = -2 \\cdot 3 = -6\\)\nThus, \\(\\operatorname *{det}(A) = -6\\)."
  },
  {
    "objectID": "la-21.html#interactive-example-optimal-expansion-2",
    "href": "la-21.html#interactive-example-optimal-expansion-2",
    "title": "Linear Algebra",
    "section": "Interactive Example: Optimal Expansion",
    "text": "Interactive Example: Optimal Expansion\nLet’s verify this with Python (using NumPy):\n\n\n\n\n\n\n\nThis example perfectly illustrates the power of strategic choice. By picking the column with most zeros, we reduced a \\(4 \\times 4\\) problem to a single \\(3 \\times 3\\) problem, and then again simplified that \\(3 \\times 3\\) problem to a single \\(2 \\times 2\\) problem. This significantly streamlines complex calculations. This computational strategy of minimizing steps is highly valued in all areas of engineering."
  },
  {
    "objectID": "la-21.html#determinants-of-triangular-matrices",
    "href": "la-21.html#determinants-of-triangular-matrices",
    "title": "Linear Algebra",
    "section": "Determinants of Triangular Matrices",
    "text": "Determinants of Triangular Matrices\nA special and very useful case involves triangular matrices, common in system analysis after methods like Gaussian elimination.\nTheorem 2.1.2 If \\(A\\) is an \\(n \\times n\\) triangular matrix (upper triangular, lower triangular, or diagonal), then \\(\\det(A)\\) is the product of the entries on the main diagonal of the matrix; that is, \\(\\det(A) = a_{11} a_{22} \\dots a_{nn}\\).\nExample: \\(4 \\times 4\\) Lower Triangular Matrix\n\\[\n\\operatorname *{det}\\left[ \\begin{array}{cccc}a_{11} & 0 & 0 & 0 \\\\ a_{21} & a_{22} & 0 & 0 \\\\ a_{31} & a_{32} & a_{33} & 0 \\\\ a_{41} & a_{42} & a_{43} & a_{44} \\end{array} \\right]\n\\] Expanding along the first row (which has many zeros) simplifies quickly: \\(= a_{11} \\cdot \\operatorname *{det}\\left[ \\begin{array}{ccc}a_{22} & 0 & 0 \\\\ a_{32} & a_{33} & 0 \\\\ a_{42} & a_{43} & a_{44} \\end{array} \\right]\\) \\(= a_{11}a_{22} \\cdot \\operatorname *{det}\\left[ \\begin{array}{cc}a_{33} & 0 \\\\ a_{43} & a_{44} \\end{array} \\right]\\) \\(= a_{11}a_{22}a_{33}a_{44}\\)\n\nTriangular matrices are frequently encountered in ECE, for instance, as a result of Gaussian elimination or LU decomposition used to solve linear systems or analyze circuit models. Knowing this property allows for quick verification or computation of determinants without resorting to a full cofactor expansion, especially useful for large systems or state-space models. This property is a huge computational shortcut."
  },
  {
    "objectID": "la-21.html#arrow-technique-for-2-times-2-and-3-times-3-matrices",
    "href": "la-21.html#arrow-technique-for-2-times-2-and-3-times-3-matrices",
    "title": "Linear Algebra",
    "section": "“Arrow Technique” for \\(2 \\times 2\\) and \\(3 \\times 3\\) Matrices",
    "text": "“Arrow Technique” for \\(2 \\times 2\\) and \\(3 \\times 3\\) Matrices\nThis technique offers a quick visual shortcut for small matrices, but it only works for \\(2 \\times 2\\) and \\(3 \\times 3\\) determinants! It does not generalize to \\(4 \\times 4\\) or higher."
  },
  {
    "objectID": "la-21.html#arrow-technique-for-2-times-2-and-3-times-3-matrices-1",
    "href": "la-21.html#arrow-technique-for-2-times-2-and-3-times-3-matrices-1",
    "title": "Linear Algebra",
    "section": "“Arrow Technique” for \\(2 \\times 2\\) and \\(3 \\times 3\\) Matrices",
    "text": "“Arrow Technique” for \\(2 \\times 2\\) and \\(3 \\times 3\\) Matrices\n\\(2 \\times 2\\) Case\nMultiply entries on the rightward arrow and subtract the product of entries on the leftward arrow. \\[\n\\left| \\begin{array}{ll}a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{array} \\right| = a_{11} a_{22} - a_{12} a_{21}\n\\]\n\\(3 \\times 3\\) Case\n\nRecopy the first and second columns to the right of the matrix.\nSum the products of entries on the three rightward (down-right) arrows.\nSubtract the sum of the products of entries on the three leftward (down-left) arrows.\n\n\nThis mnemonic is great for quick calculations on assessments or for checking results, but it’s vital that students understand its limitations. For larger matrices, this visual trick completely breaks down and applying it incorrectly is a common mistake. It’s an application-specific shortcut, not a general theorem or method."
  },
  {
    "objectID": "la-21.html#interactive-example-arrow-technique-for-2-times-2",
    "href": "la-21.html#interactive-example-arrow-technique-for-2-times-2",
    "title": "Linear Algebra",
    "section": "Interactive Example: “Arrow Technique” for \\(2 \\times 2\\)",
    "text": "Interactive Example: “Arrow Technique” for \\(2 \\times 2\\)\nCalculate the determinant of \\(\\left[ \\begin{array}{cc}3 & 1 \\\\ 4 & -2 \\end{array} \\right]\\) using the arrow technique.\n\\[\n\\left| \\begin{array}{cc}3 & 1 \\\\ 4 & -2 \\end{array} \\right| = (3)(-2) - (1)(4) = -6 - 4 = -10\n\\]\nVerify this with Python:\n\n\n\n\n\n\n\nHere’s a chance to quickly test the \\(2 \\times 2\\) arrow method. Encourage students to try changing the numbers in the Python code and re-running it to see how the determinant changes. This interactive element helps solidify their understanding of the calculation process."
  },
  {
    "objectID": "la-21.html#interactive-example-arrow-technique-for-3-times-3",
    "href": "la-21.html#interactive-example-arrow-technique-for-3-times-3",
    "title": "Linear Algebra",
    "section": "Interactive Example: “Arrow Technique” for \\(3 \\times 3\\)",
    "text": "Interactive Example: “Arrow Technique” for \\(3 \\times 3\\)\nCalculate the determinant of \\(\\left[ \\begin{array}{ccc}1 & 2 & 3 \\\\ -4 & 5 & 6 \\\\ 7 & -8 & 9 \\end{array} \\right]\\) using the arrow technique.\n\\[\n\\left[ (1)(5)(9) + (2)(6)(7) + (3)(-4)(-8) \\right] \\\\\n- \\left[ (3)(5)(7) + (1)(6)(-8) + (2)(-4)(9) \\right]\n\\] \\[\n= [45 + 84 + 96] - [105 - 48 - 72]\n\\] \\[\n= [225] - [-15] = 225 + 15 = 240\n\\]"
  },
  {
    "objectID": "la-21.html#interactive-example-arrow-technique-for-3-times-3-1",
    "href": "la-21.html#interactive-example-arrow-technique-for-3-times-3-1",
    "title": "Linear Algebra",
    "section": "Interactive Example: “Arrow Technique” for \\(3 \\times 3\\)",
    "text": "Interactive Example: “Arrow Technique” for \\(3 \\times 3\\)\nVerify with Python:\n\n\n\n\n\n\n\nSimilar to the \\(2 \\times 2\\) example, let students experiment with different 3x3 matrices in the Python code. This interactive check reinforces the “arrow technique” and demonstrates its equivalence to more general methods for these specific matrix sizes. It also provides confidence by comparing with NumPy’s robust determinant calculation."
  },
  {
    "objectID": "la-21.html#engineering-applications-of-determinants",
    "href": "la-21.html#engineering-applications-of-determinants",
    "title": "Linear Algebra",
    "section": "Engineering Applications of Determinants",
    "text": "Engineering Applications of Determinants\nDeterminants are more than just mathematical curiosities; they have critical applications in Electrical and Computer Engineering.\nCircuit Analysis\n\nKVL/KCL systems: Determinants confirm if a unique solution exists for nodal voltages or mesh currents.\nCramer’s Rule: Can solve small systems of circuit equations directly.\nNetwork stability: Related to matrix eigenvalues (roots of characteristic equation), which involve determinants.\n\nControl Systems\n\nCharacteristic Equation: Finding eigenvalues involves calculating \\(\\operatorname{det}(sI - A) = 0\\), which dictates system stability and response.\nControllability/Observability: Gramian matrices, which assess system properties, involve determinants."
  },
  {
    "objectID": "la-21.html#engineering-applications-of-determinants-1",
    "href": "la-21.html#engineering-applications-of-determinants-1",
    "title": "Linear Algebra",
    "section": "Engineering Applications of Determinants",
    "text": "Engineering Applications of Determinants\nSignal Processing\n\nFilter Design: Determinants can be involved in analyzing and verifying properties of digital or analog filters.\nPrincipal Component Analysis (PCA): The determinant of the covariance matrix is related to the generalized variance preserved by the principal components.\n\nSimple Resistive Circuit\nA common engineering problem is solving for unknown currents or voltages in a circuit. This often leads to a system of linear equations represented by a matrix.\nA unique solution for voltages/currents exists if the determinant of the circuit’s system matrix is non-zero. If the determinant is zero, it might indicate a redundant equation, a short circuit, or an open circuit, leading to non-unique or no solution for certain variables.\n\nHighlight that while full cofactor expansion for large matrices isn’t always practical for direct computation in large-scale real-world problems (other numerical methods like LU decomposition are faster), the concept of the determinant and its properties derived from these definitions are indispensable for theoretical understanding and diagnosing system behavior in ECE."
  },
  {
    "objectID": "la-21.html#summary-and-key-takeaways",
    "href": "la-21.html#summary-and-key-takeaways",
    "title": "Linear Algebra",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\nCore Concepts\n\nDefinition of Determinant: Scalar value that quantifies properties of a square matrix.\nMinors and Cofactors: Building blocks for inductive definition of determinants, especially useful for higher orders.\nCofactor Expansion: A method to calculate determinants by expanding along any chosen row or column.\n\nStrategic Choice: Always choose the row or column with the most zeros to simplify calculations.\n\nDeterminant of Triangular Matrices: Simply the product of diagonal entries! A major shortcut for ECE systems.\n“Arrow Technique”: A quick, visual method specific to \\(2 \\times 2\\) and \\(3 \\times 3\\) matrices (do not apply to higher orders!)."
  },
  {
    "objectID": "la-21.html#summary-and-key-takeaways-1",
    "href": "la-21.html#summary-and-key-takeaways-1",
    "title": "Linear Algebra",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\nRelevance to ECE\n\nSystem Analysis: Crucial for determining invertibility of system matrices (e.g., in circuits, control systems) and ensuring unique solutions.\nEigenvalue Problems: Formation of characteristic equations, fundamental for stability analysis in control and signal processing.\nNumerical Efficiency: Understanding these concepts informs more advanced numerical algorithms used in computational engineering.\n\n\nWe’ve covered the theoretical foundation and practical methods for computing determinants. Understanding these concepts is essential not just for passing Linear Algebra, but for practical problem-solving and deeper analysis across many ECE disciplines. Make sure to practice these techniques!"
  },
  {
    "objectID": "la-18.html#matrix-transformations",
    "href": "la-18.html#matrix-transformations",
    "title": "Linear Algebra",
    "section": "1.8 Matrix Transformations",
    "text": "1.8 Matrix Transformations\nImron Rosyadi\n  Understanding how matrices elegantly represent geometric and algebraic transformations, crucial for engineering applications.\n\nGood morning everyone! Today, we’re delving into a concept that truly bridges the gap between abstract linear algebra and its concrete applications in Electrical and Computer Engineering: matrix transformations. These are special functions arising from matrix multiplication, and they are absolutely fundamental to understanding everything from image processing and computer graphics to robotics, control systems, and machine learning. We’ll see how matrix transformations provide a powerful and intuitive way to describe linear mappings between vector spaces."
  },
  {
    "objectID": "la-18.html#introduction-vectors-in-rn",
    "href": "la-18.html#introduction-vectors-in-rn",
    "title": "Linear Algebra",
    "section": "Introduction: Vectors in \\(R^n\\)",
    "text": "Introduction: Vectors in \\(R^n\\)\n\nAn ordered \\(n\\)-tuple of real numbers can be seen as a vector.\n\nExample: \\((s_1, s_2, \\ldots, s_n)\\)\n\nThe set of all ordered \\(n\\)-tuples forms the space \\(R^n\\).\nWe primarily use column vector form: \\[\n\\mathbf{x} = \\left[ \\begin{array}{c}s_{1} \\\\ s_{2} \\\\ \\vdots \\\\ s_{n} \\end{array} \\right]\n\\]"
  },
  {
    "objectID": "la-18.html#introduction-vectors-in-rn-1",
    "href": "la-18.html#introduction-vectors-in-rn-1",
    "title": "Linear Algebra",
    "section": "Introduction: Vectors in \\(R^n\\)",
    "text": "Introduction: Vectors in \\(R^n\\)\n\nStandard Basis Vectors for \\(R^n\\): \\(\\mathbf{e}_1, \\mathbf{e}_2, \\ldots, \\mathbf{e}_n\\).\n\n\\(\\mathbf{e}_i\\) has a 1 in the \\(i\\)-th position and zeros elsewhere.\nAny vector \\(\\mathbf{x}\\) in \\(R^n\\) can be uniquely written as: \\[ \\mathbf{x} = x_{1} \\mathbf{e}_{1} + x_{2} \\mathbf{e}_{2} + \\dots + x_{n} \\mathbf{e}_{n} \\] For example, in \\(R^3\\): \\(\\mathbf{e}_1 = \\left[ \\begin{smallmatrix} 1 \\\\ 0 \\\\ 0 \\end{smallmatrix} \\right]\\), \\(\\mathbf{e}_2 = \\left[ \\begin{smallmatrix} 0 \\\\ 1 \\\\ 0 \\end{smallmatrix} \\right]\\), \\(\\mathbf{e}_3 = \\left[ \\begin{smallmatrix} 0 \\\\ 0 \\\\ 1 \\end{smallmatrix} \\right]\\).\n\n\n\nBefore we talk about transformations, let’s quickly recall our concept of vectors. In linear algebra, a vector in \\(R^n\\) is simply an ordered list of \\(n\\) real numbers. While we can write them as tuples, for matrix operations, we almost always prefer the column vector form. These vectors live in a space called \\(R^n\\). For example, \\(R^2\\) is the Cartesian plane, and \\(R^3\\) is 3D space.\nA crucial concept is the ‘standard basis vectors’ for \\(R^n\\). These are like the fundamental building blocks of the space. For example, in \\(R^3\\), we have \\(\\mathbf{e}_1\\), \\(\\mathbf{e}_2\\), and \\(\\mathbf{e}_3\\). Any vector in \\(R^n\\) can be expressed as a unique linear combination of these basis vectors. This property will be incredibly important when we learn how to define matrix transformations."
  },
  {
    "objectID": "la-18.html#functions-and-transformations",
    "href": "la-18.html#functions-and-transformations",
    "title": "Linear Algebra",
    "section": "Functions and Transformations",
    "text": "Functions and Transformations\nRecall that a function is a rule that associates each element of a set A (the domain) with exactly one element in a set B (the codomain). The set of all possible output elements is called the range.\n\n\n\nIf a function \\(f\\) maps from \\(R^n\\) to \\(R^m\\), we call \\(f\\) a transformation from \\(R^n\\) to \\(R^m\\), denoted \\(f: R^n \\to R^m\\). If \\(m=n\\), it’s sometimes called an operator on \\(R^n\\).\n\nWe’re all familiar with functions like \\(y = f(x)\\). In linear algebra, we’re interested in functions where the input is a vector and the output is also a vector. When the domain is \\(R^n\\) and the codomain is \\(R^m\\), we call such a function a ‘transformation’. This terminology suggests a change, a mapping from one space to another. If the input and output spaces are the same, \\(R^n\\) to \\(R^n\\), we often refer to it as an ‘operator’. Think of it as taking an input vector, applying a rule, and getting an output vector."
  },
  {
    "objectID": "la-18.html#matrix-transformations-the-core-idea",
    "href": "la-18.html#matrix-transformations-the-core-idea",
    "title": "Linear Algebra",
    "section": "Matrix Transformations: The Core Idea",
    "text": "Matrix Transformations: The Core Idea\nA matrix transformation is a transformation from \\(R^n\\) to \\(R^m\\) that arises from matrix multiplication.\nConsider a system of linear equations: \\[\n\\begin{array}{l}w_{1} = a_{11}x_{1} + a_{12}x_{2} + \\dots + a_{1n}x_{n} \\\\ w_{2} = a_{21}x_{1} + a_{22}x_{2} + \\dots + a_{2n}x_{n} \\\\ \\vdots \\qquad \\vdots \\qquad \\vdots \\qquad \\vdots \\\\ w_{m} = a_{m1}x_{1} + a_{m2}x_{2} + \\dots + a_{mn}x_{n} \\end{array}\n\\] This can be written in matrix notation as \\(\\mathbf{w} = A\\mathbf{x}\\), where: \\[\n\\left[ \\begin{array}{c}w_{1} \\\\ w_{2} \\\\ \\vdots \\\\ w_{m} \\end{array} \\right] = \\left[ \\begin{array}{cccc}a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & & \\vdots \\\\ a_{m1} & a_{m2} & \\dots & a_{mn} \\end{array} \\right]\\left[ \\begin{array}{c}x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{n} \\end{array} \\right]\n\\]"
  },
  {
    "objectID": "la-18.html#matrix-transformations-the-core-idea-1",
    "href": "la-18.html#matrix-transformations-the-core-idea-1",
    "title": "Linear Algebra",
    "section": "Matrix Transformations: The Core Idea",
    "text": "Matrix Transformations: The Core Idea\nWe view this as a transformation \\(T_A: R^n \\to R^m\\) such that \\(T_A(\\mathbf{x}) = A\\mathbf{x}\\).\n\n\n\n\nThe core idea is that any system of linear equations can be expressed as a matrix-vector product \\(A\\mathbf{x}\\). When we conceive of this product as an input-output mapping, where \\(\\mathbf{x}\\) is the input vector from \\(R^n\\) and \\(\\mathbf{w}\\) is the output vector from \\(R^m\\), we have a matrix transformation. The matrix \\(A\\) ‘transforms’ vector \\(\\mathbf{x}\\) into vector \\(\\mathbf{w}\\). The diagram visually shows this mapping process."
  },
  {
    "objectID": "la-18.html#example-1-a-matrix-transformation-from-r4-to-r3",
    "href": "la-18.html#example-1-a-matrix-transformation-from-r4-to-r3",
    "title": "Linear Algebra",
    "section": "Example 1: A Matrix Transformation from \\(R^4\\) to \\(R^3\\)",
    "text": "Example 1: A Matrix Transformation from \\(R^4\\) to \\(R^3\\)\nConsider the transformation defined by: \\[\n\\begin{array}{l}w_{1} = 2x_{1} - 3x_{2} + x_{3} - 5x_{4} \\\\ w_{2} = 4x_{1} + x_{2} - 2x_{3} + x_{4} \\\\ w_{3} = 5x_{1} - x_{2} + 4x_{3} \\end{array}\n\\]"
  },
  {
    "objectID": "la-18.html#example-1-a-matrix-transformation-from-r4-to-r3-1",
    "href": "la-18.html#example-1-a-matrix-transformation-from-r4-to-r3-1",
    "title": "Linear Algebra",
    "section": "Example 1: A Matrix Transformation from \\(R^4\\) to \\(R^3\\)",
    "text": "Example 1: A Matrix Transformation from \\(R^4\\) to \\(R^3\\)\nThis transformation maps vectors from \\(R^4\\) to \\(R^3\\). Its matrix form is \\(\\mathbf{w} = A\\mathbf{x}\\): \\[\n\\left[ \\begin{array}{c}w_{1} \\\\ w_{2} \\\\ w_{3} \\end{array} \\right] = \\left[ \\begin{array}{cccc}2 & -3 & 1 & -5 \\\\ 4 & 1 & -2 & 1 \\\\ 5 & -1 & 4 & 0 \\end{array} \\right]\\left[ \\begin{array}{c}x_{1} \\\\ x_{2} \\\\ x_{3} \\\\ x_{4} \\end{array} \\right]\n\\] So, the transformation matrix is \\(A = \\left[ \\begin{array}{cccc}2 & -3 & 1 & -5 \\\\ 4 & 1 & -2 & 1 \\\\ 5 & -1 & 4 & 0 \\end{array} \\right]\\).\nLet’s find the image of \\(\\mathbf{x} = \\left[ \\begin{smallmatrix} 1 \\\\ -3 \\\\ 0 \\\\ 2 \\end{smallmatrix} \\right]\\) under \\(T_A\\).\n\nHere’s a concrete example. We have a set of equations defining \\(w_1, w_2, w_3\\) in terms of \\(x_1, x_2, x_3, x_4\\). This forms a transformation from \\(R^4\\) to \\(R^3\\). We can easily extract the coefficient matrix \\(A\\). Now, to find the image of a specific vector \\(\\mathbf{x}\\), we just perform the matrix-vector multiplication \\(A\\mathbf{x}\\)."
  },
  {
    "objectID": "la-18.html#example-1-python-calculation",
    "href": "la-18.html#example-1-python-calculation",
    "title": "Linear Algebra",
    "section": "Example 1: Python Calculation",
    "text": "Example 1: Python Calculation\nCompute \\(T_A(\\mathbf{x}) = A\\mathbf{x}\\) for the given \\(A\\) and \\(\\mathbf{x}\\).\n\n\n\n\n\n\nThe image of \\(\\mathbf{x}\\) is \\(\\mathbf{w} = \\left[ \\begin{smallmatrix} 1 \\\\ 3 \\\\ 8 \\end{smallmatrix} \\right]\\).\n\nUsing numpy, this is a straightforward calculation. We define the transformation matrix \\(A\\) and the input vector \\(\\mathbf{x}\\). The @ operator in Python performs matrix multiplication. The result, \\(\\mathbf{w}\\), is the output vector. This direct computation is what happens behind the scenes in many ECE applications when transforming data, for instance, applying a filter to a signal represented as a vector, or transforming coordinates in robotics."
  },
  {
    "objectID": "la-18.html#special-matrix-transformations",
    "href": "la-18.html#special-matrix-transformations",
    "title": "Linear Algebra",
    "section": "Special Matrix Transformations",
    "text": "Special Matrix Transformations\n\nZero Transformation (\\(T_0: R^n \\to R^m\\)): If \\(0\\) is the \\(m \\times n\\) zero matrix, then \\(T_0(\\mathbf{x}) = 0\\mathbf{x} = \\mathbf{0}\\). (Maps every vector in \\(R^n\\) to the zero vector in \\(R^m\\).)\nIdentity Operator (\\(T_I: R^n \\to R^n\\)): If \\(I\\) is the \\(n \\times n\\) identity matrix, then \\(T_I(\\mathbf{x}) = I\\mathbf{x} = \\mathbf{x}\\). (Maps every vector in \\(R^n\\) to itself.)\n\n\nJust like with numbers, we have special matrices that represent ‘nothing’ or ‘identity’. The zero matrix, when used as a transformation, maps any input vector to the zero vector. This can represent, for example, a system with no output regardless of input. The identity matrix, as a transformation, simply maps a vector to itself, meaning no change occurs. This is like a pass-through in a signal chain."
  },
  {
    "objectID": "la-18.html#properties-of-matrix-transformations-theorem-1.8.1",
    "href": "la-18.html#properties-of-matrix-transformations-theorem-1.8.1",
    "title": "Linear Algebra",
    "section": "Properties of Matrix Transformations (Theorem 1.8.1)",
    "text": "Properties of Matrix Transformations (Theorem 1.8.1)\nFor every matrix \\(A\\), the matrix transformation \\(T_A: R^n \\to R^m\\) has the following properties:\n\n\\(T_A(\\mathbf{0}) = \\mathbf{0}\\)\n\\(T_A(k\\mathbf{u}) = kT_A(\\mathbf{u})\\) (Homogeneity property)\n\\(T_A(\\mathbf{u} + \\mathbf{v}) = T_A(\\mathbf{u}) + T_A(\\mathbf{v})\\) (Additivity property)\n\\(T_A(\\mathbf{u} - \\mathbf{v}) = T_A(\\mathbf{u}) - T_A(\\mathbf{v})\\)\n\nThese properties are direct consequences of matrix arithmetic axioms. Collectively, properties (b) and (c) are known as linearity conditions. They imply that matrix transformations preserve linear combinations: \\[ T_A(k_1\\mathbf{u}_1 + \\dots + k_r\\mathbf{u}_r) = k_1T_A(\\mathbf{u}_1) + \\dots + k_rT_A(\\mathbf{u}_r) \\]\n\nThese properties are fundamental, stemming directly from how matrix multiplication works. The homogeneity property means scaling the input vector by a scalar \\(k\\) before applying the transformation is the same as scaling the output vector by \\(k\\) after the transformation. The additivity property means transforming the sum of two vectors is the same as summing their individual transformations. These two properties, homogeneity and additivity, define what we call a ‘linear transformation’. They allow us to break down complex inputs into simpler components, transform them, and then combine the results, which is key in superposition, a common concept in circuit analysis."
  },
  {
    "objectID": "la-18.html#linear-transformations-defined",
    "href": "la-18.html#linear-transformations-defined",
    "title": "Linear Algebra",
    "section": "Linear Transformations Defined",
    "text": "Linear Transformations Defined\nNot all transformations are matrix transformations. For example, \\(w_1 = x_1^2 + x_2^2\\) is not. This leads to two crucial questions:\n\nHow do we know if a transformation \\(T: R^n \\to R^m\\) is a matrix transformation?\nIf it is, how do we find its unique matrix?\n\nTHEOREM 1.8.2: A transformation \\(T: R^n \\to R^m\\) is a matrix transformation if and only if it satisfies the following linearity conditions:\n(i) \\(T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})\\) (Additivity)\n(ii) \\(T(k\\mathbf{u}) = k T(\\mathbf{u})\\) (Homogeneity)\nTHEOREM 1.8.3:\nEvery linear transformation from \\(R^n\\) to \\(R^m\\) is a matrix transformation, and conversely, every matrix transformation from \\(R^n\\) to \\(R^m\\) is a linear transformation.\n\nThis theorem is the bridge. It states that matrix transformations are exactly the same as linear transformations. The two conditions, additivity and homogeneity, are the litmus test. If a transformation satisfies these two seemingly simple rules, then it’s a linear transformation, and it must have a corresponding matrix. This is incredibly powerful. No matter how complicated the linear transformation seems, as long as it respects scaling and addition, we can represent it with a matrix."
  },
  {
    "objectID": "la-18.html#geometric-interpretation-mapping-points",
    "href": "la-18.html#geometric-interpretation-mapping-points",
    "title": "Linear Algebra",
    "section": "Geometric Interpretation (Mapping Points)",
    "text": "Geometric Interpretation (Mapping Points)\n\n\n\nA matrix transformation \\(T_A: R^n \\to R^m\\) maps each vector (point) in \\(R^n\\) into a vector (point) in \\(R^m\\). This is the basis of transformations in computer graphics and robotics.\n\nFrom a geometric perspective, a matrix transformation takes a point (or vector) in \\(R^n\\) and maps it to a new point (or vector) in \\(R^m\\). Think of rotating an object in 3D space, scaling an image, or translating a robot arm. These are all examples of geometric transformations that can be represented by matrices. This mapping concept is central to understanding how coordinates change under various operations."
  },
  {
    "objectID": "la-18.html#uniqueness-of-the-standard-matrix-theorem-1.8.4",
    "href": "la-18.html#uniqueness-of-the-standard-matrix-theorem-1.8.4",
    "title": "Linear Algebra",
    "section": "Uniqueness of the Standard Matrix (Theorem 1.8.4)",
    "text": "Uniqueness of the Standard Matrix (Theorem 1.8.4)\nIf \\(T_A: R^n \\to R^m\\) and \\(T_B: R^n \\to R^m\\) are matrix transformations, and if \\(T_A(\\mathbf{x}) = T_B(\\mathbf{x})\\) for every vector \\(\\mathbf{x}\\) in \\(R^n\\), then \\(A = B\\).\nSignificance: There is a one-to-one correspondence between linear transformations from \\(R^n\\) to \\(R^m\\) and \\(m \\times n\\) matrices. Every such linear transformation arises from exactly one \\(m \\times n\\) matrix. This matrix is known as the standard matrix for the transformation.\n\nThis theorem is incredibly important because it tells us that the matrix representing a linear transformation is unique. If two linear transformations map every vector to the same image, then their representing matrices must be identical. This means we don’t have to worry about finding multiple matrices for the same transformation. It establishes a perfect one-to-one link between all linear transformations and all matrices of the appropriate size. This uniquely defined matrix is called the standard matrix."
  },
  {
    "objectID": "la-18.html#procedure-for-finding-standard-matrices",
    "href": "la-18.html#procedure-for-finding-standard-matrices",
    "title": "Linear Algebra",
    "section": "Procedure for Finding Standard Matrices",
    "text": "Procedure for Finding Standard Matrices\nTo find the standard matrix \\(A\\) for a linear transformation \\(T: R^n \\to R^m\\):\nStep 1: Find the images of the standard basis vectors for \\(R^n\\): \\(T(\\mathbf{e}_1), T(\\mathbf{e}_2), \\ldots, T(\\mathbf{e}_n)\\).\nRecall $\\mathbf{e}_1 = \\left[\\begin{smallmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{smallmatrix}\\right], \\ldots, \\mathbf{e}_n = \\left[\\begin{smallmatrix} 0 \\\\ \\vdots \\\\ 0 \\\\ 1 \\end{smallmatrix}\\right]$. These vectors are from $R^n$. The images $T(\\mathbf{e}_i)$ will be vectors in $R^m$.\nStep 2: Construct the matrix \\(A\\) that has these images as its successive columns: \\[\nA = [T(\\mathbf{e}_1) \\mid T(\\mathbf{e}_2) \\mid \\dots \\mid T(\\mathbf{e}_n)]\n\\] This matrix \\(A\\) is the standard matrix for \\(T\\).\n\nThis is the practical algorithm for finding the standard matrix. The key insight is that because linear transformations preserve linear combinations, knowing how the transformation acts on the simple basis vectors \\(\\mathbf{e}_i\\) tells you everything you need to know about how it acts on any vector. You simply apply the transformation rule to each basis vector, and the resulting image vectors become the columns of your standard matrix. This procedure is widely used in ECE to derive transformation matrices for various operations."
  },
  {
    "objectID": "la-18.html#example-4-finding-a-standard-matrix",
    "href": "la-18.html#example-4-finding-a-standard-matrix",
    "title": "Linear Algebra",
    "section": "Example 4: Finding a Standard Matrix",
    "text": "Example 4: Finding a Standard Matrix\nFind the standard matrix \\(A\\) for the linear transformation \\(T: R^2 \\to R^3\\) defined by: \\[\nT\\left(\\left[ \\begin{array}{c}{x_{1}}\\\\ {x_{2}} \\end{array} \\right]\\right) = \\left[ \\begin{array}{c}{2x_{1} + x_{2}}\\\\ {x_{1} - 3x_{2}}\\\\ {-x_{1} + x_{2}} \\end{array} \\right]\n\\]"
  },
  {
    "objectID": "la-18.html#example-4-finding-a-standard-matrix-1",
    "href": "la-18.html#example-4-finding-a-standard-matrix-1",
    "title": "Linear Algebra",
    "section": "Example 4: Finding a Standard Matrix",
    "text": "Example 4: Finding a Standard Matrix\nStep 1: Find images of standard basis vectors for \\(R^2\\). \\(\\mathbf{e}_1 = \\left[ \\begin{smallmatrix} 1 \\\\ 0 \\end{smallmatrix} \\right]\\) and \\(\\mathbf{e}_2 = \\left[ \\begin{smallmatrix} 0 \\\\ 1 \\end{smallmatrix} \\right]\\).\n\\(T(\\mathbf{e}_1) = T\\left(\\left[ \\begin{smallmatrix} 1 \\\\ 0 \\end{smallmatrix} \\right]\\right) = \\left[ \\begin{array}{c}{2(1) + 0}\\\\ {1 - 3(0)}\\\\ {-1 + 0} \\end{array} \\right] = \\left[ \\begin{array}{r}{2}\\\\ {1}\\\\ {-1} \\end{array} \\right]\\) \\(T(\\mathbf{e}_2) = T\\left(\\left[ \\begin{smallmatrix} 0 \\\\ 1 \\end{smallmatrix} \\right]\\right) = \\left[ \\begin{array}{c}{2(0) + 1}\\\\ {0 - 3(1)}\\\\ {-0 + 1} \\end{array} \\right] = \\left[ \\begin{array}{r}{1}\\\\ {-3}\\\\ {1} \\end{array} \\right]\\)\nStep 2: Construct the standard matrix A. \\(A = [T(\\mathbf{e}_1) \\mid T(\\mathbf{e}_2)]\\) \\[\nA = \\left[ \\begin{array}{rr}{2} & {1}\\\\ {1} & {-3}\\\\ {-1} & {1} \\end{array} \\right]\n\\]\n\nLet’s apply the procedure. Our transformation takes a 2-D vector and outputs a 3-D vector. So our standard matrix A will be a 3x2 matrix. We need to find how T acts on the standard basis vectors of \\(R^2\\). First, for \\(\\mathbf{e}_1\\), we substitute \\(x_1=1, x_2=0\\) into the definition of \\(T\\). This gives us the first column of \\(A\\). Then, for \\(\\mathbf{e}_2\\), we substitute \\(x_1=0, x_2=1\\). This gives us the second column of \\(A\\). The resulting matrix \\(A\\) is the unique standard matrix for this transformation."
  },
  {
    "objectID": "la-18.html#example-5-computing-with-standard-matrices",
    "href": "la-18.html#example-5-computing-with-standard-matrices",
    "title": "Linear Algebra",
    "section": "Example 5: Computing with Standard Matrices",
    "text": "Example 5: Computing with Standard Matrices\nFor the linear transformation in Example 4, use the standard matrix \\(A\\) to find \\(T\\left(\\left[ \\begin{smallmatrix} 1 \\\\ 4 \\end{smallmatrix} \\right]\\right)\\).\nThe standard matrix is \\(A = \\left[ \\begin{array}{rr}{2} & {1}\\\\ {1} & {-3}\\\\ {-1} & {1} \\end{array} \\right]\\). Let \\(\\mathbf{x} = \\left[ \\begin{smallmatrix} 1 \\\\ 4 \\end{smallmatrix} \\right]\\).\nThe transformation is multiplication by \\(A\\), so \\(T(\\mathbf{x}) = A\\mathbf{x}\\).\n\n\n\n\n\n\nThe image is \\(T\\left(\\left[ \\begin{smallmatrix} 1 \\\\ 4 \\end{smallmatrix} \\right]\\right) = \\left[ \\begin{smallmatrix} 6 \\\\ -11 \\\\ 3 \\end{smallmatrix} \\right]\\).\n\nNow that we have the standard matrix \\(A\\), applying the transformation to any vector \\(\\mathbf{x}\\) is simply a matrix-vector multiplication \\(A\\mathbf{x}\\). This is computationally very efficient, especially for many different input vectors. Our numpy calculation confirms the result. This approach is what enables efficient transformations in applications like graphics rendering, where an image (a collection of points/vectors) is transformed by a single matrix."
  },
  {
    "objectID": "la-18.html#example-6-finding-a-standard-matrix-comma-delimited-form",
    "href": "la-18.html#example-6-finding-a-standard-matrix-comma-delimited-form",
    "title": "Linear Algebra",
    "section": "Example 6: Finding a Standard Matrix (Comma-Delimited Form)",
    "text": "Example 6: Finding a Standard Matrix (Comma-Delimited Form)\nRewrite the transformation \\(T(x_1, x_2) = (3x_1 + x_2, 2x_1 - 4x_2)\\) in column-vector form and find its standard matrix.\nStep 1: Rewrite in column-vector form. \\[\nT\\left(\\left[ \\begin{array}{c}{x_{1}}\\\\ {x_{2}} \\end{array} \\right]\\right) = \\left[ \\begin{array}{c}{3x_{1} + x_{2}}\\\\ {2x_{1} - 4x_{2}} \\end{array} \\right]\n\\]"
  },
  {
    "objectID": "la-18.html#example-6-finding-a-standard-matrix-comma-delimited-form-1",
    "href": "la-18.html#example-6-finding-a-standard-matrix-comma-delimited-form-1",
    "title": "Linear Algebra",
    "section": "Example 6: Finding a Standard Matrix (Comma-Delimited Form)",
    "text": "Example 6: Finding a Standard Matrix (Comma-Delimited Form)\nStep 2: Identify the coefficients to form the matrix directly. We observe that the output vector is a linear combination of \\(x_1\\) and \\(x_2\\) with coefficients forming columns: \\[\n\\left[ \\begin{array}{c}{3x_{1} + x_{2}}\\\\ {2x_{1} - 4x_{2}} \\end{array} \\right] = x_1\\left[ \\begin{array}{c}{3}\\\\ {2} \\end{array} \\right] + x_2\\left[ \\begin{array}{r}{1}\\\\ {-4} \\end{array} \\right] = \\left[ \\begin{array}{rr}{3} & {1}\\\\ {2} & {-4} \\end{array} \\right]\\left[ \\begin{array}{c}{x_{1}}\\\\ {x_{2}} \\end{array} \\right]\n\\] Thus, the standard matrix is: \\[\nA = \\left[ \\begin{array}{rr}{3} & {1}\\\\ {2} & {-4} \\end{array} \\right]\n\\]\nAlternatively, explicitly apply the procedure:\n\\(T(\\mathbf{e}_1) = T(1,0) = (3(1)+0, 2(1)-4(0)) = (3,2)\\), so \\(\\left[ \\begin{smallmatrix} 3 \\\\ 2 \\end{smallmatrix} \\right]\\).\n\\(T(\\mathbf{e}_2) = T(0,1) = (3(0)+1, 2(0)-4(1)) = (1,-4)\\), so \\(\\left[ \\begin{smallmatrix} 1 \\\\ -4 \\end{smallmatrix} \\right]\\).\nThe standard matrix is then \\(\\left[ \\begin{array}{rr}{3} & {1}\\\\ {2} & {-4} \\end{array} \\right]\\).\n\nSometimes transformations are given in a more compact, comma-delimited form. The first step is always to convert it to the column-vector form, which makes the coefficients more obvious. Once in that form, you can either directly write down the coefficient matrix as the standard matrix, or, if less obvious, explicitly apply the \\(T(\\mathbf{e}_i)\\) procedure as before. The result is always the same unique standard matrix."
  },
  {
    "objectID": "la-18.html#ece-applications-summary",
    "href": "la-18.html#ece-applications-summary",
    "title": "Linear Algebra",
    "section": "ECE Applications & Summary",
    "text": "ECE Applications & Summary\nKey Concepts for ECE:\n\nComputer Graphics & Image Processing: Rotations, scaling, translations (affine transformations, built on linear transformations) of 2D/3D objects and images.\nRobotics: Kinematics (mapping joint angles to end-effector positions), robot path planning, sensor data transformation.\nControl Systems: State-space representation, system linearization, transformations between different coordinate frames (e.g., in aerospace engineering).\nSignal Processing: Filtering operations (e.g., convolution), Fourier transforms, wavelet transforms can often be represented as linear transformations.\nCircuit Analysis: Impedance transformations, network analysis can involve linear mappings.\nData Science/Machine Learning: Feature transformations, dimensionality reduction (e.g., PCA), neural network layers often perform linear transformations."
  },
  {
    "objectID": "la-18.html#ece-applications-summary-1",
    "href": "la-18.html#ece-applications-summary-1",
    "title": "Linear Algebra",
    "section": "ECE Applications & Summary",
    "text": "ECE Applications & Summary\nToday We Covered:\n\nVectors in \\(R^n\\) and transformations from \\(R^n\\) to \\(R^m\\).\nThe definition of a matrix transformation as \\(T_A(\\mathbf{x}) = A\\mathbf{x}\\).\nThe essential linearity conditions (additivity & homogeneity) that uniquely define linear transformations.\nThe one-to-one correspondence between linear transformations and their standard matrices.\nA practical procedure for finding the standard matrix by transforming standard basis vectors.\n\n\nMatrix transformations are everywhere in ECE. They are the mathematical backbone behind how we manipulate data, control physical systems, and render virtual environments. Whether you’re rotating a CAD model, filtering noise from a signal, designing a controller for an autonomous vehicle, or processing images, you’re constantly working with matrix transformations. Understanding them is not just an academic exercise; it’s a fundamental skill for any engineer. Today, we laid the groundwork by defining what matrix transformations are, identifying their core properties, and learning the crucial method to find their unique standard matrix."
  },
  {
    "objectID": "la-16.html#more-on-linear-systems-and-invertible-matrices",
    "href": "la-16.html#more-on-linear-systems-and-invertible-matrices",
    "title": "Linear Algebra",
    "section": "1.6 More on Linear Systems and Invertible Matrices",
    "text": "1.6 More on Linear Systems and Invertible Matrices\nImron Rosyadi\n  Leveraging matrix inverses to efficiently solve linear systems and deepen our understanding of system properties.\n\nGood morning everyone! Today, we’re building on our previous discussion about elementary matrices and invertibility. We’ll explore how the inverse of a matrix provides a direct formula for solving certain types of linear systems, and we’ll further expand our understanding of invertible matrices and their crucial role in system analysis within Electrical and Computer Engineering."
  },
  {
    "objectID": "la-16.html#number-of-solutions-of-a-linear-system",
    "href": "la-16.html#number-of-solutions-of-a-linear-system",
    "title": "Linear Algebra",
    "section": "Number of Solutions of a Linear System",
    "text": "Number of Solutions of a Linear System\nTHEOREM 1.6.1 A system of linear equations has zero, one, or infinitely many solutions. There are no other possibilities.\nProof Idea: If \\(A\\mathbf{x} = \\mathbf{b}\\) has more than one solution (\\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\)), then their difference \\(\\mathbf{x}_0 = \\mathbf{x}_1 - \\mathbf{x}_2 \\ne \\mathbf{0}\\) is a non-trivial solution to the associated homogeneous system \\(A\\mathbf{x} = \\mathbf{0}\\).\nThen, for any scalar \\(k\\), \\(\\mathbf{x}_1 + k\\mathbf{x}_0\\) is also a solution to \\(A\\mathbf{x} = \\mathbf{b}\\). \\[\nA(\\mathbf{x}_{1}+k\\mathbf{x}_{0})=A\\mathbf{x}_{1}+k(A\\mathbf{x}_{0}) = \\mathbf{b}+k\\mathbf{0}=\\mathbf{b}\n\\] Since there are infinitely many choices for \\(k\\), there are infinitely many solutions."
  },
  {
    "objectID": "la-16.html#number-of-solutions-of-a-linear-system-1",
    "href": "la-16.html#number-of-solutions-of-a-linear-system-1",
    "title": "Linear Algebra",
    "section": "Number of Solutions of a Linear System",
    "text": "Number of Solutions of a Linear System\nVisualizing Solution Spaces\n\n\n\n\n\ngraph TD\n    A[Linear System Ax=b] --&gt; B{Number of Solutions?};\n    B --&gt; C{Zero};\n    B --&gt; D{One};\n    B --&gt; E{Infinitely Many};\n\n\n\n\n\n\nThis fundamental theorem applies to any linear system, regardless of its size or the properties of its coefficient matrix.\n\nWe’ve seen this concept intuitively in earlier sections by looking at lines and planes. Now, Theorem 1.6.1 formally proves that these are the only three outcomes for any linear system. The key insight in the proof is that if you have two distinct solutions, their difference forms a non-trivial solution to the homogeneous system. This non-trivial solution can then be scaled by any real number and added to one of the original solutions to generate an infinite number of new solutions. This has implications for understanding degrees of freedom in engineering problems, such as current flow in a network."
  },
  {
    "objectID": "la-16.html#solving-linear-systems-by-matrix-inversion",
    "href": "la-16.html#solving-linear-systems-by-matrix-inversion",
    "title": "Linear Algebra",
    "section": "Solving Linear Systems by Matrix Inversion",
    "text": "Solving Linear Systems by Matrix Inversion\nTHEOREM 1.6.2 If \\(A\\) is an invertible \\(n \\times n\\) matrix, then for each \\(n \\times 1\\) matrix \\(\\mathbf{b}\\), the system of equations \\(A\\mathbf{x} = \\mathbf{b}\\) has exactly one solution, namely, \\(\\mathbf{x} = A^{-1}\\mathbf{b}\\).\nProof Idea:\n\nExistence:\n\nVerify that \\(\\mathbf{x} = A^{-1}\\mathbf{b}\\) is indeed a solution: \\(A(A^{-1}\\mathbf{b}) = (AA^{-1})\\mathbf{b} = I\\mathbf{b} = \\mathbf{b}\\).\n\nUniqueness:\n\nAssume \\(\\mathbf{x}_0\\) is any solution, so \\(A\\mathbf{x}_0 = \\mathbf{b}\\).\nMultiply by \\(A^{-1}\\) from the left: \\(A^{-1}(A\\mathbf{x}_0) = A^{-1}\\mathbf{b} \\implies (A^{-1}A)\\mathbf{x}_0 = A^{-1}\\mathbf{b} \\implies I\\mathbf{x}_0 = A^{-1}\\mathbf{b} \\implies \\mathbf{x}_0 = A^{-1}\\mathbf{b}\\).\nThis method provides a direct formula to find the solution.\n\nTheorem 1.6.2 is incredibly important. It’s often the first “formula” students encounter for solving a system of linear equations beyond Gaussian elimination. In ECE, many problems, like solving for node voltages in a circuit using Kirchhoff’s laws or analyzing steady-state responses of control systems, boil down to \\(A\\mathbf{x}=\\mathbf{b}\\). If the system’s matrix \\(A\\) is invertible, we can directly compute the solution \\(\\mathbf{x}\\) by finding \\(A^{-1}\\) and multiplying it by \\(\\mathbf{b}\\). This is particularly useful in computer simulations and numerical analysis where matrix inversion libraries are readily available."
  },
  {
    "objectID": "la-16.html#example-1-solution-of-a-linear-system-using-a-1",
    "href": "la-16.html#example-1-solution-of-a-linear-system-using-a-1",
    "title": "Linear Algebra",
    "section": "Example 1: Solution of a Linear System Using \\(A^{-1}\\)",
    "text": "Example 1: Solution of a Linear System Using \\(A^{-1}\\)\nConsider the system: \\[\n\\begin{array}{r}{x_{1} + 2x_{2} + 3x_{3} = 5}\\\\ {2x_{1} + 5x_{2} + 3x_{3} = 3}\\\\ {x_{1}\\qquad +8x_{3} = 17} \\end{array}\n\\] In matrix form \\(A\\mathbf{x} = \\mathbf{b}\\): \\[\nA = {\\left[ \\begin{array}{l l l}{1} & {2} & {3}\\\\ {2} & {5} & {3}\\\\ {1} & {0} & {8} \\end{array} \\right]},\\quad \\mathbf{x} = {\\left[ \\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\\\ {x_{3}} \\end{array} \\right]},\\quad \\mathbf{b} = {\\left[ \\begin{array}{l}{5}\\\\ {3}\\\\ {17} \\end{array} \\right]}\n\\] From Example 4 of the preceding section, we know \\(A^{-1}\\): \\[\nA^{-1} = \\left[ \\begin{array}{rrr} - 40 & 16 & 9 \\\\ 13 & -5 & -3 \\\\ 5 & -2 & -1 \\end{array} \\right]\n\\] By Theorem 1.6.2, the solution is \\(\\mathbf{x} = A^{-1}\\mathbf{b}\\).\n\nLet’s apply Theorem 1.6.2 to a concrete example. This is the same coefficient matrix \\(A\\) we used in Example 4 from the previous lecture, where we found its inverse. We have a new right-hand side vector \\(\\mathbf{b}\\). Now, all we need to do is perform the matrix-vector multiplication \\(A^{-1}\\mathbf{b}\\) to get the solution \\(\\mathbf{x}\\). This direct computational approach bypasses the need for Gaussian elimination for each new \\(\\mathbf{b}\\), provided \\(A^{-1}\\) is already known or easily computed."
  },
  {
    "objectID": "la-16.html#example-1-python-calculation-of-solution",
    "href": "la-16.html#example-1-python-calculation-of-solution",
    "title": "Linear Algebra",
    "section": "Example 1: Python Calculation of Solution",
    "text": "Example 1: Python Calculation of Solution\nLet’s compute \\(\\mathbf{x} = A^{-1}\\mathbf{b}\\) using numpy.\n\n\n\n\n\n\nThe solution is \\(x_1 = 1, x_2 = -1, x_3 = 2\\).\nThis method is efficient when \\(A^{-1}\\) is already known.\n\nHere’s the calculation in Python. We define the inverse matrix \\(A^{-1}\\) and the vector \\(\\mathbf{b}\\). Using np.dot for matrix-vector multiplication, we quickly find the solution vector \\(\\mathbf{x}\\). As you can see, \\(x_1 = 1, x_2 = -1, x_3 = 2\\). This demonstrates the beauty and efficiency of using matrix inversion for solving linear systems, especially when you need to solve many systems with the same coefficient matrix but different right-hand sides, a common scenario in signal processing filters or circuit analysis configurations."
  },
  {
    "objectID": "la-16.html#linear-systems-with-a-common-coefficient-matrix",
    "href": "la-16.html#linear-systems-with-a-common-coefficient-matrix",
    "title": "Linear Algebra",
    "section": "Linear Systems with a Common Coefficient Matrix",
    "text": "Linear Systems with a Common Coefficient Matrix\nOften, we need to solve sequences of systems: \\[\nA\\mathbf{x} = \\mathbf{b}_1,\\quad A\\mathbf{x} = \\mathbf{b}_2,\\quad \\ldots ,\\quad A\\mathbf{x} = \\mathbf{b}_k\n\\] all sharing the same square coefficient matrix \\(A\\).\nIf \\(A\\) is invertible, solutions are \\(\\mathbf{x}_j = A^{-1}\\mathbf{b}_j\\). This involves one inversion and \\(k\\) multiplications.\nEfficient Approach (Gauss-Jordan combined): Form the partitioned matrix: \\[\n[A\\mid \\mathbf{b}_{1}\\mid \\mathbf{b}_{2}\\mid \\dots \\mid \\mathbf{b}_{k}]\n\\] Reduce this entire matrix to reduced row echelon form. The solutions \\(\\mathbf{x}_j\\) will appear in the columns corresponding to \\(\\mathbf{b}_j\\) on the right side once \\(A\\) is reduced to \\(I\\).\nThis method works even if \\(A\\) is not invertible (you’d still see the consistency conditions and solutions, if any).\n\nIn engineering, it’s very common to encounter multiple linear systems that share the same underlying structure, meaning they have the same coefficient matrix \\(A\\), but different input or output vectors \\(\\mathbf{b}\\). Think about analyzing a circuit under different input voltage conditions, or a control system responding to various disturbances. Instead of solving each system individually or inverting \\(A\\) once and then multiplying \\(k\\) times, we can use a highly efficient approach: augment the coefficient matrix with all the right-hand side vectors at once and perform a single Gauss-Jordan elimination. This simultaneously solves all systems, saving significant computational effort."
  },
  {
    "objectID": "la-16.html#example-2-solving-two-linear-systems-at-once",
    "href": "la-16.html#example-2-solving-two-linear-systems-at-once",
    "title": "Linear Algebra",
    "section": "Example 2: Solving Two Linear Systems at Once",
    "text": "Example 2: Solving Two Linear Systems at Once\nSolve the systems:\n\n\\(x_1+2x_2+3x_3=4\\)\n\\(2x_1+5x_2+3x_3=5\\)\n\\(x_1\\qquad+8x_3=9\\)\n\\(x_1+2x_2+3x_3=1\\)\n\\(2x_1+5x_2+3x_3=6\\)\n\\(x_1\\qquad+8x_3=-6\\)\n\nCoefficient matrix is \\(A = \\left[ \\begin{array}{lll}1 & 2 & 3 \\\\ 2 & 5 & 3 \\\\ 1 & 0 & 8 \\end{array} \\right]\\) for both."
  },
  {
    "objectID": "la-16.html#example-2-solving-two-linear-systems-at-once-1",
    "href": "la-16.html#example-2-solving-two-linear-systems-at-once-1",
    "title": "Linear Algebra",
    "section": "Example 2: Solving Two Linear Systems at Once",
    "text": "Example 2: Solving Two Linear Systems at Once\nAugmented matrix \\([A \\mid \\mathbf{b}_a \\mid \\mathbf{b}_b]\\): \\[\n\\left[ \\begin{array}{lllll}1 & 2 & 3 & 4 & 1 \\\\ 2 & 5 & 3 & 5 & 6 \\\\ 1 & 0 & 8 & 9 & -6 \\end{array} \\right]\n\\] Reducing this to reduced row echelon form yields: \\[\n\\left[ \\begin{array}{lllll}1 & 0 & 0 & 1 & 2 \\\\ 0 & 1 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 1 & -1 \\end{array} \\right]\n\\] Solution for (a):\n\\(x_1 = 1, x_2 = 0, x_3 = 1\\).\nSolution for (b):\n\\(x_1 = 2, x_2 = 1, x_3 = -1\\).\n\nHere, we have two systems with the same \\(A\\) matrix, which we already know is invertible from previous examples. We form the combined augmented matrix. After performing Gauss-Jordan elimination (the steps are omitted here for brevity, but you can verify them), the RREF directly gives us the solutions. The fourth column corresponds to the solution of system (a), and the fifth column to the solution of system (b). This illustrates how compact and efficient solving multiple systems simultaneously can be."
  },
  {
    "objectID": "la-16.html#example-2-python-verification-of-combined-solution",
    "href": "la-16.html#example-2-python-verification-of-combined-solution",
    "title": "Linear Algebra",
    "section": "Example 2: Python Verification of Combined Solution",
    "text": "Example 2: Python Verification of Combined Solution\nLet’s use numpy.linalg.solve to confirm the solutions.\n\n\n\n\n\n\nThe computed solutions match the results from the combined Gauss-Jordan reduction.\n\nWhile the previous slide showed the manual Gauss-Jordan approach, numpy provides a convenient linalg.solve function, which is designed for solving \\(A\\mathbf{x}=\\mathbf{b}\\). It’s often more numerically stable and efficient than directly computing \\(A^{-1}\\) and then multiplying. Here, we confirm that the solutions found by the simultaneous reduction are indeed correct by solving each system individually using numpy. This confirms the power of our linear algebra tools."
  },
  {
    "objectID": "la-16.html#properties-of-invertible-matrices",
    "href": "la-16.html#properties-of-invertible-matrices",
    "title": "Linear Algebra",
    "section": "Properties of Invertible Matrices",
    "text": "Properties of Invertible Matrices\nConventionally, for a matrix \\(B\\) to be the inverse of \\(A\\), we need both \\(AB=I\\) and \\(BA=I\\).\nTHEOREM 1.6.3 Let \\(A\\) be a square matrix.\n\nIf \\(B\\) is a square matrix satisfying \\(BA = I\\), then \\(B = A^{-1}\\).\nIf \\(B\\) is a square matrix satisfying \\(AB = I\\), then \\(B = A^{-1}\\).\n\nSignificance: To show that \\(B\\) is the inverse of \\(A\\), you only need to verify one of the conditions (\\(AB=I\\) or \\(BA=I\\)). The other condition holds automatically for square matrices.\n\nProof Idea for (a):\nAssume \\(BA=I\\). We need to show \\(A\\) is invertible. If \\(A\\) is invertible, then \\((BA)A^{-1} = IA^{-1} \\implies B(AA^{-1}) = A^{-1} \\implies BI = A^{-1} \\implies B=A^{-1}\\).\nTo show \\(A\\) is invertible, we use Theorem 1.5.3: show \\(A\\mathbf{x}=\\mathbf{0}\\) has only the trivial solution. If \\(A\\mathbf{x}_0 = \\mathbf{0}\\), then \\(B(A\\mathbf{x}_0) = B\\mathbf{0} \\implies I\\mathbf{x}_0 = \\mathbf{0} \\implies \\mathbf{x}_0 = \\mathbf{0}\\). So \\(A\\) is invertible.\nThis theorem often simplifies theoretical work and proofs involving matrix inverses. You might wonder, given the definition of an inverse requires both \\(AB=I\\) and \\(BA=I\\), why do we need both? Theorem 1.6.3 states that for square matrices, if you find a matrix that satisfies one of these conditions, it automatically satisfies the other. This isn’t true for non-square matrices, but it’s a very helpful property for square matrices. The proof relies on connecting back to the equivalent statements theorem from the last lecture, particularly that if \\(A\\mathbf{x}=\\mathbf{0}\\) has only the trivial solution, then \\(A\\) is invertible."
  },
  {
    "objectID": "la-16.html#theorem-1.6.4-extended-equivalent-statements",
    "href": "la-16.html#theorem-1.6.4-extended-equivalent-statements",
    "title": "Linear Algebra",
    "section": "THEOREM 1.6.4: Extended Equivalent Statements",
    "text": "THEOREM 1.6.4: Extended Equivalent Statements\nIf \\(A\\) is an \\(n \\times n\\) matrix, then the following are equivalent:\n\n\\(A\\) is invertible.\n\\(A\\mathbf{x} = \\mathbf{0}\\) has only the trivial solution.\nThe reduced row echelon form of \\(A\\) is \\(I_{n}\\).\n\\(A\\) is expressible as a product of elementary matrices.\n\\(A\\mathbf{x} = \\mathbf{b}\\) is consistent for every \\(n \\times 1\\) matrix \\(\\mathbf{b}\\).\n\\(A\\mathbf{x} = \\mathbf{b}\\) has exactly one solution for every \\(n \\times 1\\) matrix \\(\\mathbf{b}\\).\n\n\nWe’re expanding our powerful Equivalence Theorem! We previously established the equivalence of (a), (b), (c), and (d). Now, we add two more critical conditions that relate to the solvability of general linear systems. (e) means that for an invertible matrix A, no matter what right-hand side vector \\(\\mathbf{b}\\) you put in, a solution always exists. (f) takes it a step further: not only does a solution exist, but that solution is unique. These new equivalences are intuitively linked to parts (a) and (b) of the theorem. In ECE, this means if your system is ‘well-behaved’ (matrix A is invertible), you always get a predictive, unique output for any given input."
  },
  {
    "objectID": "la-16.html#invertibility-of-product-matrices",
    "href": "la-16.html#invertibility-of-product-matrices",
    "title": "Linear Algebra",
    "section": "Invertibility of Product Matrices",
    "text": "Invertibility of Product Matrices\nTHEOREM 1.6.5 Let \\(A\\) and \\(B\\) be square matrices of the same size. If \\(AB\\) is invertible, then \\(A\\) and \\(B\\) must also be invertible.\nECE Relevance: This theorem is important when considering cascading systems or transformations. If you have two operations represented by matrices \\(A\\) and \\(B\\) performing sequentially (\\(AB\\)), and their combined effect is reversible (invertible), then each individual operation (\\(A\\) and \\(B\\)) must also be reversible. This applies to signal processing chains, control system components, or multi-stage filters.\n\nProof Idea for B invertible: Assume \\(\\mathbf{x}_0\\) is a solution to \\(B\\mathbf{x}=\\mathbf{0}\\). Then \\((AB)\\mathbf{x}_0 = A(B\\mathbf{x}_0) = A\\mathbf{0} = \\mathbf{0}\\). Since \\(AB\\) is invertible, by Theorem 1.6.4 (a) \\(\\Leftrightarrow\\) (b), the system \\((AB)\\mathbf{x}=\\mathbf{0}\\) has only the trivial solution. Thus \\(\\mathbf{x}_0 = \\mathbf{0}\\). Therefore, \\(B\\) is invertible. Once \\(B\\) is invertible, then \\(A = (AB)B^{-1}\\), which is a product of invertible matrices, therefore \\(A\\) is also invertible.\nThis theorem is quite intuitive in a practical sense. If you combine two devices or processes in series, and the overall system is reversible (meaning you can undo its effect), then each individual device or process must also be reversible. If either \\(A\\) or \\(B\\) were singular (not invertible), it would “destroy” information and make the combined product \\(AB\\) singular as well, preventing it from being invertible. This theorem assures us that for a reversible cascade, all components must also be reversible."
  },
  {
    "objectID": "la-16.html#the-fundamental-problem-consistency-conditions",
    "href": "la-16.html#the-fundamental-problem-consistency-conditions",
    "title": "Linear Algebra",
    "section": "The Fundamental Problem: Consistency Conditions",
    "text": "The Fundamental Problem: Consistency Conditions\nProblem: Let \\(A\\) be a fixed \\(m \\times n\\) matrix. Find all \\(m \\times 1\\) matrices \\(\\mathbf{b}\\) such that the system of equations \\(A\\mathbf{x} = \\mathbf{b}\\) is consistent.\n\nCase 1: \\(A\\) is invertible (\\(n \\times n\\) square matrix)\n\nBy Theorem 1.6.2 and 1.6.4, \\(A\\mathbf{x}=\\mathbf{b}\\) has a unique solution \\(\\mathbf{x} = A^{-1}\\mathbf{b}\\) for every \\(\\mathbf{b}\\). So, no conditions on \\(\\mathbf{b}\\).\n\nCase 2: \\(A\\) is not square, or \\(A\\) is square but not invertible.\n\nTheorem 1.6.2 does not apply.\n\\(\\mathbf{b}\\) must usually satisfy certain conditions for \\(A\\mathbf{x}=\\mathbf{b}\\) to be consistent.\nWe use row reduction on \\([A \\mid \\mathbf{b}]\\) to identify these conditions.\n\n\n\nThis “fundamental problem” drives many investigations in linear algebra and its applications. It asks: for what inputs \\(\\mathbf{b}\\) will a given system \\(A\\mathbf{x}=\\mathbf{b}\\) actually produce a solution? If \\(A\\) is an invertible square matrix, the answer is simple: any \\(\\mathbf{b}\\) will work, and the solution is unique. This is ideal; it means your system can process any valid input. However, if \\(A\\) is not square (e.g., more equations than unknowns, or vice versa) or is a singular square matrix, then not all vectors \\(\\mathbf{b}\\) will result in a consistent system. We saw an example of this in the previous lecture with singular matrices leading to inconsistent systems visually. In these cases, we must find specific conditions on \\(\\mathbf{b}\\). This often tells engineers about the range or subspace of possible outputs from their system."
  },
  {
    "objectID": "la-16.html#example-3-determining-consistency-by-elimination",
    "href": "la-16.html#example-3-determining-consistency-by-elimination",
    "title": "Linear Algebra",
    "section": "Example 3: Determining Consistency by Elimination",
    "text": "Example 3: Determining Consistency by Elimination\nWhat conditions must \\(b_1, b_2, b_3\\) satisfy for the system to be consistent? \\[\n\\begin{array}{r}x_{1} + x_{2} + 2x_{3} = b_{1} \\\\ x_{1} + x_{3} = b_{2} \\\\ 2x_{1} + x_{2} + 3x_{3} = b_{3} \\end{array}\n\\] Augmented matrix \\([A \\mid \\mathbf{b}]\\): \\[\n\\left[ \\begin{array}{llll}1 & 1 & 2 & b_{1} \\\\ 1 & 0 & 1 & b_{2} \\\\ 2 & 1 & 3 & b_{3} \\end{array} \\right]\n\\] Row reducing to row echelon form: \\[\n\\left[ \\begin{array}{rrrr}1 & 1 & 2 & b_{1} \\\\ 0 & -1 & -1 & b_{2} - b_{1} \\\\ 0 & -1 & -1 & b_{3} - 2b_{1} \\end{array} \\right] \\quad \\xrightarrow{R_2 \\leftarrow -R_2} \\quad \\left[ \\begin{array}{rrrr}1 & 1 & 2 & b_{1} \\\\ 0 & 1 & 1 & b_{1} - b_{2} \\\\ 0 & -1 & -1 & b_{3} - 2b_{1} \\end{array} \\right]\n\\] \\[\n\\xrightarrow{R_3 \\leftarrow R_3 + R_2} \\quad \\left[ \\begin{array}{rrrr}1 & 1 & 2 & b_{1} \\\\ 0 & 1 & 1 & b_{1} - b_{2} \\\\ 0 & 0 & 0 & b_{3} - b_{2} - b_{1} \\end{array} \\right]\n\\] For consistency, the last row implies: \\(b_3 - b_2 - b_1 = 0 \\implies b_3 = b_1 + b_2\\). So, \\(\\mathbf{b}\\) must be of the form \\(\\left[ \\begin{array}{c}b_{1} \\\\ b_{2} \\\\ b_{1} + b_{2} \\end{array} \\right]\\).\n\nLet’s see an example of Case 2 where \\(A\\) is singular or not square. Here, as we reduce the augmented matrix, we notice that the left side develops a row of zeros. For the system to be consistent, the corresponding entry on the right-hand side of that zero row must also be zero. This gives us a condition on \\(b_1, b_2, b_3\\). In this case, \\(b_3\\) must be the sum of \\(b_1\\) and \\(b_2\\). This means not all \\(\\mathbf{b}\\) vectors will lead to a solution for this particular system. In an ECE context, this might indicate that your system cannot produce arbitrary outputs, but rather its outputs are constrained to a specific subspace."
  },
  {
    "objectID": "la-16.html#example-4-determining-consistency-by-elimination-system-from-ex.-1",
    "href": "la-16.html#example-4-determining-consistency-by-elimination-system-from-ex.-1",
    "title": "Linear Algebra",
    "section": "Example 4: Determining Consistency by Elimination (System from Ex. 1)",
    "text": "Example 4: Determining Consistency by Elimination (System from Ex. 1)\nWhat conditions must \\(b_1, b_2, b_3\\) satisfy for the system to be consistent? \\[\n\\begin{array}{r}x_{1} + 2x_{2} + 3x_{3} = b_{1} \\\\ 2x_{1} + 5x_{2} + 3x_{3} = b_{2} \\\\ x_{1} + 8x_{3} = b_{3} \\end{array}\n\\] (This is \\(A\\mathbf{x}=\\mathbf{b}\\) where \\(A\\) is the invertible matrix from Example 1).\nAugmented matrix \\([A \\mid \\mathbf{b}]\\): \\[\n\\left[ \\begin{array}{llll}1 & 2 & 3 & b_1\\\\ 2 & 5 & 3 & b_2\\\\ 1 & 0 & 8 & b_3 \\end{array} \\right]\n\\] Reducing this to reduced row echelon form yields (as seen when finding \\(A^{-1}\\)): \\[\n\\begin{array}{r}\\left[ \\begin{array}{cccc}1 & 0 & 0 & -40b_1 + 16b_2 + 9b_3\\\\ 0 & 1 & 0 & 13b_1 - 5b_2 - 3b_3\\\\ 0 & 0 & 1 & 5b_1 - 2b_2 - b_3 \\end{array} \\right] \\end{array}\n\\] Result: There are no restrictions on \\(b_1, b_2, b_3\\). The system has a unique solution for all values of \\(b_1, b_2, b_3\\). This confirms Theorem 1.6.4 (e) and (f) for invertible matrices.\n\nLet’s reconsider the matrix from Example 1, which we know is invertible. If we augment it with a generic \\(\\mathbf{b}\\) vector and perform Gauss-Jordan elimination, we’ll see that the left side reduces to the identity matrix. This means there are no rows of zeros on the left, and therefore no consistency conditions on the \\(b_i\\) values. The right side simply provides the unique solution terms in terms of \\(b_1, b_2, b_3\\). This directly demonstrates Theorem 1.6.4 parts (e) and (f) in action: if \\(A\\) is invertible, then \\(A\\mathbf{x}=\\mathbf{b}\\) is consistent for every \\(\\mathbf{b}\\) and has exactly one solution. This is the ideal behavior for many ECE applications."
  },
  {
    "objectID": "la-16.html#ece-applications-summary",
    "href": "la-16.html#ece-applications-summary",
    "title": "Linear Algebra",
    "section": "ECE Applications & Summary",
    "text": "ECE Applications & Summary\nKey Concepts for ECE:\n\nExistence and Uniqueness of Solutions: Critical for ensuring that models of circuits, control systems, or communication links have predictable and solvable outcomes.\n\nSystem behaves consistently.\nOutput is uniquely determined by input.\n\nEfficient System Solving:\n\nUsing \\(A^{-1}\\) for direct \\(\\mathbf{x} = A^{-1}\\mathbf{b}\\) when \\(A\\) is invertible.\nSolving multiple systems simultaneously via augmented matrices. Reduces computation in large-scale simulations.\n\nSystem Properties: Invertibility of component matrices and their product defines the overall reversibility and predictability of complex engineering systems."
  },
  {
    "objectID": "la-16.html#ece-applications-summary-1",
    "href": "la-16.html#ece-applications-summary-1",
    "title": "Linear Algebra",
    "section": "ECE Applications & Summary",
    "text": "ECE Applications & Summary\nToday We Covered:\n\nThe fundamental theorem that linear systems have 0, 1, or ∞ solutions.\nSolving \\(A\\mathbf{x}=\\mathbf{b}\\) using \\(A^{-1}\\) for invertible \\(A\\).\nSimultaneous solution of multiple linear systems with a common coefficient matrix.\nProperties of invertible matrices, including the extended Equivalence Theorem.\nHow to determine consistency conditions for \\(A\\mathbf{x}=\\mathbf{b}\\) by elimination.\n\n\nTo conclude, today’s lecture reinforced the practicality of matrix inverses and the theoretical backbone of linear system solutions. We formally proved that linear systems can only have zero, one, or infinitely many solutions. Crucially, for invertible square matrices, we learned how to directly solve \\(A\\mathbf{x}=\\mathbf{b}\\) using \\(A^{-1}\\), and how to efficiently tackle multiple systems at once. We also expanded our understanding of invertible matrices through new equivalences and the product invertibility theorem. Engineers must understand these concepts to ensure their models yield consistent and unique solutions."
  },
  {
    "objectID": "la-14.html#inverses-algebraic-properties-of-matrices",
    "href": "la-14.html#inverses-algebraic-properties-of-matrices",
    "title": "Linear Algebra",
    "section": "Inverses; Algebraic Properties of Matrices",
    "text": "Inverses; Algebraic Properties of Matrices\nIn this section, we’ll explore fundamental algebraic properties of matrix operations. We’ll discover similarities with real number arithmetic but also highlight crucial differences that are vital for engineers.\nWe will cover:\n\nProperties of matrix addition and scalar multiplication.\nKey distinctions in matrix multiplication, including non-commutativity.\nSpecial matrices: zero matrices and identity matrices.\nThe concept of a matrix inverse and methods for \\(2 \\times 2\\) matrices.\nProperties of matrix powers, polynomials, and transposes.\n\n\nGood morning, everyone! Building on our understanding of basic matrix operations from last time, today we dive into the fascinating world of matrix algebra. Just like real numbers have rules for addition, subtraction, and multiplication, matrices do too. However, matrix arithmetic has some unique behaviors that make it distinct and powerful.\nFor electrical and computer engineers, understanding these algebraic properties is paramount. They govern how linear systems behave, how transformations unfold, and how we computationally manage large datasets. We’ll start with the familiar, like commutativity for addition, then quickly move to the less intuitive, like why simple cancellation rules don’t always apply to matrices. Get ready for some crucial insights!"
  },
  {
    "objectID": "la-14.html#properties-of-matrix-arithmetic-theorem-1.4.1",
    "href": "la-14.html#properties-of-matrix-arithmetic-theorem-1.4.1",
    "title": "Linear Algebra",
    "section": "Properties of Matrix Arithmetic (Theorem 1.4.1)",
    "text": "Properties of Matrix Arithmetic (Theorem 1.4.1)\nAssuming matrix sizes allow operations, these rules hold:\n\nCommutative Law for Addition: \\((a) \\ A + B = B + A\\)\nAssociative Law for Addition: \\((b) \\ A + (B + C) = (A + B) + C\\)\nAssociative Law for Multiplication: \\((c) \\ A(BC) = (AB)C\\)\nDistributive Laws: \\((d) \\ A(B + C) = AB + AC\\) (Left distributive)\n\\((e) \\ (B + C)A = BA + CA\\) (Right distributive)\nScalar Properties: \\((l) \\ a(bC) = (ab)C\\)\n\\((m) \\ a(BC) = (aB)C = B(aC)\\)\n\n\nLet’s start with the good news: many properties you’re familiar with from real number arithmetic do apply to matrices, provided the dimensions are compatible for the operations.\n\nCommutativity of Addition: Just like \\(2+3 = 3+2\\), matrix addition order doesn’t matter. This makes sense as it’s element-wise.\nAssociativity of Addition and Multiplication: This means you don’t need parentheses for sums or products of three or more matrices. The grouping of terms doesn’t change the final result. For example, in control systems, if you apply several linear transformations in sequence, the associative property ensures the order of grouping calculations doesn’t matter.\nDistributive Laws: Multiplication distributes over addition, from both the left and right. This is highly useful for simplifying matrix expressions, much like in scalar algebra.\nScalar Properties: Scalars can be factored out, grouped, or moved around in products, which simplifies many computations.\n\nThese properties form the bedrock of matrix algebra, allowing us to manipulate matrix expressions with confidence, much like we do with scalar variables."
  },
  {
    "objectID": "la-14.html#example-associativity-of-matrix-multiplication",
    "href": "la-14.html#example-associativity-of-matrix-multiplication",
    "title": "Linear Algebra",
    "section": "Example: Associativity of Matrix Multiplication",
    "text": "Example: Associativity of Matrix Multiplication\nLet’s illustrate the Associative Law for matrix multiplication: \\(A(BC) = (AB)C\\).\nGiven matrices: \\[\nA={\\left[\\begin{array}{l l}{1}&{2}\\\\{3}&{4}\\\\{0}&{1}\\end{array}\\right]},\\quad B={\\left[\\begin{array}{l l}{4}&{3}\\\\{2}&{1}\\end{array}\\right]},\\quad C={\\left[\\begin{array}{l l}{1}&{0}\\\\{2}&{3}\\end{array}\\right]}\n\\]\nExpected result: \\((AB)C\\) should equal \\(A(BC)\\).\n\n\n\n\n\n\n\nLet’s use Python to verify the associative property for matrix multiplication immediately. This visual confirmation is quite powerful.\nWe’ve defined three matrices: A (3x2), B (2x2), and C (2x2).\nFirst, we compute (AB)C: We multiply A and B, then multiply the result by C.\nThen, we compute A(BC): We multiply B and C, then multiply A by that result.\nAs the output verifies using np.array_equal(), both calculations yield the exact same matrix. This is a crucial property for manipulating complex transformation sequences in computer graphics, signal processing chains where filters are applied sequentially, or in control systems where multiple feedback loops interact. The order of applying the sequence of transformations doesn’t matter if you maintain the relative order."
  },
  {
    "objectID": "la-14.html#differences-from-real-number-arithmetic-non-commutativity",
    "href": "la-14.html#differences-from-real-number-arithmetic-non-commutativity",
    "title": "Linear Algebra",
    "section": "Differences from Real Number Arithmetic: Non-Commutativity",
    "text": "Differences from Real Number Arithmetic: Non-Commutativity\nA major difference from real number arithmetic (\\(ab=ba\\)) is that matrix multiplication is generally NOT commutative: \\(AB \\ne BA\\).\nThis can happen for three reasons:\n\n\\(AB\\) may be defined, but \\(BA\\) may not be. (e.g., \\(A\\) is \\(2 \\times 3\\), \\(B\\) is \\(3 \\times 4\\))\nBoth \\(AB\\) and \\(BA\\) may be defined, but have different sizes. (e.g., \\(A\\) is \\(2 \\times 3\\), \\(B\\) is \\(3 \\times 2\\))\nBoth \\(AB\\) and \\(BA\\) may be defined and have the same size, but \\(AB \\ne BA\\).\n\n\nNow for the critical difference: unlike real numbers, matrix multiplication is not commutative. This is a source of common errors if you blindly apply rules from scalar algebra.\nI’ve sketched out three scenarios using Mermaid diagrams: 1. Dimension Mismatch: The simplest case. If \\(A\\) is 2x3 and \\(B\\) is 3x4, \\(AB\\) is defined (2x4), but \\(BA\\) is not defined because \\(B\\)’s columns (4) do not match \\(A\\)’s rows (2). 2. Size Mismatch: What if \\(A\\) is 2x3 and \\(B\\) is 3x2? Both \\(AB\\) (2x2) and \\(BA\\) (3x3) are defined, but they result in matrices of different sizes, so they cannot be equal. 3. Same Size, Different Result: Even if both products are defined and result in matrices of the same size, they might not be equal. This is the most subtle case.\nThis non-commutativity is fundamental in many engineering fields. For example, in computer graphics, applying a rotation then a translation is generally not the same as applying a translation then a rotation. In control systems, the order of cascaded system components can significantly alter overall system behavior because their transfer functions are multiplied as matrices."
  },
  {
    "objectID": "la-14.html#example-2-order-matters-in-matrix-multiplication-ab-ne-ba",
    "href": "la-14.html#example-2-order-matters-in-matrix-multiplication-ab-ne-ba",
    "title": "Linear Algebra",
    "section": "Example 2: Order Matters in Matrix Multiplication (\\(AB \\ne BA\\))",
    "text": "Example 2: Order Matters in Matrix Multiplication (\\(AB \\ne BA\\))\nConsider matrices \\(A\\) and \\(B\\): \\[\nA={\\left[\\begin{array}{l l}{-1}&{0}\\\\{2}&{3}\\end{array}\\right]}\\quad{\\mathrm{and}}\\quad B={\\left[\\begin{array}{l l}{1}&{2}\\\\{3}&{0}\\end{array}\\right]}\n\\] Both are \\(2 \\times 2\\) matrices, so \\(AB\\) and \\(BA\\) are both defined and are both \\(2 \\times 2\\).\nLet’s perform the multiplications:\n\n\n\n\n\n\nAs you can see, \\(AB \\ne BA\\).\n\nLet’s see the third scenario of non-commutativity in action with a concrete example. Here, matrices A and B are both 2x2, so both products AB and BA are defined and will result in 2x2 matrices.\nHowever, when we compute them, the results are clearly different. This visually confirms that even if sizes align, the order of matrix multiplication is generally critical. You cannot simply swap the order.\nThis behavior is not a flaw; it reflects the underlying operations these matrices represent. In ECE, this means that the sequence of operations matters.\n\nIn signal processing, applying Filter 1 then Filter 2 is rarely the same as Filter 2 then Filter 1.\nIn robotics, the order of joint movements or transformations (e.g., rotating a robot arm then extending it, versus extending then rotating) directly corresponds to matrix multiplication. If the order is swapped, the end effector winds up in a different position."
  },
  {
    "objectID": "la-14.html#zero-matrices-and-their-properties",
    "href": "la-14.html#zero-matrices-and-their-properties",
    "title": "Linear Algebra",
    "section": "Zero Matrices and Their Properties",
    "text": "Zero Matrices and Their Properties\nA zero matrix (\\(\\boldsymbol{\\theta}\\)) is a matrix whose entries are all zero. Examples: \\(\\left[\\begin{smallmatrix} 0 & 0 \\\\ 0 & 0 \\end{smallmatrix}\\right]\\), \\(\\left[\\begin{smallmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{smallmatrix}\\right]\\).\nProperties of Zero Matrices (Theorem 1.4.2):\n\n\\((a) \\ A + \\boldsymbol{\\theta} = \\boldsymbol{\\theta} + A = A\\) (Zero matrix acts as additive identity)\n\\((c) \\ A - A = \\boldsymbol{\\theta}\\)\n\\((d) \\ \\boldsymbol{\\theta} A = \\boldsymbol{\\theta}\\) and \\(A \\boldsymbol{\\theta} = \\boldsymbol{\\theta}\\) (Multiplication by zero matrix gives zero matrix)\n\nKey Difference with Scalars (Failure of Zero Product Property): Unlike real numbers (\\(ab=0 \\implies a=0\\) or \\(b=0\\)), for matrices:\n\nIf \\(AB = \\boldsymbol{\\theta}\\), it does not imply \\(A = \\boldsymbol{\\theta}\\) or \\(B = \\boldsymbol{\\theta}\\).\n\n\nJust as scalar arithmetic has the concept of zero, matrix algebra has zero matrices, which are simply matrices filled entirely with zeros.\nThe first few properties are completely analogous to scalar zero: adding a zero matrix doesn’t change a matrix, and subtracting a matrix from itself gives the zero matrix. Also, multiplying any matrix by a zero matrix (if compatible dimensions) will always result in a zero matrix.\nHowever, here’s another crucial departure from scalar arithmetic that engineers must be aware of: the zero product property does not hold for matrices. In regular algebra, if \\(ab=0\\), then either \\(a\\) or \\(b\\) must be zero. This is NOT true for matrices. You can multiply two non-zero matrices together and still get a zero matrix. This has implications in areas like control systems where you might be looking for conditions that generate no output but it doesn’t mean the input or the system itself is zero."
  },
  {
    "objectID": "la-14.html#failure-of-the-cancellation-law-zero-product-example",
    "href": "la-14.html#failure-of-the-cancellation-law-zero-product-example",
    "title": "Linear Algebra",
    "section": "Failure of the Cancellation Law & Zero Product Example",
    "text": "Failure of the Cancellation Law & Zero Product Example\nFailure of the Cancellation Law (\\(AB=AC \\implies B=C\\) ?)\nIf \\(A \\ne \\boldsymbol{\\theta}\\) and \\(AB=AC\\), we cannot generally conclude \\(B=C\\).\nExample: \\(A={\\left[\\begin{smallmatrix}{0}&{1}\\\\{0}&{2}\\end{smallmatrix}\\right]}\\), \\(B={\\left[\\begin{smallmatrix}{1}&{1}\\\\{3}&{4}\\end{smallmatrix}\\right]}\\), \\(C={\\left[\\begin{smallmatrix}{2}&{5}\\\\{3}&{4}\\end{smallmatrix}\\right]}\\)\n\n\n\n\n\n\nHere, \\(AB=AC\\) but \\(B \\ne C\\)."
  },
  {
    "objectID": "la-14.html#failure-of-the-cancellation-law-zero-product-example-1",
    "href": "la-14.html#failure-of-the-cancellation-law-zero-product-example-1",
    "title": "Linear Algebra",
    "section": "Failure of the Cancellation Law & Zero Product Example",
    "text": "Failure of the Cancellation Law & Zero Product Example\nA Zero Product with Nonzero Factors\nExample: \\(A={\\left[\\begin{smallmatrix}{0}&{1}\\\\{0}&{2}\\end{smallmatrix}\\right]}\\), \\(B={\\left[\\begin{smallmatrix}{3}&{7}\\\\{0}&{0}\\end{smallmatrix}\\right]}\\) \\(A \\ne \\boldsymbol{\\theta}\\) and \\(B \\ne \\boldsymbol{\\theta}\\), but \\(AB = \\boldsymbol{\\theta}\\).\n\n\n\n\n\n\n\nThese two examples illustrate the critical departures from scalar arithmetic.\nFirst, the cancellation law (\\(if \\ ab=ac \\ and \\ a \\ne 0, then \\ b=c\\)) does not generally hold for matrices. In the example, matrix A is clearly non-zero. When we compute AB and AC, they turn out to be identical. However, matrix B and matrix C are distinctly different. This means you cannot simply “cancel” A from both sides of a matrix equation like you would with scalars. This is crucial for avoiding incorrect conclusions in areas like signal processing, where certain inputs might produce the same output from a linear system, but the inputs themselves are not identical.\nSecond, the zero product property (\\(if \\ ab=0, then \\ a=0 \\ or \\ b=0\\)) also fails for matrices. The second example shows two matrices A and B, neither of which is the zero matrix. Yet, their product AB results in the zero matrix. This highlights that a system can produce a zero output using non-zero components or inputs, which is important in control engineering and understanding system redundancies."
  },
  {
    "objectID": "la-14.html#identity-matrices",
    "href": "la-14.html#identity-matrices",
    "title": "Linear Algebra",
    "section": "Identity Matrices",
    "text": "Identity Matrices\nAn identity matrix (\\(I_n\\)) is a square matrix with 1s on the main diagonal and 0s elsewhere. Examples: \\[\nI_2 = {\\left[\\begin{array}{l l}{1}&{0}\\\\{0}&{1}\\end{array}\\right]},\\quad I_3 = {\\left[\\begin{array}{l l l}{1}&{0}&{0}\\\\{0}&{1}&{0}\\\\{0}&{0}&{1}\\end{array}\\right]}\n\\] The identity matrix acts as the multiplicative identity:\n\nIf \\(A\\) is \\(m \\times n\\), then \\(A I_n = A\\) and \\(I_m A = A\\).\n\nTheorem 1.4.3: If \\(R\\) is the reduced row echelon form (RREF) of an \\(n \\times n\\) matrix \\(A\\), then either \\(R\\) has a row of zeros OR \\(R\\) is the identity matrix \\(I_n\\). This implies a fundamental test for invertibility for square matrices.\n\n\n\n\n\n\n\nJust as the number “1” is the multiplicative identity in scalar arithmetic (\\(a \\cdot 1 = a\\)), identity matrices play the same role in matrix multiplication. An identity matrix is always square, with \\(1\\)s on the main diagonal and \\(0\\)s everywhere else. We denote the \\(n \\times n\\) identity matrix as \\(I_n\\).\nWhen you multiply any matrix \\(A\\) by an appropriately sized identity matrix, the matrix \\(A\\) remains unchanged. This is demonstrated by the Python code.\nTheorem 1.4.3 is significant: for any square matrix, its RREF will either be the identity matrix or it will contain at least one row of zeros. This provides a quick check from the row reduction process whether a square matrix has certain properties related to its invertibility, which we discuss next."
  },
  {
    "objectID": "la-14.html#inverse-of-a-matrix",
    "href": "la-14.html#inverse-of-a-matrix",
    "title": "Linear Algebra",
    "section": "Inverse of a Matrix",
    "text": "Inverse of a Matrix\nFor real numbers, \\(a^{-1}\\) is the reciprocal (\\(a \\cdot a^{-1} = 1\\)). For a square matrix \\(A\\), its inverse (if it exists) is a matrix \\(B\\) of the same size such that: \\[\nAB = BA = I\n\\]\n\nIf such a matrix \\(B\\) can be found, \\(A\\) is invertible (or nonsingular), and \\(B\\) is the inverse of \\(A\\).\nIf no such \\(B\\) exists, \\(A\\) is singular.\n\nTheorem 1.4.4: If \\(B\\) and \\(C\\) are both inverses of the matrix \\(A\\), then \\(B=C\\).\n\nThis means an invertible matrix has exactly one unique inverse, which we denote \\(A^{-1}\\). (\\(A A^{-1} = I\\) and \\(A^{-1} A = I\\)).\n\nExample 5: \\(A = {\\left[\\begin{smallmatrix}{2}&{-5}\\\\{-1}&{3}\\end{smallmatrix}\\right]}\\) and \\(B = {\\left[\\begin{smallmatrix}{3}&{5}\\\\{1}&{2}\\end{smallmatrix}\\right]}\\). \\(AB = I\\) and \\(BA = I\\). Thus, \\(A\\) and \\(B\\) are inverses of each other.\nApplication in ECE: Essential for solving linear systems (\\(A\\mathbf{x}=\\mathbf{b} \\implies \\mathbf{x}=A^{-1}\\mathbf{b}\\)), system control, signal reconstruction, and filter design.\n\nThe concept of a matrix inverse is analogous to a reciprocal in scalar arithmetic. If you can “un-do” a matrix transformation, the matrix is invertible.\nFor a square matrix \\(A\\), its inverse, \\(A^{-1}\\), is a matrix that, when multiplied by \\(A\\) in either order, results in the identity matrix \\(I\\). This implies that the transformation represented by \\(A^{-1}\\) effectively nullifies the transformation represented by \\(A\\). If no such inverse exists, the matrix is singular.\nThe uniqueness of the inverse (Theorem 1.4.4) is very important. It means we can speak of “the” inverse, not “an” inverse. This simplifies notation and ensures consistency.\nIn engineering, if you have a linear system described by \\(A\\mathbf{x} = \\mathbf{b}\\), and \\(A\\) is invertible, you can find the input \\(\\mathbf{x}\\) simply by multiplying \\(\\mathbf{b}\\) by \\(A^{-1}\\). This is invaluable for solving circuit problems, identifying sources from sensor data, optimizing communication channels, and in control theory to achieve desired system states."
  },
  {
    "objectID": "la-14.html#invertibility-of-2-times-2-matrices",
    "href": "la-14.html#invertibility-of-2-times-2-matrices",
    "title": "Linear Algebra",
    "section": "Invertibility of \\(2 \\times 2\\) Matrices",
    "text": "Invertibility of \\(2 \\times 2\\) Matrices\nTheorem 1.4.5: The matrix \\(A = \\left[\\begin{smallmatrix} a & b \\\\ c & d \\end{smallmatrix}\\right]\\) is invertible if and only if \\(ad - bc \\ne 0\\). In this case, the inverse is given by: \\[\nA^{-1} = \\frac{1}{ad - bc} \\left[ \\begin{array}{cc}d & -b \\\\ -c & a \\end{array} \\right]\n\\] The value \\(ad - bc\\) is called the determinant of \\(A\\), denoted \\(\\operatorname{det}(A)\\) or \\(|A|\\).\nExample 7(a): \\(A={\\left[\\begin{smallmatrix}{6}&{1}\\\\{5}&{2}\\end{smallmatrix}\\right]}\\) \\(\\operatorname{det}(A) = (6)(2) - (1)(5) = 12 - 5 = 7\\). Since \\(7 \\ne 0\\), \\(A\\) is invertible. \\(A^{-1} = \\frac{1}{7} \\left[\\begin{smallmatrix}{2}&{-1}\\\\{-5}&{6}\\end{smallmatrix}\\right] = \\left[\\begin{smallmatrix}{\\frac{2}{7}}&{-\\frac{1}{7}}\\\\{-\\frac{5}{7}}&{\\frac{6}{7}}\\end{smallmatrix}\\right]\\)\nExample 7(b): \\(A={\\left[\\begin{smallmatrix}{-1}&{2}\\\\{3}&{-6}\\end{smallmatrix}\\right]}\\) \\(\\operatorname{det}(A) = (-1)(-6) - (2)(3) = 6 - 6 = 0\\). Since \\(\\operatorname{det}(A) = 0\\), \\(A\\) is not invertible (it’s singular).\n\nFor \\(2 \\times 2\\) matrices, there’s a simple, direct formula for the inverse. This is the first place we encounter the concept of the determinant, which is a single scalar value derived from a square matrix.\nFor a \\(2 \\times 2\\) matrix \\(\\left[\\begin{smallmatrix} a & b \\\\ c & d \\end{smallmatrix}\\right]\\), the determinant is \\(ad - bc\\). The key takeaway here is that a \\(2 \\times 2\\) matrix is invertible if and only if its determinant is non-zero. This tiny value \\(ad-bc\\) holds immense power over invertibility.\nThe examples demonstrate this. For the first matrix, \\(det(A)=7\\), which means an inverse exists and can be computed directly using the formula. For the second, \\(det(A)=0\\), so it’s singular, with no inverse. This determinant concept extends to larger matrices and is central to topics like eigenvalues and system stability in ECE."
  },
  {
    "objectID": "la-14.html#solving-linear-systems-using-matrix-inversion",
    "href": "la-14.html#solving-linear-systems-using-matrix-inversion",
    "title": "Linear Algebra",
    "section": "Solving Linear Systems using Matrix Inversion",
    "text": "Solving Linear Systems using Matrix Inversion\nA linear system \\(A\\mathbf{x} = \\mathbf{b}\\) can be solved by matrix inversion (if \\(A\\) is invertible): \\[\nA\\mathbf{x} = \\mathbf{b} \\implies A^{-1}(A\\mathbf{x}) = A^{-1}\\mathbf{b} \\implies (A^{-1}A)\\mathbf{x} = A^{-1}\\mathbf{b} \\implies I\\mathbf{x} = A^{-1}\\mathbf{b} \\implies \\mathbf{x} = A^{-1}\\mathbf{b}\n\\]\nExample 8: Solve for \\(x, y\\) in terms of \\(u, v\\): \\(u = ax + by\\)\n\\(v = cx + dy\\)\nIn matrix form: \\(\\left[\\begin{smallmatrix} u \\\\ v \\end{smallmatrix}\\right] = \\left[\\begin{smallmatrix} a & b \\\\ c & d \\end{smallmatrix}\\right] \\left[\\begin{smallmatrix} x \\\\ y \\end{smallmatrix}\\right]\\)\nLet \\(A = \\left[\\begin{smallmatrix} a & b \\\\ c & d \\end{smallmatrix}\\right]\\). If \\(\\operatorname{det}(A) = ad-bc \\ne 0\\), then: \\[\n\\left[\\begin{smallmatrix} x \\\\ y \\end{smallmatrix}\\right] = A^{-1} \\left[\\begin{smallmatrix} u \\\\ v \\end{smallmatrix}\\right] = \\frac{1}{ad - bc} \\left[\\begin{smallmatrix} d & -b \\\\ -c & a \\end{smallmatrix}\\right] \\left[\\begin{smallmatrix} u \\\\ v \\end{smallmatrix}\\right]\n\\] Resulting in: \\[\nx = \\frac{du - bv}{ad - bc}, \\quad y = \\frac{av - cu}{ad - bc}\n\\]\n\nOne of the most immediate and practical applications of the matrix inverse for engineers is solving systems of linear equations. If you recall \\(A\\mathbf{x} = \\mathbf{b}\\) from our previous section, when \\(A\\) is an invertible square matrix, we can directly solve for the unknown vector \\(\\mathbf{x}\\) by multiplying both sides by \\(A^{-1}\\) from the left.\nThis method is extremely powerful. Instead of using Gaussian elimination every time, if \\(A^{-1}\\) is known or can be efficiently computed, then solving for \\(\\mathbf{x}\\) is a simple matrix-vector multiplication.\nThe example shows how a general \\(2 \\times 2\\) system, often seen in circuit equations or transformation problems, can be solved explicitly using the inverse formula. This direct solution for \\(\\mathbf{x}\\) is a cornerstone for designing and analyzing linear systems where inputs and outputs are related by a matrix. It is the basis for simulation tools that automatically determine circuit currents or voltages given specific system configurations."
  },
  {
    "objectID": "la-14.html#properties-of-inverses",
    "href": "la-14.html#properties-of-inverses",
    "title": "Linear Algebra",
    "section": "Properties of Inverses",
    "text": "Properties of Inverses\nTheorem 1.4.6 (Inverse of a Product): If \\(A\\) and \\(B\\) are invertible matrices of the same size, then \\(AB\\) is invertible, and \\[(A B)^{-1} = B^{-1}A^{-1}\\]\nThis generalizes: \\((A_1 A_2 \\dots A_n)^{-1} = A_n^{-1} \\dots A_2^{-1} A_1^{-1}\\).\nTheorem 1.4.7 (Powers of a Matrix & Other Properties): If \\(A\\) is invertible and \\(n\\) is a nonnegative integer:\n\n\\((a) \\ (A^{-1})^{-1} = A\\)\n\\((b) \\ (A^n)^{-1} = A^{-n} = (A^{-1})^n\\)\n\\((c) \\ (kA)^{-1} = k^{-1}A^{-1}\\) for any nonzero scalar \\(k\\).\n\nWe define integer powers for square matrices:\n\n\\(A^0 = I\\)\n\\(A^n = A \\cdot A \\dots A\\) (\\(n\\) factors for \\(n &gt; 0\\))\n\\(A^{-n} = (A^{-1})^n\\) (\\(n\\) factors for \\(n &gt; 0\\), if \\(A\\) invertible)\n\n\nThe inverse operation also has important algebraic properties.\nTheorem 1.4.6 is incredibly important and often counter-intuitive: the inverse of a product of matrices is the product of their inverses in reverse order. This is a direct consequence of non-commutativity. Think of it as carefully “un-doing” a sequence of operations. If you put on your socks then shoes, to reverse, you take off shoes then socks.\nTheorem 1.4.7 extends these ideas to powers of matrices. These properties mean you can work with matrix powers (positive and negative) much like scalar powers, as long as you respect the order when products are involved.\nThese properties are fundamental in control systems for analyzing cascades of systems, in cryptography using matrix transformations, and in circuit analysis, especially for time-varying systems where matrices might represent discrete time steps. For example, if \\(A\\) describes a transformation, \\((A^n)\\) describes applying that transformation \\(n\\) times."
  },
  {
    "objectID": "la-14.html#example-9-the-inverse-of-a-product",
    "href": "la-14.html#example-9-the-inverse-of-a-product",
    "title": "Linear Algebra",
    "section": "Example 9: The Inverse of a Product",
    "text": "Example 9: The Inverse of a Product\nLet \\(A={\\left[\\begin{smallmatrix}{1}&{2}\\\\{1}&{3}\\end{smallmatrix}\\right]}\\) and \\(B={\\left[\\begin{smallmatrix}{3}&{2}\\\\{2}&{2}\\end{smallmatrix}\\right]}\\). Let’s verify that \\((AB)^{-1} = B^{-1}A^{-1}\\).\n\n\n\n\n\n\n\nLet’s confirm Theorem 1.4.6 with another Python example. We’ll compute \\((AB)^{-1}\\) directly and then \\(B^{-1}A^{-1}\\) and check if they’re equal.\nThe code performs step-by-step calculation: first computes \\(AB\\) and its inverse, then computes individual inverses \\(A^{-1}\\) and \\(B^{-1}\\), and finally their product \\(B^{-1}A^{-1}\\). As you can see, the final matrices match, confirming the theorem.\nI’ve also added a quick demo of matrix powers using np.linalg.matrix_power() and np.linalg.inv(). You can see that \\((A^3)^{-1}\\) is indeed equal to \\((A^{-1})^3\\).\nThis property is crucial in areas like linear time-invariant systems in control theory and signal processing, where transformations are often applied in sequence, and you need to reverse the entire process. For instance, if \\(A\\) represents a system at time \\(t_1\\) and \\(B\\) at \\(t_2\\), then \\((AB)^{-1}\\) could represent the inverse process going from \\(t_2\\) back to \\(t_0\\), and this involves reversing the sequence of operations."
  },
  {
    "objectID": "la-14.html#matrix-polynomials",
    "href": "la-14.html#matrix-polynomials",
    "title": "Linear Algebra",
    "section": "Matrix Polynomials",
    "text": "Matrix Polynomials\nIf \\(A\\) is a square matrix, and \\(p(x) = a_0 + a_1x + a_2x^2 + \\dots + a_m x^m\\) is a polynomial, then the matrix polynomial \\(p(A)\\) is defined as: \\[\np(A) = a_0I + a_1A + a_2A^2 + \\dots + a_mA^m\n\\] Here, \\(I\\) is the identity matrix of the same size as \\(A\\), ensuring all terms are compatible for addition.\nExample 12: Find \\(p(A)\\) for \\(p(x) = x^2 - 2x - 3\\) and \\(A = \\begin{bmatrix} -1 & 2 \\\\ 0 & 3 \\end{bmatrix}\\).\nSolution: \\(p(A) = A^2 - 2A - 3I\\)\n\\(A^2 = \\begin{bmatrix} -1 & 2 \\\\ 0 & 3 \\end{bmatrix} \\begin{bmatrix} -1 & 2 \\\\ 0 & 3 \\end{bmatrix} = \\begin{bmatrix} 1 & 4 \\\\ 0 & 9 \\end{bmatrix}\\)\n\\(2A = \\begin{bmatrix} -2 & 4 \\\\ 0 & 6 \\end{bmatrix}\\)\n\\(3I = \\begin{bmatrix} 3 & 0 \\\\ 0 & 3 \\end{bmatrix}\\)\n\\(p(A) = \\begin{bmatrix} 1 & 4 \\\\ 0 & 9 \\end{bmatrix} - \\begin{bmatrix} -2 & 4 \\\\ 0 & 6 \\end{bmatrix} - \\begin{bmatrix} 3 & 0 \\\\ 0 & 3 \\end{bmatrix} = \\begin{bmatrix} 1-(-2)-3 & 4-4-0 \\\\ 0-0-0 & 9-6-3 \\end{bmatrix} = \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix}\\)\nSo, \\(p(A) = \\boldsymbol{\\theta}\\).\n\nMatrix polynomials are an extension of the polynomial concept to matrices. Instead of plugging in a scalar value for \\(x\\), we plug in a matrix \\(A\\). The constant term \\(a_0\\) is replaced by \\(a_0I\\) to ensure that all terms in the sum are matrices of the same size and can be added.\nThe example shows a calculation where a quadratic polynomial applied to a specific matrix results in the zero matrix. This is not a coincidence; it’s a profound result related to the Cayley-Hamilton Theorem, which states that every square matrix satisfies its own characteristic equation (a specific polynomial).\nIn ECE, matrix polynomials are fundamental in:\n\nLinear system theory: Describing the behavior of dynamic systems over time, where \\(A\\) might represent system dynamics.\nControl engineering: Analyzing system stability and response characteristics, defining state-space models.\nLinear algebra applications in digital signal processing and image processing: For filtering and transformations. Understanding matrix polynomials provides deeper insight into how matrix properties manifest in the temporal evolution or transformation of systems."
  },
  {
    "objectID": "la-14.html#properties-of-the-transpose",
    "href": "la-14.html#properties-of-the-transpose",
    "title": "Linear Algebra",
    "section": "Properties of the Transpose",
    "text": "Properties of the Transpose\nTheorem 1.4.8: If sizes allow:\n\n\\((a) \\ (A^T)^T = A\\) (Transposing twice returns original matrix)\n\\((b) \\ (A + B)^T = A^T + B^T\\) (Transpose distributes over addition)\n\\((c) \\ (A - B)^T = A^T - B^T\\) (Transpose distributes over subtraction)\n\\((d) \\ (kA)^T = kA^T\\) (Scalar multiple distributes over transpose)\n\\((e) \\ (AB)^T = B^T A^T\\) (Transpose of a product is product of transposes in reverse order)  —  another crucial reversal!\n\nTheorem 1.4.9 (Inverse of Transpose): If \\(A\\) is an invertible matrix, then \\(A^T\\) is also invertible and: \\[\n(A^{T})^{-1} = (A^{-1})^{T}\n\\]\n\nThe transpose operation also has a set of intuitive and some not-so-intuitive properties. Properties (a) through (d) are quite straightforward and intuitive. For example, transposing a matrix twice just gets you back to the original matrix. Scalar multiplication and matrix addition/subtraction behave as expected with the transpose operation.\nHowever, Property (e) is another critical one involving a reversal of order, similar to the inverse of a product: the transpose of a product \\(AB\\) is \\(B^T A^T\\). Remember this as part of the “socks and shoes” rule – to transpose a sequence of operations, you reverse the order of operations and take the transpose of each.\nFinally, Theorem 1.4.9 is valuable: the inverse of a transpose is the transpose of the inverse. This means you can compute them in any order and get the same result.\nThese properties are indispensable in various ECE applications:\n\nIn optimizing calculations in signal processing and machine learning, particularly in algorithms involving gradients and covariance matrices.\nFor simplifying complex matrix expressions in controls design.\nFor understanding symmetry and duality in electrical circuits or electromagnetic fields."
  },
  {
    "objectID": "la-14.html#example-13-inverse-of-a-transpose",
    "href": "la-14.html#example-13-inverse-of-a-transpose",
    "title": "Linear Algebra",
    "section": "Example 13: Inverse of a Transpose",
    "text": "Example 13: Inverse of a Transpose\nConsider a general \\(2 \\times 2\\) matrix and its transpose: \\(A = \\left[\\begin{smallmatrix} a & b \\\\ c & d \\end{smallmatrix}\\right] \\quad \\text{and} \\quad A^T = \\left[\\begin{smallmatrix} a & c \\\\ b & d \\end{smallmatrix}\\right]\\) Assume \\(A\\) is invertible, i.e., \\(ad-bc \\ne 0\\).\nLet’s compute \\((A^T)^{-1}\\) and \\((A^{-1})^T\\) and verify they are equal.\n\n\n\n\n\n\n\nLet’s use a numerical example in Python to confirm this theorem. We take a \\(2 \\times 2\\) matrix \\(A\\).\nFirst, we compute the inverse of \\(A\\), then transpose that result. Second, we transpose \\(A\\) first, and then compute the inverse of \\(A^T\\). As the np.array_equal confirms, both results are the same. This means you can perform these operations in any order and get the same valid outcome.\nThis property is useful in various optimizations and theoretical proofs. For example, in electrical engineering, when dealing with passive networks and their impedance matrices, this property assures certain symmetrical relationships in analysis. It also appears in antenna array processing and in the formulation of least squares problems, where properties of the transpose are used to find optimal solutions."
  },
  {
    "objectID": "la-14.html#conclusion",
    "href": "la-14.html#conclusion",
    "title": "Linear Algebra",
    "section": "Conclusion",
    "text": "Conclusion\n\nMatrix Arithmetic Rules: Many scalar arithmetic rules (associative, distributive) apply to matrices, but key differences exist.\nNon-Commutativity (\\(AB \\ne BA\\)): Order in matrix multiplication matters; it’s generally NOT commutative.\nFailure of Laws: Cancellation law and zero product property usually fail for matrices.\nSpecial Matrices:\n\nZero Matrix (\\(\\boldsymbol{\\theta}\\)): Additive identity.\nIdentity Matrix (\\(I\\)): Multiplicative identity.\n\nMatrix Inverse (\\(A^{-1}\\)): “Un-does” a matrix operation (\\(AA^{-1}=I\\)). Exists iff \\(\\operatorname{det}(A) \\ne 0\\) for \\(2 \\times 2\\) matrices.\nTranspose (\\(A^T\\)): Swaps rows and columns. \\((AB)^T = B^T A^T\\).\nMatrix Powers & Polynomials: Apply scalar polynomial concepts to matrices.\nDirect Application in ECE: These properties are fundamental for understanding and solving problems in:\n\nCircuit Analysis (e.g., node-voltage method, mesh analysis)\nControl Systems (e.g., state-space representations, system stability)\nSignal Processing (e.g., filters, transformations)\nComputer Graphics (e.g., 3D transformations)\nMachine Learning (e.g., linear regression, deep learning)\n\n\n\nTo conclude our detailed exploration of matrix algebraic properties:\nWe’ve seen that while matrices share many algebraic properties with real numbers, particularly for addition and scalar multiplication, they fundamentally differ in one critical aspect: matrix multiplication is generally not commutative. This non-commutativity then leads to the failure of other familiar laws, like the cancellation law and the zero product property from scalar algebra.\nWe introduced special matrices – the zero matrix (additive identity) and the identity matrix (multiplicative identity) – which play analogous roles to 0 and 1 in scalar arithmetic.\nCrucially, we defined the concept of the matrix inverse, a powerful tool for “undoing” matrix transformations, and learned how to compute it for \\(2 \\times 2\\) matrices using the determinant. We also explored properties of matrix powers and how polynomials can be applied to matrices. Finally, we revisited the transpose operation and its important properties, especially how it interacts with matrix products and inverses.\nThese algebraic properties are not just theoretical constructs; they are the foundation upon which nearly all advanced topics in linear algebra, and by extension, most computational and analytical methods in Electrical and Computer Engineering are built. Mastering these rules is absolutely essential for your success in this course and in your future engineering career, as they provide the language and tools to precisely describe, analyze, and manipulate complex systems.\nThank you. Next, we will be discussing elementary matrices and a method for finding the inverse of any invertible square matrix using row operations."
  },
  {
    "objectID": "la-14.html#exercises",
    "href": "la-14.html#exercises",
    "title": "Linear Algebra",
    "section": "Exercises",
    "text": "Exercises\ngrasple_exercise_3_4_11\nTo show: if \\(AB\\) is invertible, then so are \\(A\\) and \\(B\\)."
  },
  {
    "objectID": "la-14.html#exercises-1",
    "href": "la-14.html#exercises-1",
    "title": "Linear Algebra",
    "section": "Exercises",
    "text": "Exercises\ngrasple_exercise_3_4_12\nWhat about \\((AB)^{-1} = A^{-1}B^{-1}\\)?"
  },
  {
    "objectID": "la-14.html#exercises-2",
    "href": "la-14.html#exercises-2",
    "title": "Linear Algebra",
    "section": "Exercises",
    "text": "Exercises\ngrasple_exercise_3_4_13\nWhat about \\(((AB)^T)^{-1} = (A^T)^{-1}(B^T)^{-1}\\)?"
  },
  {
    "objectID": "la-12.html#introduction-to-gaussian-elimination",
    "href": "la-12.html#introduction-to-gaussian-elimination",
    "title": "Linear Algebra",
    "section": "Introduction to Gaussian Elimination",
    "text": "Introduction to Gaussian Elimination\nIn the previous section, we saw how elementary row operations can transform a linear system’s augmented matrix into a simpler form from which solutions are evident. This section formalizes that process.\nWe will cover:\n\nDefining Row Echelon Form (REF) and Reduced Row Echelon Form (RREF).\nInterpreting solutions from REF/RREF matrices.\nThe systematic procedures: Gaussian Elimination and Gauss-Jordan Elimination.\nSpecial considerations for homogeneous linear systems.\n\n\nGood morning, everyone! Today, we delve deeper into solving linear systems, moving beyond ad-hoc elimination to a systematic, algorithmic approach. This is where linear algebra truly shines, providing robust methods that are essential for both manual calculations and, more importantly, for computational solutions in engineering.\nMany real-world engineering problems involve systems with thousands or even millions of unknowns, far too large to solve by hand. The concepts we’ll discuss today are the foundation of the numerical algorithms used in computer software for these massive systems, even with considerations like memory and roundoff errors. Understanding these procedures is crucial for any ECE student. Let’s define the formal forms matrices take and the steps to achieve them."
  },
  {
    "objectID": "la-12.html#echelon-forms-of-matrices",
    "href": "la-12.html#echelon-forms-of-matrices",
    "title": "Linear Algebra",
    "section": "Echelon Forms of Matrices",
    "text": "Echelon Forms of Matrices\nA matrix is in Row Echelon Form (REF) if it has the following properties:\n\nIf a row doesn’t consist entirely of zeros, its first nonzero number (called a leading 1) is 1.\nAny rows consisting entirely of zeros are grouped at the bottom.\nIn any two successive non-zero rows, the leading 1 in the lower row is farther to the right than the leading 1 in the higher row.\n\nA matrix in Reduced Row Echelon Form (RREF) has all three REF properties, plus:\n\nEach column containing a leading 1 has zeros everywhere else in that column (above and below the leading 1).\n\n\nThe goal of our systematic procedures is to transform any augmented matrix into one of these specific “echelon forms.” These forms simplify the interpretation of solutions.\nThink of “echelon” like steps or a staircase. * Leading 1: This is our pivot element, the first non-zero entry in a non-zero row, and we make it 1. * Zero rows at bottom: This isolates trivial equations (0=0). * Staircase pattern: The leading 1s move progressively to the right, creating a clear structure. * Zeros below leading 1s (REF): This is the “staircase” effect. For example, once the first element of the first row is a leading 1, you can use it to zero out all elements below it in that column. * Zeros above AND below leading 1s (RREF): This is stricter. Not only are zeros below a leading 1, but also above it in the same column. Once a matrix is in RREF, the solution can often be directly read from the augmented matrix, or derived with minimal effort. Example 6 from our last lecture was a perfect example of a matrix in RREF."
  },
  {
    "objectID": "la-12.html#example-1-row-echelon-vs.-reduced-row-echelon-forms",
    "href": "la-12.html#example-1-row-echelon-vs.-reduced-row-echelon-forms",
    "title": "Linear Algebra",
    "section": "Example 1: Row Echelon vs. Reduced Row Echelon Forms",
    "text": "Example 1: Row Echelon vs. Reduced Row Echelon Forms\n\n\nReduced Row Echelon Form Examples: (All follow properties 1-4) \\[\n\\left[{\\begin{array}{r r r | r}{1}&{0}&{0}&{4}\\\\ {0}&{1}&{0}&{7}\\\\ {0}&{0}&{1}&{-1}\\end{array}}\\right]\n\\] \\[\n\\left[{\\begin{array}{r r r}{1}&{0}&{0}\\\\ {0}&{1}&{0}\\\\ {0}&{0}&{1}\\end{array}}\\right]\n\\] \\[\n\\left[{\\begin{array}{r r r r | r}{0}&{1}&{-2}&{0}&{1}\\\\ {0}&{0}&{0}&{1}&{3}\\\\ {0}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{0}\\end{array}}\\right]\n\\]\n\nRow Echelon Form Examples (NOT Reduced): (Follow properties 1-3, but NOT 4) \\[\n\\left[{\\begin{array}{r r r | r}{1}&{4}&{-3}&{7}\\\\ {0}&{1}&{6}&{2}\\\\ {0}&{0}&{1}&{5}\\end{array}}\\right]\n\\]\n\\[\n\\left[{\\begin{array}{r r r}{1}&{1}&{0}\\\\ {0}&{1}&{0}\\\\ {0}&{0}&{0}\\end{array}}\\right]\n\\]\n\\[\n\\left[{\\begin{array}{r r r r | r}{0}&{1}&{2}&{6}&{0}\\\\ {0}&{0}&{1}&{-1}&{0}\\\\ {0}&{0}&{0}&{0}&{1}\\end{array}}\\right]\n\\]\n\n\nLet’s look at concrete examples of these forms. On the left, you see matrices in Reduced Row Echelon Form. Observe that in any column where there’s a leading 1 (the first non-zero entry in its row), all other entries in that column are zero. This is the defining characteristic of RREF. It looks like an identity matrix (or part of one) with zeros around the leading 1s.\nOn the right, these matrices are in Row Echelon Form but not Reduced Row Echelon Form. They have leading 1s, the staircase pattern, and zero rows at the bottom (if any). However, they also have non-zero entries above the leading 1s. For example, in the first matrix on the right, the 4, -3, and 7 in the first row are above leading 1s. While these are perfectly valid REF matrices, they require a bit more work (back-substitution) to get the final solution. The goal of Gauss-Jordan is to reach the RREF, which gives the solution directly."
  },
  {
    "objectID": "la-12.html#solving-systems-from-echelon-forms",
    "href": "la-12.html#solving-systems-from-echelon-forms",
    "title": "Linear Algebra",
    "section": "Solving Systems from Echelon Forms",
    "text": "Solving Systems from Echelon Forms\nOnce an augmented matrix is in REF or RREF, the solution to the linear system can be obtained.\nExample 3: Unique Solution (From RREF)\nSuppose RREF of augmented matrix for \\(x_1, x_2, x_3, x_4\\) is: \\[\n\\left[{\\begin{array}{c c c c | c}{1}&{0}&{0}&{0}&{3}\\\\ {0}&{1}&{0}&{0}&{-1}\\\\ {0}&{0}&{1}&{0}&{0}\\\\ {0}&{0}&{0}&{1}&{5}\\end{array}}\\right]\n\\] This directly translates to: \\[\nx_1 = 3, \\quad x_2 = -1, \\quad x_3 = 0, \\quad x_4 = 5\n\\] This is a unique solution.\n\nThe power of echelon forms lies in how easily we can extract the solutions. When the augmented matrix is in Reduced Row Echelon Form, especially like this example, reading the solution is instantaneous. The left side of the augmented matrix resembles an identity matrix, meaning each variable is isolated. We can simply read off the values from the rightmost column. This tells us there is exactly one solution."
  },
  {
    "objectID": "la-12.html#solving-systems-from-echelon-forms-cont.",
    "href": "la-12.html#solving-systems-from-echelon-forms-cont.",
    "title": "Linear Algebra",
    "section": "Solving Systems from Echelon Forms (Cont.)",
    "text": "Solving Systems from Echelon Forms (Cont.)\nExample 4(a): No Solution\nSuppose RREF of augmented matrix for \\(x, y, z\\) is: \\[\n\\left[{\\begin{array}{l l l | l}{1}&{0}&{0}&{0}\\\\ {0}&{1}&{2}&{0}\\\\ {0}&{0}&{0}&{1}\\end{array}}\\right]\n\\] The last row corresponds to the equation: \\[\n0x + 0y + 0z = 1 \\implies 0=1\n\\] This is a contradiction. The system is inconsistent and has no solution.\n\nNot all linear systems have solutions. When you convert an augmented matrix to RREF and encounter a row like this, it immediately tells you the system is inconsistent. The row [0 0 0 | 1] translates to \\(0x + 0y + 0z = 1\\), or simply \\(0=1\\). This is logically impossible. Therefore, no values of \\(x, y, z\\) can satisfy this equation, meaning no solution exists for the entire system."
  },
  {
    "objectID": "la-12.html#solving-systems-from-echelon-forms-cont.-1",
    "href": "la-12.html#solving-systems-from-echelon-forms-cont.-1",
    "title": "Linear Algebra",
    "section": "Solving Systems from Echelon Forms (Cont.)",
    "text": "Solving Systems from Echelon Forms (Cont.)\nExample 4(b): Infinitely Many Solutions\nSuppose RREF of augmented matrix for \\(x, y, z\\) is: \\[\n\\left[{\\begin{array}{l l l | l}{1}&{0}&{3}&{-1}\\\\ {0}&{1}&{-4}&{2}\\\\ {0}&{0}&{0}&{0}\\end{array}}\\right]\n\\] The last row \\(0=0\\) can be omitted. The corresponding system is: \\[\n\\begin{array}{r}x + 3z = -1 \\\\ y - 4z = 2 \\end{array}\n\\] Variables corresponding to leading 1s (\\(x, y\\)) are leading variables. Others (\\(z\\)) are free variables. Solve for leading variables in terms of free variables: \\[\n\\begin{array}{l}x = -1 - 3z \\\\ y = 2 + 4z \\end{array}\n\\] Assign an arbitrary value (parameter) \\(t\\) to the free variable \\(z\\). The general solution (parametric equations) is: \\[\nx = -1 - 3t, \\quad y = 2 + 4t, \\quad z = t\n\\] Specific solutions can be found by choosing values for \\(t\\). For example, if \\(t=0\\), then \\((x,y,z)=(-1,2,0)\\). If \\(t=1\\), then \\((x,y,z)=(-4,6,1)\\).\n\nThis scenario leads to infinitely many solutions, as we discussed in the previous lecture. When the RREF contains a row of all zeros for coefficients (like [0 0 0 | 0]), it means that equation provides no new information. It’s simply \\(0=0\\).\nIn this example, \\(x\\) and \\(y\\) are our leading variables because they correspond to the columns with leading 1s. \\(z\\) is a free variable because it does not have a leading 1 in its column. Free variables can take any arbitrary real value.\nTo represent all infinitely many solutions, we express the leading variables in terms of the free variables. By introducing a parameter, say \\(t\\), for the free variable \\(z\\), we obtain parametric equations. These equations form what’s called a general solution because by varying the parameter \\(t\\), you can generate every possible solution to the system. This method is common in control and signal processing, where you might have underdetermined systems."
  },
  {
    "objectID": "la-12.html#solving-systems-from-echelon-forms-cont.-2",
    "href": "la-12.html#solving-systems-from-echelon-forms-cont.-2",
    "title": "Linear Algebra",
    "section": "Solving Systems from Echelon Forms (Cont.)",
    "text": "Solving Systems from Echelon Forms (Cont.)\nExample 4(c): Infinitely Many Solutions (Another form)\nSuppose RREF of augmented matrix for \\(x, y, z\\) is: \\[\n\\left[{\\begin{array}{l l l | l}{1}&{-5}&{1}&{4}\\\\ {0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}\\end{array}}\\right]\n\\] The system reduces to a single equation: \\[\nx - 5y + z = 4\n\\] Here, \\(x\\) is the only leading variable. \\(y\\) and \\(z\\) are free variables. Let \\(y = s\\) and \\(z = t\\) (parameters). Then \\(x = 4 + 5y - z \\implies x = 4 + 5s - t\\).\nThe general solution is: \\[\nx = 4 + 5s - t, \\quad y = s, \\quad z = t\n\\] This represents a plane in 3D space.\n\nHere’s another case of infinitely many solutions. This time, after reduction, we’re left with only one non-zero row. The rest are zero rows, offering no information.\nThis means we have only one effective equation: \\(x - 5y + z = 4\\). With three unknowns and only one equation, we have degrees of freedom. \\(x\\) is the leading variable here. Both \\(y\\) and \\(z\\) are free variables, allowing us to assign them arbitrary parameters, \\(s\\) and \\(t\\).\nThis again results in a parametric solution, but with two parameters, because the solution set is a plane in three-dimensional space, which requires two dimensions (or two parameters) to describe. This type of solution arises in engineering when systems are flexible or when there are more variables than independent constraints, such as in process control optimization."
  },
  {
    "objectID": "la-12.html#gauss-jordan-elimination-step-by-step-procedure",
    "href": "la-12.html#gauss-jordan-elimination-step-by-step-procedure",
    "title": "Linear Algebra",
    "section": "Gauss-Jordan Elimination: Step-by-Step Procedure",
    "text": "Gauss-Jordan Elimination: Step-by-Step Procedure\nThis algorithm reduces any matrix to its unique Reduced Row Echelon Form.\nPhase 1: Forward Elimination (Gaussian Elimination to REF) Goal: Create leading 1s and zeros below them.\n\nLocate Leftmost Non-Zero Column: This will be your first pivot column.\nMove Non-Zero Entry to Top: If needed, interchange the top row with another row to get a non-zero entry at the top of the pivot column.\nCreate Leading 1: Multiply the top row by the reciprocal of its new leading entry to make it 1.\nCreate Zeros Below Leading 1: Add suitable multiples of the new first row to rows below it to make all entries below the leading 1 zero.\nRepeat for Submatrix: Cover the top row and repeat steps 1-4 on the remaining (sub)matrix. Continue until the entire matrix is in Row Echelon Form."
  },
  {
    "objectID": "la-12.html#gauss-jordan-elimination-step-by-step-procedure-1",
    "href": "la-12.html#gauss-jordan-elimination-step-by-step-procedure-1",
    "title": "Linear Algebra",
    "section": "Gauss-Jordan Elimination: Step-by-Step Procedure",
    "text": "Gauss-Jordan Elimination: Step-by-Step Procedure\nPhase 2: Backward Elimination (To RREF) Goal: Create zeros above the leading 1s.\n\nCreate Zeros Above Leading 1s: Starting with the last non-zero row and working upwards, add suitable multiples of each row to the rows above to introduce zeros above the leading 1s.\n\nThe final matrix is in Reduced Row Echelon Form.\n\nNow that we understand what REF and RREF are, let’s learn the systematic procedure to achieve them. This two-phase algorithm is called Gauss-Jordan elimination.\nThe first phase, Forward Elimination, is essentially what’s known as Gaussian Elimination. Its objective is to transform the matrix into Row Echelon Form. You identify the leftmost column that isn’t all zeros – this is your “pivot” column for that step. You then ensure a non-zero entry is at the top of this column, make it a “leading 1” by scaling the row, and then use that leading 1 to “zero out” all the entries directly below it. You then ignore this processed row and column and repeat the process on the remaining “submatrix”. This process continues until you have the staircase-like REF structure, with all zeros below the leading 1s.\nThe second phase, Backward Elimination, is what distinguishes Gauss-Jordan from just Gaussian Elimination. Once in REF, you work upwards from the last leading 1. You use this leading 1 to zero out any non-zero entries above it in its column. You repeat this for all leading 1s until all entries above and below them are zero. The result is the cleaner, reduced row echelon form. This systematic process is what computers execute to solve large-scale linear systems."
  },
  {
    "objectID": "la-12.html#interactive-example-gauss-jordan-elimination",
    "href": "la-12.html#interactive-example-gauss-jordan-elimination",
    "title": "Linear Algebra",
    "section": "Interactive Example: Gauss-Jordan Elimination",
    "text": "Interactive Example: Gauss-Jordan Elimination\nLet’s apply Gauss-Jordan Elimination to the augmented matrix for the system: \\(x + y + 2z = 9\\)\n\\(2x + 4y - 3z = 1\\)\n\\(3x + 6y - 5z = 0\\)\nInitial Augmented Matrix: \\[\n\\left[ \\begin{array}{r r r | r}{1} & 1 & 2 & 9\\\\ {2} & 4 & {-3} & 1\\\\ {3} & 6 & {-5} & 0 \\end{array} \\right]\n\\]"
  },
  {
    "objectID": "la-12.html#interactive-example-gauss-jordan-elimination-1",
    "href": "la-12.html#interactive-example-gauss-jordan-elimination-1",
    "title": "Linear Algebra",
    "section": "Interactive Example: Gauss-Jordan Elimination",
    "text": "Interactive Example: Gauss-Jordan Elimination\n\n\n\n\n\n\n\nLet’s see the Gauss-Jordan process in action using a simple Python script in Pyodide. This code block will perform the exact elementary row operations we discussed and show the matrix transformation at each step.\nYou can click on the ‘Show Code’ button to see how these operations are implemented using NumPy arrays. Notice how each step carefully targets one position to become zero or one.\n\nSteps 1-4 correspond to the forward elimination phase, bringing the matrix to Row Echelon Form.\nSteps 5-8 correspond to the backward elimination phase, bringing it to Reduced Row Echelon Form.\n\nThe final matrix is [[1. 0. 0. 1.], [0. 1. 0. 2.], [0. 0. 1. 3.]]. This directly implies \\(x=1, y=2, z=3\\), demonstrating the power of Gauss-Jordan elimination for direct solution retrieval. This method is the basis for many numerical linear algebra libraries used in engineering software, where speed and accuracy are paramount."
  },
  {
    "objectID": "la-12.html#interactive-reduced-row-echelon-form-calculator",
    "href": "la-12.html#interactive-reduced-row-echelon-form-calculator",
    "title": "Linear Algebra",
    "section": "Interactive: Reduced Row Echelon Form Calculator",
    "text": "Interactive: Reduced Row Echelon Form Calculator"
  },
  {
    "objectID": "la-12.html#homogeneous-linear-systems",
    "href": "la-12.html#homogeneous-linear-systems",
    "title": "Linear Algebra",
    "section": "Homogeneous Linear Systems",
    "text": "Homogeneous Linear Systems\nA system of linear equations is homogeneous if all constant terms are zero: \\[\n\\begin{array}{r l} a_{11}x_{1} + \\dots +a_{1n}x_{n} &= 0\\\\ a_{21}x_{1} + \\dots +a_{2n}x_{n} &= 0\\\\ \\vdots\\qquad\\vdots\\qquad\\vdots\\qquad\\vdots \\\\ a_{m1}x_{1} + \\dots +a_{mn}x_{n} &= 0 \\end{array}\n\\]"
  },
  {
    "objectID": "la-12.html#homogeneous-linear-systems-1",
    "href": "la-12.html#homogeneous-linear-systems-1",
    "title": "Linear Algebra",
    "section": "Homogeneous Linear Systems",
    "text": "Homogeneous Linear Systems\nKey properties of homogeneous systems:\n\nAlways consistent: \\(x_1=0, x_2=0, \\ldots, x_n=0\\) is always a solution (the trivial solution).\nCan have only two types of solutions:\n\nOnly the trivial solution.\nInfinitely many solutions (including the trivial solution, which are then called nontrivial solutions)."
  },
  {
    "objectID": "la-12.html#homogeneous-linear-systems-2",
    "href": "la-12.html#homogeneous-linear-systems-2",
    "title": "Linear Algebra",
    "section": "Homogeneous Linear Systems",
    "text": "Homogeneous Linear Systems\nGeometrically: For 2D, lines pass through the origin. If they’re distinct and intersect only at origin \\(\\implies\\) trivial solution. If they coincide \\(\\implies\\) infinitely many solutions.\n\n\n\n\nHomogeneous System Geometry\n\n\n\n\nHomogeneous linear systems are a special but incredibly important class of linear systems. They are defined by having all zeros on the right-hand side of the equations.\nA critical property is that they are always consistent. This is because \\(x_1=0, x_2=0, \\ldots, x_n=0\\) (the trivial solution) will always satisfy every equation. If adding more variables or equations results in having actual non-zero solutions, these are called nontrivial solutions.\nGeometrically, in 2D, homogeneous equations represent lines passing through the origin \\((0,0)\\). The trivial solution corresponds to the origin itself. If the lines are distinct, they only intersect at the origin (unique trivial solution). If they are the same line, they have infinitely many solutions (all points on the line, including the origin). These systems appear often in physical modeling; for example, determining if there are non-zero currents in a circuit without any external voltage sources applied."
  },
  {
    "objectID": "la-12.html#homogeneous-linear-systems-cont.",
    "href": "la-12.html#homogeneous-linear-systems-cont.",
    "title": "Linear Algebra",
    "section": "Homogeneous Linear Systems (Cont.)",
    "text": "Homogeneous Linear Systems (Cont.)\nTheorem 1.2.1: Free Variable Theorem for Homogeneous Systems\nIf a homogeneous linear system has \\(n\\) unknowns, and if the reduced row echelon form of its augmented matrix has \\(r\\) nonzero rows, then the system has \\(n - r\\) free variables.\nTheorem 1.2.2: Existence of Nontrivial Solutions\nA homogeneous linear system with more unknowns than equations (i.e., \\(n &gt; m\\)) has infinitely many solutions.\n\nProof Sketch: If \\(m &lt; n\\) (equations &lt; unknowns), then when reducing to RREF, the number of leading 1s (\\(r\\)) must be less than or equal to \\(m\\). So, \\(r \\le m &lt; n\\). This means \\(n - r &gt; 0\\), guaranteeing at least one free variable. Since free variables can take infinite values, there are infinitely many solutions (including the trivial one).\n\nThis theorem is very powerful and has applications in fields like signal processing (e.g., finding the null space of a matrix where the outputs of a system are zero) and control theory (e.g., determining system stability).\n\nThese two theorems are cornerstone results for homogeneous systems.\nTheorem 1.2.1 tells us exactly how many free variables we’ll have. Each non-zero row in the RREF gives us a leading variable. The remaining variables are free. So, if you have \\(n\\) unknowns and \\(r\\) leading variables (which is the number of non-zero rows), you’ll have \\(n-r\\) free variables. This is a direct measure of the “degree of freedom” in your solution set.\nTheorem 1.2.2 is a very practical shortcut. It says that if you have fewer equations than unknowns in a homogeneous system, you are guaranteed to have infinitely many solutions. This is because you won’t have enough independent constraints to pin down all your variables to unique values. In an ECE context, this might occur when you’re analyzing a network, and you have more unknown currents than independent Kirchhoff’s Current Law junctions or Kirchhoff’s Voltage Law loops, implying multiple current distributions can satisfy the zero-source conditions."
  },
  {
    "objectID": "la-12.html#example-6-a-homogeneous-system",
    "href": "la-12.html#example-6-a-homogeneous-system",
    "title": "Linear Algebra",
    "section": "Example 6: A Homogeneous System",
    "text": "Example 6: A Homogeneous System\nSolve the homogeneous linear system: \\[\n\\begin{array}{r r r r r r}\n{x_{1} + 3x_{2}}  & {-2x_{3}} & {}          & {+2x_{5}} & {}         & = 0\\\\\n{2x_{1} + 6x_{2}} & {-5x_{3}} & {-2x_{4}}   & {+4x_{5}} & {- 3x_{6}} & = 0\\\\\n{}                & {5x_{3}}  & {+10x_{4}}  & {}        & {+15x_{6}} & = 0\\\\\n{2x_{1} + 6x_{2}} & {}        & {+8x_{4}}   & {+4x_{5}} & {+18x_{6}} & = 0\n\\end{array}\n\\] This is the same system as in Example 5, but with zeros on the right-hand side. The augmented matrix is: \\[\n\\left[{\\begin{array}{r r r r r r | r}{1}&{3}&{-2}&{0}&{2}&{0}&{0}\\\\ {2}&{6}&{-5}&{-2}&{4}&{-3}&{0}\\\\ {0}&{0}&{5}&{10}&{0}&{15}&{0}\\\\ {2}&{6}&{0}&{8}&{4}&{18}&{0}\\end{array}}\\right]\n\\] Note that elementary row operations will preserve the column of zeros."
  },
  {
    "objectID": "la-12.html#example-6-a-homogeneous-system-1",
    "href": "la-12.html#example-6-a-homogeneous-system-1",
    "title": "Linear Algebra",
    "section": "Example 6: A Homogeneous System",
    "text": "Example 6: A Homogeneous System\nIts RREF (from Example 5’s reduction) will be: \\[\n\\left[{\\begin{array}{r r r r r r | r}{1}&{3}&{0}&{4}&{2}&{0}&{0}\\\\ {0}&{0}&{1}&{2}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{0}&{1}&{0}\\\\ {0}&{0}&{0}&{0}&{0}&{0}&{0}\\end{array}}\\right]\n\\] The corresponding system of equations is: \\[\n\\begin{array}{r r r r r r}\n{x_{1} + 3x_{2}} & {}      & {+4x_{4}} & {+ 2x_{5}} & {}      & {= 0}\\\\\n{}               & {x_{3}} & {+2x_{4}} & {}         & {}      & {= 0}\\\\\n{}               & {}      & {}        & {}         & {x_{6}}  & {= 0}\n\\end{array}\n\\] Leading variables: \\(x_1, x_3, x_6\\). Free variables: \\(x_2, x_4, x_5\\). Let \\(x_2 = r, x_4 = s, x_5 = t\\). Solution: \\[\nx_{1} = -3r - 4s - 2t, \\quad x_{2} = r, \\quad x_{3} = -2s, \\quad x_{4} = s, \\quad x_{5} = t, \\quad x_{6} = 0\n\\] Since there are free variables, this homogeneous system has infinitely many solutions. The trivial solution is when \\(r=s=t=0\\).\n\nWe use the same matrix transformation as in Example 5 because the coefficients are identical, and the right-hand side, being all zeros, will remain all zeros throughout the elementary row operations. This is a very convenient property of homogeneous systems.\nFrom the RREF, we identify our leading variables (\\(x_1, x_3, x_6\\)) and our free variables (\\(x_2, x_4, x_5\\)). Here, we have \\(n=6\\) unknowns and \\(r=3\\) non-zero rows, so we have \\(n-r = 6-3=3\\) free variables, as per Theorem 1.2.1. This directly tells us we will have infinitely many solutions, including the trivial solution when all parameters are zero.\nThis example ties directly into the concept of a matrix’s null space (or kernel) - the set of all vectors that are mapped to the zero vector by the linear transformation defined by the matrix. Finding the null space involves solving just such a homogeneous system, a common task in image processing, machine learning, and control theory."
  },
  {
    "objectID": "la-12.html#gaussian-elimination-and-back-substitution",
    "href": "la-12.html#gaussian-elimination-and-back-substitution",
    "title": "Linear Algebra",
    "section": "Gaussian Elimination and Back-Substitution",
    "text": "Gaussian Elimination and Back-Substitution\nFor computer-based solutions of large systems, Gaussian Elimination (reducing to Row Echelon Form) followed by back-substitution is generally more efficient than full Gauss-Jordan elimination.\nProcedure:\n\nUse elementary row operations to reduce the augmented matrix to Row Echelon Form (REF) (Forward Elimination).\nWrite the corresponding system of equations.\nSolve the equations for the leading variables.\nStarting from the bottom equation and working upwards, substitute the values of determined variables (or parametric expressions) into the equations above.\nAssign arbitrary values to any remaining free variables to get the general solution."
  },
  {
    "objectID": "la-12.html#gaussian-elimination-and-back-substitution-1",
    "href": "la-12.html#gaussian-elimination-and-back-substitution-1",
    "title": "Linear Algebra",
    "section": "Gaussian Elimination and Back-Substitution",
    "text": "Gaussian Elimination and Back-Substitution\nExample 7: Example 5 Solved by Back-Substitution\nREF from Example 5: \\[\n\\left[{\\begin{array}{r r r r r r | r}{1}&{3}&{-2}&{0}&{2}&{0}&{0}\\\\ {0}&{0}&{1}&{2}&{0}&{3}&{1}\\\\ {0}&{0}&{0}&{0}&{0}&{1}&{{\\frac{1}{3}}}\\\\ {0}&{0}&{0}&{0}&{0}&{0}&{0}\\end{array}}\\right]\n\\] Corresponding system: \\[\n\\begin{array}{l} x_1 + 3x_2 - 2x_3 + 2x_5 = 0 \\\\ x_3 + 2x_4 + 3x_6 = 1 \\\\ x_6 = \\frac{1}{3} \\end{array}\n\\] Solving for leading variables (\\(x_1, x_3, x_6\\)): \\[\n\\begin{array}{l} {x_1 = -3x_2 + 2x_3 - 2x_5} \\\\ {x_3 = 1 - 2x_4 - 3x_6} \\\\ {x_6 = \\frac{1}{3}} \\end{array}\n\\]"
  },
  {
    "objectID": "la-12.html#gaussian-elimination-and-back-substitution-2",
    "href": "la-12.html#gaussian-elimination-and-back-substitution-2",
    "title": "Linear Algebra",
    "section": "Gaussian Elimination and Back-Substitution",
    "text": "Gaussian Elimination and Back-Substitution\nNow, back-substitute:\n\nSubstitute \\(x_6 = \\frac{1}{3}\\) into the second equation: \\(x_3 = 1 - 2x_4 - 3\\left(\\frac{1}{3}\\right) \\implies x_3 = 1 - 2x_4 - 1 \\implies x_3 = -2x_4\\)\nSubstitute \\(x_3 = -2x_4\\) into the first equation: \\(x_1 = -3x_2 + 2(-2x_4) - 2x_5 \\implies x_1 = -3x_2 - 4x_4 - 2x_5\\)\n\nAssign free variables \\(x_2=r, x_4=s, x_5=t\\). Resulting general solution: \\[\nx_1 = -3r - 4s - 2t, \\quad x_2 = r, \\quad x_3 = -2s, \\quad x_4 = s, \\quad x_5 = t, \\quad x_6 = \\frac{1}{3}\n\\] This confirms the same solution as Gauss-Jordan elimination for Example 5.\n\nWhile Gauss-Jordan gives the direct solution in RREF, for very large systems, the “backward elimination” phase (making zeros above leading 1s) is computationally more expensive than simply using back-substitution. That’s why Gaussian Elimination (to REF) followed by back-substitution is preferred for computational efficiency.\nLet’s revisit Example 5 with this method. We already have the REF from our previous steps. Now, convert it back into equations. The key here is the back-substitution part. You start from the bottommost equation that provides a direct value (like \\(x_6 = 1/3\\)). You substitute this value into the equation above it, then solve for the next leading variable. You continue this upward process, substituting known values (or expressions in terms of free variables) into higher equations, until all leading variables are expressed either as unique numerical values or in terms of the free variables.\nThe final parametric solution is identical to what we got with Gauss-Jordan, demonstrating that both methods yield the same correct result, but Gaussian elimination + back-substitution can be more efficient for computers. This process is fundamental to how linear system solvers work in tools like MATLAB, SciPy, and other numerical analysis packages crucial for ECE applications."
  },
  {
    "objectID": "la-12.html#example-8-discussion-on-solutions-from-ref",
    "href": "la-12.html#example-8-discussion-on-solutions-from-ref",
    "title": "Linear Algebra",
    "section": "Example 8: Discussion on Solutions from REF",
    "text": "Example 8: Discussion on Solutions from REF\nSuppose these are augmented matrices for systems in \\(x_1, x_2, x_3, x_4\\). All are in REF, but not RREF.\n\n\n(a) No Solution \\[\n{\\left[\\begin{array}{l l l l | l}{1}&{-3}&{7}&{2}&{5}\\\\ {0}&{1}&{2}&{-4}&{1}\\\\ {0}&{0}&{1}&{6}&{9}\\\\ {0}&{0}&{0}&{0}&{1}\\end{array}\\right]}\n\\] Last row: \\(0=1\\). Inconsistent.\n\n(b) Infinitely Many Solutions \\[\n{\\left[\\begin{array}{l l l l | l}{1}&{-3}&{7}&{2}&{5}\\\\ {0}&{1}&{2}&{-4}&{1}\\\\ {0}&{0}&{1}&{6}&{9}\\\\ {0}&{0}&{0}&{0}&{0}\\end{array}\\right]}\n\\] Last row: \\(0=0\\). Leading variables \\(x_1, x_2, x_3\\). Free variable \\(x_4\\). Infinitely many solutions."
  },
  {
    "objectID": "la-12.html#example-8-discussion-on-solutions-from-ref-1",
    "href": "la-12.html#example-8-discussion-on-solutions-from-ref-1",
    "title": "Linear Algebra",
    "section": "Example 8: Discussion on Solutions from REF",
    "text": "Example 8: Discussion on Solutions from REF\n\n\n(c) Unique Solution \\[\n{\\left[\\begin{array}{l l l l | l}{1}&{-3}&{7}&{2}&{5}\\\\ {0}&{1}&{2}&{-4}&{1}\\\\ {0}&{0}&{1}&{6}&{9}\\\\ {0}&{0}&{0}&{1}&{0}\\end{array}\\right]}\n\\] Last row: \\(x_4=0\\). All variables are leading variables. Unique solution.\n\n\n\n\nThese examples (a), (b), and (c) reinforce how to interpret solutions directly from a matrix in Row Echelon Form.\n\n(a) No Solution: As before, a row of [0 0 0 0 | 1] or similar directly means inconsistency. This is a common situation for engineers where your model assumptions might be contradictory, leading to no possible physical solution.\n(b) Infinitely Many Solutions: A row of all zeros (including the constant term) means that equation is redundant. If you have fewer leading 1s than variables (i.e., you have free variables), you will have infinite solutions. This suggests flexibility in a system or potentially an underdefined problem.\n(c) Unique Solution: If every variable corresponds to a leading 1 (no free variables), and there are no contradictory rows, you will have a unique solution. Even though the matrix isn’t fully reduced, you can still determine uniqueness by counting leading 1’s. This often represents tightly constrained, well-defined problems where there’s only one way for the system to behave.\n\nThese insights are crucial for diagnosing circuit behaviors, analyzing control system stability, or simulating physical phenomena in engineering."
  },
  {
    "objectID": "la-12.html#important-facts-about-echelon-forms",
    "href": "la-12.html#important-facts-about-echelon-forms",
    "title": "Linear Algebra",
    "section": "Important Facts About Echelon Forms",
    "text": "Important Facts About Echelon Forms\n\nUnique RREF: Every matrix has a unique reduced row echelon form. No matter what sequence of elementary row operations you use, you will always arrive at the same RREF for a given matrix.\nNon-Unique REF: Row echelon forms (REF) are not unique. Different sequences of elementary row operations can lead to different REF forms for the same matrix.\nPivot Positions/Columns are Unique: Although REF can vary, the number of zero rows and the positions of the leading 1s (called pivot positions) are always the same for a given matrix. Columns containing pivot positions are called pivot columns."
  },
  {
    "objectID": "la-12.html#important-facts-about-echelon-forms-1",
    "href": "la-12.html#important-facts-about-echelon-forms-1",
    "title": "Linear Algebra",
    "section": "Important Facts About Echelon Forms",
    "text": "Important Facts About Echelon Forms\nExample 9: Pivot Positions and Columns\nFor the matrix \\[\nA = \\begin{bmatrix}\n0 & 0 & -2 & 0 & 7 & 12 \\\\\n2 & 4 & -10 & 6 & 12 & 28 \\\\\n2 & 4 & -5 & 6 & -5 & -1\n\\end{bmatrix}\n\\] one possible REF is: \\[\n\\left[ \\begin{array}{cccccc}1 & 2 & -5 & 3 & 6 & 14 \\\\ 0 & 0 & 1 & 0 & -\\frac{7}{2} & -6 \\\\ 0 & 0 & 0 & 0 & 1 & 2 \\end{array} \\right]\n\\] The leading 1s are in positions (row 1, column 1), (row 2, column 3), and (row 3, column 5). These are the pivot positions. The pivot columns are columns 1, 3, and 5. In a linear system, pivot columns identify the leading variables. Here, \\(x_1, x_3, x_5\\) would be leading variables.\n\nBefore we conclude, let’s highlight some critical theoretical properties of echelon forms that are important in advanced linear algebra and numerical analysis.\n\nUniqueness of RREF: This is a very powerful guarantee. It means that regardless of the path you take with elementary row operations, if you reduce a matrix all the way to RREF, you will always get the same final matrix. This provides a definitive “canonical form” for any given matrix.\nNon-Uniqueness of REF: In contrast, there can be multiple valid Row Echelon Forms for a single matrix. This is because the choices of row operations in the forward phase can sometimes be arbitrary.\nUniqueness of Pivot Positions: Despite the non-uniqueness of REF, the locations of the leading 1s (the pivot positions) are always fixed. This means the number of leading variables and the specific variables that are leading (the pivot columns) are invariant properties of the matrix. This is a crucial concept for understanding matrix rank, which we’ll cover later, and for identifying independent variables in engineering models.\n\nFor instance, in the example, columns 1, 3, and 5 are pivot columns. This immediately tells us that if this were an augmented matrix for a system with variables \\(x_1, \\ldots, x_5\\), then \\(x_1, x_3, x_5\\) would be our leading variables. Understanding pivot columns is vital in fields like data compression and feature selection in machine learning, which heavily rely on linear algebra."
  },
  {
    "objectID": "la-12.html#roundoff-error-and-stability",
    "href": "la-12.html#roundoff-error-and-stability",
    "title": "Linear Algebra",
    "section": "Roundoff Error and Stability",
    "text": "Roundoff Error and Stability\nWhile Gaussian and Gauss-Jordan elimination are mathematically sound, their practical implementation on computers faces challenges:\n\nRoundoff Errors: Computers use finite-precision arithmetic, leading to small errors in calculations.\nInstability: Successive calculations can amplify these roundoff errors, rendering results inaccurate or “unstable.”\n\nFor large systems (e.g., in computational fluid dynamics, large-scale circuit simulations), specialized numerical techniques are used to minimize these errors. Gaussian elimination is generally preferred over Gauss-Jordan due to fewer operations, reducing error propagation.\n\nFinally, a quick but important note for future ECE endeavors. While we’ve learned the exact mathematical procedures for solving linear systems, real-world computations using computers introduce complexities.\nComputers can only represent numbers with a finite number of decimal places. This “roundoff” can introduce tiny errors. When you perform millions of elementary row operations on a large matrix, these tiny errors can accumulate and, in some cases, magnify, leading to a completely incorrect answer. An algorithm that is susceptible to this error magnification is called “unstable.”\nTherefore, numerical linear algebra, a specialized field, focuses on developing “stable” algorithms that minimize roundoff error. It’s why engineers often use pre-built, robust libraries (like NumPy’s linear algebra functions) instead of coding these algorithms from scratch, as these libraries employ sophisticated techniques to manage these numerical challenges. Gaussian elimination is often favored over Gauss-Jordan in these libraries because it generally involves fewer arithmetic operations, thus less opportunity for error accumulation."
  },
  {
    "objectID": "la-12.html#conclusion",
    "href": "la-12.html#conclusion",
    "title": "Linear Algebra",
    "section": "Conclusion",
    "text": "Conclusion\n\nEchelon Forms (REF & RREF): Provide structured representations of matrices for systematic problem-solving.\nGaussian Elimination & Gauss-Jordan Elimination: Algorithms for transforming matrices into echelon forms.\nSolutions: Determined by the final echelon form (unique, infinite, or no solution).\nHomogeneous Systems: Always consistent, trivial solution always exists, nontrivial solutions if free variables exist.\nPractical Importance: These methods are fundamental to numerical algorithms for systems of equations in all ECE applications, from circuit analysis to control systems, signal processing, and machine learning.\n\n\nTo conclude our discussion on Gaussian Elimination: We’ve defined the two critical forms matrices can take: Row Echelon Form and Reduced Row Echelon Form, understanding their properties and how they differ. We’ve learned the step-by-step algorithms, Gauss-Jordan and Gaussian elimination with back-substitution, for achieving these forms. We’ve reviewed how to interpret the solution types (unique, infinite, no solution) directly from the echelon forms. And we covered homogeneous systems, a special class that always has at least the trivial solution, with the key insight that more unknowns than equations guarantees infinitely many solutions.\nThe systematic approach of Gaussian and Gauss-Jordan elimination is not just an academic exercise. It is the bedrock of how computers solve linear systems efficiently and accurately. Whether you’re analyzing the behavior of op-amp circuits, designing digital filters, or optimizing power grids, you will encounter linear systems, and these methods are the backbone of their solutions.\nNext time, we will explore vectors and their properties, building further on these foundational concepts. Thank you!"
  },
  {
    "objectID": "la-12.html#exercises",
    "href": "la-12.html#exercises",
    "title": "Linear Algebra",
    "section": "Exercises",
    "text": "Exercises\ngrasple_exercise_2_1_B\nTo perform row operations on a matrix."
  },
  {
    "objectID": "la-12.html#exercises-1",
    "href": "la-12.html#exercises-1",
    "title": "Linear Algebra",
    "section": "Exercises",
    "text": "Exercises\ngrasple_exercise_2_1_C\nChecking whether a matrix is in (reduced) echelon form."
  },
  {
    "objectID": "la-12.html#exercises-2",
    "href": "la-12.html#exercises-2",
    "title": "Linear Algebra",
    "section": "Exercises",
    "text": "Exercises\ngrasple_exercise_2_1_E\nApplying the algorithm to compute a solution of a linear system."
  },
  {
    "objectID": "la-104.html#introduction-to-markov-processes",
    "href": "la-104.html#introduction-to-markov-processes",
    "title": "Linear Algebra",
    "section": "Introduction to Markov Processes",
    "text": "Introduction to Markov Processes\nA Markov Chain or Markov Process models a system that changes from state to state over time.\nExample States:\n\nWeather: Sunny, Cloudy, Rainy\nTraffic signal: Green, Yellow, Red\nDigital circuit: High, Low\nCommunication Channel: Good, Noisy\n\n\n\nMemoryless Property: This is crucial. It simplifies modeling as we only need the current state to predict the next, not the entire history. This is often applicable in real-world engineering systems where exact past sequences are too complex to track.\nIn ECE, think of device states (e.g., working, failed), network node status (busy, idle), or process control (above threshold, below threshold)."
  },
  {
    "objectID": "la-104.html#transition-probabilities-and-matrices",
    "href": "la-104.html#transition-probabilities-and-matrices",
    "title": "Linear Algebra",
    "section": "Transition Probabilities and Matrices",
    "text": "Transition Probabilities and Matrices\nIf a Markov chain has \\(k\\) possible states, labeled \\(1, 2, \\ldots, k\\):\nTransition Probability (\\(p_{ij}\\)):\n\nThe probability that the system is in state \\(i\\) at the next observation, given it was in state \\(j\\) at the preceding observation.\n\nTransition Matrix (\\(P\\)):\n\n\\(P = [p_{ij}]\\)\nColumns represent preceding states.\nRows represent next states.\n\n\\[\nP = \\left[ \\begin{array}{llll}\np_{11} & p_{12} & \\dots & p_{1k} \\\\\np_{21} & p_{22} & \\dots & p_{2k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\np_{k1} & p_{k2} & \\dots & p_{kk}\n\\end{array} \\right] \\begin{array}{l}\n\\longleftarrow \\text{ Next State 1} \\\\\n\\longleftarrow \\text{ Next State 2} \\\\\n\\longleftarrow \\vdots \\\\\n\\longleftarrow \\text{ Next State k}\n\\end{array}\n\\] \\[\\uparrow \\uparrow \\quad \\uparrow\\] \\[\\text{Prev. State 1} \\quad \\text{Prev. State 2} \\quad \\text{Prev. State k}\\]\n\n\nEmphasize that the columns sum to 1 because from any given preceding state \\(j\\), the system must transition to some state (could be itself).\nThis structure of \\(P\\) is vital for understanding how probabilities propagate through the system.\n\\(p_{32}\\) is the probability to go FROM state 2 TO state 3."
  },
  {
    "objectID": "la-104.html#example-1-car-rental-agency",
    "href": "la-104.html#example-1-car-rental-agency",
    "title": "Linear Algebra",
    "section": "Example 1: Car Rental Agency",
    "text": "Example 1: Car Rental Agency\nA car rental agency has three locations (1, 2, 3). The manager finds customer return patterns:\n\n\n\nFrom Location 1:\n\nReturn to 1: 80%\nReturn to 2: 10%\nReturn to 3: 10%\n\nFrom Location 2:\n\nReturn to 1: 30%\nReturn to 2: 20%\nReturn to 3: 50%\n\nFrom Location 3:\n\nReturn to 1: 20%\nReturn to 2: 60%\nReturn to 3: 20%\n\n\n\nTransition Matrix (\\(P\\)): \\[\nP = \\left[ \\begin{array}{ccc}\n.8 & .3 & .2 \\\\\n.1 & .2 & .6 \\\\\n.1 & .5 & .2\n\\end{array} \\right]\n\\] This matrix is a Stochastic Matrix: all entries are non-negative, and columns sum to 1. \\[\np_{1j} + p_{2j} + \\dots + p_{kj} = 1\n\\]\n\n\n\nWalk through how the percentages map to the matrix entries. For example, the first column corresponds to cars rented from Location 1. 80% return to 1 (\\(p_{11}=.8\\)), 10% to 2 (\\(p_{21}=.1\\)), 10% to 3 (\\(p_{31}=.1\\)). Sum is \\(0.8+0.1+0.1 = 1\\).\nExplain the term “Stochastic matrix” and its importance. It’s a fundamental property of valid transition matrices."
  },
  {
    "objectID": "la-104.html#state-vectors",
    "href": "la-104.html#state-vectors",
    "title": "Linear Algebra",
    "section": "State Vectors",
    "text": "State Vectors\nDefinition 2: State Vector (\\(\\mathbf{x}\\)) A column vector \\(\\mathbf{x}\\) whose \\(i\\)-th component \\(x_i\\) is the probability that the system is in the \\(i\\)-th state at a given time.\nExample for a 3-state system: \\[\n\\mathbf{x} = \\left[ \\begin{array}{l}\nx_{1} \\\\\nx_{2} \\\\\nx_{3}\n\\end{array} \\right]\n\\]\n\n\\(x_i \\ge 0\\) for all \\(i\\).\n\\(\\sum x_i = 1\\) (the system must be in some state).\nA vector with these properties is called a probability vector.\n\nTheorem 10.4.1: Future State Prediction If \\(P\\) is the transition matrix of a Markov chain and \\(\\mathbf{x}^{(n)}\\) is the state vector at the \\(n\\)-th observation, then: \\[ \\mathbf{x}^{(n+1)} = P \\mathbf{x}^{(n)} \\] This implies: \\[ \\mathbf{x}^{(n)} = P^n \\mathbf{x}^{(0)} \\]\n\n\nThe state vector provides a probabilistic snapshot of the system’s distribution across its states.\nThis theorem is the core engine of Markov chain dynamics: matrix-vector multiplication propagates uncertain states through time.\nHighlight the power of linear algebra: complex probabilistic evolution reduces to repeated matrix multiplication."
  },
  {
    "objectID": "la-104.html#example-2-alumni-donation-probabilistic-evolution",
    "href": "la-104.html#example-2-alumni-donation-probabilistic-evolution",
    "title": "Linear Algebra",
    "section": "Example 2: Alumni Donation (Probabilistic Evolution)",
    "text": "Example 2: Alumni Donation (Probabilistic Evolution)\nAn alumni office finds:\n\n80% of contributors contribute next year.\n30% of non-contributors contribute next year.\n\nStates:\n\nContributor\nNon-contributor\n\nTransition Matrix: \\[\nP = \\left[ \\begin{array}{ll}\n.8 & .3 \\\\\n.2 & .7\n\\end{array} \\right]\n\\]\nLet’s track a new graduate who did not contribute initially.\nInitial state \\(\\mathbf{x}^{(0)}\\): certainty in state 2 (non-contributor). \\[\n\\mathbf{x}^{(0)} = \\left[ \\begin{array}{l}\n0 \\\\\n1\n\\end{array} \\right]\n\\]\n\n\nExplain derivation of \\(P\\):\n\nCol 1 (from Contributor): 80% stay Contributor (\\(p_{11}=.8\\)), 20% become Non-contributor (\\(p_{21}=.2\\)).\nCol 2 (from Non-contributor): 30% become Contributor (\\(p_{12}=.3\\)), 70% stay Non-contributor (\\(p_{22}=.7\\)).\n\nThe initial state vector \\(\\mathbf{x}^{(0)}\\) captures the starting condition. Since the person did not contribute, probability of being in state 1 is 0, probability of being in state 2 is 1."
  },
  {
    "objectID": "la-104.html#example-2-alumni-donation-interactive-simulation",
    "href": "la-104.html#example-2-alumni-donation-interactive-simulation",
    "title": "Linear Algebra",
    "section": "Example 2: Alumni Donation (Interactive Simulation)",
    "text": "Example 2: Alumni Donation (Interactive Simulation)\nLet’s simulate the state vector evolution for the alumni example over several years.\n\n\n\n\n\n\n\n\nPyodide interaction: This is a live Python simulation. Students can interact with it, change x0, or P to see the effect.\nObserve how the probabilities shift and eventually stabilize. This hints at a “long-term” behavior.\nFor ECE, this relates to steady-state analysis of dynamic systems or iterative algorithms converging."
  },
  {
    "objectID": "la-104.html#example-4-car-rental-agency-interactive-simulation",
    "href": "la-104.html#example-4-car-rental-agency-interactive-simulation",
    "title": "Linear Algebra",
    "section": "Example 4: Car Rental Agency (Interactive Simulation)",
    "text": "Example 4: Car Rental Agency (Interactive Simulation)\nRevisiting Example 1 (\\(P\\) is 3x3). Let’s simulate if a car is initially rented from Location 2.\nInitial state \\(\\mathbf{x}^{(0)}\\): \\[\n\\mathbf{x}^{(0)} = \\left[ \\begin{array}{l}\n0 \\\\\n1 \\\\\n0\n\\end{array} \\right]\n\\]\n\n\n\n\n\n\n\n\nAgain, observe the convergence. Even with a different starting state (car rented from location 2), the system still approaches a stable distribution.\nThis suggests that the long-term behavior might be independent of the initial conditions for certain types of Markov chains."
  },
  {
    "objectID": "la-104.html#example-5-traffic-officer-network-flow",
    "href": "la-104.html#example-5-traffic-officer-network-flow",
    "title": "Linear Algebra",
    "section": "Example 5: Traffic Officer (Network Flow)",
    "text": "Example 5: Traffic Officer (Network Flow)\nA traffic officer moves between 8 intersections. They remain at the current intersection or move to an adjacent one, choosing randomly (equally likely).\n\n\n\n\n\ngraph LR\n    subgraph Intersections\n        1 --- 2\n        1 --- 3\n        2 --- 4\n        2 --- 5\n        3 --- 6\n        4 --- 7\n        4 --- 8\n        5 --- 6\n        5 --- 8\n        6 --- 7\n        7 --- 8\n    end\n    style 1 fill:#f9f,stroke:#333,stroke-width:2px\n    style 2 fill:#f9f,stroke:#333,stroke-width:2px\n    style 3 fill:#f9f,stroke:#333,stroke-width:2px\n    style 4 fill:#f9f,stroke:#333,stroke-width:2px\n    style 5 fill:#f9f,stroke:#333,stroke-width:2px\n    style 6 fill:#f9f,stroke:#333,stroke-width:2px\n    style 7 fill:#f9f,stroke:#333,stroke-width:2px\n    style 8 fill:#f9f,stroke:#333,stroke-width:2px\n\n\n\n\n\n\nIf at intersection 5 (neighbors 2, 4, 8), she can go to 2, 4, 5, or 8, each with probability \\(1/4\\). The transition matrix \\(P\\) is 8x8.\n\n\nMermaid diagram visualizes the “connectivity” of states. In engineering, this could represent communication network nodes, power grid connections, or process flow paths.\nChallenge students: how would you construct the 8x8 matrix based on this rule? For example, the 5th column would have \\(p_{25}=.25, p_{45}=.25, p_{55}=.25, p_{85}=.25\\), and others zero.\nThis is a good example of how graph theory (network diagram) and linear algebra (transition matrix) are intertwined in practical problems."
  },
  {
    "objectID": "la-104.html#example-5-transition-matrix",
    "href": "la-104.html#example-5-transition-matrix",
    "title": "Linear Algebra",
    "section": "Example 5: Transition Matrix",
    "text": "Example 5: Transition Matrix\n\\(P_{8\\times8}\\) is given. If the officer starts at Intersection 5, initial state \\(\\mathbf{x}^{(0)}\\) is: \\[\n\\mathbf{x}^{(0)} = \\left[ \\begin{array}{l}\n0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\n\\end{array} \\right]\n\\]"
  },
  {
    "objectID": "la-104.html#example-5-transition-matrix-1",
    "href": "la-104.html#example-5-transition-matrix-1",
    "title": "Linear Algebra",
    "section": "Example 5: Transition Matrix",
    "text": "Example 5: Transition Matrix\nLet’s see how her location probabilities evolve.\n\n\n\n\n\n\n\n\nWhy Transpose? The given problem text’s \\(P\\) matrix (image) implicitly follows a row-stochastic convention or has columns summing to 1 by chance. The common definition of a transition matrix (\\(P_{ij}\\) from \\(j\\) to \\(i\\)) makes it column-stochastic. I’ve explicitly transposed the numpy array initialization to ensure it’s column-stochastic for \\(P \\mathbf{x}\\).\nThe results show a distribution of probabilities across intersections, not just for intersection 5. This signifies the probable location over time.\nNotice how the probabilities converge to very specific values, regardless of the initial starting point. This is the crucial characteristic of a “regular” Markov chain.\nThis type of analysis is relevant in ECE for resource allocation, load balancing, or network traffic routing."
  },
  {
    "objectID": "la-104.html#visualization-1",
    "href": "la-104.html#visualization-1",
    "title": "Linear Algebra",
    "section": "Visualization (1)",
    "text": "Visualization (1)"
  },
  {
    "objectID": "la-104.html#visualization-2",
    "href": "la-104.html#visualization-2",
    "title": "Linear Algebra",
    "section": "Visualization (2)",
    "text": "Visualization (2)\nVisualizing a Markov Chain - Will Hipson"
  },
  {
    "objectID": "la-104.html#limiting-behavior-and-steady-state-vectors",
    "href": "la-104.html#limiting-behavior-and-steady-state-vectors",
    "title": "Linear Algebra",
    "section": "Limiting Behavior and Steady-State Vectors",
    "text": "Limiting Behavior and Steady-State Vectors\nDoes \\(\\mathbf{x}^{(n)}\\) always converge to a fixed vector?\n\nNo! Example 6 shows it can oscillate.\n\n\\(P = \\left[ \\begin{array}{cc} 0 & 1 \\\\ 1 & 0 \\end{array} \\right]\\), \\(\\mathbf{x}^{(0)} = \\left[ \\begin{array}{c} 1 \\\\ 0 \\end{array} \\right]\\)\n\\(\\mathbf{x}^{(0)} = \\left[ \\begin{array}{c} 1 \\\\ 0 \\end{array} \\right]\\), \\(\\mathbf{x}^{(1)} = \\left[ \\begin{array}{c} 0 \\\\ 1 \\end{array} \\right]\\), \\(\\mathbf{x}^{(2)} = \\left[ \\begin{array}{c} 1 \\\\ 0 \\end{array} \\right], \\ldots\\)\n\n\nDefinition 3: Regular Transition Matrix A transition matrix \\(P\\) is regular if some integer power of it, \\(P^m\\), has all positive entries (no zeros). * Examples 1, 2, 5 were regular (\\(m=1\\) or \\(m=4\\)). This ensures convergence."
  },
  {
    "objectID": "la-104.html#limiting-behavior-and-steady-state-vectors-1",
    "href": "la-104.html#limiting-behavior-and-steady-state-vectors-1",
    "title": "Linear Algebra",
    "section": "Limiting Behavior and Steady-State Vectors",
    "text": "Limiting Behavior and Steady-State Vectors\nTheorem 10.4.3: Behavior of \\(P^n \\mathbf{x}\\) as \\(n \\rightarrow \\infty\\) If \\(P\\) is a regular transition matrix and \\(\\mathbf{x}\\) is any probability vector, then as \\(n \\rightarrow \\infty\\): \\[ P^n \\mathbf{x} \\rightarrow \\mathbf{q} \\] where \\(\\mathbf{q}\\) is a fixed probability vector, called the steady-state vector.\n\n\\(\\mathbf{q}\\) is independent of the initial state \\(\\mathbf{x}^{(0)}\\).\nAll entries of \\(\\mathbf{q}\\) are positive.\n\n\n\nThe oscillation example shows why the “regular” condition is necessary. If \\(P\\) can prevent transitions to certain states over many steps, it might not converge.\nThe term “steady-state” is very familiar in ECE (e.g., in circuit analysis, control systems). Here, it applies to the long-term probability distribution of states.\nThis means that given enough time, the system will settle down into a predictable long-term distribution of probabilities across its states, regardless of how it started."
  },
  {
    "objectID": "la-104.html#calculating-the-steady-state-vector-mathbfq",
    "href": "la-104.html#calculating-the-steady-state-vector-mathbfq",
    "title": "Linear Algebra",
    "section": "Calculating the Steady-State Vector (\\(\\mathbf{q}\\))",
    "text": "Calculating the Steady-State Vector (\\(\\mathbf{q}\\))\nTheorem 10.4.4: Steady-State Vector Property The steady-state vector \\(\\mathbf{q}\\) of a regular transition matrix \\(P\\) is the unique probability vector that satisfies the equation: \\[ P \\mathbf{q} = \\mathbf{q} \\] This is an eigenvector problem! \\(\\mathbf{q}\\) is an eigenvector of \\(P\\) corresponding to the eigenvalue \\(\\lambda = 1\\).\nRearranging the equation: \\[ (I - P) \\mathbf{q} = \\mathbf{0} \\] This is a homogeneous linear system. We need to find the unique probability vector (entries sum to 1) in the null space of \\((I-P)\\).\n\n\nThis connection to eigenvectors is a powerful application of linear algebra in probability theory.\nSince matrices describing physical systems often have stable long-term behaviors, finding the steady-state vector is a common task in various ECE fields, from queueing theory to network analysis.\nThe matrix \\((I-P)\\) will be singular, meaning it has a non-trivial null space. We are looking for a specific vector in that null space."
  },
  {
    "objectID": "la-104.html#example-7-alumni-donation-steady-state-calculation",
    "href": "la-104.html#example-7-alumni-donation-steady-state-calculation",
    "title": "Linear Algebra",
    "section": "Example 7: Alumni Donation (Steady-State Calculation)",
    "text": "Example 7: Alumni Donation (Steady-State Calculation)\nRecall \\(P = \\left[ \\begin{array}{ll} .8 & .3 \\\\ .2 & .7 \\end{array} \\right]\\). We want to solve \\((I - P) \\mathbf{q} = \\mathbf{0}\\) \\[\n\\left( \\left[ \\begin{array}{cc} 1 & 0 \\\\ 0 & 1 \\end{array} \\right] - \\left[ \\begin{array}{cc} .8 & .3 \\\\ .2 & .7 \\end{array} \\right] \\right) \\left[ \\begin{array}{c} q_{1} \\\\ q_{2} \\end{array} \\right] = \\left[ \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right]\n\\] \\[\n\\left[ \\begin{array}{cc} .2 & -.3 \\\\ -.2 & .3 \\end{array} \\right] \\left[ \\begin{array}{c} q_{1} \\\\ q_{2} \\end{array} \\right] = \\left[ \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right]\n\\] This gives \\(.2q_1 - .3q_2 = 0 \\implies q_1 = 1.5 q_2\\).\nAlso, \\(q_1 + q_2 = 1\\). Substitute \\(q_1\\): \\(1.5q_2 + q_2 = 1 \\implies 2.5q_2 = 1 \\implies q_2 = 0.4\\).\nThen \\(q_1 = 1.5 \\times 0.4 = 0.6\\).\nThus, \\(\\mathbf{q} = \\left[ \\begin{array}{c} .6 \\\\ .4 \\end{array} \\right]\\)."
  },
  {
    "objectID": "la-104.html#example-7-alumni-donation-steady-state-calculation-1",
    "href": "la-104.html#example-7-alumni-donation-steady-state-calculation-1",
    "title": "Linear Algebra",
    "section": "Example 7: Alumni Donation (Steady-State Calculation)",
    "text": "Example 7: Alumni Donation (Steady-State Calculation)\n\n\n\n\n\n\n\n\nThe solution shows that in the long run, 60% of alumni will contribute and 40% will not. This provides valuable insight for fundraising strategies.\nExplain the augmented system trick: since \\((I-P)\\) is singular, we add the condition that components sum to 1 to make the system uniquely solvable. This is a common numerical technique."
  },
  {
    "objectID": "la-104.html#example-8-car-rental-agency-steady-state",
    "href": "la-104.html#example-8-car-rental-agency-steady-state",
    "title": "Linear Algebra",
    "section": "Example 8: Car Rental Agency (Steady-State)",
    "text": "Example 8: Car Rental Agency (Steady-State)\n\\(P = \\left[ \\begin{array}{ccc} .8 & .3 & .2 \\\\ .1 & .2 & .6 \\\\ .1 & .5 & .2 \\end{array} \\right]\\) The system \\((I - P) \\mathbf{q} = \\mathbf{0}\\) is: \\[\n\\left[ \\begin{array}{ccc}\n.2 & -.3 & -.2 \\\\\n-.1 & .8 & -.6 \\\\\n-.1 & -.5 & .8\n\\end{array} \\right] \\left[ \\begin{array}{c}\nq_{1} \\\\\nq_{2} \\\\\nq_{3}\n\\end{array} \\right] = \\left[ \\begin{array}{c}\n0 \\\\\n0 \\\\\n0\n\\end{array} \\right]\n\\]\n\n\n\n\n\n\nInterpretation for ECE / Facility Design:\n\n\\(q_1 \\approx 0.5573\\): % of cars at Location 1.\n\\(q_2 \\approx 0.2295\\): % of cars at Location 2.\n\\(q_3 \\approx 0.2131\\): % of cars at Location 3.\n\nIf the agency has 1000 cars, roughly 557 will be at Location 1, 230 at Location 2, and 213 at Location 3 in the long run. This informs optimal parking space allocation.\n\n\nHere, linear algebra directly informs real-world engineering decisions. Facility planning, resource allocation, and inventory management can all benefit from this long-term probabilistic insight.\nThis example demonstrates how mathematical models simplify seemingly complex real-world dynamics into actionable numbers."
  },
  {
    "objectID": "la-104.html#example-9-traffic-officer-steady-state",
    "href": "la-104.html#example-9-traffic-officer-steady-state",
    "title": "Linear Algebra",
    "section": "Example 9: Traffic Officer (Steady-State)",
    "text": "Example 9: Traffic Officer (Steady-State)\nLet’s find the long-term probability distribution for the traffic officer’s location.\n\n\n\n\n\n\nThe resulting steady-state vector shows the proportion of time the officer spends at each intersection over the long term. Are these proportions equal? Why or why not?\n\n\nThe results show uneven distribution: some intersections (e.g., 4 and 5, 7 and 8) have higher probabilities. This is because these intersections are more “central” or have more connections in the network.\nDiscussion Point: If the objective was to spend equal time at each intersection, this “random movement with equal probabilities to neighbors” strategy is not optimal. This highlights how complex systems behave, and mathematical analysis can reveal non-intuitive results.\nThis has applications in load balancing in distributed systems, or resource allocation in networks. You might want to deliberately create an uneven distribution or aim for a perfectly even one, and Markov chains can model this."
  },
  {
    "objectID": "la-104.html#summary-and-key-takeaways",
    "href": "la-104.html#summary-and-key-takeaways",
    "title": "Linear Algebra",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\nLinear Algebra in Markov Chains:\n\nTransition Matrix (P): Describes state-to-state probabilities.\nState Vector (\\(\\mathbf{x}\\)): Represents probability distribution across states.\nTime Evolution: \\(\\mathbf{x}^{(n+1)} = P \\mathbf{x}^{(n)}\\) (Matrix-vector multiplication).\nSteady-State Vector (\\(\\mathbf{q}\\)): Long-term, stable probability distribution.\n\nFound by solving \\(P \\mathbf{q} = \\mathbf{q}\\) or \\((I-P) \\mathbf{q} = \\mathbf{0}\\).\nAn eigenvector corresponding to eigenvalue \\(\\lambda=1\\)."
  },
  {
    "objectID": "la-104.html#summary-and-key-takeaways-1",
    "href": "la-104.html#summary-and-key-takeaways-1",
    "title": "Linear Algebra",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\nECE Applications:\n\nSystem Reliability: Modeling component failure and repair states.\nNetwork Performance: Analyzing packet routing, queueing, node status.\nControl Systems: Describing controller states or system mode transitions.\nResource Allocation: Optimizing resource distribution based on usage patterns.\nSignal Processing: Analyzing state transitions in digital filters or communication channels.\n\n\n\nReinforce the interdisciplinary nature: probability and linear algebra combine to powerful modeling tools.\nEncourage students to think of other systems in ECE that could be modeled as Markov chains (e.g., a finite state machine, a sensor in different modes, a memory cell).\nEmphasize the practical utility of understanding long-term behavior (steady-state) in system design and optimization."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Linear Algebra",
    "section": "",
    "text": "This is Linear Algebra Lecture Notes"
  },
  {
    "objectID": "index.html#week-1",
    "href": "index.html#week-1",
    "title": "Linear Algebra",
    "section": "Week 1",
    "text": "Week 1\n\nRPS\nLinear Algebra 1.1: ntroduction to Systems of Linear Equations\nLinear Algebra 1.2: Gaussian Elimination\nLinear Algebra 1.3: Matrices and Matrix Operations\nLinear Algebra 1.4: Inverses; Algebraic Properties of Matrices"
  },
  {
    "objectID": "index.html#week-2",
    "href": "index.html#week-2",
    "title": "Linear Algebra",
    "section": "Week 2",
    "text": "Week 2\n\nLinear Algebra 1.5: Elementary Matrices and a Method for Finding \\(A^{-1}\\)\nLinear Algebra 1.6: More on Linear Systems and Invertible Matrices\nLinear Algebra 1.7: Diagonal, Triangular, and Symmetric Matrices\nLinear Algebra 1.8: Matrix Transformations\nLinear Algebra 1.9: Applications of Linear Systems\nLinear Algebra 10.4: Markov Chains"
  },
  {
    "objectID": "index.html#week-3",
    "href": "index.html#week-3",
    "title": "Linear Algebra",
    "section": "Week 3",
    "text": "Week 3\n\nLinear Algebra 2.1: Determinants by Cofactor Expansion\nLinear Algebra 2.2: Evaluating Determinants by Row Reduction\nLinear Algebra 2.3: Properties of Determinants; Cramer’s Rule\nOctave Tutorial\nPyton Tutorial\nMarkov Chain Tutorial"
  },
  {
    "objectID": "index.html#week-4",
    "href": "index.html#week-4",
    "title": "Linear Algebra",
    "section": "Week 4",
    "text": "Week 4\n\nUjian CPMK 1\nBullying"
  },
  {
    "objectID": "index.html#week-5",
    "href": "index.html#week-5",
    "title": "Linear Algebra",
    "section": "Week 5",
    "text": "Week 5\n\nLinear Algebra 3.1: Vectors in 2-Space, 3-Space, and \\(n\\) -Space\nLinear Algebra 3.2: Norm, Dot Product, and Distance in \\(R^n\\)"
  },
  {
    "objectID": "index.html#week-6",
    "href": "index.html#week-6",
    "title": "Linear Algebra",
    "section": "Week 6",
    "text": "Week 6\n\nLinear Algebra 3.3: Orthogonality\nLinear Algebra 3.4: The Geometry of Linear Systems\nLinear Algebra 3.5: Cross Product"
  },
  {
    "objectID": "la-11.html#introduction-to-systems-of-linear-equations",
    "href": "la-11.html#introduction-to-systems-of-linear-equations",
    "title": "Linear Algebra",
    "section": "Introduction to Systems of Linear Equations",
    "text": "Introduction to Systems of Linear Equations\nSystems of linear equations are fundamental to various engineering disciplines. In this section, we’ll introduce key terminology and explore methods for solving such systems.\n\nGood morning, everyone! Today, we embark on a crucial journey into the world of Linear Algebra, starting with the very foundation: Systems of Linear Equations. These systems are not just abstract mathematical constructs; they are the bedrock for understanding many real-world phenomena and engineering problems, from analyzing electrical circuits to optimizing control systems.\nOur main goals for this session are: 1. To define what constitutes a linear equation and a system of linear equations. 2. To understand the different types of solutions a linear system can have. 3. To introduce the concept of augmented matrices and elementary row operations as a systematic way to solve these systems. 4. To see how these concepts are visualized geometrically, especially in 2D and 3D.\nLet’s dive in!"
  },
  {
    "objectID": "la-11.html#what-is-a-linear-equation",
    "href": "la-11.html#what-is-a-linear-equation",
    "title": "Linear Algebra",
    "section": "What is a Linear Equation?",
    "text": "What is a Linear Equation?\nA linear equation is an algebraic equation of the form:\n\nIn two variables (\\(x, y\\)): \\[\nax + by = c \\quad (a, b \\text{ not both } 0)\n\\]\nIn three variables (\\(x, y, z\\)): \\[\nax + by + cz = d \\quad (a, b, c \\text{ not all } 0)\n\\]\nIn \\(n\\) variables (\\(x_1, x_2, \\ldots, x_n\\)): \\[\na_1x_1 + a_2x_2 + \\cdots + a_nx_n = b \\quad (a_i \\text{ not all } 0)\n\\]\n\nA special case where \\(b=0\\) is called a homogeneous linear equation: \\[\na_1x_1 + a_2x_2 + \\cdots + a_nx_n = 0\n\\]\n\nLet’s start with the basic building block: the linear equation. You’ve encountered these since high school, especially in two dimensions, where they represent straight lines. Think about the equation \\(y = mx + b\\); it can be rewritten as \\(mx - y = -b\\), which fits our definition.\nIn three dimensions, a linear equation represents a plane. For example, in circuit analysis, Kirchhoff’s voltage and current laws often lead to linear equations.\nThe key characteristic of a linear equation is that variables only appear to the first power, and there are no products or roots of variables. They also cannot be arguments of functions like sine, log, or exponential. This linearity is what makes these systems so powerful and solvable through systematic methods."
  },
  {
    "objectID": "la-11.html#linear-vs.-non-linear-equations",
    "href": "la-11.html#linear-vs.-non-linear-equations",
    "title": "Linear Algebra",
    "section": "Linear vs. Non-Linear Equations",
    "text": "Linear vs. Non-Linear Equations\n\n\nLinear Equations\nCharacterized by:\n\nVariables only to the first power.\nNo products or roots of variables.\nNo variables in trigonometric, logarithmic, or exponential functions.\n\nExamples: \\[\n\\begin{array}{l} x + 3y = 7 \\\\ \\frac{1}{2}x - y + 3z = -1 \\end{array}\n\\] \\[\n\\begin{array}{l} x_1 - 2x_2 - 3x_3 + x_4 = 0 \\\\ x_1 + x_2 + \\cdots + x_n = 1 \\end{array}\n\\]\n\nNon-Linear Equations\nViolate linearity conditions.\nExamples: \\[\n\\begin{array}{l} x + 3y^2 = 4 \\\\ \\sin x + y = 0 \\end{array}\n\\] \\[\n\\begin{array}{l} 3x + 2y - xy = 5 \\\\ \\sqrt{x_1} + 2x_2 + x_3 = 1 \\end{array}\n\\]\n\n\nIt’s crucial to distinguish between linear and non-linear equations, as the methods we’ll learn are specifically for linear systems. Look at the examples on the left: all variables are simply \\(x\\), \\(y\\), \\(z\\), or \\(x_i\\) to the power of 1.\nOn the right, we see squared terms like \\(y^2\\), products like \\(xy\\), variables inside functions like \\(\\sin x\\), or under radicals like \\(\\sqrt{x_1}\\). These violate the linearity conditions. While non-linear systems exist and are vital in many applications, their solution methods are much more complex and outside the scope of this course."
  },
  {
    "objectID": "la-11.html#systems-of-linear-equations",
    "href": "la-11.html#systems-of-linear-equations",
    "title": "Linear Algebra",
    "section": "Systems of Linear Equations",
    "text": "Systems of Linear Equations\nA system of linear equations (or linear system) is a finite set of linear equations. The variables are called unknowns.\nGeneral system of \\(m\\) equations in \\(n\\) unknowns: \\[\n\\begin{array}{c} a_{11}x_{1}+a_{12}x_{2}+\\cdots+a_{1n}x_{n}=b_{1} \\\\ a_{21}x_{1}+a_{22}x_{2}+\\cdots+a_{2n}x_{n}=b_{2} \\\\ \\vdots\\qquad\\vdots\\qquad\\vdots\\qquad\\vdots\\qquad\\vdots \\\\ a_{m1}x_{1}+a_{m2}x_{2}+\\cdots+a_{mn}x_{n}=b_{m} \\end{array}\n\\] Here, \\(a_{ij}\\) (coefficients) indicate location: \\(a_{12}\\) is in the first equation, multiplying \\(x_2\\). \\(b_k\\) are constants.\nA solution is a sequence of \\(n\\) numbers \\((s_1, s_2, \\ldots, s_n)\\) that makes each equation a true statement when \\(x_1=s_1, \\ldots, x_n=s_n\\). This is also called an ordered \\(n\\)-tuple.\n\nWhen we have multiple linear equations that must be satisfied simultaneously, we call that a system of linear equations. These are very common in engineering for modeling multiple interacting components or constraints.\nFor instance, in circuit analysis, if you apply Kirchhoff’s Voltage Law to several loops in a circuit, you’ll end up with a system of linear equations where the unknowns might be the currents flowing through different branches.\nThe double subscript \\(a_{ij}\\) might look intimidating at first but it’s very logical: the first index i tells you which equation the coefficient belongs to, and the second index j tells you which variable it’s multiplying. So \\(a_{23}\\) would be the coefficient of \\(x_3\\) in the second equation.\nA solution to such a system is simply a set of values for the variables that satisfy all equations simultaneously. We can write these solutions as an ordered n-tuple, which gives us a concise way to represent the point in n-dimensional space where all the equations intersect."
  },
  {
    "objectID": "la-11.html#geometric-interpretation-2d",
    "href": "la-11.html#geometric-interpretation-2d",
    "title": "Linear Algebra",
    "section": "Geometric Interpretation (2D)",
    "text": "Geometric Interpretation (2D)\nFor a system of two linear equations in two unknowns, graphs are lines in the \\(xy\\)-plane. Consider: \\[\n\\begin{array}{r}a_{1}x + b_{1}y = c_{1} \\\\ a_{2}x + b_{2}y = c_{2} \\end{array}\n\\]\nThere are three possibilities for the intersection of two lines, hence three possibilities for the solution set:\n\nNo Solution: Lines are parallel and distinct.\nExactly One Solution: Lines intersect at a single point.\nInfinitely Many Solutions: Lines coincide."
  },
  {
    "objectID": "la-11.html#geometric-interpretation-2d-1",
    "href": "la-11.html#geometric-interpretation-2d-1",
    "title": "Linear Algebra",
    "section": "Geometric Interpretation (2D)",
    "text": "Geometric Interpretation (2D)\n\n\n\n\nFigure 1.1.1\n\n\n\n\nLet’s first visualize this in two dimensions, which is intuitive for most of us. Each linear equation here represents a straight line. The solution to the system is the point or points where these lines intersect.\nAs you can see from the diagram, there are only three ways two lines can interact: 1. They are parallel and never meet. In this case, there is no solution that satisfies both equations simultaneously. 2. They cross at exactly one point. This point is the unique solution to the system. 3. They are the exact same line, meaning they overlap everywhere. In this case, every point on that line is a solution, leading to infinitely many solutions.\nThis visualization helps set the stage for a fundamental theorem in linear algebra: A linear system cannot have, for example, exactly two solutions. It’s either none, one, or infinitely many."
  },
  {
    "objectID": "la-11.html#geometric-interpretation-3d",
    "href": "la-11.html#geometric-interpretation-3d",
    "title": "Linear Algebra",
    "section": "Geometric Interpretation (3D)",
    "text": "Geometric Interpretation (3D)\nSimilarly, for a system of three linear equations in three unknowns, graphs are planes in the \\(xyz\\)-plane. Consider: \\[\n\\begin{array}{r}a_{1}x + b_{1}y + c_{1}z = d_{1} \\\\ a_{2}x + b_{2}y + c_{2}z = d_{2} \\\\ a_{3}x + b_{3}y + c_{3}z = d_{3} \\end{array}\n\\] Solutions correspond to points where all three planes intersect. Again, there are only three possibilities: No solutions, one solution, or infinitely many solutions."
  },
  {
    "objectID": "la-11.html#geometric-interpretation-3d-1",
    "href": "la-11.html#geometric-interpretation-3d-1",
    "title": "Linear Algebra",
    "section": "Geometric Interpretation (3D)",
    "text": "Geometric Interpretation (3D)\n\n\n\n\nFigure 1.1.2\n\n\n\n\nExtending this concept to three dimensions, each linear equation now represents a plane. The solution to a system of three linear equations in three unknowns is the point or set of points where all three planes intersect.\nWhile visualizing this is a bit harder than lines, the fundamental principle remains the same. You can have: * No common intersection (e.g., three parallel planes, or two parallel and one intersecting, or planes intersecting pairwise but not all at one common line/point). * A single point of intersection (e.g., like the corner of a room where three walls meet). * An infinite number of solutions (e.g., all three planes coincide, or intersect along a common line).\nThe core takeaway is that the number of solutions for any linear system is restricted to these three categories."
  },
  {
    "objectID": "la-11.html#the-fundamental-theorem-of-linear-systems",
    "href": "la-11.html#the-fundamental-theorem-of-linear-systems",
    "title": "Linear Algebra",
    "section": "The Fundamental Theorem of Linear Systems",
    "text": "The Fundamental Theorem of Linear Systems\nEvery system of linear equations has zero, one, or infinitely many solutions. There are no other possibilities.\n\nConsistent system: Has at least one solution (one or infinitely many).\nInconsistent system: Has no solutions.\n\nThis principle holds true regardless of the number of equations or variables.\n\nThis theorem is incredibly powerful and simplifies our understanding of linear systems. Unlike non-linear systems, which can exhibit complex behaviors like having exactly two solutions, linear systems are strict. This property is a direct consequence of their linear nature and the methods we use to solve them.\nFor engineers, understanding this classification is crucial. If you’re designing a control system and end up with a linear system, knowing that it will either have a unique solution, no solution (indicating a problem in your design or model), or infinite solutions (implying design flexibility or redundancy) helps in troubleshooting and interpretation."
  },
  {
    "objectID": "la-11.html#example-2-a-linear-system-with-one-solution",
    "href": "la-11.html#example-2-a-linear-system-with-one-solution",
    "title": "Linear Algebra",
    "section": "Example 2: A Linear System with One Solution",
    "text": "Example 2: A Linear System with One Solution\nSolve the linear system: \\[\n\\begin{array}{c}x - y = 1 \\\\ 2x + y = 6 \\end{array}\n\\]\nSolution:\n\nAdd \\(2 \\times\\) (first equation) to (second equation) to eliminate \\(x\\):\n\nEquation 1: \\(x - y = 1\\)\nEquation 2: \\(2x + y = 6\\)\n\nAdd Eq1 and Eq2 directly to eliminate \\(y\\):\n\\((x - y) + (2x + y) = 1 + 6\\)\n\\(3x = 7 \\implies x = \\frac{7}{3}\\)\nSubstitute \\(x\\) into Eq1:\n\\(\\frac{7}{3} - y = 1\\)\n\\(y = \\frac{7}{3} - 1 = \\frac{7}{3} - \\frac{3}{3} = \\frac{4}{3}\\)\n\nThus, the unique solution is \\((x, y) = \\left(\\frac{7}{3}, \\frac{4}{3}\\right)\\)."
  },
  {
    "objectID": "la-11.html#example-2-a-linear-system-with-one-solution-1",
    "href": "la-11.html#example-2-a-linear-system-with-one-solution-1",
    "title": "Linear Algebra",
    "section": "Example 2: A Linear System with One Solution",
    "text": "Example 2: A Linear System with One Solution\n\n\n\n\n\n\n\nLet’s walk through our first example of solving a system. The original source text had a small typo in the elimination example, so I’ve corrected it here to show the simplest way to solve this system.\nHere, we have a simple system of two equations. Notice that the ‘y’ terms have coefficients -1 and +1. This makes it very easy to eliminate ‘y’ by simply adding the two equations together.\nWhen we add Equation 1 to Equation 2, the ‘y’ terms cancel out, leaving us with \\(3x = 7\\), which quickly gives us \\(x = 7/3\\).\nOnce we have \\(x\\), we can substitute it back into either original equation to find \\(y\\). Using the first equation, \\(7/3 - y = 1\\), we get \\(y = 4/3\\).\nSo, the unique solution is \\((7/3, 4/3)\\). Geometrically, this is the single point where these two lines intersect, as shown in the plot."
  },
  {
    "objectID": "la-11.html#example-3-a-linear-system-with-no-solutions",
    "href": "la-11.html#example-3-a-linear-system-with-no-solutions",
    "title": "Linear Algebra",
    "section": "Example 3: A Linear System with No Solutions",
    "text": "Example 3: A Linear System with No Solutions\nSolve the linear system: \\[\n\\begin{array}{c}x + y = 4 \\\\ 3x + 3y = 6 \\end{array}\n\\]\nSolution:\n\nMultiply the first equation by -3 and add it to the second equation:\n\nEquation 1: \\(x + y = 4\\)\n\\((-3)(x + y) + (3x + 3y) = (-3)(4) + 6\\)\n\\(-3x - 3y + 3x + 3y = -12 + 6\\)\n\\(0 = -6\\)\nThe resulting equation \\(0 = -6\\) is a contradiction. Thus, the system has no solution. The lines are parallel and distinct."
  },
  {
    "objectID": "la-11.html#example-3-a-linear-system-with-no-solutions-1",
    "href": "la-11.html#example-3-a-linear-system-with-no-solutions-1",
    "title": "Linear Algebra",
    "section": "Example 3: A Linear System with No Solutions",
    "text": "Example 3: A Linear System with No Solutions\n\n\n\n\n\n\n\nNext, let’s explore a system with no solutions. We have \\(x + y = 4\\) and \\(3x + 3y = 6\\). If we try to eliminate one variable, say \\(x\\), we can multiply the first equation by -3 and add it to the second. This straightforward algebraic manipulation leads to the equation \\(0 = -6\\). This is a mathematical impossibility. A statement like “0 equals -6” is a contradiction.\nWhat does this mean for our system? It means there are no values of \\(x\\) and \\(y\\) that can simultaneously satisfy both equations. Geometrically, as the plot shows, these two lines are parallel and distinct. They have the same slope (which you can see by rewriting them as \\(y=-x+4\\) and \\(y=-x+2\\)), but different y-intercepts, meaning they will never intersect. This inconsistency is a clear indicator of “no solution.”"
  },
  {
    "objectID": "la-11.html#example-4-a-linear-system-with-infinitely-many-solutions",
    "href": "la-11.html#example-4-a-linear-system-with-infinitely-many-solutions",
    "title": "Linear Algebra",
    "section": "Example 4: A Linear System with Infinitely Many Solutions",
    "text": "Example 4: A Linear System with Infinitely Many Solutions\nSolve the linear system: \\[\n\\begin{array}{r}4x - 2y = 1 \\\\ 16x - 8y = 4 \\end{array}\n\\]\nSolution:\n\nMultiply the first equation by -4 and add it to the second equation:\n\nEquation 1: \\(4x - 2y = 1\\)\n\\((-4)(4x - 2y) + (16x - 8y) = (-4)(1) + 4\\)\n\\(-16x + 8y + 16x - 8y = -4 + 4\\) \\(0 = 0\\)\n\nThe resulting equation \\(0 = 0\\) is always true and imposes no additional restriction.\nThe solution set is given by the single equation \\(4x - 2y = 1\\).\nTo describe the infinite solutions, we use parametric equations. Let \\(y = t\\) (where \\(t\\) is any real number). Then \\(4x - 2t = 1 \\implies 4x = 1 + 2t \\implies x = \\frac{1}{4} + \\frac{1}{2}t\\).\nThe solution is \\((x, y) = \\left(\\frac{1}{4} + \\frac{1}{2}t, t\\right)\\)."
  },
  {
    "objectID": "la-11.html#example-4-a-linear-system-with-infinitely-many-solutions-1",
    "href": "la-11.html#example-4-a-linear-system-with-infinitely-many-solutions-1",
    "title": "Linear Algebra",
    "section": "Example 4: A Linear System with Infinitely Many Solutions",
    "text": "Example 4: A Linear System with Infinitely Many Solutions\n\n\n\n\n\n\n\nOur final common case is a system with infinitely many solutions. Consider \\(4x - 2y = 1\\) and \\(16x - 8y = 4\\). If we multiply the first equation by -4 and add it to the second equation, we get \\(0 = 0\\).\nUnlike the previous case, \\(0 = 0\\) is a true statement, but it doesn’t give us any information about \\(x\\) or \\(y\\). This indicates that the two original equations are essentially the same equation; one is a multiple of the other.\nGeometrically, as you can see, the lines are coincident—they overlap perfectly. Every point on that line is a solution. To represent these infinite solutions, we use a technique called parametrization. We introduce a dummy variable, often called a parameter (like \\(t\\) or \\(s\\)), and express our variables in terms of this parameter. Here, we let \\(y=t\\). Then, from \\(4x-2y=1\\), we solve for \\(x\\) in terms of \\(t\\), giving \\(x = 1/4 + 1/2t\\). This pair of equations allows us to generate specific solutions by picking any value for \\(t\\). For example, if \\(t=0\\), we get \\((1/4, 0)\\). If \\(t=1\\), we get \\((3/4, 1)\\). Both of these points lie on the line and satisfy both original equations."
  },
  {
    "objectID": "la-11.html#example-5a-a-linear-system-with-infinitely-many-solutions-3d",
    "href": "la-11.html#example-5a-a-linear-system-with-infinitely-many-solutions-3d",
    "title": "Linear Algebra",
    "section": "Example 5a: A Linear System with Infinitely Many Solutions (3D)",
    "text": "Example 5a: A Linear System with Infinitely Many Solutions (3D)\nSolve the linear system: \\[\n\\begin{array}{r}x - y + 2z = 5 \\\\ 2x - 2y + 4z = 10 \\\\ 3x - 3y + 6z = 15 \\end{array}\n\\]\nSolution: Observe that the second equation is \\(2 \\times\\) (first equation), and the third equation is \\(3 \\times\\) (first equation). This means all three equations represent the same plane. Thus, finding solutions to this system is equivalent to finding solutions to the single equation: \\[\nx - y + 2z = 5\n\\] To describe the infinite solutions, we use two parameters since we have one equation and three unknowns. Let \\(y = r\\) and \\(z = s\\) (where \\(r, s\\) are any real numbers). Substitute these into the equation: \\(x - r + 2s = 5 \\implies x = 5 + r - 2s\\).\nThe solution is given by the parametric equations: \\[\nx = 5 + r - 2s, \\quad y = r, \\quad z = s\n\\] For example, taking \\(r=1, s=0\\) yields the solution \\((6, 1, 0)\\).\n\nNow, let’s extend this idea of infinitely many solutions to three dimensions. Here, we have a system of three equations in three unknowns. If we look closely, the second equation (\\(2x - 2y + 4z = 10\\)) is simply two times the first equation (\\(x - y + 2z = 5\\)). Similarly, the third equation (\\(3x - 3y + 6z = 15\\)) is three times the first equation.\nThis means all three equations describe the exact same plane in 3D space. Any point on this plane is a solution to the system. To represent these infinite solutions, we need not one, but two parameters because we have a 3D space and our solution is a 2D plane within it. We can choose any two variables to be our parameters. Here, we’ve chosen \\(y=r\\) and \\(z=s\\). Then, we solve for \\(x\\) in terms of \\(r\\) and \\(s\\), getting \\(x = 5 + r - 2s\\). This gives us a set of parametric equations that describes every point on the plane. You can pick any real numbers for \\(r\\) and \\(s\\), and you’ll get a valid solution \\((x,y,z)\\). For example, if \\(r=1\\) and \\(s=0\\), we get the solution \\((6,1,0)\\)."
  },
  {
    "objectID": "la-11.html#example-5b-a-linear-system-with-infinitely-many-solutions-3d",
    "href": "la-11.html#example-5b-a-linear-system-with-infinitely-many-solutions-3d",
    "title": "Linear Algebra",
    "section": "Example 5b: A Linear System with Infinitely Many Solutions (3D)",
    "text": "Example 5b: A Linear System with Infinitely Many Solutions (3D)\nConsider the system of two linear equations\n\\(x+y+z=1\\)\n\\(x−z=0\\)\nEach equation individually defines a plane in space. The solutions of the system of both equations are the points that lie on both planes. We can see in the picture below that the planes intersect in a line. In particular, this system has infinitely many solutions."
  },
  {
    "objectID": "la-11.html#augmented-matrices",
    "href": "la-11.html#augmented-matrices",
    "title": "Linear Algebra",
    "section": "Augmented Matrices",
    "text": "Augmented Matrices\nSolving linear systems by algebraic substitution can become cumbersome. We can simplify notation using augmented matrices.\nA system of linear equations: \\[\n\\begin{array}{c} a_{11}x_{1}+a_{12}x_{2}+\\cdots+a_{1n}x_{n}=b_{1} \\\\ a_{21}x_{1}+a_{22}x_{2}+\\cdots+a_{2n}x_{n}=b_{2} \\\\ \\vdots\\qquad\\vdots\\qquad\\vdots\\qquad\\vdots\\qquad\\vdots \\\\ a_{m1}x_{1}+a_{m2}x_{2}+\\cdots+a_{mn}x_{n}=b_{m} \\end{array}\n\\] can be abbreviated by its augmented matrix: \\[\n\\left[ \\begin{array}{c c c c | c}{a_{11}} & {a_{12}} & \\dots & {a_{1n}} & {b_{1}}\\\\ {a_{21}} & {a_{22}} & \\dots & {a_{2n}} & {b_{2}}\\\\ \\vdots & \\vdots & & \\vdots & \\vdots \\\\ {a_{m1}} & {a_{m2}} & \\dots & {a_{mn}} & {b_{m}} \\end{array} \\right]\n\\] The vertical line conceptually separates the coefficient matrix from the constant terms."
  },
  {
    "objectID": "la-11.html#augmented-matrices-1",
    "href": "la-11.html#augmented-matrices-1",
    "title": "Linear Algebra",
    "section": "Augmented Matrices",
    "text": "Augmented Matrices\nExample: For \\(x_{1} + x_{2} + 2x_{3} = 9\\), \\(2x_{1} + 4x_{2} - 3x_{3} = 1\\), \\(3x_{1} + 6x_{2} - 5x_{3} = 0\\), the augmented matrix is: \\[\n\\left[ \\begin{array}{r r r | r}{1} & 1 & 2 & 9\\\\ 2 & 4 & {-3} & 1\\\\ 3 & 6 & {-5} & 0 \\end{array} \\right]\n\\]\n\nAs systems grow larger, keeping track of variables and ‘plus’ signs becomes tedious. This is where the brilliant idea of augmented matrices comes in. A matrix is simply a rectangular array of numbers.\nWe can represent the entire system just by its coefficients and the constant terms on the right-hand side. The augmented matrix captures all the essential information needed to solve the system without the clutter of variables and operators. The vertical line helps visually distinguish the coefficients from the constants, but mathematically it’s just a single matrix. This compact notation is incredibly useful for organization and computation."
  },
  {
    "objectID": "la-11.html#elementary-row-operations",
    "href": "la-11.html#elementary-row-operations",
    "title": "Linear Algebra",
    "section": "Elementary Row Operations",
    "text": "Elementary Row Operations\nThe algebraic operations on a system that do not alter the solution set correspond to Elementary Row Operations on the augmented matrix:\n\nMultiply a row through by a nonzero constant.\n\n(Corresponds to multiplying an equation by a nonzero constant.)\n\nInterchange two rows.\n\n(Corresponds to interchanging two equations.)\n\nAdd a constant times one row to another.\n\n(Corresponds to adding a constant times one equation to another.)\n\n\nThese operations allow us to systematically simplify the matrix (and thus the system) to a form from which the solution can be easily found.\n\nWhy are these matrix operations useful? Because they directly mirror the standard algebraic operations we perform on equations, and crucially, these operations do not change the solution set of the system. If \\((x,y,z)\\) is a solution to the original system, it will still be a solution after any of these operations are applied.\n\nMultiplying an entire row by a non-zero constant is like scaling an equation. If \\(2x+4y=6\\) (scaled by 2) is true, then \\(x+2y=3\\) (original) is also true, and vice-versa.\nInterchanging two rows simply means swapping the order of two equations, which doesn’t affect the overall solution.\nAdding a multiple of one row to another is exactly what we did in the previous examples to eliminate variables. This is the most powerful operation for simplification.\n\nBy applying these elementary row operations, we can transform a complex system into a much simpler, equivalent system that’s easy to solve, often in a triangle-like form. This systematic process is what we’ll explore more deeply in the next section."
  },
  {
    "objectID": "la-11.html#example-6-using-elementary-row-operations-step-1",
    "href": "la-11.html#example-6-using-elementary-row-operations-step-1",
    "title": "Linear Algebra",
    "section": "Example 6: Using Elementary Row Operations (Step 1)",
    "text": "Example 6: Using Elementary Row Operations (Step 1)\nLet’s solve the system alongside its augmented matrix: \\[\n\\begin{array}{r}x + y + 2z = 9\\\\ 2x + 4y - 3z = 1\\\\ 3x + 6y - 5z = 0 \\end{array} \\qquad \\left[ \\begin{array}{r r r | r}{1} & 1 & 2 & 9\\\\ {2} & 4 & {-3} & 1\\\\ {3} & 6 & {-5} & 0 \\end{array} \\right]\n\\]\nOperation: Add \\(- 2\\) times the first equation to the second.\nMatrix Operation: Add \\(- 2\\) times the first row to the second row (Notation: \\(R_2 \\leftarrow R_2 - 2R_1\\)).\n\\[\n\\begin{array}{r}x + y + 2z = 9\\\\ \\quad (2-2)x + (4-2)y + (-3-4)z = 1-18 \\\\ 3x + 6y - 5z = 0 \\end{array} \\rightarrow \\begin{array}{r}x + y + 2z = 9\\\\ 2y - 7z = -17\\\\ 3x + 6y - 5z = 0 \\end{array}\n\\] \\[\n\\left[ \\begin{array}{r r r | r}{1} & 1 & 2 & 9\\\\ 0 & 2 & {-7} & {-17}\\\\ 3 & 6 & {-5} & 0 \\end{array} \\right]\n\\]\n\nNow, let’s see how elementary row operations work in practice, side-by-side with the algebraic operations. Our goal is to transform the system into a simpler form where x, y, and z values are easily found, usually in a triangular or diagonal matrix form.\nOur first step is to eliminate \\(x\\) from the second equation. We do this by multiplying the first equation by -2 and adding it to the second equation. This ensures that the coefficient of \\(x\\) in the second equation becomes zero.\nCorrespondingly, in the augmented matrix, we perform the same operation on the rows: \\(-2\\) times Row 1 added to Row 2. Notice how the first column in the second row becomes zero, effectively eliminating \\(x\\) from that equation."
  },
  {
    "objectID": "la-11.html#example-6-using-elementary-row-operations-step-2",
    "href": "la-11.html#example-6-using-elementary-row-operations-step-2",
    "title": "Linear Algebra",
    "section": "Example 6: Using Elementary Row Operations (Step 2)",
    "text": "Example 6: Using Elementary Row Operations (Step 2)\nOperation: Add \\(- 3\\) times the first equation to the third.\nMatrix Operation: Add \\(- 3\\) times the first row to the third row (Notation: \\(R_3 \\leftarrow R_3 - 3R_1\\)).\nCurrent system/matrix: \\[\n\\begin{array}{r}x + y + 2z = 9\\\\ 2y - 7z = -17\\\\ 3x + 6y - 5z = 0 \\end{array} \\qquad \\left[ \\begin{array}{r r r | r}{1} & 1 & 2 & 9\\\\ 0 & 2 & {-7} & {-17}\\\\ 3 & 6 & {-5} & 0 \\end{array} \\right]\n\\]\nResult: \\[\n\\begin{array}{r}x + y + 2z = 9\\\\ 2y - 7z = -17\\\\ (3-3)x + (6-3)y + (-5-6)z = 0-27 \\end{array} \\rightarrow \\begin{array}{r}x + y + 2z = 9\\\\ 2y - 7z = -17\\\\ 3y - 11z = -27 \\end{array}\n\\] \\[\n\\left[{\\begin{array}{r r r | r}{1}&{1}&{2}&{9}\\\\ {0}&{2}&{-7}&{-17}\\\\ {0}&{3}&{-11}&{-27}\\end{array}}\\right]\n\\]\n\nContinuing our elimination process, our next target is to eliminate \\(x\\) from the third equation. We apply a similar logic: multiply the first equation by -3 and add it to the third equation. This will make the \\(x\\) coefficient in the third equation zero.\nIn the matrix, this means adding \\(-3\\) times Row 1 to Row 3. This operation creates a zero in the first column of the third row as well. We are slowly getting the matrix into an upper triangular form, where coefficients below the main diagonal are zero. This structure makes it very easy to solve later using back-substitution."
  },
  {
    "objectID": "la-11.html#example-6-using-elementary-row-operations-step-3",
    "href": "la-11.html#example-6-using-elementary-row-operations-step-3",
    "title": "Linear Algebra",
    "section": "Example 6: Using Elementary Row Operations (Step 3)",
    "text": "Example 6: Using Elementary Row Operations (Step 3)\nOperation: Multiply the second equation by \\(\\frac{1}{2}\\).\nMatrix Operation: Multiply the second row by \\(\\frac{1}{2}\\) (Notation: \\(R_2 \\leftarrow \\frac{1}{2}R_2\\)).\nCurrent system/matrix: \\[\n\\begin{array}{r}x + y + 2z = 9\\\\ 2y - 7z = -17\\\\ 3y - 11z = -27 \\end{array} \\qquad \\left[{\\begin{array}{r r r | r}{1}&{1}&{2}&{9}\\\\ {0}&{2}&{-7}&{-17}\\\\ {0}&{3}&{-11}&{-27}\\end{array}}\\right]\n\\]\nResult: \\[\n\\begin{array}{r}x + y + 2z = 9\\\\ \\frac{1}{2}(2y - 7z) = \\frac{1}{2}(-17)\\\\ 3y - 11z = -27 \\end{array} \\rightarrow \\begin{array}{r}x + y + 2z = 9\\\\ y - \\frac{7}{2} z = -\\frac{17}{2}\\\\ 3y - 11z = -27 \\end{array}\n\\] \\[\n\\left[{\\begin{array}{r r r | r}{1}&{1}&{2}&{9}\\\\ {0}&{1}&{-{\\frac{7}{2}}}&{-{\\frac{17}{2}}}\\\\ {0}&{3}&{-11}&{-27}\\end{array}}\\right]\n\\]\n\nWith \\(x\\) coefficients now zero in the second and third rows (equations), our next strategic move is to simplify the second equation. We want the coefficient of \\(y\\) in the second equation to be 1. This is often called creating a “leading 1”.\nWe achieve this by multiplying the entire second equation by \\(1/2\\). In the matrix, this means multiplying Row 2 by \\(1/2\\). This makes the leading entry in the second row equal to 1, which is a standard step in Gaussian elimination."
  },
  {
    "objectID": "la-11.html#example-6-using-elementary-row-operations-step-4",
    "href": "la-11.html#example-6-using-elementary-row-operations-step-4",
    "title": "Linear Algebra",
    "section": "Example 6: Using Elementary Row Operations (Step 4)",
    "text": "Example 6: Using Elementary Row Operations (Step 4)\nOperation: Add \\(- 3\\) times the second equation to the third.\nMatrix Operation: Add \\(- 3\\) times the second row to the third row (Notation: \\(R_3 \\leftarrow R_3 - 3R_2\\)).\nCurrent system/matrix: \\[\n\\begin{array}{r}x + y + 2z = 9\\\\ y - \\frac{7}{2} z = -\\frac{17}{2}\\\\ 3y - 11z = -27 \\end{array} \\qquad \\left[{\\begin{array}{r r r | r}{1}&{1}&{2}&{9}\\\\ {0}&{1}&{-{\\frac{7}{2}}}&{-{\\frac{17}{2}}}\\\\ {0}&{3}&{-11}&{-27}\\end{array}}\\right]\n\\]\nResult: \\[\n\\begin{array}{r}\nx+y+2z=9\\\\\ny-\\frac{7}{2}z=-\\frac{17}{2}\\\\\n(3-3)y + (-11 - 3(-\\frac{7}{2}))z = -27 - 3(-\\frac{17}{2})\n\\end{array}\n\\rightarrow\n\\begin{array}{r}\nx+y+2z={9}\\\\\n{y-\\frac{7}{2}z=-\\frac{17}{2}}\\\\\n-\\frac{1}{2}z=-\\frac{3}{2}\n\\end{array}\n\\] \\[\n\\left[{\\begin{array}{r r r | r}{1}&{1}&{2}&{9}\\\\ {0}&{1}&{-{\\frac{7}{2}}}&{-{\\frac{17}{2}}}\\\\ {0}&{0}&{-{\\frac{1}{2}}}&{-{\\frac{3}{2}}}\\end{array}}\\right]\n\\]\n\nNow that the leading ‘1’ is in place for \\(y\\) in the second equation, we use it to eliminate \\(y\\) from the third equation. We perform the operation of adding \\(-3\\) times the second equation to the third equation. This will make the \\(y\\) coefficient in the third equation zero.\nOn the matrix side, this is applying \\(-3\\) times Row 2 to Row 3. This completes the “forward elimination” phase of Gaussian elimination, yielding an upper triangular matrix. From this form, we can easily solve for \\(z\\), then back-substitute to find \\(y\\), then \\(x\\). This process is often called back-substitution."
  },
  {
    "objectID": "la-11.html#example-6-using-elementary-row-operations-step-5",
    "href": "la-11.html#example-6-using-elementary-row-operations-step-5",
    "title": "Linear Algebra",
    "section": "Example 6: Using Elementary Row Operations (Step 5)",
    "text": "Example 6: Using Elementary Row Operations (Step 5)\nOperation: Multiply the third equation by \\(- 2\\).\nMatrix Operation: Multiply the third row by \\(- 2\\) (Notation: \\(R_3 \\leftarrow -2R_3\\)).\nCurrent system/matrix: \\[\n\\begin{array}{r}x+y+2z=\\begin{array}{r}{9}\\\\ {y-\\frac{7}{2}z=-\\frac{17}{2}}\\end{array}\\\\ {-\\frac{1}{2}z=-\\frac{3}{2}}\\end{array} \\qquad \\left[{\\begin{array}{r r r | r}{1}&{1}&{2}&{9}\\\\ {0}&{1}&{-{\\frac{7}{2}}}&{-{\\frac{17}{2}}}\\\\ {0}&{0}&{-{\\frac{1}{2}}}&{-{\\frac{3}{2}}}\\end{array}}\\right]\n\\]\nResult: \\[\n\\begin{array}{r}\nx+y+2z=9\\\\\n{y-\\frac{7}{2}z=-\\frac{17}{2}}\\\\\n{(-2)(-\\frac{1}{2}z)=(-2)}{(-\\frac{3}{2})}\n\\end{array}\n\\rightarrow\n\\begin{array}{r}\nx+y+2z=9\\\\\n{y-\\frac{7}{2}z=-\\frac{17}{2}}\\\\\n{z=3}\n\\end{array}\n\\] \\[\n\\left[{\\begin{array}{r r r | r}{1}&{1}&{2}&{9}\\\\ {0}&{1}&{-{\\frac{7}{2}}}&{-{\\frac{17}{2}}}\\\\ {0}&{0}&{1}&{3}\\end{array}}\\right]\n\\]\n\nNow we have our system in an “echelon form.” We can see that \\(-1/2 z = -3/2\\). To get a clean value for \\(z\\), we multiply the third equation by -2. This results in \\(z=3\\). In the matrix, multiplying Row 3 by -2 gives us a leading ‘1’ in the third row, which is desirable standard form for solving this system. Now that we have \\(z=3\\), we can use back-substitution."
  },
  {
    "objectID": "la-11.html#example-6-using-elementary-row-operations-step-6",
    "href": "la-11.html#example-6-using-elementary-row-operations-step-6",
    "title": "Linear Algebra",
    "section": "Example 6: Using Elementary Row Operations (Step 6)",
    "text": "Example 6: Using Elementary Row Operations (Step 6)\nOperation (Part 1): Add \\(- 1\\) times the second equation to the first.\nMatrix Operation (Part 1): (\\(R_1 \\leftarrow R_1 - R_2\\)) to eliminate \\(y\\) from the first equation.\nCurrent system/matrix:\n\\[\n\\begin{array}{r}x + y + 2z = 9\\\\ y - \\frac{7}{2} z = -\\frac{17}{2}\\\\ z = 3 \\end{array} \\qquad \\left[{\\begin{array}{r r r | r}{1}&{1}&{2}&{9}\\\\ {0}&{1}&{-{\\frac{7}{2}}}&{-{\\frac{17}{2}}}\\\\ {0}&{0}&{1}&{3}\\end{array}}\\right]\n\\]\nResulting simplified terms in \\(R_1\\): (This intermediate step is a conceptual one leading to x = ... + z related terms directly)\n\\[\n\\begin{array}{r}\n{x + (1-1)y + (2-(-\\frac{7}{2}))z}=9-(-\\frac{17}{2})\\\\\n{y-\\frac{7}{2}z}=-\\frac{17}{2}\\\\\n{z}=3\n\\end{array}\n\\rightarrow\n\\begin{array}{r}\n{x+\\frac{11}{2}z}=\\frac{35}{2}\\\\\n{y-\\frac{7}{2}z}=-\\frac{17}{2}\\\\\n{z}={3}\n\\end{array}\n\\]\n\\[\n\\left[{\\begin{array}{r r r | r}{1}&{0}&{{\\frac{11}{2}}}&{{\\frac{35}{2}}}\\\\ {0}&{1}&{-{\\frac{7}{2}}}&{-{\\frac{17}{2}}}\\\\ {0}&{0}&{1}&{3}\\end{array}}\\right]\n\\]\n\nThis is the start of the “backward elimination” or reduction phase, aiming to put the matrix into “reduced row echelon form” (identity matrix on the left). First, we want to eliminate the \\(y\\) term from the first equation. We do this by adding \\(-1\\) times the second equation to the first equation. In matrix form (\\(R_1 \\leftarrow R_1 - R_2\\)), this operation clears the \\(y\\) coefficient in the first row. We are now closer to having a diagonal matrix on the left side, which directly gives us the values of \\(x, y, z\\)."
  },
  {
    "objectID": "la-11.html#example-6-using-elementary-row-operations-step-7",
    "href": "la-11.html#example-6-using-elementary-row-operations-step-7",
    "title": "Linear Algebra",
    "section": "Example 6: Using Elementary Row Operations (Step 7)",
    "text": "Example 6: Using Elementary Row Operations (Step 7)\nOperation (Part 2): Substitute \\(z=3\\) into the first two equations (or eliminate \\(z\\) directly).\nAdd \\(-\\frac{11}{2}\\) times the third equation to the first. (\\(R_1 \\leftarrow R_1 - \\frac{11}{2}R_3\\))\nAdd \\(\\frac{7}{2}\\) times the third equation to the second. (\\(R_2 \\leftarrow R_2 + \\frac{7}{2}R_3\\))\nCurrent system/matrix: \\[\n\\begin{array}{r}\n{x+\\frac{11}{2}z=\\frac{35}{2}}\\\\\n{y-\\frac{7}{2}z=-\\frac{17}{2}}\\\\\n{z=3}\n\\end{array}\n\\qquad\n\\left[{\\begin{array}{r r r | r}{1}&{0}&{{\\frac{11}{2}}}&{{\\frac{35}{2}}}\\\\ {0}&{1}&{-{\\frac{7}{2}}}&{-{\\frac{17}{2}}}\\\\ {0}&{0}&{1}&{3}\\end{array}}\\right]\n\\]\nFinal Result: \\[\n\\begin{array}{r}\n{x=1}\\\\\n{y=2}\\\\\n{z=3}\n\\end{array}\n\\]\n\\[\n\\left[{\\begin{array}{r r r | r}{1}&{0}&{0}&{1}\\\\ {0}&{1}&{0}&{2}\\\\ {0}&{0}&{1}&{3}\\end{array}}\\right]\n\\]\nThe solution is \\((x, y, z) = (1, 2, 3)\\).\n\nThis is the final step in the systematic solution process, called Gauss-Jordan elimination. Already we know \\(z=3\\). We now use this value to simplify the first two equations further.\nWe want to eliminate the \\(z\\) terms from the first and second equations directly, making their coefficients zero. For the first equation, we add \\(-\\frac{11}{2}\\) times the third equation to it. For the second equation, we add \\(\\frac{7}{2}\\) times the third equation to it.\nThe amazing result is that we arrive at a matrix where the left side is the identity matrix. This directly tells us the solution: \\(x=1, y=2, z=3\\). This methodical approach, using elementary row operations on the augmented matrix, is extremely efficient for solving large systems of linear equations and is the basis for many computational algorithms in engineering, such as those used in finite element analysis or optimization."
  },
  {
    "objectID": "la-11.html#interactive-reduced-row-echelon-form-calculator",
    "href": "la-11.html#interactive-reduced-row-echelon-form-calculator",
    "title": "Linear Algebra",
    "section": "Interactive: Reduced Row Echelon Form Calculator",
    "text": "Interactive: Reduced Row Echelon Form Calculator"
  },
  {
    "objectID": "la-11.html#conclusion",
    "href": "la-11.html#conclusion",
    "title": "Linear Algebra",
    "section": "Conclusion",
    "text": "Conclusion\n\nLinear equations are fundamental building blocks.\nSystems of linear equations can have zero, one, or infinitely many solutions.\nAugmented matrices provide a compact way to represent linear systems.\nElementary row operations are systematic tools to solve linear systems efficiently, preserving the solution set.\n\nThese concepts are crucial for solving problems in control systems, signal processing, circuit analysis, optimization, and many other areas in Electrical and Computer Engineering.\n\nTo summarize today’s discussion: We started by defining linear equations and understanding their distinct properties compared to non-linear ones. We then introduced systems of linear equations and classified their solutions into three fundamental categories: no solution, exactly one solution, or infinitely many solutions. Finally, we learned about augmented matrices as a powerful notation and elementary row operations as systematic tools to transform linear systems into simpler forms, making them solvable.\nThese foundational concepts are not just theoretical; they are the bedrock of many advanced topics you’ll encounter in your ECE curriculum. From analyzing complex electrical circuits using Kirchhoff’s laws, to designing controllers for robotic systems, to optimizing resource allocation in networks, linear algebra provides the essential mathematical framework.\nIn the next section, we’ll formalize the process of using elementary row operations to solve systems, moving towards a robust algorithm called Gaussian elimination and Gauss-Jordan elimination. Thank you!"
  },
  {
    "objectID": "la-13.html#matrices-and-matrix-operations",
    "href": "la-13.html#matrices-and-matrix-operations",
    "title": "Linear Algebra",
    "section": "Matrices and Matrix Operations",
    "text": "Matrices and Matrix Operations\nBeyond augmented matrices for linear systems, rectangular arrays of numbers are fundamental entities in their own right. In this section, we define matrices and their basic arithmetic operations.\nWe will cover:\n\nMatrix notation and terminology.\nBasic matrix operations: equality, addition, subtraction, scalar multiplication.\nThe crucial operation of matrix multiplication.\nSpecial concepts: partitioned matrices, linear combinations, transpose, and trace.\n\n\nGood morning, everyone! Today, we transition from using matrices primarily as tools for solving linear equations to viewing them as mathematical objects with their own rules of arithmetic. Matrices are incredibly versatile and appear in almost every branch of engineering, from representing signals and images to modeling control systems and electrical networks.\nUnderstanding matrix operations is not just for theory; it’s essential for practical applications. When you’re trying to combine sensor data, apply transformations in computer graphics, or analyze complex circuit interactions, you’ll be performing matrix operations. We’ll start with basic definitions and then dive into the operations, especially matrix multiplication, which powers many transformations and computations in ECE."
  },
  {
    "objectID": "la-13.html#matrix-notation-and-terminology",
    "href": "la-13.html#matrix-notation-and-terminology",
    "title": "Linear Algebra",
    "section": "Matrix Notation and Terminology",
    "text": "Matrix Notation and Terminology\nA matrix is a rectangular array of numbers. The numbers are called entries.\nExample: Student study hours (Mon-Sun over 3 subjects) \\[\n\\left[{\\begin{array}{c c c c c c c}{2}&{3}&{2}&{4}&{1}&{4}&{2}\\\\ {0}&{3}&{1}&{4}&{3}&{2}&{2}\\\\ {4}&{1}&{3}&{1}&{0}&{0}&{2}\\end{array}}\\right]\n\\]\n\nSize: Described by rows x columns (e.g., \\(3 \\times 7\\)).\nRow Vector (Row Matrix): One row (e.g., \\([2 \\ 1 \\ 0 \\ -3]\\)).\nColumn Vector (Column Matrix): One column (e.g., \\(\\left[\\begin{smallmatrix} 1 \\\\ 3 \\\\ 3 \\end{smallmatrix}\\right]\\)).\nSquare Matrix of order \\(n\\): \\(n\\) rows and \\(n\\) columns.\nEntry Notation: \\(a_{ij}\\) (or \\((A)_{ij}\\)) denotes the entry in row \\(i\\) and column \\(j\\).\nMain Diagonal: For a square matrix, entries \\(a_{11}, a_{22}, \\ldots, a_{nn}\\).\n\n\\[\nA = \\left[ \\begin{array}{cccc}a_{11} & a_{12} & \\dots & a_{1n}\\\\ a_{21} & a_{22} & \\dots & a_{2n}\\\\ \\vdots & \\vdots & & \\vdots \\\\ a_{m1} & a_{m2} & \\dots & a_{mn} \\end{array} \\right] \\quad \\text{general } m \\times n \\text{ matrix}\n\\]\n\nLet’s start with the basics of how we refer to matrices. A matrix is simply an organized collection of numbers, arranged in rows and columns. This structured format makes them ideal for representing various kinds of data in engineering. For example, a matrix could represent: * Pixel values in an image (rows \\(\\times\\) columns of pixel intensity). * Sensor readings over time (rows for sensors, columns for time points). * System states in a dynamic model.\nWe describe its size as “rows by columns.” So, a \\(3 \\times 7\\) matrix has three rows and seven columns. Special cases include row and column vectors, which are just matrices with a single row or column, respectively. Entries are identified by their row and column index, \\(a_{ij}\\), where \\(i\\) is the row and \\(j\\) is the column. This notation is super important for pinpointing individual elements. For a square matrix (same number of rows and columns), the elements running from top-left to bottom-right form the “main diagonal.” This diagonal has special significance in many matrix operations."
  },
  {
    "objectID": "la-13.html#matrix-equality-addition-and-subtraction",
    "href": "la-13.html#matrix-equality-addition-and-subtraction",
    "title": "Linear Algebra",
    "section": "Matrix Equality, Addition, and Subtraction",
    "text": "Matrix Equality, Addition, and Subtraction\nMatrix Equality\nTwo matrices \\(A\\) and \\(B\\) are equal if:\n\nThey have the same size.\nTheir corresponding entries are equal (\\(a_{ij} = b_{ij}\\) for all \\(i,j\\)).\n\nExample: \\(A = \\left[\\begin{smallmatrix} 2 & 1 \\\\ 3 & x \\end{smallmatrix}\\right]\\), \\(B = \\left[\\begin{smallmatrix} 2 & 1 \\\\ 3 & 5 \\end{smallmatrix}\\right]\\). \\(A=B\\) iff \\(x=5\\).\nMatrix Addition and Subtraction\nIf \\(A\\) and \\(B\\) are matrices of the same size:\n\nSum \\(A+B\\): Obtained by adding corresponding entries. \\((A+B)_{ij} = a_{ij} + b_{ij}\\).\nDifference \\(A-B\\): Obtained by subtracting corresponding entries. \\((A-B)_{ij} = a_{ij} - b_{ij}\\). Matrices of different sizes cannot be added or subtracted.\n\n\nMuch like scalars, matrices can be equal, added, and subtracted, but with specific rules. Equality is straightforward: they must be identical in terms of shape (same number of rows and columns) and content (every single entry must match). The example clearly shows this; if even one entry doesn’t match, the matrices are not equal.\nAddition and Subtraction are elemental. They are defined only when the matrices have the exact same dimensions. You simply add or subtract the entries that are in the same position in both matrices. This is analogous to combining two sets of sensor data where each sensor corresponds to a specific location or type. For instance, if you have two snapshots of temperature readings from a grid of sensors at two different times, adding them might represent the total heat absorbed, or subtracting them would show the change in temperature at each point."
  },
  {
    "objectID": "la-13.html#interactive-example-matrix-operations",
    "href": "la-13.html#interactive-example-matrix-operations",
    "title": "Linear Algebra",
    "section": "Interactive Example: Matrix Operations",
    "text": "Interactive Example: Matrix Operations\nLet’s demonstrate matrix addition and subtraction with Python:\n\n\n\n\n\n\n\nLet’s use Python’s NumPy library to quickly perform these operations. NumPy is the standard for numerical computing in Python and is highly optimized for matrix operations.\nAs you can see from the output, both addition and subtraction are done “element-wise”. This means the corresponding entries are simply added or subtracted. For example, the top-left entry of A+B is 2 + (-4) = -2.\nThe last part of the code also demonstrates scalar multiplication, where the scalar 2 is multiplied by every single entry in matrix A. These basic operations are the building blocks for more complex matrix algebra and are frequently used in digital signal processing, for instance, to scale or offset signals."
  },
  {
    "objectID": "la-13.html#scalar-multiplication",
    "href": "la-13.html#scalar-multiplication",
    "title": "Linear Algebra",
    "section": "Scalar Multiplication",
    "text": "Scalar Multiplication\nIf \\(A\\) is any matrix and \\(c\\) is any scalar (real number), then the product \\(cA\\) is the matrix obtained by multiplying each entry of \\(A\\) by \\(c\\). \\(cA\\) is called a scalar multiple of \\(A\\).\nIn matrix notation: \\((cA)_{ij} = c(A)_{ij} = ca_{ij}\\).\nExample: For \\(A = \\left[\\begin{smallmatrix} 2 & 3 & 4 \\\\ 1 & 3 & 1 \\end{smallmatrix}\\right]\\), then \\(2A = \\left[\\begin{smallmatrix} 4 & 6 & 8 \\\\ 2 & 6 & 2 \\end{smallmatrix}\\right]\\).\nIt is common practice to denote \\((-1)B\\) by \\(-B\\).\nApplication in ECE: Scaling measurements from sensors, adjusting signal amplitudes, or weighting different components in a multi-input system.\n\nScalar multiplication is straightforward: to multiply a matrix by a scalar, you simply multiply every single entry in the matrix by that scalar. It’s like uniformly scaling every element of the matrix.\nIn an ECE context, imagine you have a matrix representing voltage readings from multiple points in a circuit. If you want to convert these readings from millivolts to volts, you would multiply the entire matrix by a scalar factor of \\(1/1000\\). Or, if you need to double the gain of an amplifier that outputs signals represented as a matrix, you’d multiply that matrix by 2. It’s a very common operation for adjusting magnitudes."
  },
  {
    "objectID": "la-13.html#matrix-multiplication-product",
    "href": "la-13.html#matrix-multiplication-product",
    "title": "Linear Algebra",
    "section": "Matrix Multiplication (Product)",
    "text": "Matrix Multiplication (Product)\nThis is the most complex, but most powerful, basic matrix operation.\nIf \\(A\\) is an \\(m \\times r\\) matrix and \\(B\\) is an \\(r \\times n\\) matrix, then the product \\(AB\\) is the \\(m \\times n\\) matrix.\nCrucial Compatibility Rule: The number of columns of the first matrix (\\(A\\)’s columns, \\(r\\)) must equal the number of rows of the second matrix (\\(B\\)’s rows, \\(r\\)).\n\nIf compatible (\\(m \\times \\mathbf{r} \\cdot \\mathbf{r} \\times n\\)), the resulting product matrix has size \\(m \\times n\\).\n\nRow-Column Rule: To find the entry \\((AB)_{ij}\\) (row \\(i\\), column \\(j\\) of \\(AB\\)), you single out row \\(i\\) from \\(A\\) and column \\(j\\) from \\(B\\). Multiply corresponding entries from the selected row and column, and then add up the resulting products. \\[\n(AB)_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + \\dots + a_{ir}b_{rj}\n\\]\n\nMatrix multiplication is fundamentally different from the previous operations because it’s not element-wise. It’s a “row-column” operation and is the cornerstone of linear transformations.\nThe most critical rule is the compatibility constraint: the “inner dimensions” must match. If A is m x r and B is s x n, then for AB to be defined, r must equal s. If they don’t match, the multiplication is undefined.\nThe row-column rule is what defines the actual computation. To get any specific entry in the product matrix, say the entry in the \\(i\\)-th row and \\(j\\)-th column, you take the \\(i\\)-th row of the first matrix (A) and the \\(j\\)-th column of the second matrix (B). You then perform a dot product: multiply the first element of the row by the first element of the column, the second by the second, and so on, then sum all these products. This might seem abstract, but it’s how linear transformations are applied, how signals are convolved, and how components in a system interact."
  },
  {
    "objectID": "la-13.html#example-5-multiplying-matrices",
    "href": "la-13.html#example-5-multiplying-matrices",
    "title": "Linear Algebra",
    "section": "Example 5: Multiplying Matrices",
    "text": "Example 5: Multiplying Matrices\nFind \\(AB\\) for: \\[\nA={\\left[\\begin{array}{l l l}{1}&{2}&{4}\\\\ {2}&{6}&{0}\\end{array}\\right]},\\quad B={\\left[\\begin{array}{l l l l}{4}&{1}&{4}&{3}\\\\ {0}&{-1}&{3}&{1}\\\\ {2}&{7}&{5}&{2}\\end{array}\\right]}\n\\] \\(A\\) is \\(2 \\times \\mathbf{3}\\) and \\(B\\) is \\(\\mathbf{3} \\times 4\\). Product \\(AB\\) will be \\(2 \\times 4\\).\nLet’s compute \\((AB)_{23}\\) (row 2, col 3): \\((2 \\cdot 4) + (6 \\cdot 3) + (0 \\cdot 5) = 8 + 18 + 0 = 26\\)\nLet’s compute \\((AB)_{14}\\) (row 1, col 4): \\((1 \\cdot 3) + (2 \\cdot 1) + (4 \\cdot 2) = 3 + 2 + 8 = 13\\)\nThe full product: \\[\nA B={\\left[\\begin{array}{l l l l}{12}&{27}&{30}&{13}\\\\ {8}&{-4}&{26}&{12}\\end{array}\\right]}\n\\]\nApplications in ECE:\n\nTransformations: Rotating, scaling, or projecting vectors in graphics and robotics.\nCircuit Analysis: Solving network equations (e.g., \\(V = IZ\\)), impedance, and admittance matrices.\nDigital Signal Processing (DSP): Filtering operations, Fourier transforms can be expressed as matrix multiplications.\n\n\nLet’s see this in action with an example. Matrix A is 2x3, and Matrix B is 3x4. Since the inner dimensions (3 and 3) match, the product AB is defined and will be a 2x4 matrix.\nLet’s calculate a couple of entries, for instance, the entry in the second row, third column of AB. We take the second row of A: [2 6 0] and the third column of B: [4 3 5]^T. We then perform the dot product: (2*4) + (6*3) + (0*5) = 8 + 18 + 0 = 26. This is the entry (AB)23.\nThis seemingly complex operation is essential in ECE because it represents the application of linear transformations.\n\nIn computer graphics, multiplying a vector (like a point in 3D space) by a transformation matrix rotates or scales that point.\nIn circuit analysis, matrix multiplication is used to represent the relationship between current, voltage, and impedance in complex networks, allowing you to solve for unknown quantities using \\(V=IZ\\) in matrix form.\nIn DSP, applying filters to signals, or performing Fourier transforms, are often conceptually (and computationally) done via matrix multiplication. It’s how signals are manipulated and transformed."
  },
  {
    "objectID": "la-13.html#interactive-example-matrix-multiplication",
    "href": "la-13.html#interactive-example-matrix-multiplication",
    "title": "Linear Algebra",
    "section": "Interactive Example: Matrix Multiplication",
    "text": "Interactive Example: Matrix Multiplication\nLet’s use Python to compute the full matrix product:\n\n\n\n\n\n\n\nHere, we use NumPy again, which provides a very convenient @ operator for matrix multiplication.\nThe code precisely demonstrates the calculation from the previous slide. You can see how NumPy handles all the dot products for each entry, giving us the full 2x4 product matrix. It also verifies the dimensions, showing that a (2x3) multiplied by a (3x4) yields a (2x4) matrix.\nThis interactivity allows you to verify the row-column rule yourself by picking any entry and manually computing its value. Understanding the inner workings of matrix multiplication is key to grasping how linear transformations map inputs to outputs in various engineering systems."
  },
  {
    "objectID": "la-13.html#matrix-multiplication-by-columns-and-by-rows",
    "href": "la-13.html#matrix-multiplication-by-columns-and-by-rows",
    "title": "Linear Algebra",
    "section": "Matrix Multiplication by Columns and By Rows",
    "text": "Matrix Multiplication by Columns and By Rows\nMatrix multiplication can be viewed column by column or row by row:\nIf \\(AB\\) is defined,\n\n\\(j\\)-th column of \\(AB\\) is \\(A\\) times the \\(j\\)-th column of \\(B\\). \\[\nAB = A[\\mathbf{b}_1\\quad \\mathbf{b}_2\\quad \\dots \\quad \\mathbf{b}_n] = [A\\mathbf{b}_1\\quad A\\mathbf{b}_2\\quad \\dots \\quad A\\mathbf{b}_n]\n\\]\n\\(i\\)-th row of \\(AB\\) is \\(i\\)-th row of \\(A\\) times \\(B\\). \\[\nAB = \\left[ \\begin{array}{c}\\mathbf{a}_1\\\\ \\mathbf{a}_2\\\\ \\vdots \\\\ \\mathbf{a}_m \\end{array} \\right]B = \\left[ \\begin{array}{c}\\mathbf{a}_1B\\\\ \\mathbf{a}_2B\\\\ \\vdots \\\\ \\mathbf{a}_mB \\end{array} \\right]\n\\]\n\nThis is useful for computing specific rows or columns without the full product.\n\nWhile the row-column rule calculates each entry, there are also useful ways to think about computing entire rows or columns of the product. These ideas are particularly important for understanding the structure of matrix transformations.\nThe first formula states that each column of the product \\(AB\\) is formed by multiplying matrix \\(A\\) by the corresponding column of matrix \\(B\\). This means if you want just the second column of \\(AB\\), you multiply \\(A\\) by the second column vector of \\(B\\).\nSimilarly, the second formula indicates that each row of \\(AB\\) is obtained by multiplying the corresponding row of \\(A\\) by the entire matrix \\(B\\). This is beneficial when you only need specific output rows translated by the transformation. These different perspectives are very important in optimizing computations, particularly in parallel computing environments. In signal processing, for instance, you might only care about the output of a specific filter (one row) applied to different input signals."
  },
  {
    "objectID": "la-13.html#matrix-products-as-linear-combinations",
    "href": "la-13.html#matrix-products-as-linear-combinations",
    "title": "Linear Algebra",
    "section": "Matrix Products as Linear Combinations",
    "text": "Matrix Products as Linear Combinations\nA linear combination of matrices \\(A_1, \\ldots, A_r\\) with scalars \\(c_1, \\ldots, c_r\\) is \\(c_1A_1 + c_2A_2 + \\dots + c_rA_r\\).\nTheorem 1.3.1: If \\(A\\) is an \\(m \\times n\\) matrix and \\(\\mathbf{x}\\) is an \\(n \\times 1\\) column vector, then the product \\(A\\mathbf{x}\\) can be expressed as a linear combination of the column vectors of \\(A\\), in which the coefficients are the entries of \\(\\mathbf{x}\\).\n\\(A\\mathbf{x} = x_1(\\text{col }_1 \\text{ of } A) + x_2(\\text{col }_2 \\text{ of } A) + \\dots + x_n(\\text{col }_n \\text{ of } A)\\)\nExample 8: \\[\n{\\left[\\begin{array}{l l l}{-1}&{3}&{2}\\\\ {1}&{2}&{-3}\\\\ {2}&{1}&{-2}\\end{array}\\right]}{\\left[\\begin{array}{l}{2}\\\\ {-1}\\\\ {3}\\end{array}\\right]}={\\left[\\begin{array}{l}{1}\\\\ {-9}\\\\ {-3}\\end{array}\\right]}\n\\] This is equivalent to: \\[\n2{\\left[\\begin{array}{l}{-1}\\\\ {1}\\\\ {2}\\end{array}\\right]} - 1{\\left[\\begin{array}{l}{3}\\\\ {2}\\\\ {1}\\end{array}\\right]} + 3{\\left[\\begin{array}{l}{2}\\\\ {-3}\\\\ {-2}\\end{array}\\right]} = {\\left[\\begin{array}{l}{1}\\\\ {-9}\\\\ {-3}\\end{array}\\right]}\n\\]\nApplication in ECE: This is fundamental for understanding superposition principles in circuits, representing signals as combinations of basis functions (like Fourier series), or combining effects of multiple inputs in control systems.\n\nThis concept is incredibly important and is often revisited throughout linear algebra: matrix products can be seen as linear combinations. A linear combination, in simple terms, is a sum of vectors or matrices scaled by constants.\nTheorem 1.3.1 is key: If you multiply a matrix \\(A\\) by a column vector \\(\\mathbf{x}\\), the result (\\(A\\mathbf{x}\\)) is a linear combination of the columns of \\(A\\), where the scaling coefficients are the entries of \\(\\mathbf{x}\\). This provides a powerful geometric and conceptual understanding of what matrix multiplication does.\nThe example clearly illustrates this. The product of the matrix and vector is exactly equal to scaling each column of the matrix by the corresponding entry in the vector and then summing those scaled columns. This is not just a mathematical curiosity; it has profound implications: * In circuit analysis, if the columns of A represent responses to individual input signals, then \\(A\\mathbf{x}\\) represents the total response to a combined input signal \\(\\mathbf{x}\\), illustrating the superposition principle. * In signal processing, it allows us to express complex signals as combinations of simpler “basis” signals, which is the core idea behind Fourier analysis and wavelets. * In control systems, it helps analyze how different control inputs combine to affect the system’s state."
  },
  {
    "objectID": "la-13.html#matrix-form-of-a-linear-system",
    "href": "la-13.html#matrix-form-of-a-linear-system",
    "title": "Linear Algebra",
    "section": "Matrix Form of a Linear System",
    "text": "Matrix Form of a Linear System\nAny system of \\(m\\) linear equations in \\(n\\) unknowns can be written compactly as a single matrix equation: \\[\nA\\mathbf{x} = \\mathbf{b}\n\\] Where:\n\n\\(A\\) is the coefficient matrix (\\(m \\times n\\)).\n\\(\\mathbf{x}\\) is the unknown vector (\\(n \\times 1\\)).\n\\(\\mathbf{b}\\) is the constant vector (\\(m \\times 1\\)).\n\nExample: \\[\n\\begin{array}{c}x_{1} - 2x_{2} + x_{3} = 0\\\\ 2x_{1} + 0x_{2} + 3x_{3} = 5\\end{array} \\quad \\rightarrow \\quad \\left[ \\begin{array}{r r r}{1} & -2 & 1\\\\ 2 & 0 & 3 \\end{array} \\right]\\left[ \\begin{array}{c}{x_{1}}\\\\{x_{2}}\\\\{x_{3}}\\end{array} \\right] = \\left[ \\begin{array}{c}{0}\\\\{5}\\end{array} \\right]\n\\]\nThe augmented matrix for the system is \\([A \\mid \\mathbf{b}]\\).\nApplication in ECE: This compact notation simplifies the representation and analysis of large, complex systems found in circuits, control, and power systems. Rather than writing out pages of individual equations, we can deal with the entire system as one matrix equation.\n\nBringing it back to our first topic, systems of linear equations. Matrix multiplication provides an elegant way to represent an entire system of linear equations as a single matrix equation: \\(A\\mathbf{x} = \\mathbf{b}\\).\nHere, \\(A\\) collects all the coefficients, \\(\\mathbf{x}\\) is the column vector of all your unknown variables, and \\(\\mathbf{b}\\) is the column vector of all the constants on the right-hand side. The example vividly shows how a typical system translates into this matrix form.\nThis matrix representation is more than just a compact notation; it’s a fundamental shift in how we think about solving these problems. Instead of seeing individual equations, we see a single transformation (\\(A\\)) acting on an input (\\(\\mathbf{x}\\)) to produce an output (\\(\\mathbf{b}\\)). This unified view is crucial in all areas of ECE, from power system load flow analysis to solving for node voltages in a circuit network, where you might be dealing with systems of hundreds or thousands of equations."
  },
  {
    "objectID": "la-13.html#transpose-of-a-matrix",
    "href": "la-13.html#transpose-of-a-matrix",
    "title": "Linear Algebra",
    "section": "Transpose of a Matrix",
    "text": "Transpose of a Matrix\nThe transpose of an \\(m \\times n\\) matrix \\(A\\), denoted by \\(A^T\\), is the \\(n \\times m\\) matrix obtained by interchanging the rows and columns of \\(A\\).\n\nThe first column of \\(A^T\\) is the first row of \\(A\\), and so on.\nThe entry in row \\(i\\), column \\(j\\) of \\(A^T\\) is \\((A^T)_{ij} = (A)_{ji}\\).\n\nExample: \\[\nA=\\left[\\begin{array}{c c c}{{1}}&{{-2}}&{{4}}\\\\ {{3}}&{{7}}&{{0}}\\\\ {{-5}}&{{8}}&{{6}}\\end{array}\\right] \\quad \\rightarrow \\quad A^{T}=\\left[\\begin{array}{c c c}{{1}}&{{3}}&{{-5}}\\\\ {{-2}}&{{7}}&{{8}}\\\\ {{4}}&{{0}}&{{6}}\\end{array}\\right]\n\\] For a square matrix, this is like “reflecting” the matrix entries about its main diagonal.\nApplication in ECE:\n\nSignal Processing: Analyzing properties of signals and systems, e.g., in covariance matrices for noisy signals.\nData Science/ML: Reshaping data for algorithms, in least squares regression and principal component analysis (PCA).\nQuantum Mechanics: Representing operators and states in quantum computing."
  },
  {
    "objectID": "la-13.html#transpose-of-a-matrix-1",
    "href": "la-13.html#transpose-of-a-matrix-1",
    "title": "Linear Algebra",
    "section": "Transpose of a Matrix",
    "text": "Transpose of a Matrix\n\n\n\n\n\n\n\nThe transpose is an operation unique to matrices and has no direct scalar counterpart. It simply flips the matrix over its main diagonal, effectively swapping rows and columns. What was the first row becomes the first column, and so on.\nThe example shows how a square matrix is reflected. For a non-square matrix, the dimensions also swap: a 3x2 matrix becomes a 2x3 matrix when transposed.\nIn ECE, the transpose is extremely common:\n\nIn signal processing, when you’re dealing with matrices of data, you often need to transpose them to align dimensions for operations like matrix multiplication, particularly for calculating correlation or covariance matrices. For example, if you have a matrix where rows are time points and columns are sensor data, transposing it means columns are time points and rows are sensor data.\nIt’s integral to the mathematics of least squares regression, where you try to find the best fit line for a set of data points, common in system identification and control.\nIn quantum computing, the adjoint (conjugate transpose) of a matrix is a fundamental operation representing a reverse process or observation."
  },
  {
    "objectID": "la-13.html#trace-of-a-matrix",
    "href": "la-13.html#trace-of-a-matrix",
    "title": "Linear Algebra",
    "section": "Trace of a Matrix",
    "text": "Trace of a Matrix\nIf \\(A\\) is a square matrix, then the trace of \\(A\\), denoted by \\(\\operatorname{tr}(A)\\), is defined to be the sum of the entries on the main diagonal of \\(A\\). The trace of \\(A\\) is undefined if \\(A\\) is not a square matrix.\nExample: For \\(A = \\left[\\begin{smallmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{smallmatrix}\\right]\\), \\(\\operatorname{tr}(A) = a_{11} + a_{22} + a_{33}\\).\nFor \\(B = \\left[\\begin{smallmatrix} -1 & 2 & 7 & 0 \\\\ 3 & 5 & -8 & 4 \\\\ 1 & 2 & 7 & -3 \\\\ 4 & -2 & 1 & 0 \\end{smallmatrix}\\right]\\), \\(\\operatorname{tr}(B) = -1 + 5 + 7 + 0 = 11\\).\nApplication in ECE:\n\nControl Systems: Used in system analysis, e.g., in stability criteria and Lyapunov equations.\nQuantum Mechanics: Represents observable quantities; used in density matrices.\nNumerical Analysis: Useful properties for eigenvalue calculations."
  },
  {
    "objectID": "la-13.html#trace-of-a-matrix-1",
    "href": "la-13.html#trace-of-a-matrix-1",
    "title": "Linear Algebra",
    "section": "Trace of a Matrix",
    "text": "Trace of a Matrix\n\n\n\n\n\n\n\nThe trace is another special operation, defined only for square matrices. It’s simply the sum of the elements along the main diagonal.\nThe example clearly shows how to compute the trace by summing the highlighted diagonal elements. For a larger matrix, it’s the sum of \\(a_{11}, a_{22}, a_{33}\\), and so on.\nWhile seemingly simple, the trace has deep theoretical significance in various engineering fields: * In control theory, the trace of certain matrices can reveal information about system stability or how quickly a system converges to a desired state. * In quantum mechanics, the trace of a density matrix gives the total probability of all states, which must be 1. * It plays a role in finding eigenvalues, which are critical for understanding the fundamental behavior of linear systems like resonators or RLC circuits. Python’s NumPy library has a convenient np.trace() function that computes this directly."
  },
  {
    "objectID": "la-13.html#conclusion",
    "href": "la-13.html#conclusion",
    "title": "Linear Algebra",
    "section": "Conclusion",
    "text": "Conclusion\n\nMatrices are powerful data structures: Representing data and relationships in a structured way (e.g., sensor arrays, system states).\nCore Operations:\n\nAddition & Subtraction: Element-wise (same size required).\nScalar Multiplication: Scales all entries by a constant.\nMatrix Multiplication: Row-column rule (inner dimensions must match)  —  represents linear transformations and data interaction.\n\nConceptual Depth: Matrix products as linear combinations (superposition, basis vectors).\nSpecial Operations:\n\nTranspose (\\(A^T\\)): Swaps rows and columns (data reorientation).\nTrace (\\(\\operatorname{tr}(A)\\)): Sum of main diagonal elements (for square matrices, revealing system properties).\n\nUbiquitous in ECE: These operations form the bedrock of numerical methods for solving problems in circuits, signals, control, robotics, imaging, communication, and machine learning.\n\nMastering these matrix operations is essential for your engineering toolkit!\n\nTo wrap up our discussion on matrices and matrix operations from Section 1.3:\nWe started by defining matrices as flexible data structures for organizing numbers, crucial for representing everything from sensor data to system coefficients in ECE.\nWe covered the fundamental arithmetic operations: * Addition and subtraction are element-wise, meaning they combine corresponding entries. These are like combining multiple sets of measurements or changes in system states. * Scalar multiplication uniformly scales data, * And most importantly, matrix multiplication, which is a row-column operation with strict dimension compatibility rules. This operation is the mathematical language of linear transformations, describing how systems evolve or how signals are processed.\nWe also discussed the conceptual power of viewing matrix products as linear combinations of column vectors, which directly translates to the principle of superposition in many physical systems.\nFinally, we introduced the transpose, which reorients matrices, and the trace, which sums diagonal elements, both having significant applications in advanced analysis across ECE.\nThese operations are not just abstract mathematical concepts; they are the fundamental “verbs” of linear algebra that you will use daily in circuit simulation, signal processing, control system design, image processing, and even in novel areas like quantum computing and AI. Without a solid grasp of these operations, tackling complex engineering challenges becomes significantly harder. Keep practicing these, as they are foundational for every topic moving forward.\nThank you! I look forward to seeing you next time as we delve into properties of matrix operations."
  },
  {
    "objectID": "la-13.html#exercises",
    "href": "la-13.html#exercises",
    "title": "Linear Algebra",
    "section": "Exercises",
    "text": "Exercises\ngrasple_exercise_3_2_2\nTo compute \\(c_1A + c_2B\\)."
  },
  {
    "objectID": "la-13.html#exercises-1",
    "href": "la-13.html#exercises-1",
    "title": "Linear Algebra",
    "section": "Exercises",
    "text": "Exercises\ngrasple_exercise_3_2_3\nTo compute \\(c_1A + c_2B\\)."
  },
  {
    "objectID": "la-15.html#elementary-matrices-and-a-method-for-finding-a-1",
    "href": "la-15.html#elementary-matrices-and-a-method-for-finding-a-1",
    "title": "Linear Algebra",
    "section": "1.5 Elementary Matrices and a Method for Finding \\(A^{-1}\\)",
    "text": "1.5 Elementary Matrices and a Method for Finding \\(A^{-1}\\)\nImron Rosyadi\n  An essential tool for understanding and solving linear systems in engineering.\n\nGood morning everyone! Today, we’re diving into a crucial topic in linear algebra: elementary matrices and how they help us find the inverse of a matrix. This might sound abstract, but it’s fundamental to many areas in Electrical and Computer Engineering, from circuit analysis to signal processing and control systems. We’ll explore the theoretical underpinnings and practical computational methods."
  },
  {
    "objectID": "la-15.html#moment-of-silence",
    "href": "la-15.html#moment-of-silence",
    "title": "Linear Algebra",
    "section": "Moment of Silence",
    "text": "Moment of Silence"
  },
  {
    "objectID": "la-15.html#d-elastic-collision",
    "href": "la-15.html#d-elastic-collision",
    "title": "Linear Algebra",
    "section": "1D elastic collision",
    "text": "1D elastic collision\nA mass \\(M1\\) (moving) with initial velocity \\(U_1\\) hits another mass \\(m_2\\) that is initially at rest. We assume a perfectly elastic collision in one dimension. Unknown final velocities are:\n\n\\(V_1\\) — final velocity of mass \\(M1\\),\n\\(v_2\\) — final velocity of mass \\(m2\\).\n\nTwo equations govern the collision:\n\nConservation of linear momentum\n\n\\[\nM_1 U_1 = M_1 V_1 + m_2 v_2\n\\]\n\nElastic collision condition (relative velocity reversal)\n\n\\[\nV_1 - v_2 = -(U_1 - 0) = -U_1\n\\]\nRewrite as a linear system \\(A\\mathbf{x}=\\mathbf{b}\\) with \\(\\mathbf{x}=[V_1,\\,v_2]^\\top\\):\n\\[\n\\begin{bmatrix} M_1 & m_2 \\\\[4pt] 1 & -1 \\end{bmatrix}\n\\begin{bmatrix} V_1 \\\\ v_2 \\end{bmatrix}\n=\n\\begin{bmatrix} M_1 U_1 \\\\ -U_1 \\end{bmatrix}.\n\\]"
  },
  {
    "objectID": "la-15.html#elementary-matrices-and-a-method-for-finding-a-1-1",
    "href": "la-15.html#elementary-matrices-and-a-method-for-finding-a-1-1",
    "title": "Linear Algebra",
    "section": "1.5 Elementary Matrices and a Method for Finding \\(A^{-1}\\)",
    "text": "1.5 Elementary Matrices and a Method for Finding \\(A^{-1}\\)"
  },
  {
    "objectID": "la-15.html#introduction-why-elementary-matrices",
    "href": "la-15.html#introduction-why-elementary-matrices",
    "title": "Linear Algebra",
    "section": "Introduction: Why Elementary Matrices?",
    "text": "Introduction: Why Elementary Matrices?\n\nHow do we systematically find the inverse of a matrix?\nHow do basic row operations relate to matrix multiplication?\n\nIn this section, we’ll develop an algorithm for finding the inverse of a matrix and discuss key properties of invertible matrices. We’ll start by revisiting elementary row operations.\n\nWe’ve touched upon matrix inverses before, but how do we actually compute them for larger matrices systematically? And how do the basic row operations that we’ve been using, like scaling a row or swapping rows, relate to matrix multiplication? These are the questions we’ll answer today. Understanding this connection is key, as it provides a powerful framework for matrix manipulation and solving linear systems."
  },
  {
    "objectID": "la-15.html#elementary-row-operations",
    "href": "la-15.html#elementary-row-operations",
    "title": "Linear Algebra",
    "section": "Elementary Row Operations",
    "text": "Elementary Row Operations\nRecall the three elementary row operations on a matrix \\(A\\):\n\nScaling: Multiply a row by a nonzero constant \\(c\\).\n\nInverse Operation: Multiply the same row by \\(1/c\\).\n\nInterchange: Interchange two rows.\n\nInverse Operation: Interchange the same two rows.\n\nReplacement: Add a constant \\(c\\) times one row to another.\n\nInverse Operation: Add \\(-c\\) times the same row to the other.\n\n\n\nRemember these operations from Section 1.1? They are the fundamental tools for Gaussian elimination and Gauss-Jordan elimination. It’s important to note that each of these operations is reversible. If you perform an operation, you can always undo it with a corresponding inverse operation. This concept of reversibility is crucial when we talk about matrix inverses."
  },
  {
    "objectID": "la-15.html#row-equivalence-elementary-matrices",
    "href": "la-15.html#row-equivalence-elementary-matrices",
    "title": "Linear Algebra",
    "section": "Row Equivalence & Elementary Matrices",
    "text": "Row Equivalence & Elementary Matrices\nDEFINITION 1: Row Equivalence\nMatrices \\(A\\) and \\(B\\) are row equivalent if either (hence each) can be obtained from the other by a sequence of elementary row operations.\nDEFINITION 2: Elementary Matrix\nA matrix \\(E\\) is called an elementary matrix if it can be obtained from an identity matrix by performing a single elementary row operation.\n\nThese two definitions are foundational for today’s topic. Row equivalence establishes a relationship between matrices that can be transformed into one another using elementary row operations. The concept of an “elementary matrix” is where things get interesting: it’s a matrix that represents a single elementary row operation."
  },
  {
    "objectID": "la-15.html#example-1-elementary-matrices",
    "href": "la-15.html#example-1-elementary-matrices",
    "title": "Linear Algebra",
    "section": "Example 1: Elementary Matrices",
    "text": "Example 1: Elementary Matrices\nListed below are four elementary matrices and the operations that produce them from an identity matrix \\(I\\).\n\n\n\n\nHere you can see concrete examples. The first matrix on the left is formed by multiplying the second row of \\(I_3\\) by 7. The second, by swapping the first and third rows of \\(I_3\\). The third, by adding -2 times the first row to the third row of \\(I_3\\). And the fourth, by multiplying the first row of \\(I_2\\) by 1. Notice how they all originate from an identity matrix and correspond to one specific elementary row operation."
  },
  {
    "objectID": "la-15.html#theorem-1.5.1-row-operations-by-matrix-multiplication",
    "href": "la-15.html#theorem-1.5.1-row-operations-by-matrix-multiplication",
    "title": "Linear Algebra",
    "section": "Theorem 1.5.1: Row Operations by Matrix Multiplication",
    "text": "Theorem 1.5.1: Row Operations by Matrix Multiplication\nIf the elementary matrix \\(E\\) results from performing a certain row operation on \\(I_m\\) and if \\(A\\) is an \\(m \\times n\\) matrix, then the product \\(EA\\) is the matrix that results when this same row operation is performed on \\(A\\).\nThis is a powerful result: performing a row operation on \\(A\\) is equivalent to multiplying \\(A\\) by an elementary matrix from the left.\n\nThis theorem is a game-changer. It means that seemingly simple row operations, which we’ve performed directly on matrices, can actually be represented as matrix multiplications. This connection is super important because it allows us to analyze complicated sequences of row operations using the algebra of matrices. Later, we will see how this concept has profound implications for understanding matrix invertibility and factorization in signal processing or control systems."
  },
  {
    "objectID": "la-15.html#example-2-using-elementary-matrices",
    "href": "la-15.html#example-2-using-elementary-matrices",
    "title": "Linear Algebra",
    "section": "Example 2: Using Elementary Matrices",
    "text": "Example 2: Using Elementary Matrices\nConsider the matrix: \\[\nA = \\left[ \\begin{array}{rrrr}1 & 0 & 2 & 3 \\\\ 2 & -1 & 3 & 6 \\\\ 1 & 4 & 4 & 0 \\end{array} \\right]\n\\]\nAnd the elementary matrix: \\[\nE = \\left[ \\begin{array}{lll}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 3 & 0 & 1 \\end{array} \\right]\n\\]\n\\(E\\) results from adding 3 times the first row of \\(I_3\\) to its third row.\nThe product \\(EA\\) is:\n\\[\nEA = \\left[ \\begin{array}{rrrr}1 & 0 & 2 & 3 \\\\ 2 & -1 & 3 & 6 \\\\ 4 & 4 & 10 & 9 \\end{array} \\right]\n\\]\nThis is precisely what you get by adding 3 times row 1 of \\(A\\) to row 3 of \\(A\\).\n\nLet’s see this in action. We have matrix A and elementary matrix E. E was formed by taking I3 and adding 3 times its first row to its third row. Now, if we explicitly multiply E by A, we get a new matrix. If you look closely, you’ll see that this new matrix is exactly what you would get if you performed that same row operation (add 3 times row 1 to row 3) directly on matrix A. This example beautifully illustrates Theorem 1.5.1. While we often perform row operations directly in practice, understanding this matrix multiplication equivalent is crucial for theoretical proofs and more advanced matrix decompositions."
  },
  {
    "objectID": "la-15.html#example-2-python-demonstration",
    "href": "la-15.html#example-2-python-demonstration",
    "title": "Linear Algebra",
    "section": "Example 2: Python Demonstration",
    "text": "Example 2: Python Demonstration\nLet’s compute \\(EA\\).\n\n\n\n\n\n\nThe result of EA confirms the effect of adding 3 times the first row of \\(A\\) to the third row.\n\nNow, let’s confirm this using a live Python environment right here in our presentation. We’ll use the numpy library, which is a staple in ECE for numerical computations. We define our matrix A and our elementary matrix E. Then, a simple np.dot(E, A) gives us the product. You can see how the third row of the resulting matrix accurately reflects the row operation. This hands-on computation reinforces the theorem."
  },
  {
    "objectID": "la-15.html#inverse-operations",
    "href": "la-15.html#inverse-operations",
    "title": "Linear Algebra",
    "section": "Inverse Operations",
    "text": "Inverse Operations\nFor every elementary matrix \\(E\\), there’s an inverse elementary row operation that transforms \\(E\\) back to the identity matrix \\(I\\).\n\n\nRow Operation on \\(I\\) That Produces \\(E\\)\n\nMultiply row \\(i\\) by \\(c \\neq 0\\)\nInterchange rows \\(i\\) and \\(j\\)\nAdd \\(c\\) times row \\(i\\) to row \\(j\\)\n\n\nRow Operation on \\(E\\) That Reproduces \\(I\\)\n\nMultiply row \\(i\\) by \\(1/c\\)\nInterchange rows \\(i\\) and \\(j\\)\nAdd \\(-c\\) times row \\(i\\) to row \\(j\\)\n\n\n\nWe discussed this briefly earlier: every elementary row operation has an inverse. This table summarizes those inverse operations. For example, if you scaled a row by \\(c\\), you can undo it by scaling it by \\(1/c\\). If you swapped two rows, swapping them again undoes the operation. And if you added \\(c\\) times one row to another, you undo it by adding \\(-c\\) times that same row. This reversibility leads us to an important theorem about elementary matrices themselves."
  },
  {
    "objectID": "la-15.html#theorem-1.5.2-invertibility-of-elementary-matrices",
    "href": "la-15.html#theorem-1.5.2-invertibility-of-elementary-matrices",
    "title": "Linear Algebra",
    "section": "Theorem 1.5.2: Invertibility of Elementary Matrices",
    "text": "Theorem 1.5.2: Invertibility of Elementary Matrices\nEvery elementary matrix is invertible, and its inverse is also an elementary matrix.\nThis theorem is a building block for many results that follow.\n\nThis theorem is quite intuitive given what we just saw. Since every elementary row operation is reversible by another elementary row operation, and elementary matrices represent these operations, it logically follows that elementary matrices must be invertible. And furthermore, their inverse is also an elementary matrix. This is not just a theoretical curiosity; it’s a critical property that enables us to express any invertible matrix as a product of elementary matrices, which is a powerful concept in areas like digital signal processing and control system design."
  },
  {
    "objectID": "la-15.html#theorem-1.5.3-equivalent-statements",
    "href": "la-15.html#theorem-1.5.3-equivalent-statements",
    "title": "Linear Algebra",
    "section": "Theorem 1.5.3: Equivalent Statements",
    "text": "Theorem 1.5.3: Equivalent Statements\nIf \\(A\\) is an \\(n \\times n\\) matrix, then the following statements are equivalent (all true or all false):\n\n\\(A\\) is invertible.\n\\(A\\mathbf{x} = \\mathbf{0}\\) has only the trivial solution (\\(\\mathbf{x}=\\mathbf{0}\\)).\nThe reduced row echelon form (RREF) of \\(A\\) is \\(I_n\\).\n\\(A\\) is expressible as a product of elementary matrices.\n\nThis theorem connects seemingly diverse ideas in linear algebra!\n\nNow we come to one of the most significant theorems in introductory linear algebra: the Equivalent Statements Theorem. This theorem beautifully ties together concepts we’ve already learned and new ones like elementary matrices. It tells us that these four conditions are inextricably linked. If one is true, all are true. If one is false, all are false. This isn’t just theory; it has practical implications in ECE. For example, knowing that “A is invertible” is equivalent to “Ax=0 has only the trivial solution” tells us that a system of equations describing a circuit has a unique solution if and only if its coefficient matrix is invertible."
  },
  {
    "objectID": "la-15.html#a-method-for-inverting-matrices",
    "href": "la-15.html#a-method-for-inverting-matrices",
    "title": "Linear Algebra",
    "section": "A Method for Inverting Matrices",
    "text": "A Method for Inverting Matrices\nInversion Algorithm:\nTo find the inverse of an invertible matrix \\(A\\):\n\nForm the augmented matrix \\([A \\mid I_n]\\).\nPerform a sequence of elementary row operations to reduce \\(A\\) to the identity matrix \\(I_n\\).\nAs you perform these operations on \\(A\\), simultaneously apply them to \\(I_n\\).\nIf \\(A\\) reduces to \\(I_n\\), the right side of the augmented matrix will be \\(A^{-1}\\). The final matrix will have the form \\([I_n \\mid A^{-1}]\\).\n\nThis method leverages the fact that the same sequence of operations that transforms \\(A\\) to \\(I_n\\) will transform \\(I_n\\) to \\(A^{-1}\\).\n\nThis algorithm directly comes from the equivalence theorem and the idea of elementary matrices. If A is invertible, we know its RREF is the identity matrix. Each step of row reduction corresponds to multiplying by an elementary matrix. So, if \\(E_k \\dots E_1 A = I_n\\), then multiplying by \\(A^{-1}\\) on the right, we get \\(E_k \\dots E_1 = A^{-1}\\). This means the sequence of elementary matrices that transform A to I, when applied to I, will transform I to A inverse. This is the foundation of the Gauss-Jordan method for finding an inverse, a very practical algorithm in many engineering computations."
  },
  {
    "objectID": "la-15.html#example-4-using-row-operations-to-find-a-1",
    "href": "la-15.html#example-4-using-row-operations-to-find-a-1",
    "title": "Linear Algebra",
    "section": "Example 4: Using Row Operations to Find \\(A^{-1}\\)",
    "text": "Example 4: Using Row Operations to Find \\(A^{-1}\\)\nFind the inverse of \\[\nA = \\left[ \\begin{array}{lll}1 & 2 & 3 \\\\ 2 & 5 & 3 \\\\ 1 & 0 & 8 \\end{array} \\right]\n\\]\nWe start with the augmented matrix \\([A \\mid I]\\):\n\\[\n\\left[ \\begin{array}{r r r | r r r}{1}&{2}&{3} & {1}&{0}&{0}\\\\ {2}&{5}&{3} & {0}&{1}&{0}\\\\ {1}&{0}&{8} & {0}&{0}&{1}\\end{array} \\right]\n\\] Apply row operations: 1. \\(R_2 \\leftarrow R_2 - 2R_1\\) 2. \\(R_3 \\leftarrow R_3 - R_1\\)\n\\[\n\\left[ \\begin{array}{r r r | r r r}{1}&{2}&{3} & {1}&{0}&{0}\\\\ {0}&{1}&{-3} & {-2}&{1}&{0}\\\\ {0}&{-2}&{5} & {-1}&{0}&{1}\\end{array} \\right]\n\\]\n\nLet’s walk through an example. We want to find the inverse of matrix A. We set up the augmented matrix by placing A on the left and the identity matrix on the right. Our goal is to transform the left side into the identity matrix using elementary row operations, and whatever we do to the left, we do to the right. The first step for Gauss-Jordan is to get zeros below the leading 1 in the first column. We perform R2 - 2R1 and R3 - R1. Observe how the operations are applied to both sides of the partition."
  },
  {
    "objectID": "la-15.html#example-4-using-row-operations-to-find-a-1-cont.",
    "href": "la-15.html#example-4-using-row-operations-to-find-a-1-cont.",
    "title": "Linear Algebra",
    "section": "Example 4: Using Row Operations to Find \\(A^{-1}\\) (Cont.)",
    "text": "Example 4: Using Row Operations to Find \\(A^{-1}\\) (Cont.)\nFrom previous step: \\[\n\\left[ \\begin{array}{r r r | r r r}{1}&{2}&{3} & {1}&{0}&{0}\\\\ {0}&{1}&{-3} & {-2}&{1}&{0}\\\\ {0}&{-2}&{5} & {-1}&{0}&{1}\\end{array} \\right]\n\\] Apply row operations: 3. \\(R_3 \\leftarrow R_3 + 2R_2\\)\n\\[\n\\left[ \\begin{array}{r r r | r r r}{1}&{2}&{3} & {1}&{0}&{0}\\\\ {0}&{1}&{-3} & {-2}&{1}&{0}\\\\ {0}&{0}&{-1} & {-5}&{2}&{1}\\end{array} \\right]\n\\] 4. \\(R_3 \\leftarrow -1 \\cdot R_3\\)\n\\[\n\\left[ \\begin{array}{r r r | r r r}{1}&{2}&{3} & {1}&{0}&{0}\\\\ {0}&{1}&{-3} & {-2}&{1}&{0}\\\\ {0}&{0}&{1} & {5}&{-2}&{-1}\\end{array} \\right]\n\\]\n\nContinuing the process, we want to clear the entry below the leading 1 in the second column. So we perform R3 + 2R2. This gives us a 0 in the (3,2) position. Finally, to get the identity matrix on the left, we need a 1 in the (3,3) position. We achieve this by multiplying R3 by -1. Now, we have successfully created an upper triangular form, moving towards full Gauss-Jordan."
  },
  {
    "objectID": "la-15.html#example-4-using-row-operations-to-find-a-1-cont.-1",
    "href": "la-15.html#example-4-using-row-operations-to-find-a-1-cont.-1",
    "title": "Linear Algebra",
    "section": "Example 4: Using Row Operations to Find \\(A^{-1}\\) (Cont.)",
    "text": "Example 4: Using Row Operations to Find \\(A^{-1}\\) (Cont.)\nFrom previous step: \\[\n\\left[ \\begin{array}{r r r | r r r}{1}&{2}&{3} & {1}&{0}&{0}\\\\ {0}&{1}&{-3} & {-2}&{1}&{0}\\\\ {0}&{0}&{1} & {5}&{-2}&{-1}\\end{array} \\right]\n\\] Apply row operations: 5. \\(R_2 \\leftarrow R_2 + 3R_3\\) 6. \\(R_1 \\leftarrow R_1 - 3R_3\\)\n\\[\n\\left[ \\begin{array}{r r r | r r r}{1}&{2}&{0} & {-14}&{6}&{3}\\\\ {0}&{1}&{0} & {13}&{-5}&{-3}\\\\ {0}&{0}&{1} & {5}&{-2}&{-1}\\end{array} \\right]\n\\] 7. \\(R_1 \\leftarrow R_1 - 2R_2\\)\n\\[\n\\left[ \\begin{array}{r r r | r r r}{1}&{0}&{0} & {-40}&{16}&{9}\\\\ {0}&{1}&{0} & {13}&{-5}&{-3}\\\\ {0}&{0}&{1} & {5}&{-2}&{-1}\\end{array} \\right]\n\\]\nThus, the inverse is: \\[\nA^{-1} = \\left[ \\begin{array}{rrr} - 40 & 16 & 9 \\\\ 13 & -5 & -3 \\\\ 5 & -2 & -1 \\end{array} \\right]\n\\]\n\nNow we move to the “reverse” part of Gauss-Jordan, clearing elements above the leading 1s. First, we make the elements above the leading 1 in the third column zero. We do R2 + 3R3 and R1 - 3R3. This gives us zeros in the (1,3) and (2,3) positions. Finally, we clear the element above the leading 1 in the second column by performing R1 - 2R2. Successfully, we have reduced the left side to the identity matrix. The matrix on the right is now transformed into A inverse. This systematic approach is extremely reliable for finding matrix inverses."
  },
  {
    "objectID": "la-15.html#example-4-python-verification",
    "href": "la-15.html#example-4-python-verification",
    "title": "Linear Algebra",
    "section": "Example 4: Python Verification",
    "text": "Example 4: Python Verification\nLet’s verify the inverse using numpy.linalg.inv.\n\n\n\n\n\n\nThe computed inverse matches our manual calculation, and \\(A A^{-1} \\approx I\\).\n\nTo double-check our manual calculations, we can use numpy.linalg.inv, which is a highly optimized function for calculating matrix inverses. As you can see, the result from numpy matches our step-by-step example. We can also perform a quick check by multiplying the original matrix by its inverse, which, if correct, should yield the identity matrix. This is a common practice in engineering to verify calculations."
  },
  {
    "objectID": "la-15.html#example-5-showing-that-a-matrix-is-not-invertible",
    "href": "la-15.html#example-5-showing-that-a-matrix-is-not-invertible",
    "title": "Linear Algebra",
    "section": "Example 5: Showing That a Matrix Is Not Invertible",
    "text": "Example 5: Showing That a Matrix Is Not Invertible\nConsider the matrix: \\[\nA = \\left[ \\begin{array}{rrr}1 & 6 & 4 \\\\ 2 & 4 & -1 \\\\ -1 & 2 & 5 \\end{array} \\right]\n\\] Start with \\([A \\mid I]\\) and apply row operations:\n\\[\n\\quad \\left[ \\begin{array}{r r r | r r r}{1} & {6} & {4} & {1} & {0} & {0}\\\\ {2} & {4} & {-1} & {0} & {1} & {0}\\\\ {-1} & {2} & {5} & {0} & {0} & {1} \\end{array} \\right]\n\\] 1. \\(R_2 \\leftarrow R_2 - 2R_1\\) 2. \\(R_3 \\leftarrow R_3 + R_1\\)\n\\[\n\\left[ \\begin{array}{r r r | r r r}{1} & {6} & {4} & {1} & {0} & {0}\\\\ {0} & {-8} & {-9} & {-2} & {1} & {0}\\\\ {0} & {8} & {9} & {1} & {0} & {1} \\end{array} \\right]\n\\] 3. \\(R_3 \\leftarrow R_3 + R_2\\)\n\\[\n\\left[ \\begin{array}{r r r | r r r}{1} & {6} & {4} & {1} & {0} & {0}\\\\ {0} & {-8} & {-9} & {-2} & {1} & {0}\\\\ {0} & {0} & {0} & {-1} & {1} & {1} \\end{array} \\right]\n\\]\nSince we obtained a row of zeros on the left side, \\(A\\) is not invertible.\n\nWhat if a matrix isn’t invertible? How does this algorithm tell us? Let’s take matrix A here. We’ll start the inversion process the same way. After the first set of row operations to clear the first column, we get to the second matrix. Now, when we try to clear the elements below the second pivot, specifically the (3,2) element, by adding R2 to R3, we find something crucial: the entire third row of the left part of the augmented matrix becomes zero. According to Theorem 1.5.3 part (c), if the reduced row echelon form of \\(A\\) is not \\(I_n\\), then \\(A\\) is not invertible. A row of zeros means we cannot reduce it to the identity matrix, hence \\(A\\) is singular, or non-invertible."
  },
  {
    "objectID": "la-15.html#example-5-python-check-for-non-invertibility",
    "href": "la-15.html#example-5-python-check-for-non-invertibility",
    "title": "Linear Algebra",
    "section": "Example 5: Python Check for Non-Invertibility",
    "text": "Example 5: Python Check for Non-Invertibility\nWhat happens when numpy.linalg.inv tries to invert a singular matrix?\n\n\n\n\n\n\nnumpy correctly identifies the matrix as singular, consistent with our manual process.\n\nLet’s see how numpy handles this. We use a try-except block because we expect an error. When np.linalg.inv is called on a singular matrix, it raises a LinAlgError, specifically indicating the matrix is singular. This confirms our manual observation: the appearance of a row of zeros on the left side during the inversion process is the clear signal that the matrix is not invertible. This concept is critical in ECE. For example, if a system matrix in circuit analysis turns out to be singular, it implies that the circuit equations do not have a unique solution, perhaps indicating a design flaw or a dependent set of equations."
  },
  {
    "objectID": "la-15.html#example-6-analyzing-homogeneous-systems",
    "href": "la-15.html#example-6-analyzing-homogeneous-systems",
    "title": "Linear Algebra",
    "section": "Example 6: Analyzing Homogeneous Systems",
    "text": "Example 6: Analyzing Homogeneous Systems\nUse Theorem 1.5.3 to determine whether the given homogeneous systems have nontrivial solutions.\nSystem (a): \\[\n\\begin{array}{r l} & {x_{1} + 2x_{2} + 3x_{3} = 0}\\\\ & {2x_{1} + 5x_{2} + 3x_{3} = 0}\\\\ & {x_{1} + 0x_{2} + 8x_{3} = 0} \\end{array}\n\\] Coefficient matrix: \\(A = \\left[ \\begin{array}{lll}1 & 2 & 3 \\\\ 2 & 5 & 3 \\\\ 1 & 0 & 8 \\end{array} \\right]\\) (from Example 4)"
  },
  {
    "objectID": "la-15.html#example-6-analyzing-homogeneous-systems-1",
    "href": "la-15.html#example-6-analyzing-homogeneous-systems-1",
    "title": "Linear Algebra",
    "section": "Example 6: Analyzing Homogeneous Systems",
    "text": "Example 6: Analyzing Homogeneous Systems\nSystem (b): \\[\n\\begin{array}{r l} & {x_{1} + 6x_{2} + 4x_{3} = 0}\\\\ & {2x_{1} + 4x_{2} - x_{3} = 0}\\\\ & {-x_{1} + 2x_{2} + 5x_{3} = 0} \\end{array}\n\\] Coefficient matrix: \\(A = \\left[ \\begin{array}{rrr}1 & 6 & 4 \\\\ 2 & 4 & -1 \\\\ -1 & 2 & 5 \\end{array} \\right]\\) (from Example 5)\nSolution: From parts (a) and (b) of Theorem 1.5.3: a homogeneous linear system \\(A\\mathbf{x}=\\mathbf{0}\\) has only the trivial solution if and only if its coefficient matrix \\(A\\) is invertible.\n\nSystem (a)’s coefficient matrix (Example 4) is invertible. Thus, system (a) has only the trivial solution.\nSystem (b)’s coefficient matrix (Example 5) is NOT invertible. Thus, system (b) has nontrivial solutions.\n\n\nThis example directly shows the power of Theorem 1.5.3. We don’t even need to solve the systems explicitly. We can simply look at the invertibility of their coefficient matrices. For system (a), its coefficient matrix is the same one we successfully inverted in Example 4. Since it’s invertible, Theorem 1.5.3(a) implies Theorem 1.5.3(b) - the system \\(A\\mathbf{x}=\\mathbf{0}\\) has only the trivial solution. For system (b), its coefficient matrix is the singular matrix from Example 5. Since it’s not invertible, Theorem 1.5.3(a) is false, which means Theorem 1.5.3(b) is also false. Therefore, \\(A\\mathbf{x}=\\mathbf{0}\\) for system (b) must have non-trivial solutions. This is a very efficient way to determine the nature of solutions for homogeneous systems, which often arise in finding null spaces of linear transformations or analyzing stable states in dynamic systems in ECE."
  },
  {
    "objectID": "la-15.html#ece-applications-summary",
    "href": "la-15.html#ece-applications-summary",
    "title": "Linear Algebra",
    "section": "ECE Applications & Summary",
    "text": "ECE Applications & Summary\nKey Concepts for ECE:\n\nSystem Analysis: Inverting matrices is crucial for solving systems of linear equations that model electrical circuits, control systems, and communication networks.\nTransformations: Elementary matrices represent fundamental transformations that can be applied to data (e.g., in signal processing or computer graphics).\nInvertibility: Determines if a unique solution exists for a given engineering problem (e.g., unique current/voltage in a circuit, unique control input for a desired output)."
  },
  {
    "objectID": "la-15.html#ece-applications-summary-1",
    "href": "la-15.html#ece-applications-summary-1",
    "title": "Linear Algebra",
    "section": "ECE Applications & Summary",
    "text": "ECE Applications & Summary\nToday We Covered:\n\nElementary Row Operations and their matrix representation.\nThe definition and invertibility of Elementary Matrices.\nThe fundamental Equivalence Theorem linking invertibility, homogeneous solutions, RREF, and elementary matrices.\nA practical algorithm for finding \\(A^{-1}\\) using row operations.\nHow to identify non-invertible matrices during the inversion process.\n\n\nLet’s briefly tie back the theoretical concepts to practical ECE applications. The ability to solve systems of equations by inverting matrices is fundamental in circuit analysis to find unknown currents and voltages. In control systems, matrix inverses are used in state-space representations to design controllers. In signal processing, transformations often involve matrix multiplications. The concept of invertibility itself is critical: it tells us if a unique solution exists for a system of equations. For example, if a circuit’s matrix is singular, it might imply a redundant or ill-defined circuit.\nTo summarize, today we first revisited elementary row operations and introduced elementary matrices as their matrix multiplication equivalents. We proved that elementary matrices are always invertible. Then, we explored the powerful Equivalence Theorem, which unifies several core linear algebra concepts, providing deep insights into matrix properties. Finally, we learned and practiced a systematic algorithm to find matrix inverses and how to recognize when a matrix is not invertible. This forms a robust foundation for more advanced topics in matrices and linear transformations."
  },
  {
    "objectID": "la-17.html#diagonal-triangular-and-symmetric-matrices",
    "href": "la-17.html#diagonal-triangular-and-symmetric-matrices",
    "title": "Linear Algebra",
    "section": "1.7 Diagonal, Triangular, and Symmetric Matrices",
    "text": "1.7 Diagonal, Triangular, and Symmetric Matrices\nImron Rosyadi\n  Exploring special matrix forms critical for efficient computation and understanding system properties in ECE.\n\nGood morning everyone! Today, we’re going to dive into some special types of matrices: diagonal, triangular, and symmetric matrices. These matrices are not just theoretical curiosities; they show up constantly in various engineering applications, from signal processing and image compression to control systems and machine learning. Understanding their unique properties allows for more efficient computations and deeper insights into the systems they represent."
  },
  {
    "objectID": "la-17.html#diagonal-matrices",
    "href": "la-17.html#diagonal-matrices",
    "title": "Linear Algebra",
    "section": "Diagonal Matrices",
    "text": "Diagonal Matrices\nA diagonal matrix is a square matrix in which all the entries off the main diagonal are zero.\nExamples: \\[\n{\\left[\\begin{array}{l l}{2}&{0}\\\\ {0}&{-5}\\end{array}\\right]},\\quad{\\left[\\begin{array}{l l l}{1}&{0}&{0}\\\\ {0}&{1}&{0}\\\\ {0}&{0}&{1}\\end{array}\\right]},\\quad{\\left[\\begin{array}{l l l l}{6}&{0}&{0}&{0}\\\\ {0}&{-4}&{0}&{0}\\\\ {0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{8}\\end{array}\\right]}\n\\]\nA general \\(n \\times n\\) diagonal matrix \\(D\\) can be written as: \\[\nD = \\left[ \\begin{array}{c c c c}{d_{1}} & 0 & \\dots & 0\\\\ 0 & {d_{2}} & \\dots & 0\\\\ \\vdots & \\vdots & & \\vdots \\\\ 0 & 0 & \\dots & {d_{n}} \\end{array} \\right]\n\\]\n\nDiagonal matrices are perhaps the simplest form of matrices beyond the identity matrix. They only have non-zero entries on the main diagonal. They are incredibly common in ECE applications. For instance, in digital signal processing, if you’re scaling different frequency components independently, this can often be represented by multiplication with a diagonal matrix. Similarly, in circuit analysis, if components are perfectly decoupled, their impedance matrix might be diagonal."
  },
  {
    "objectID": "la-17.html#diagonal-matrices-inverses-and-powers",
    "href": "la-17.html#diagonal-matrices-inverses-and-powers",
    "title": "Linear Algebra",
    "section": "Diagonal Matrices: Inverses and Powers",
    "text": "Diagonal Matrices: Inverses and Powers\nIf \\(D = \\text{diag}(d_1, d_2, \\ldots, d_n)\\) is a diagonal matrix:\n\nInverse: \\(D\\) is invertible if and only if all diagonal entries \\(d_i\\) are nonzero. In this case, \\[\nD^{-1} = \\left[ \\begin{array}{c c c c}{1/d_{1}} & 0 & \\dots & 0\\\\ 0 & {1/d_{2}} & \\dots & 0\\\\ \\vdots & \\vdots & & \\vdots \\\\ 0 & 0 & \\dots & {1/d_{n}} \\end{array} \\right]\n\\]\nPowers: For any positive integer \\(k\\), \\[\nD^{k} = \\left[ \\begin{array}{c c c c}{d_{1}^{k}} & 0 & \\dots & 0\\\\ 0 & {d_{2}^{k}} & \\dots & 0\\\\ \\vdots & \\vdots & & \\vdots \\\\ 0 & 0 & \\dots & {d_{n}^{k}} \\end{array} \\right]\n\\] This property also extends to negative powers if \\(D\\) is invertible (\\(D^{-k} = (D^{-1})^k\\)).\n\n\nOne of the greatest advantages of diagonal matrices is how simple their inverses and powers are to compute. You simply invert or raise each diagonal element to the desired power. There’s no complex matrix multiplication involved. This is why transforming matrices into diagonal or near-diagonal forms, like in eigenvalue decomposition, is such a powerful technique in numerical linear algebra and various areas of ECE for simplifying complex problems."
  },
  {
    "objectID": "la-17.html#example-1-inverses-and-powers-of-diagonal-matrices",
    "href": "la-17.html#example-1-inverses-and-powers-of-diagonal-matrices",
    "title": "Linear Algebra",
    "section": "Example 1: Inverses and Powers of Diagonal Matrices",
    "text": "Example 1: Inverses and Powers of Diagonal Matrices\nGiven \\(A = \\left[ \\begin{array}{ccc}1 & 0 & 0 \\\\ 0 & -3 & 0 \\\\ 0 & 0 & 2 \\end{array} \\right]\\), compute \\(A^{-1}, A^5, A^{-5}\\).\n\n\n\n\n\n\nThe computations are straightforward due to the diagonal structure.\n\nLet’s see this in action using Python. For diagonal matrices, numpy.diag is very useful. To get the diagonal elements of A, we use np.diag(A). To construct a diagonal matrix from an array of diagonal elements, we use np.diag(array). You can see how easy it is to perform these operations, confirming the formulas for inverse and powers. Compare this to the effort of inverting or raising a full matrix to a power. This efficiency makes diagonal matrices highly desirable in computational applications like image processing filters, where specific channels or features can be scaled independently."
  },
  {
    "objectID": "la-17.html#diagonal-matrices-matrix-products",
    "href": "la-17.html#diagonal-matrices-matrix-products",
    "title": "Linear Algebra",
    "section": "Diagonal Matrices: Matrix Products",
    "text": "Diagonal Matrices: Matrix Products\nMatrix products involving diagonal factors are especially easy to compute:\n\nMultiplying on the left by \\(D\\) (scales rows): \\[\n\\left[ \\begin{array}{ccc}d_{1} & 0 & 0 \\\\ 0 & d_{2} & 0 \\\\ 0 & 0 & d_{3} \\end{array} \\right]\\left[ \\begin{array}{cccc}a_{11} & a_{12} & a_{13} & a_{14} \\\\ a_{21} & a_{22} & a_{23} & a_{24} \\\\ a_{31} & a_{32} & a_{33} & a_{34} \\end{array} \\right] = \\left[ \\begin{array}{cccc}d_{1}a_{11} & d_{1}a_{12} & d_{1}a_{13} & d_{1}a_{14} \\\\ d_{2}a_{21} & d_{2}a_{22} & d_{2}a_{23} & d_{2}a_{24} \\\\ d_{3}a_{31} & d_{3}a_{32} & d_{3}a_{33} & d_{3}a_{34} \\end{array} \\right]\n\\] (Multiply successive rows of \\(A\\) by successive diagonal entries of \\(D\\).)\nMultiplying on the right by \\(D\\) (scales columns): \\[\n\\left[ \\begin{array}{ccc}a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\\\ a_{41} & a_{42} & a_{43} \\end{array} \\right]\\left[ \\begin{array}{ccc}d_{1} & 0 & 0 \\\\ 0 & d_{2} & 0 \\\\ 0 & 0 & d_{3} \\end{array} \\right] = \\left[ \\begin{array}{ccc}d_{1}a_{11} & d_{2}a_{12} & d_{3}a_{13} \\\\ d_{1}a_{21} & d_{2}a_{22} & d_{3}a_{23} \\\\ d_{1}a_{31} & d_{2}a_{32} & d_{3}a_{33} \\\\ d_{1}a_{41} & d_{2}a_{42} & d_{3}a_{43} \\end{array} \\right]\n\\] (Multiply successive columns of \\(A\\) by successive diagonal entries of \\(D\\).)\n\n\nThese properties are incredibly useful for constructing and manipulating matrices in certain applications. When a diagonal matrix multiplies another matrix from the left, it scales the rows of that matrix. This is common in sensor data normalization or signal amplification. When it multiplies from the right, it scales the columns, which might be seen in adjusting gains for different features or parameters."
  },
  {
    "objectID": "la-17.html#diagonal-matrix-multiplication-example",
    "href": "la-17.html#diagonal-matrix-multiplication-example",
    "title": "Linear Algebra",
    "section": "Diagonal Matrix Multiplication Example",
    "text": "Diagonal Matrix Multiplication Example\nLet’s confirm the column-scaling property with Pyodide.\n\n\n\n\n\n\nObserve how each column of \\(A\\) is scaled by the corresponding diagonal entry of \\(D\\).\n\nLet’s verify the column-scaling property using Python’s numpy library. We define a sample matrix A and a diagonal matrix D. When we perform matrix multiplication A @ D, you can clearly see that the first column of A is multiplied by 2, the second by 0.5, and the third by 3, which are the diagonal entries of D. This direct and simple scaling behavior is very valuable when you need to apply different weights or factors to different features or variables represented by columns in your data."
  },
  {
    "objectID": "la-17.html#triangular-matrices",
    "href": "la-17.html#triangular-matrices",
    "title": "Linear Algebra",
    "section": "Triangular Matrices",
    "text": "Triangular Matrices\nA square matrix is triangular if all entries above the main diagonal are zero (lower triangular) or all entries below the main diagonal are zero (upper triangular).\n\nLower Triangular: \\(a_{ij} = 0\\) if \\(i &lt; j\\) (entries above diagonal are zero).\nUpper Triangular: \\(a_{ij} = 0\\) if \\(i &gt; j\\) (entries below diagonal are zero).\n\nExample 2: Upper and Lower Triangular Matrices\n\n\n\n\nTriangular matrices are fundamental in linear algebra because they naturally arise during Gaussian elimination and LU decomposition, which are workhorses for solving linear systems and performing matrix factorization. They are also prevalent in areas like circuit simulation and numerical analysis. The image shows clear examples of both upper and lower triangular matrices. Notice how all the non-zero elements are confined to either the upper or lower triangle, including the diagonal."
  },
  {
    "objectID": "la-17.html#properties-of-triangular-matrices-theorem-1.7.1",
    "href": "la-17.html#properties-of-triangular-matrices-theorem-1.7.1",
    "title": "Linear Algebra",
    "section": "Properties of Triangular Matrices (Theorem 1.7.1)",
    "text": "Properties of Triangular Matrices (Theorem 1.7.1)\n\nThe transpose of a lower triangular matrix is upper triangular, and vice versa.\nThe product of lower triangular matrices is lower triangular, and the product of upper triangular matrices is upper triangular.\nA triangular matrix is invertible if and only if its diagonal entries are all nonzero.\nThe inverse of an invertible lower triangular matrix is lower triangular, and the inverse of an invertible upper triangular matrix is upper triangular.\n\nWhy these matter in ECE:\n\nLU decomposition (e.g., \\(A=LU\\)) is a critical tool for solving systems efficiently, where L and U are triangular. The product property ensures this structure is maintained.\nProvides a quick check for invertibility without full row reduction. This helps identify if a system represented by a triangular matrix has a unique solution.\nInverse computations for triangular matrices are significantly simpler than for general matrices, making them computationally attractive.\n\n\nThese properties make triangular matrices very practical. Property (b) is crucial for justifying why LU decomposition works, allowing us to break down complex matrix problems into simpler, triangular ones. Property (c) offers a very fast way to check if a matrix is invertible. For instance, in a circuit simulation if you end up with a triangular system matrix, you can instantly tell if a unique solution exists by just checking the diagonal elements. Property (d) means that when you invert a triangular matrix, you don’t lose that useful sparsity, which speeds up further calculations."
  },
  {
    "objectID": "la-17.html#example-3-computations-with-triangular-matrices",
    "href": "la-17.html#example-3-computations-with-triangular-matrices",
    "title": "Linear Algebra",
    "section": "Example 3: Computations with Triangular Matrices",
    "text": "Example 3: Computations with Triangular Matrices\nConsider the upper triangular matrices: \\[\nA = \\left[ \\begin{array}{rrr}1 & 3 & -1 \\\\ 0 & 2 & 4 \\\\ 0 & 0 & 5 \\end{array} \\right], \\quad B = \\left[ \\begin{array}{rrr}3 & -2 & 2 \\\\ 0 & 0 & -1 \\\\ 0 & 0 & 1 \\end{array} \\right]\n\\] From Theorem 1.7.1(c):\n\n\\(A\\) is invertible because its diagonal entries (1, 2, 5) are all nonzero.\n\\(B\\) is not invertible because one of its diagonal entries (0) is zero.\n\nTheorem 1.7.1 also tells us that \\(A^{-1}\\), \\(AB\\), and \\(BA\\) are upper triangular. \\[\nA^{-1} = \\left[ \\begin{array}{rrr}1 & -\\frac{3}{2} & -\\frac{3}{5} \\\\ 0 & \\frac{1}{2} & -\\frac{2}{5} \\\\ 0 & 0 & \\frac{1}{5} \\end{array} \\right] \\quad AB = \\left[ \\begin{array}{rrr}3 & -2 & -2 \\\\ 0 & 0 & 2 \\\\ 0 & 0 & 5 \\end{array} \\right] \\quad BA = \\left[ \\begin{array}{rrr}3 & 5 & -1 \\\\ 0 & 0 & -5 \\\\ 0 & 0 & 5 \\end{array} \\right]\n\\]\n\nLet’s see these properties in action. Matrix A is upper triangular, and all its diagonal elements are non-zero, so it is invertible. Matrix B also upper triangular, but has a zero on its diagonal (the middle element is 0), so it is not invertible. Observe the products AB and BA. Both are still upper triangular, confirming property (b). Also, the inverse of A is upper triangular, as stated by property (d). This shows how the triangular structure is preserved through these operations."
  },
  {
    "objectID": "la-17.html#symmetric-matrices",
    "href": "la-17.html#symmetric-matrices",
    "title": "Linear Algebra",
    "section": "Symmetric Matrices",
    "text": "Symmetric Matrices\nA square matrix \\(A\\) is said to be symmetric if \\(A = A^T\\). This means that for all \\(i, j\\), the entry \\((A)_{ij} = (A)_{ji}\\).\nExample 4: Symmetric Matrices \\[\n\\left[ \\begin{array}{rr}7 & -3 \\\\ -3 & 5 \\end{array} \\right], \\quad \\left[ \\begin{array}{rrr}1 & 4 & 5 \\\\ 4 & -3 & 0 \\\\ 5 & 0 & 7 \\end{array} \\right], \\quad \\left[ \\begin{array}{llll}d_1 & 0 & 0 & 0 \\\\ 0 & d_2 & 0 & 0 \\\\ 0 & 0 & d_3 & 0 \\\\ 0 & 0 & 0 & d_4 \\end{array} \\right]\n\\]\nYou can recognize a symmetric matrix by inspection: entries are mirrored across the main diagonal.\n\nSymmetric matrices are extremely common in engineering and physics, especially when dealing with concepts like energy, variance, or relationships where the interaction between two elements is the same regardless of the order. For example, covariance matrices in statistics, stiffness matrices in structural analysis, and adjacency matrices in graphs are often symmetric. The defining characteristic is easy to check visually: elements across the main diagonal are identical."
  },
  {
    "objectID": "la-17.html#properties-of-symmetric-matrices-theorem-1.7.2",
    "href": "la-17.html#properties-of-symmetric-matrices-theorem-1.7.2",
    "title": "Linear Algebra",
    "section": "Properties of Symmetric Matrices (Theorem 1.7.2)",
    "text": "Properties of Symmetric Matrices (Theorem 1.7.2)\nIf \\(A\\) and \\(B\\) are symmetric matrices of the same size, and \\(k\\) is any scalar, then:\n\n\\(A^T\\) is symmetric (\\(A^T = A\\)).\n\\(A + B\\) and \\(A - B\\) are symmetric.\n\\(kA\\) is symmetric.\n\nImportant Note: The product of two symmetric matrices is not necessarily symmetric. \\((AB)^T = B^T A^T = BA\\). So, \\((AB)^T = AB\\) if and only if \\(AB = BA\\) (i.e., \\(A\\) and \\(B\\) commute).\nTHEOREM 1.7.3: The product of two symmetric matrices is symmetric if and only if the matrices commute.\n\nThese basic properties of symmetric matrices are straightforward consequences of the definition of symmetry and matrix operations. However, the product property is crucial: unlike sums or scalar multiples, the product of two symmetric matrices is not always symmetric. This is a common pitfall. The product is symmetric only if the matrices commute, which means their order of multiplication doesn’t matter. This has implications when modeling sequential operations—if the order matters, the combined effect might not inherit the symmetry of its parts."
  },
  {
    "objectID": "la-17.html#example-5-products-of-symmetric-matrices",
    "href": "la-17.html#example-5-products-of-symmetric-matrices",
    "title": "Linear Algebra",
    "section": "Example 5: Products of Symmetric Matrices",
    "text": "Example 5: Products of Symmetric Matrices\nLet’s confirm Theorem 1.7.3. Matrices \\(A = \\left[ \\begin{array}{rr}1 & 2 \\\\ 2 & 3 \\end{array} \\right]\\) (Symmetric)\n\nCase 1: Product NOT symmetric.\n\\(B = \\left[ \\begin{array}{rr}-4 & 1 \\\\ 1 & 0 \\end{array} \\right]\\) (Symmetric)\n\\(AB = \\left[ \\begin{array}{rr}-2 & 1 \\\\ -5 & 2 \\end{array} \\right]\\) (NOT Symmetric)\nConclusion: \\(A\\) and \\(B\\) do not commute.\nCase 2: Product IS symmetric.\n\\(C = \\left[ \\begin{array}{rr}-4 & 3 \\\\ 3 & -1 \\end{array} \\right]\\) (Symmetric)\n\\(AC = \\left[ \\begin{array}{rr}2 & 1 \\\\ 1 & 3 \\end{array} \\right]\\) (IS Symmetric)\nConclusion: \\(A\\) and \\(C\\) commute.\n\n\nTake matrix A, which is symmetric. In the first case, we multiply A by B. Both are symmetric. But their product AB is not symmetric. This tells us immediately, without computing BA, that A and B do not commute. In the second case, we multiply A by C. Both are symmetric. Their product AC is symmetric. This implies that A and C must commute. Let’s verify this numerically."
  },
  {
    "objectID": "la-17.html#example-5-python-verification",
    "href": "la-17.html#example-5-python-verification",
    "title": "Linear Algebra",
    "section": "Example 5: Python Verification",
    "text": "Example 5: Python Verification\nLet’s compute the products and check for symmetry.\n\n\n\n\n\n\nThe output confirms that symmetric products only occur when matrices commute.\n\nHere’s the live compute to prove the point. We define A, B, and C. First, we compute AB and BA. You’ll see AB is not symmetric, and AB is not equal to BA (they don’t commute). Second, we compute AC and CA. You’ll see AC is symmetric, and AC is indeed equal to CA (they commute). This perfectly illustrates Theorem 1.7.3. It emphasizes that while individual components might be symmetric, their combined effect is only symmetric if the processes commute. This is a subtle but important point in systems modeling."
  },
  {
    "objectID": "la-17.html#invertibility-of-symmetric-matrices",
    "href": "la-17.html#invertibility-of-symmetric-matrices",
    "title": "Linear Algebra",
    "section": "Invertibility of Symmetric Matrices",
    "text": "Invertibility of Symmetric Matrices\nTHEOREM 1.7.4: If \\(A\\) is an invertible symmetric matrix, then \\(A^{-1}\\) is symmetric.\nProof Idea:\nIf \\(A\\) is symmetric, \\(A = A^T\\).\nIf \\(A\\) is invertible, \\((A^{-1})^T = (A^T)^{-1}\\) (Property of Transpose).\nSubstituting \\(A^T=A\\): \\((A^{-1})^T = A^{-1}\\).\nThis means \\(A^{-1}\\) is symmetric.\nProducts \\(AA^T\\) and \\(A^TA\\) are Symmetric:\nFor any \\(m \\times n\\) matrix \\(A\\), the products \\(AA^T\\) (size \\(m \\times m\\)) and \\(A^TA\\) (size \\(n \\times n\\)) are always symmetric.\nProof:\n\\((AA^T)^T = (A^T)^T A^T = A A^T\\).\n\\((A^TA)^T = A^T (A^T)^T = A^T A\\).\n\nTheorem 1.7.4 is a comforting result: if a symmetric matrix has an inverse, that inverse also preserves its symmetry. This is very useful. For example, inverting a covariance matrix doesn’t change its symmetric properties, which is fortunate since covariance matrices are inherently symmetric. The products “\\(AA^T\\)” and “\\(A^TA\\)” are particularly important in applications like least squares approximation, principal component analysis (PCA), and in creating positive semi-definite matrices. The fact that these products are always symmetric, regardless of whether A itself is square or symmetric, is a powerful and very useful property."
  },
  {
    "objectID": "la-17.html#example-6-the-product-of-a-matrix-and-its-transpose-is-symmetric",
    "href": "la-17.html#example-6-the-product-of-a-matrix-and-its-transpose-is-symmetric",
    "title": "Linear Algebra",
    "section": "Example 6: The Product of a Matrix and Its Transpose Is Symmetric",
    "text": "Example 6: The Product of a Matrix and Its Transpose Is Symmetric\nLet \\(A\\) be the \\(2 \\times 3\\) matrix: \\[\nA = \\left[ \\begin{array}{rrr}1 & -2 & 4 \\\\ 3 & 0 & -5 \\end{array} \\right]\n\\] Compute \\(A^TA\\) and \\(AA^T\\):\n\n\n\n\n\n\nObserve that \\(A^TA\\) and \\(AA^T\\) are symmetric, as expected.\n\nLet’s confirm this using our Pyodide environment. We define a non-square matrix A. Then, we compute A transpose A and A A transpose. As you can see, even though A is not symmetric itself (it’s not even square), both of these products turn out to be symmetric. This property is heavily utilized in optimization, data analysis, and statistical modeling in ECE, as it guarantees certain positive definite properties and simplifies numerical algorithms."
  },
  {
    "objectID": "la-17.html#theorem-1.7.5-invertibility-of-aat-and-ata",
    "href": "la-17.html#theorem-1.7.5-invertibility-of-aat-and-ata",
    "title": "Linear Algebra",
    "section": "Theorem 1.7.5: Invertibility of \\(AA^T\\) and \\(A^TA\\)",
    "text": "Theorem 1.7.5: Invertibility of \\(AA^T\\) and \\(A^TA\\)\nIf \\(A\\) is an invertible matrix (and thus square), then \\(AA^T\\) and \\(A^TA\\) are also invertible.\nProof:\nSince \\(A\\) is invertible, \\(A^T\\) is also invertible (by Theorem 1.4.9).\nSince \\(AA^T\\) and \\(A^TA\\) are products of invertible matrices, they are themselves invertible (by Theorem 1.6.5).\nThis theorem is particularly useful in areas where invertibility is crucial, such as unique solutions in least squares problems or system stability analysis.\n\nThis theorem gives us even more confidence in these special products. If our original matrix \\(A\\) is invertible, then not only are \\(AA^T\\) and \\(A^TA\\) symmetric, but they are also guaranteed to be invertible. This is important because it ensures that operations involving these products, like finding inverses in optimization problems or solving associated linear systems, will lead to unique solutions. It leverages our prior theorems about the invertibility of transposes and products of invertible matrices."
  },
  {
    "objectID": "la-17.html#working-with-technology-block-diagonal-matrices",
    "href": "la-17.html#working-with-technology-block-diagonal-matrices",
    "title": "Linear Algebra",
    "section": "Working with Technology: Block Diagonal Matrices",
    "text": "Working with Technology: Block Diagonal Matrices\nA block diagonal matrix has square matrices (blocks) on its main diagonal and zero matrices elsewhere. Example: \\[\n\\left[ \\begin{array}{cc}D_{1} & 0 \\\\ 0 & D_{2} \\end{array} \\right]\n\\] where \\(D_1, D_2\\) are square matrices and \\(0\\) represents zero matrices of appropriate sizes.\nTask: If \\(D_1\\) and \\(D_2\\) are invertible, derive a formula for the inverse of this block diagonal matrix.\nSolution: The inverse is given by \\[\n\\left[ \\begin{array}{cc}D_{1}^{-1} & 0 \\\\ 0 & D_{2}^{-1} \\end{array} \\right]\n\\] This can be verified by multiplying the original matrix by this proposed inverse: \\[\n\\left[ \\begin{array}{cc}D_{1} & 0 \\\\ 0 & D_{2} \\end{array} \\right] \\left[ \\begin{array}{cc}D_{1}^{-1} & 0 \\\\ 0 & D_{2}^{-1} \\end{array} \\right] = \\left[ \\begin{array}{cc}D_{1}D_{1}^{-1} & D_{1}0 + 0D_{2}^{-1} \\\\ 0D_{1}^{-1} + D_{2}0 & D_{2}D_{2}^{-1} \\end{array} \\right] = \\left[ \\begin{array}{cc}I & 0 \\\\ 0 & I \\end{array} \\right]\n\\] This property greatly simplifies computations for systems represented by such matrices.\n\nBlock diagonal matrices are essentially a generalization of diagonal matrices. They appear in scenarios like decomposing a large, complex system into smaller, independent subsystems. The beauty of them is that inverting them is almost as easy as inverting a simple diagonal matrix: you just invert the individual blocks. This property significantly speeds up complex calculations in large-scale system analysis, such as in parallel processing of independent sensor data or in modeling modular control systems."
  },
  {
    "objectID": "la-17.html#working-with-technology-example-inverse-of-block-diagonal-matrix",
    "href": "la-17.html#working-with-technology-example-inverse-of-block-diagonal-matrix",
    "title": "Linear Algebra",
    "section": "Working with Technology Example: Inverse of Block Diagonal Matrix",
    "text": "Working with Technology Example: Inverse of Block Diagonal Matrix\nCompute the inverse of \\(M = \\left[ \\begin{array}{ccccc}1.24 & 2.37 & 0 & 0 \\\\ 3.08 & -1.01 & 0 & 0 \\\\ 0 & 0 & 2.76 & 4.92 \\\\ 0 & 0 & 3.23 & 5.54 \\end{array} \\right]\\)\nThis is a block diagonal matrix, with:\n\\(D_1 = \\left[ \\begin{array}{cc}1.24 & 2.37 \\\\ 3.08 & -1.01 \\end{array} \\right]\\)\n\\(D_2 = \\left[ \\begin{array}{cc}2.76 & 4.92 \\\\ 3.23 & 5.54 \\end{array} \\right]\\)\n\n\n\n\n\n\nThe inverse of \\(M\\) is formed by inverting its diagonal blocks.\n\nLet’s apply the block diagonal inverse formula to a specific example. Here, matrix M is a 4x4 block diagonal matrix composed of two 2x2 blocks, D1 and D2. In Python, we can easily extract these blocks using slicing. Then, we compute the inverse of each block independently using np.linalg.inv. Finally, we reassemble these inverse blocks into the overall inverse of M. This process is much faster and numerically more stable than trying to invert the entire 4x4 matrix, especially for very large, sparse block diagonal matrices that arise in big data or complex system modeling."
  },
  {
    "objectID": "la-17.html#ece-applications-summary",
    "href": "la-17.html#ece-applications-summary",
    "title": "Linear Algebra",
    "section": "ECE Applications & Summary",
    "text": "ECE Applications & Summary\nKey Concepts for ECE:\n\nComputational Efficiency: Diagonal and triangular matrices allow for significantly faster matrix inversions, powers, and multiplications. This is critical for real-time systems and large-scale simulations (e.g., in VLSI design, power system analysis).\nSystem Decomposition: Block diagonal matrices represent independent subsystems, simplifying analysis and parallel processing (e.g., modular control designs).\nProperties in Data Analysis: Symmetric matrices are fundamental in statistics (covariance matrices), optimization (Hessian matrices), and machine learning (kernel matrices, graph representations). Their properties (e.g., real eigenvalues, orthogonal eigenvectors) are heavily utilized.\nMatrix Factorizations: LU, Cholesky, and Spectral Decompositions leverage these special forms to solve complex problems in signal processing, communications, and controls."
  },
  {
    "objectID": "la-17.html#ece-applications-summary-1",
    "href": "la-17.html#ece-applications-summary-1",
    "title": "Linear Algebra",
    "section": "ECE Applications & Summary",
    "text": "ECE Applications & Summary\nToday We Covered:\n\nDefinitions and properties of Diagonal, Triangular, and Symmetric matrices.\nSimple rules for computing inverses and powers of diagonal matrices.\nBehavior of products involving these special matrices.\nConditions for invertibility and symmetry of products.\nIntroduction to Block Diagonal Matrices and their efficient inversion.\n\n\nTo summarize, understanding diagonal, triangular, and symmetric matrices goes beyond mere definitions. These special forms are computational workhorses in ECE. Their inherent structure allows for significant gains in computational efficiency, which is vital for designing real-time systems, simulating complex networks, and processing large datasets. They underpin powerful techniques like matrix factorizations, which break down daunting problems into manageable, structured components. From filters in digital signal processing to covariance estimation in communications, and numerical solvers in circuit analysis, these matrices are indispensable tools for every engineer."
  },
  {
    "objectID": "la-19.html#applications-of-linear-systems",
    "href": "la-19.html#applications-of-linear-systems",
    "title": "Linear Algebra",
    "section": "1.9 Applications of Linear Systems",
    "text": "1.9 Applications of Linear Systems\nImron Rosyadi\n  Unlocking the real-world power of linear systems in Network Analysis, Chemical Balancing, Polynomial Interpolation, and Economic Modeling.\n\nGood morning everyone! Until now, we’ve focused on the mechanics of linear systems, understanding how to solve them and exploring properties of matrices. Today, we’re transitioning to why this is all so important. We’ll explore diverse, real-world applications where systems of linear equations provide the fundamental tools for analysis and problem-solving, particularly relevant to Electrical and Computer Engineering. We’ll cover network analysis including electrical circuits, balancing chemical equations, polynomial interpolation, and even touch upon economic input-output models."
  },
  {
    "objectID": "la-19.html#network-analysis-core-principle",
    "href": "la-19.html#network-analysis-core-principle",
    "title": "Linear Algebra",
    "section": "Network Analysis: Core Principle",
    "text": "Network Analysis: Core Principle\nA network is a set of branches through which something “flows”. Branches meet at nodes or junctions.\n\nExamples: Electrical wires, pipes, traffic lanes, financial flows.\nFlow Rate: Measured in units like amperes (electricity), gallons/minute (water), vehicles/hour (traffic).\nCore Principle: Flow Conservation\n\nAt each node, rate of flow into the node = rate of flow out of the node.\nThis prevents buildup and ensures free movement.\n\n\nA common problem is to use known flow rates in some branches to find rates in all branches.\n\nNetworks are everywhere in ECE. Think of power grids, communication networks, or even data flow in a computer system. The fundamental principle governing most networks is flow conservation. Whatever amount of “stuff” enters a junction must also leave it. This simple rule translates directly into linear equations, forming the basis for analyzing complex systems. This is the first step in formulating many engineering problems as linear systems."
  },
  {
    "objectID": "la-19.html#example-1-simple-network-analysis",
    "href": "la-19.html#example-1-simple-network-analysis",
    "title": "Linear Algebra",
    "section": "Example 1: Simple Network Analysis",
    "text": "Example 1: Simple Network Analysis\nFind the flow rates and directions in the remaining branches.\n\n\n\n\n\nArbitrarily assign directions for unknown flows \\(x_1, x_2, x_3\\).\n\nEquations from Flow Conservation:\n\nNode A: \\(x_1 + x_2 = 30\\)\nNode B: \\(x_2 + x_3 = 35\\)\nNode C: \\(x_3 + 15 = 60 \\implies x_3 = 45\\)\nNode D: \\(x_1 + 15 = 55 \\implies x_1 = 40\\)\n\nLinear System: \\[\n\\begin{array}{r}x_1 + x_2 \\qquad = 30\\\\ \\qquad x_2 + x_3 = 35\\\\ x_3 = 45\\\\ x_1 = 40\\end{array}\n\\]\n\n\nHere, we have a simple network with four nodes and known incoming/outgoing flows at its boundaries. The unknown internal flows are \\(x_1, x_2, x_3\\). The first step is to arbitrarily assign directions to these unknowns. If our assumed direction is wrong, the solved value will simply be negative, indicating the actual flow is in the opposite direction. Then, for each node, we apply the conservation of flow rule: flow in equals flow out. This gives us a system of linear equations. This system is straightforward enough to be solved by inspection, starting from the bottom equations."
  },
  {
    "objectID": "la-19.html#example-1-solving-the-network-system",
    "href": "la-19.html#example-1-solving-the-network-system",
    "title": "Linear Algebra",
    "section": "Example 1: Solving the Network System",
    "text": "Example 1: Solving the Network System\nLet’s solve the system using Python.\nSystem: \\(x_1 + x_2 = 30\\)\n\\(x_2 + x_3 = 35\\)\n\\(x_3 = 45\\)\n\\(x_1 = 40\\)\n\n\n\n\n\n\nSolution: \\(x_1 = 40, x_2 = -10, x_3 = 45\\).\nThe negative value for \\(x_2\\) indicates its actual flow direction is opposite to the one assumed.\n\nEven though this system can be solved by substitution easily, converting it to matrix form and solving numerically demonstrates the general approach for larger, more complex networks. Our solution shows \\(x_1=40\\), \\(x_3=45\\), and \\(x_2=-10\\). The negative sign for \\(x_2\\) is key: it tells us that the initial direction we assigned to \\(x_2\\) in the diagram was incorrect. The actual flow for \\(x_2\\) is into node A, not out of it. This highlights how linear systems automatically reveal the real-world conditions."
  },
  {
    "objectID": "la-19.html#example-2-design-of-traffic-patterns",
    "href": "la-19.html#example-2-design-of-traffic-patterns",
    "title": "Linear Algebra",
    "section": "Example 2: Design of Traffic Patterns",
    "text": "Example 2: Design of Traffic Patterns\nAnalyzing traffic flow around a new park in Philadelphia.\n\n\n\nPart (a): Traffic Light Setting\nTotal Flow In = Total Flow Out\n\\(500 + 400 + 600 + 200 = x + 700 + 400\\)\n\\(1700 = x + 1100\\)\n\\(x = 600\\) vehicles/hour."
  },
  {
    "objectID": "la-19.html#example-2-design-of-traffic-patterns-1",
    "href": "la-19.html#example-2-design-of-traffic-patterns-1",
    "title": "Linear Algebra",
    "section": "Example 2: Design of Traffic Patterns",
    "text": "Example 2: Design of Traffic Patterns\nPart (b): Streets’ Flow Rates\nWith \\(x = 600\\), apply flow conservation at each intersection (A, B, C, D):\n\nA: \\(400 + 600 = x_1 + x_2 \\implies x_1 + x_2 = 1000\\)\nB: \\(x_2 + x_3 = 400 + x \\implies x_2 + x_3 = 1000\\)\nC: \\(500 + 200 = x_3 + x_4 \\implies x_3 + x_4 = 700\\)\nD: \\(x_1 + x_4 = 700\\)\n\nLinear System: \\[\n\\begin{array}{r l l}x_1 + x_2 \\qquad & = 1000\\\\ \\qquad x_2 + x_3 & = 1000\\\\ \\qquad x_3 + x_4 &= 700\\\\ x_1 \\qquad + x_4 &= 700 \\end{array}\n\\]\n\nTraffic management is a crucial area where linear systems are applied to prevent congestion. We first determine the necessary traffic light setting to balance total incoming and outgoing flow for the entire complex. Once that’s set, we then apply the flow conservation principle to each internal intersection. This creates a more involved system of linear equations for the flow rates on the internal streets (\\(x_1, x_2, x_3, x_4\\))."
  },
  {
    "objectID": "la-19.html#example-2-solving-the-traffic-flow-system",
    "href": "la-19.html#example-2-solving-the-traffic-flow-system",
    "title": "Linear Algebra",
    "section": "Example 2: Solving the Traffic Flow System",
    "text": "Example 2: Solving the Traffic Flow System\n\n\n\n\n\n\nThis system has infinitely many solutions, given by parametric equations. Physical constraints (e.g., non-negative flow) limit the parameter \\(t\\).\n\nWhen we set up and solve the traffic flow system, we find that it has infinitely many solutions, expressible in terms of a parameter ‘t’. This means there isn’t a single unique way to distribute the traffic flow, which can actually be a good thing for traffic engineers as it provides flexibility. However, these solutions aren’t entirely arbitrary. Physical constraints, like one-way streets implying non-negative flow rates, impose limits on what ‘t’ can be. Here, ‘t’ must be between 0 and 700, which then defines the range of average flow rates for each street. This demonstrates how linear systems, combined with real-world constraints, help in designing adaptable systems."
  },
  {
    "objectID": "la-19.html#network-analysis-electrical-circuits",
    "href": "la-19.html#network-analysis-electrical-circuits",
    "title": "Linear Algebra",
    "section": "Network Analysis: Electrical Circuits",
    "text": "Network Analysis: Electrical Circuits\nLinear systems are fundamental to circuit analysis.\n\nOhm’s Law: Voltage drop \\(E = IR\\) (Current \\(\\times\\) Resistance).\nKirchhoff’s Current Law (KCL): Sum of currents entering a node = Sum of currents leaving a node. (Flow Conservation)\nKirchhoff’s Voltage Law (KVL): In any closed loop, sum of voltage rises = sum of voltage drops.\n\n\n \n\nConventions: For KVL, we often assume clockwise loops and define voltage rises/drops for batteries and resistors consistently.\n\nFor electrical and computer engineers, applying linear systems to circuits is a daily task. Ohm’s Law gives us the relationship between voltage, current, and resistance. But to analyze complex circuits, we primarily rely on Kirchhoff’s laws. KCL is a direct application of the flow conservation principle we just discussed, but now for electron flow (current). KVL relates all the voltage changes around any closed path in the circuit. By systematically applying these laws and following specific conventions for how we assign directions and rises/drops, we can formulate systems of linear equations to solve for unknown currents or voltages in a circuit."
  },
  {
    "objectID": "la-19.html#example-3-circuit-with-one-closed-loop",
    "href": "la-19.html#example-3-circuit-with-one-closed-loop",
    "title": "Linear Algebra",
    "section": "Example 3: Circuit with One Closed Loop",
    "text": "Example 3: Circuit with One Closed Loop\nDetermine the current \\(I\\).\n\n\n\n\nResistor (3 Ω): Current \\(I\\) flows in same direction as loop \\(\\implies\\) Voltage Drop \\(E_R = IR = 3I\\).\nBattery (6 V): Loop direction from \\(-\\) to \\(+\\) through battery \\(\\implies\\) Voltage Rise \\(E_{Batt} = 6\\)V.\n\nApplying KVL: Sum of voltage rises = Sum of voltage drops\n\\(6 = 3I\\)\n\\(I = 2\\) A.\nSince \\(I\\) is positive, the assigned direction is correct.\n\nIn the simplest case, a single closed loop, KVL helps us find the current directly. We have a 6V battery providing a voltage rise and a 3 Ohm resistor causing a voltage drop. According to KVL, these must balance. The equation \\(6 = 3I\\) is a simple linear equation that immediately gives us \\(I=2\\)A. The positive value confirms our assumed current direction. This is a fundamental building block for analyzing more complex multi-loop circuits."
  },
  {
    "objectID": "la-19.html#example-4-circuit-with-three-closed-loops",
    "href": "la-19.html#example-4-circuit-with-three-closed-loops",
    "title": "Linear Algebra",
    "section": "Example 4: Circuit with Three Closed Loops",
    "text": "Example 4: Circuit with Three Closed Loops\nDetermine the currents \\(I_1, I_2, I_3\\).\n\n\n\nKCL at Node A (or B): \\(I_1 + I_2 = I_3 \\implies I_1 + I_2 - I_3 = 0\\)\nKVL for Loops (clockwise traversal):\n\nLeft Inside Loop: \\(50 = 5I_1 + 20I_3\\)\nRight Inside Loop: \\(30 = 10I_2 + 20I_3\\)\n\nCombined Linear System: \\[\n\\begin{array}{r l l}I_1 + \\quad I_2 - \\quad I_3 & = \\quad 0\\\\ 5I_1 \\qquad + 20I_3 & = \\quad 50\\\\ \\qquad 10I_2 + 20I_3 & = -30\\end{array}\n\\]\n\nNow for a more complex circuit, which is common in many ECE devices. We have three unknown currents. First, KCL is applied at either node A or B. Both yield the same equation relating \\(I_1, I_2, I_3\\). Next, KVL is applied to two independent closed loops. We chose the left and right inner loops, defining voltage rises from batteries and drops across resistors using Ohm’s law. This systematic application results in a \\(3 \\times 3\\) system of linear equations. This system is what we’ll solve to determine the unknown currents."
  },
  {
    "objectID": "la-19.html#example-4-solving-the-circuit-system",
    "href": "la-19.html#example-4-solving-the-circuit-system",
    "title": "Linear Algebra",
    "section": "Example 4: Solving the Circuit System",
    "text": "Example 4: Solving the Circuit System\nLet’s solve for \\(I_1, I_2, I_3\\) using Python.\nSystem: \\(I_1 + I_2 - I_3 = 0\\)\n\\(5I_1 + 20I_3 = 50\\)\n\\(10I_2 + 20I_3 = -30\\)\n\n\n\n\n\n\nSolution: \\(I_1 = 6\\) A, \\(I_2 = -5\\) A, \\(I_3 = 1\\) A.\nThe negative value for \\(I_2\\) indicates its actual direction is opposite to the assumption.\n\nWe use numpy.linalg.solve for a quick and accurate solution. The results are \\(I_1=6\\)A, \\(I_2=-5\\)A, and \\(I_3=1\\)A. Again, the negative sign for \\(I_2\\) simply confirms that the assumed direction in the diagram was opposite to the actual flow. Linear algebra provides us with a robust method to solve even complex circuits, which is critical for design and troubleshooting in any electrical system."
  },
  {
    "objectID": "la-19.html#balancing-chemical-equations",
    "href": "la-19.html#balancing-chemical-equations",
    "title": "Linear Algebra",
    "section": "Balancing Chemical Equations",
    "text": "Balancing Chemical Equations\nChemical equations represent reactions where atoms rearrange. A balanced equation means the same number of atoms of each type appears on both sides of the equation.\nSystematic Method using Linear Systems:\n\nAssign unknown coefficients (\\(x_1, x_2, \\ldots\\)) to each molecule.\nFor each type of atom (e.g., C, H, O), set up an equation ensuring the number of atoms on the reactant side equals the number on the product side.\nThis forms a homogeneous linear system.\nSolve the system; the smallest positive integer solution for the coefficients is the balanced equation.\n\n\nBalancing chemical equations might seem like a chemistry problem, but it’s a perfect illustration of conservation, and thus, where linear systems are very useful. For complex reactions, trial and error is inefficient. Linear algebra provides a systematic, foolproof method. The core idea is that for each type of atom involved, their count must be conserved across the reaction. This translates directly into a system of homogeneous linear equations, as all terms are brought to one side to sum to zero."
  },
  {
    "objectID": "la-19.html#example-balancing-methane-combustion",
    "href": "la-19.html#example-balancing-methane-combustion",
    "title": "Linear Algebra",
    "section": "Example: Balancing Methane Combustion",
    "text": "Example: Balancing Methane Combustion\nBalance the equation: \\(\\mathrm{CH_4} + \\mathrm{O_2} \\longrightarrow \\mathrm{CO_2} + \\mathrm{H_2O}\\)\nAssign coefficients: \\(x_1(\\mathrm{CH_4}) + x_2(\\mathrm{O_2}) \\longrightarrow x_3(\\mathrm{CO_2}) + x_4(\\mathrm{H_2O})\\)\nAtom Balance Equations:\n\nCarbon (C): \\(x_1 = x_3 \\implies x_1 - x_3 = 0\\)\nHydrogen (H): \\(4x_1 = 2x_4 \\implies 4x_1 - 2x_4 = 0\\)\nOxygen (O): \\(2x_2 = 2x_3 + x_4 \\implies 2x_2 - 2x_3 - x_4 = 0\\)\n\nHomogeneous Linear System: \\[\n\\begin{array}{r}x_1 \\qquad - x_3 \\qquad \\quad = 0\\\\ 4x_1 \\qquad \\qquad - 2x_4 = 0\\\\ \\qquad 2x_2 - 2x_3 - x_4 = 0\\end{array}\n\\]\nAugmented Matrix: \\(\\left[ \\begin{array}{rrrrr}1 & 0 & -1 & 0 & 0\\\\ 4 & 0 & 0 & -2 & 0\\\\ 0 & 2 & -2 & -1 & 0 \\end{array} \\right]\\)\n\nThe first step is to assign variables, \\(x_1, x_2, x_3, x_4\\), to represent the number of molecules of each compound. Then, for each element (Carbon, Hydrogen, Oxygen), we write an equation ensuring the number of atoms is conserved. For example, for carbon, \\(x_1\\) carbon atoms on the left must equal \\(x_3\\) carbon atoms on the right. This yields a system of homogeneous linear equations."
  },
  {
    "objectID": "la-19.html#example-solving-for-methane-combustion",
    "href": "la-19.html#example-solving-for-methane-combustion",
    "title": "Linear Algebra",
    "section": "Example: Solving for Methane Combustion",
    "text": "Example: Solving for Methane Combustion\nLet’s solve the system:\n\\(x_1 - x_3 = 0\\)\n\\(4x_1 \\qquad - 2x_4 = 0\\)\n\\(2x_2 - 2x_3 - x_4 = 0\\)\n\n\n\n\n\n\nThe general solution is \\(x_1 = t/2, x_2 = t, x_3 = t/2, x_4 = t\\).\nSmallest positive integers occur at \\(t=2\\): \\(x_1=1, x_2=2, x_3=1, x_4=2\\).\nBalanced equation: \\(\\mathrm{CH_4} + 2\\mathrm{O_2} \\longrightarrow \\mathrm{CO_2} + 2\\mathrm{H_2O}\\).\n\nWe use numpy to represent the coefficient matrix. Solving homogeneous systems like \\(A\\mathbf{x}=\\mathbf{0}\\) often involves finding the null space. For this simple case, we can deduce the general solution \\(x_1=t/2, x_2=t, x_3=t/2, x_4=t\\). The smallest positive integer solution is obtained by choosing the smallest ‘t’ that makes all variables integers. Here, \\(t=2\\) gives the coefficients 1, 2, 1, 2. This systematic method is highly effective for more complex chemical equations where trial-and-error fails."
  },
  {
    "objectID": "la-19.html#example-balancing-mathrmhcl-mathrmna_3po_4",
    "href": "la-19.html#example-balancing-mathrmhcl-mathrmna_3po_4",
    "title": "Linear Algebra",
    "section": "Example: Balancing \\(\\mathrm{HCl} + \\mathrm{Na_3PO_4}\\)",
    "text": "Example: Balancing \\(\\mathrm{HCl} + \\mathrm{Na_3PO_4}\\)\nBalance: \\(\\mathrm{HCl} + \\mathrm{Na_3PO_4} \\longrightarrow \\mathrm{H_3PO_4} + \\mathrm{NaCl}\\)\nAssign coefficients: \\(x_1(\\mathrm{HCl}) + x_2(\\mathrm{Na_3PO_4}) \\longrightarrow x_3(\\mathrm{H_3PO_4}) + x_4(\\mathrm{NaCl})\\)\nAtom Balance Equations:\n\nH: \\(x_1 = 3x_3 \\implies x_1 - 3x_3 = 0\\)\nCl: \\(x_1 = x_4 \\implies x_1 - x_4 = 0\\)\nNa: \\(3x_2 = x_4 \\implies 3x_2 - x_4 = 0\\)\nP: \\(x_2 = x_3 \\implies x_2 - x_3 = 0\\)\nO: \\(4x_2 = 4x_3 \\implies 4x_2 - 4x_3 = 0\\)\n\nHomogeneous Linear System: \\[\n\\begin{array}{r l l l l l}x_1 \\qquad \\qquad - 3x_3 \\qquad \\quad &= 0\\\\ x_1 \\qquad \\qquad \\qquad - x_4 &= 0\\\\ \\qquad 3x_2 \\qquad \\qquad - x_4 &= 0\\\\ \\qquad x_2 - x_3 \\qquad \\quad &= 0\\\\ \\qquad 4x_2 - 4x_3 \\qquad \\quad &= 0\\end{array}\n\\]\n\nThis chemical equation is slightly more complex. Again, we assign coefficients and set up balance equations for each atom type (Hydrogen, Chlorine, Sodium, Phosphorus, Oxygen). Notice that the last two equations are dependent (\\(4x_2 - 4x_3 = 0\\) is a multiple of \\(x_2 - x_3 = 0\\)), so the system will have infinitely many solutions, and we’ll seek the smallest positive integer set."
  },
  {
    "objectID": "la-19.html#example-solving-for-mathrmhcl-mathrmna_3po_4",
    "href": "la-19.html#example-solving-for-mathrmhcl-mathrmna_3po_4",
    "title": "Linear Algebra",
    "section": "Example: Solving for \\(\\mathrm{HCl} + \\mathrm{Na_3PO_4}\\)",
    "text": "Example: Solving for \\(\\mathrm{HCl} + \\mathrm{Na_3PO_4}\\)\nSystem: \\(x_1 - 3x_3 = 0\\)\n\\(x_1 - x_4 = 0\\)\n\\(3x_2 - x_4 = 0\\)\n\\(x_2 - x_3 = 0\\)\n\\(4x_2 - 4x_3 = 0\\)\n\n\n\n\n\n\nGeneral solution: \\(x_1 = t, x_2 = t/3, x_3 = t/3, x_4 = t\\).\nSmallest positive integers occur at \\(t=3\\): \\(x_1=3, x_2=1, x_3=1, x_4=3\\).\nBalanced equation: \\(3\\mathrm{HCl} + \\mathrm{Na_3PO_4} \\longrightarrow \\mathrm{H_3PO_4} + 3\\mathrm{NaCl}\\).\n\nSimilar to the previous example, this system also leads to infinitely many solutions due to the dependency among equations. The general solution reveals that we need to choose ‘t’ as a multiple of 3 to obtain integer values for \\(x_2\\) and \\(x_3\\). The smallest such positive integer ‘t’ is 3, yielding the coefficients 3, 1, 1, 3. This systematic approach ensures accurate balancing for any chemical equation, no matter how complex."
  },
  {
    "objectID": "la-19.html#polynomial-interpolation",
    "href": "la-19.html#polynomial-interpolation",
    "title": "Linear Algebra",
    "section": "Polynomial Interpolation",
    "text": "Polynomial Interpolation\nProblem: Find a polynomial whose graph passes through a specified set of points \\((x_1, y_1), \\ldots, (x_n, y_n)\\).\nTHEOREM 1.9.1 (Polynomial Interpolation): Given any \\(n\\) points in the \\(xy\\)-plane with distinct \\(x\\)-coordinates, there is a unique polynomial of degree \\(n-1\\) or less whose graph passes through those points."
  },
  {
    "objectID": "la-19.html#polynomial-interpolation-1",
    "href": "la-19.html#polynomial-interpolation-1",
    "title": "Linear Algebra",
    "section": "Polynomial Interpolation",
    "text": "Polynomial Interpolation\nWe seek a polynomial \\(p(x) = a_0 + a_1x + a_2x^2 + \\dots + a_{n-1}x^{n-1}\\). Substituting each point \\((x_i, y_i)\\) into the polynomial gives a linear system for the coefficients \\(a_0, \\ldots, a_{n-1}\\): \\[\n\\begin{array}{r l} a_{0} + a_{1}x_{1} + a_{2}x_{1}^{2} + \\dots +a_{n - 1}x_{1}^{n - 1} &= y_{1}\\\\ \\vdots \\\\ a_{0} + a_{1}x_{n} + a_{2}x_{n}^{2} + \\dots +a_{n - 1}x_{n}^{n - 1} &= y_{n} \\end{array}\n\\] This system can be written with an augmented matrix: \\[\n\\left[{\\begin{array}{l l l l l l}{1}&{x_{1}}&{x_{1}^{2}}&{\\cdots}&{x_{1}^{n-1}}&{y_{1}}\\\\ {1}&{x_{2}}&{x_{2}^{2}}&{\\cdots}&{x_{2}^{n-1}}&{y_{2}}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&&{\\vdots}&{\\vdots}\\\\ {1}&{x_{n}}&{x_{n}^{2}}&{\\cdots}&{x_{n}^{n-1}}&{y_{n}}\\end{array}}\\right]\n\\] Solving this system yields the unique coefficients of the interpolating polynomial.\n\nPolynomial interpolation is a core concept in numerical analysis and engineering. Often, we don’t have a perfect formula for a function, but we have a set of data points. Interpolation allows us to find a polynomial that neatly passes through all these points. This polynomial can then be used to approximate function values between known points. The key insight is that the requirement for the polynomial to pass through N points translates directly into a system of N linear equations for the N unknown coefficients of the polynomial. The augmented matrix of this system is related to the Vandermonde matrix, which we’ll see later has properties guaranteeing a unique solution for distinct x-coordinates."
  },
  {
    "objectID": "la-19.html#example-6-cubic-polynomial-interpolation",
    "href": "la-19.html#example-6-cubic-polynomial-interpolation",
    "title": "Linear Algebra",
    "section": "Example 6: Cubic Polynomial Interpolation",
    "text": "Example 6: Cubic Polynomial Interpolation\nFind a cubic polynomial whose graph passes through the points: \\((1,3),\\ (2, - 2),\\ (3, - 5),\\ (4,0)\\)\nThe polynomial is \\(p(x) = a_0 + a_1x + a_2x^2 + a_3x^3\\). Using the augmented matrix from the previous slide: \\[\n\\left[{\\begin{array}{r r r r r}{1}&{x_{1}}&{x_{1}^{2}}&{x_{1}^{3}}&{y_{1}}\\\\ {1}&{x_{2}}&{x_{2}^{2}}&{x_{2}^{3}}&{y_{2}}\\\\ {1}&{x_{3}}&{x_{3}^{2}}&{x_{3}^{3}}&{y_{3}}\\\\ {1}&{x_{4}}&{x_{4}^{2}}&{x_{4}^{3}}&{y_{4}}\\end{array}}\\right]={\\left[\\begin{array}{r r r r r}{1}&{1}&{1}&{1}&{3}\\\\ {1}&{2}&{4}&{8}&{-2}\\\\ {1}&{3}&{9}&{27}&{-5}\\\\ {1}&{4}&{16}&{64}&{0}\\end{array}\\right]}\n\\]\nWe will solve this system for \\(a_0, a_1, a_2, a_3\\).\n\nWe have four points, so we expect a polynomial of degree up to 3. We set up the augmented matrix directly from the given points. The columns are powers of the x-coordinates, and the last column contains the y-coordinates. Solving this system for \\(a_0, a_1, a_2, a_3\\) will give us our unique cubic interpolating polynomial."
  },
  {
    "objectID": "la-19.html#example-6-solving-and-plotting",
    "href": "la-19.html#example-6-solving-and-plotting",
    "title": "Linear Algebra",
    "section": "Example 6: Solving and Plotting",
    "text": "Example 6: Solving and Plotting\n\n\n\n\n\n\nThe coefficients obtained are \\(a_0 = 4, a_1 = 3, a_2 = -5, a_3 = 1\\). Thus, \\(p(x) = 4 + 3x - 5x^2 + x^3\\).\n\nUsing numpy.linalg.solve, we quickly find the coefficients: \\(a_0=4, a_1=3, a_2=-5, a_3=1\\). This gives us the unique cubic polynomial. The plot clearly shows how this polynomial smoothly passes through all four given points. In engineering, interpolation is used in sensor calibration, generating smooth curves for robot movements, or in signal processing to reconstruct missing data points."
  },
  {
    "objectID": "la-19.html#example-7-approximate-integration",
    "href": "la-19.html#example-7-approximate-integration",
    "title": "Linear Algebra",
    "section": "Example 7: Approximate Integration",
    "text": "Example 7: Approximate Integration\nOne application is approximating integrals of complex functions by interpolating them with simpler polynomials.\nExample: \\(\\int_{0}^{1} \\sin \\left(\\frac{\\pi x^{2}}{2}\\right) dx\\)\n\nUse points on the function’s graph (e.g., \\((x_i, f(x_i))\\)).\nFind an interpolating polynomial \\(p(x)\\) for these points.\nIntegrate the polynomial \\(\\int p(x) dx\\) as an approximation.\n\n\n\n\nThe interpolating polynomial \\(p(x) = 0.098796x + 0.762356x^2 + 2.14429x^3 - 2.00544x^4\\) closely approximates \\(f(x)\\) over \\([0,1]\\).\n\nMany functions don’t have simple antiderivatives, making their integrals hard to compute exactly. This is where approximate integration comes in. A powerful method involves using polynomial interpolation: first, you choose several points on the curve of your complex function. Then, you find an interpolating polynomial that passes through these points. Since polynomials are easy to integrate, you integrate the polynomial as an approximation for the original function’s integral. The graph visually shows how closely an interpolating polynomial can match a more complex function over an interval, leading to accurate approximations valuable in fields like signal processing or control theory."
  },
  {
    "objectID": "la-19.html#leontief-input-output-models",
    "href": "la-19.html#leontief-input-output-models",
    "title": "Linear Algebra",
    "section": "Leontief Input-Output Models",
    "text": "Leontief Input-Output Models\nDeveloped by economist Wassily Leontief (Nobel Prize, 1973). Analyzes interdependencies between sectors of an economy.\n\nSectors: Manufacturing, Agriculture, Utilities. Each produces outputs and requires inputs (from itself or other sectors).\nConsumption Matrix (C): Entries \\(c_{ij}\\) represent the dollar value of output from sector \\(i\\) required by sector \\(j\\) to produce one dollar’s worth of output.\n\nExample: \\(C = \\left[ \\begin{array}{ccc}0.5 & 0.1 & 0.1\\\\ 0.2 & 0.5 & 0.3\\\\ 0.1 & 0.3 & 0.4 \\end{array} \\right]\\)\n\n(0.5, 0.2, 0.1) in first column means manufacturing needs $0.50 of its own, $0.20 agricultural, $0.10 utilities to produce $1.00 of manufacturing output.\n\n\nOutside Demand Vector (\\(\\mathbf{d}\\)): Demand from the “open sector” (e.g., consumers).\nProduction Vector (\\(\\mathbf{x}\\)): Total output from each sector needed to satisfy internal consumption AND outside demand.\n\n\nThe Leontief Input-Output model provides a powerful framework for understanding how different sectors of an economy rely on each other. It’s a macroscopic view, crucial for economic planning and understanding supply chains. The core idea is that to produce goods, industries need inputs, which themselves are outputs from other industries. The consumption matrix, C, quantifies these interdependencies. Each column of C represents the inputs needed by a sector to produce one dollar of its own output. The model aims to find the total production for each sector to meet both its internal consumption needs and external demands."
  },
  {
    "objectID": "la-19.html#leontief-equation",
    "href": "la-19.html#leontief-equation",
    "title": "Linear Algebra",
    "section": "Leontief Equation",
    "text": "Leontief Equation\nThe total production \\(\\mathbf{x}\\) must cover: 1. Intermediate Demand (\\(C\\mathbf{x}\\)): What product-producing sectors consume from each other. 2. Outside Demand (\\(\\mathbf{d}\\)): What the open sector demands.\nThus: \\(\\mathbf{x} = C\\mathbf{x} + \\mathbf{d}\\)\nRearranging this, we get the Leontief Equation: \\[\n(I - C)\\mathbf{x} = \\mathbf{d}\n\\] The matrix \\((I - C)\\) is called the Leontief Matrix. If \\((I-C)\\) is invertible, the unique solution is \\(\\mathbf{x} = (I - C)^{-1}\\mathbf{d}\\).\n\nThis simple equation is the heart of the Leontief model. It states that the total output (production) of each sector must be equal to what is consumed internally by other industries plus what is demanded by outside consumers. Rearranging this leads to the Leontief equation, \\((I-C)\\mathbf{x}=\\mathbf{d}\\). If the Leontief matrix \\((I-C)\\) is invertible, we can directly calculate the necessary production levels \\(\\mathbf{x}\\) for any given external demand \\(\\mathbf{d}\\). This is a classic example of linear systems solving a complex economic problem."
  },
  {
    "objectID": "la-19.html#example-1-leontief-satisfying-outside-demand",
    "href": "la-19.html#example-1-leontief-satisfying-outside-demand",
    "title": "Linear Algebra",
    "section": "Example 1 (Leontief): Satisfying Outside Demand",
    "text": "Example 1 (Leontief): Satisfying Outside Demand\nConsider \\(C = \\left[ \\begin{array}{lll}0.5 & 0.1 & 0.1\\\\ 0.2 & 0.5 & 0.3\\\\ 0.1 & 0.3 & 0.4 \\end{array} \\right]\\) Outside demand \\(\\mathbf{d} = \\left[ \\begin{array}{l}7900\\\\ 3950\\\\ 1975 \\end{array} \\right]\\) (Manufactured, Agricultural, Utilities)\nWe need to solve \\((I - C)\\mathbf{x} = \\mathbf{d}\\). First, compute \\(I - C\\): \\[\nI - C = \\left[ \\begin{array}{lll}1 & 0 & 0\\\\ 0 & 1 & 0\\\\ 0 & 0 & 1 \\end{array} \\right] - \\left[ \\begin{array}{lll}0.5 & 0.1 & 0.1\\\\ 0.2 & 0.5 & 0.3\\\\ 0.1 & 0.3 & 0.4 \\end{array} \\right] = \\left[ \\begin{array}{rrr}0.5 & -0.1 & -0.1\\\\ -0.2 & 0.5 & -0.3\\\\ -0.1 & -0.3 & 0.6 \\end{array} \\right]\n\\] Now, solve the system \\(\\left[ \\begin{array}{rrr}0.5 & -0.1 & -0.1\\\\ -0.2 & 0.5 & -0.3\\\\ -0.1 & -0.3 & 0.6 \\end{array} \\right]\\left[ \\begin{array}{c}x_{1}\\\\ x_{2}\\\\ x_{3} \\end{array} \\right] = \\left[ \\begin{array}{c}7900\\\\ 3950\\\\ 1975 \\end{array} \\right]\\).\n\nLet’s apply the Leontief model to a specific scenario. We’re given the consumption matrix \\(C\\) and an external demand vector \\(\\mathbf{d}\\). The first step is to construct the Leontief matrix \\((I-C)\\). Then, we set up the linear system \\((I-C)\\mathbf{x}=\\mathbf{d}\\). Solving this system will directly give us the production vector \\(\\mathbf{x}\\), which tells us how much each sector needs to produce to satisfy both internal and external demands."
  },
  {
    "objectID": "la-19.html#example-1-leontief-solving-for-mathbfx",
    "href": "la-19.html#example-1-leontief-solving-for-mathbfx",
    "title": "Linear Algebra",
    "section": "Example 1 (Leontief): Solving for \\(\\mathbf{x}\\)",
    "text": "Example 1 (Leontief): Solving for \\(\\mathbf{x}\\)\n\n\n\n\n\n\nSolution: \\(\\mathbf{x} = \\left[ \\begin{array}{l}27500\\\\ 33750\\\\ 24750 \\end{array} \\right]\\)\nThe economy can meet the demand by producing these amounts.\n\nUsing numpy.linalg.solve, we find the production vector \\(\\mathbf{x}\\). The result shows that the manufacturing sector needs to produce $27,500, agriculture $33,750, and utilities $24,750. These are the gross output values needed to cover all intermediate consumption within the economy and to satisfy the specified external demand. This exact solution and the non-negative values indicate that the economy can indeed meet the demand."
  },
  {
    "objectID": "la-19.html#productive-open-economies",
    "href": "la-19.html#productive-open-economies",
    "title": "Linear Algebra",
    "section": "Productive Open Economies",
    "text": "Productive Open Economies\nFor \\(\\mathbf{x}\\) to be a valid production vector, its entries must be nonnegative. An economy is productive if \\((I-C)^{-1}\\) exists and has nonnegative entries for every demand vector \\(\\mathbf{d}\\).\nTHEOREM 1.10.1: If \\(C\\) is the consumption matrix for an open economy, and if all of the column sums are less than 1, then the matrix \\(I - C\\) is invertible, the entries of \\((I - C)^{-1}\\) are nonnegative, and the economy is productive.\n\nA sector is profitable if its column sum in \\(C\\) is less than 1 (it needs less than $1 of input to produce $1 of output).\nThus, if all product-producing sectors are profitable, the economy is productive. (Or if all row sums are less than 1, also productive.)\n\n\nA crucial aspect of Leontief models is whether an economy is “productive”—meaning it can actually meet any reasonable outside demand. This hinges on two conditions: the Leontief matrix \\((I-C)\\) must be invertible, and its inverse \\((I-C)^{-1}\\) must have all nonnegative entries (because you can’t have negative production). Theorem 1.10.1 provides a simple test: if the sum of inputs required for each dollar of output (i.e., column sums of C) is less than $1, then the economy is productive. This means each sector is profitable, in a gross sense, leading to stable economic conditions."
  },
  {
    "objectID": "la-19.html#example-2-leontief-confirming-productive-economy",
    "href": "la-19.html#example-2-leontief-confirming-productive-economy",
    "title": "Linear Algebra",
    "section": "Example 2 (Leontief): Confirming Productive Economy",
    "text": "Example 2 (Leontief): Confirming Productive Economy\nRecall \\(C = \\left[ \\begin{array}{lll}0.5 & 0.1 & 0.1\\\\ 0.2 & 0.5 & 0.3\\\\ 0.1 & 0.3 & 0.4 \\end{array} \\right]\\) Column sums:\n\nMfg: \\(0.5 + 0.2 + 0.1 = 0.8 &lt; 1\\)\nAg: \\(0.1 + 0.5 + 0.3 = 0.9 &lt; 1\\)\nUtil: \\(0.1 + 0.3 + 0.4 = 0.8 &lt; 1\\)\n\nAll column sums are less than 1, so by Theorem 1.10.1, the economy is productive! \\((I-C)^{-1}\\) should exist and have nonnegative entries.\n\n\n\n\n\n\nThe entries of \\((I-C)^{-1}\\) are indeed nonnegative, confirming the economy’s productivity. The production vector matches the previous calculation.\n\nLet’s confirm the theorem for our example. We calculate the column sums of the consumption matrix C. All are less than 1, so the theorem guarantees productivity. This means \\((I-C)^{-1}\\) should exist and have only non-negative entries. Using numpy.linalg.inv, we compute the inverse. As shown, all entries are positive, verifying the theorem. We can then use this inverse to quickly calculate the production vector for any demand, showing the strength of this model for economic planning. This integration of linear algebra into economics highlights its broad applicability beyond traditional engineering fields."
  },
  {
    "objectID": "la-19.html#ece-applications-summary",
    "href": "la-19.html#ece-applications-summary",
    "title": "Linear Algebra",
    "section": "ECE Applications & Summary",
    "text": "ECE Applications & Summary\nKey Areas for ECE where Linear Systems are Applied:\n\nNetwork Analysis:\n\nElectrical Circuits: Solving currents and voltages using Kirchhoff’s Laws (KCL, KVL) – fundamental to circuit design & diagnostics.\nTraffic Management: Optimizing flow, designing road networks to prevent congestion.\nData Networks: Analyzing data packet flow, optimizing routing.\n\nSignal & Image Processing:\n\nPolynomial Interpolation: Reconstructing signals, approximating complex functions, noise reduction, image scaling.\nFilter Design: Many digital filters are based on linear relationships.\n\nControl Systems:\n\nSystem Identification: Using input/output data to model a system as a set of linear equations.\nState-Space Models: Describing system dynamics as linear systems.\n\nOptimization: Many engineering optimization problems are linear or can be linearized."
  },
  {
    "objectID": "la-19.html#ece-applications-summary-1",
    "href": "la-19.html#ece-applications-summary-1",
    "title": "Linear Algebra",
    "section": "ECE Applications & Summary",
    "text": "ECE Applications & Summary\nToday We Covered:\n\nSystematic methods to solve real-world problems using linear systems.\nNetwork Analysis: Flow conservation in general networks and specific electrical circuits (Ohm’s, KCL, KVL).\nBalancing Chemical Equations: Stoichiometry via homogeneous systems.\nPolynomial Interpolation: Fitting curves to data points leading to unique solutions.\nLeontief Input-Output Models: Economic interdependencies and production planning.\n\n\nTo summarize, what we’ve seen today is just a glimpse of the vast applicability of linear systems in ECE. From optimizing traffic in Philadelphia to designing complex electrical circuits, accurately balancing chemical reactions, interpolating data points for smooth functions, and even analyzing economic interdependencies, linear algebra provides the universal language and tools. The ability to formulate these real-world challenges as systems of linear equations and solve them is a critical skill for any aspiring engineer. Linear systems are truly the workhorses of quantitative analysis across virtually all engineering disciplines."
  },
  {
    "objectID": "la-22.html#evaluating-determinants-by-row-reduction",
    "href": "la-22.html#evaluating-determinants-by-row-reduction",
    "title": "Linear Algebra",
    "section": "2.2 Evaluating Determinants by Row Reduction",
    "text": "2.2 Evaluating Determinants by Row Reduction\nImron Rosyadi"
  },
  {
    "objectID": "la-22.html#introduction-why-row-reduction",
    "href": "la-22.html#introduction-why-row-reduction",
    "title": "Linear Algebra",
    "section": "Introduction: Why Row Reduction?",
    "text": "Introduction: Why Row Reduction?\nIn the previous section, we learned about cofactor expansion. While conceptually important, it becomes computationally intensive for large matrices.\nOur Goal Today: Faster Determinant Calculation\n\nEvaluate determinants by reducing matrices to row echelon form.\nThis method is generally more efficient for larger matrices than cofactor expansion.\n\nRelevance to ECE\n\nComputational Efficiency: In engineering, matrices often represent large systems (e.g., signal processing, power networks). Efficient determinant calculation is critical for performance.\nSystem Analysis: Determinants tell us about matrix invertibility, which in turn tells us if a system has a unique solution.\n\n\nFor a \\(25 \\times 25\\) matrix, cofactor expansion would take millions of years even with today’s fastest computers. This highlights the necessity of more efficient methods like row reduction. While we’ll mostly see smaller matrices in examples, understanding the principles for large matrices is key for future advanced ECE courses involving numerical methods."
  },
  {
    "objectID": "la-22.html#basic-properties-of-determinants",
    "href": "la-22.html#basic-properties-of-determinants",
    "title": "Linear Algebra",
    "section": "Basic Properties of Determinants",
    "text": "Basic Properties of Determinants\nWe start with two fundamental theorems.\nTheorem 2.2.1: Zero Rows or Columns\nIf a square matrix \\(A\\) has a row of zeros or a column of zeros, then \\(\\operatorname *{det}(A) = 0\\).\nProof sketch: We can find the determinant by a cofactor expansion along the row or column of zeros. Since every entry in that row or column is zero, each term in the expansion \\(a_{ij}C_{ij}\\) will be zero. \\[\n\\operatorname *{det}(A) = 0 \\cdot C_1 + 0 \\cdot C_2 + \\dots + 0 \\cdot C_n = 0\n\\]\nTheorem 2.2.2: Determinant of a Transpose\nLet \\(A\\) be a square matrix. Then \\(\\operatorname *{det}(A) = \\operatorname *{det}(A^T)\\).\nProof sketch: Transposing a matrix swaps its rows and columns. A cofactor expansion of \\(A\\) along any row is equivalent to a cofactor expansion of \\(A^T\\) along the corresponding column. Thus, their determinants are the same.\n\nThe first theorem is an immediate shortcut: spotting a row or column of zeros instantly tells you the determinant is zero. This can save a lot of computation time. The second theorem is powerful because it implies that any property about rows of a matrix’s determinant also applies to its columns, and vice versa. This duality simplifies many future proofs and derivations in linear algebra, useful for understanding matrix symmetry in applications like finite element analysis or graph theory."
  },
  {
    "objectID": "la-22.html#impact-of-elementary-row-operations-on-determinants",
    "href": "la-22.html#impact-of-elementary-row-operations-on-determinants",
    "title": "Linear Algebra",
    "section": "Impact of Elementary Row Operations on Determinants",
    "text": "Impact of Elementary Row Operations on Determinants\nElementary row operations (EROs) are the backbone of Gaussian elimination. They systematically transform a matrix. How do they affect the determinant?\nTheorem 2.2.3: Effects of EROs on \\(\\operatorname{det}(A)\\)\nLet \\(A\\) be an \\(n \\times n\\) matrix.\n\nIf \\(B\\) is formed by multiplying a single row (or column) of \\(A\\) by a scalar \\(k\\), then \\(\\operatorname{det}(B) = k \\operatorname{det}(A)\\).\n\nExample for \\(3 \\times 3\\) (multiplying row 1 by \\(k\\)): \\(\\operatorname *{det}\\left[ \\begin{array}{ccc}k a_{11} & k a_{12} & k a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{array} \\right] = k a_{11}C_{11} + k a_{12}C_{12} + k a_{13}C_{13}\\) \\(= k(a_{11}C_{11} + a_{12}C_{12} + a_{13}C_{13}) = k \\operatorname{det}(A)\\)"
  },
  {
    "objectID": "la-22.html#impact-of-elementary-row-operations-on-determinants-1",
    "href": "la-22.html#impact-of-elementary-row-operations-on-determinants-1",
    "title": "Linear Algebra",
    "section": "Impact of Elementary Row Operations on Determinants",
    "text": "Impact of Elementary Row Operations on Determinants\n\nIf \\(B\\) is formed by interchanging two rows (or columns) of \\(A\\), then \\(\\operatorname{det}(B) = - \\operatorname{det}(A)\\).\n\nA row swap flips the sign of the determinant.\n\nIf \\(B\\) is formed by adding a multiple of one row (or column) of \\(A\\) to another, then \\(\\operatorname{det}(B) = \\operatorname{det}(A)\\).\n\nThis operation does not change the determinant value. This is the most useful ERO for row reduction in this context.\n\n\n\nThese properties are critical because they allow us to track changes to the determinant as we perform row reduction. For ECE: imagine a system of equations where each row represents a constraint or an equation (e.g., KVL loop, KCL node). Scaling a single row corresponds to scaling one of those equations, which scales the system. Swapping rows is just reordering equations, which changes the sign of the determinant but not its magnitude. Adding multiples of rows is equivalent to linear combinations of system equations, which preserve the solution set, and thus the determinant’s value."
  },
  {
    "objectID": "la-22.html#determinants-of-elementary-matrices",
    "href": "la-22.html#determinants-of-elementary-matrices",
    "title": "Linear Algebra",
    "section": "Determinants of Elementary Matrices",
    "text": "Determinants of Elementary Matrices\nRecall that an elementary matrix \\(E\\) is formed by performing a single ERO on the identity matrix \\(I_n\\). Theorem 2.2.3 has a special case for \\(A = I_n\\).\nTheorem 2.2.4: Determinants of Elementary Matrices\nLet \\(E\\) be an \\(n \\times n\\) elementary matrix.\n\nIf \\(E\\) results from multiplying a row of \\(I_n\\) by a nonzero scalar \\(k\\), then \\(\\operatorname{det}(E) = k\\).\nIf \\(E\\) results from interchanging two rows of \\(I_n\\), then \\(\\operatorname{det}(E) = -1\\).\nIf \\(E\\) results from adding a multiple of one row of \\(I_n\\) to another, then \\(\\operatorname{det}(E) = 1\\).\n\nObservation: The determinant of an elementary matrix can never be zero."
  },
  {
    "objectID": "la-22.html#determinants-of-elementary-matrices-1",
    "href": "la-22.html#determinants-of-elementary-matrices-1",
    "title": "Linear Algebra",
    "section": "Determinants of Elementary Matrices",
    "text": "Determinants of Elementary Matrices\nExample 1: Determinants of Elementary Matrices\n\\[\nE_1 = {\\left|\\begin{array}{llll}{1}&{0}&{0}&{0}\\\\ {0}&{\\bf{3}}&{0}&{0}\\\\ {0}&{0}&{1}&{0}\\\\ {0}&{0}&{0}&{1}\\end{array}\\right|}=3 \\quad \\text{(Row 2 of } I_4 \\text{ multiplied by 3)}\n\\] \\[\nE_2 = {\\left|\\begin{array}{llll}{0}&{0}&{0}&{\\bf{1}}\\\\ {0}&{1}&{0}&{0}\\\\ {0}&{0}&{1}&{0}\\\\ {\\bf{1}}&{0}&{0}&{0}\\end{array}\\right|}=-1 \\quad \\text{(Row 1 and Row 4 of } I_4 \\text{ interchanged)}\n\\] \\[\nE_3 = {\\left|\\begin{array}{llll}{1}&{0}&{0}&{\\bf{7}}\\\\ {0}&{1}&{0}&{0}\\\\ {0}&{0}&{1}&{0}\\\\ {0}&{0}&{0}&{1}\\end{array}\\right|}=1 \\quad \\text{(7 times Row 4 of } I_4 \\text{ added to Row 1)}\n\\]\n\nEvery elementary row operation can be represented by multiplying by an elementary matrix. These determinant rules allow us to track how the determinant changes through a sequence of elementary operations. This is crucial for numerical stability in large-scale computations often found in embedded systems or scientific computing within ECE."
  },
  {
    "objectID": "la-22.html#interactive-example-elementary-matrices",
    "href": "la-22.html#interactive-example-elementary-matrices",
    "title": "Linear Algebra",
    "section": "Interactive Example: Elementary Matrices",
    "text": "Interactive Example: Elementary Matrices\nLet’s compute the determinant of some elementary matrices using Pyodide to confirm Theorem 2.2.4.\n\n\n\n\n\n\n\nYou can observe how each operation on the identity matrix immediately gives the determinant specified by the theorem. Try changing the scalar in E1 or the rows swapped in E2, or the multiple in E3, and see how the determinant changes. This quick interactive check visually and numerically confirms the properties of elementary matrices’ determinants."
  },
  {
    "objectID": "la-22.html#matrices-with-proportional-rows-or-columns",
    "href": "la-22.html#matrices-with-proportional-rows-or-columns",
    "title": "Linear Algebra",
    "section": "Matrices with Proportional Rows or Columns",
    "text": "Matrices with Proportional Rows or Columns\nThis is another shortcut for finding zero determinants.\nTheorem 2.2.5\nIf \\(A\\) is a square matrix with two proportional rows or two proportional columns, then \\(\\det(A) = 0\\).\nProof sketch:\nIf a matrix \\(A\\) has two proportional rows (e.g., row \\(i = k \\cdot\\) row \\(j\\)), we can perform an ERO of type (c) (adding a multiple of one row to another) to create a row of zeros.\nFor example, add \\(-k\\) times row \\(j\\) to row \\(i\\). This operation does not change the determinant (Theorem 2.2.3c). Since the resulting matrix now has a row of zeros, its determinant is 0 (Theorem 2.2.1).\nTherefore, \\(\\det(A)=0\\)."
  },
  {
    "objectID": "la-22.html#matrices-with-proportional-rows-or-columns-1",
    "href": "la-22.html#matrices-with-proportional-rows-or-columns-1",
    "title": "Linear Algebra",
    "section": "Matrices with Proportional Rows or Columns",
    "text": "Matrices with Proportional Rows or Columns\nExample 2: Proportional Rows or Columns\nEach of the following matrices has a determinant of zero because they have proportional rows or columns:\n\\[\n{\\left[\\begin{array}{l l}{-1}&{4}\\\\ {-2}&{8}\\end{array}\\right]} \\quad (\\text{Row 2} = 2 \\times \\text{Row 1})\n\\]\n\\[\n{\\left[\\begin{array}{l l l}{1}&{-2}&{7}\\\\ {-4}&{8}&{5}\\\\ {2}&{-4}&{3}\\end{array}\\right]} \\quad (\\text{Col 2} = -2 \\times \\text{Col 1})\n\\]\n\\[\n{\\left[\\begin{array}{l l l l}{3}&{-1}&{4}&{-5}\\\\ {6}&{-2}&{5}&{2}\\\\ {5}&{8}&{1}&{4}\\\\ {-9}&{3}&{-12}&{15}\\end{array}\\right]} \\quad (\\text{Row 4} = -3 \\times \\text{Row 1})\n\\]\n\nThis theorem is a very quick way to identify singular (non-invertible) matrices without any complex calculations. In ECE, if a system’s matrix has proportional rows or columns, it implies a linear dependency, meaning redundant information or an under-constrained/over-constrained system, which prevents a unique solution. For example, in circuit analysis, two KVL loops that are just scalar multiples of each other would lead to a singular matrix, meaning the system of equations is linearly dependent."
  },
  {
    "objectID": "la-22.html#evaluating-determinants-by-row-reduction-the-method",
    "href": "la-22.html#evaluating-determinants-by-row-reduction-the-method",
    "title": "Linear Algebra",
    "section": "Evaluating Determinants by Row Reduction: The Method",
    "text": "Evaluating Determinants by Row Reduction: The Method\nThe strategy is:\n\nReduce to Triangular Form: Use elementary row operations to transform the given matrix \\(A\\) into a row echelon form \\(U\\) (which is always upper triangular).\nKeep Track of Changes: As you perform EROs, record how each operation affects the determinant according to Theorem 2.2.3.\n\nRow swap: multiply determinant by \\(-1\\).\nRow scalar multiplication: factor out the scalar.\nAdding multiple of row: no change.\n\nCompute \\(\\operatorname{det}(U)\\): Since \\(U\\) is triangular, \\(\\operatorname{det}(U)\\) is simply the product of its diagonal entries (Theorem 2.1.2).\nRelate Back to \\(\\operatorname{det}(A)\\): Use the accumulated factors from step 2 to find \\(\\operatorname{det}(A)\\).\n\nThis method is generally much faster than cofactor expansion for large matrices, as it converts the problem into one of simple diagonal multiplication after reduction.\n\nThis procedure is the workhorse for computational linear algebra. While numerical libraries handle the specifics, understanding this process for smaller matrices builds intuition for how larger systems are handled efficiently in practice, particularly in optimized algorithms used in engineering software."
  },
  {
    "objectID": "la-22.html#example-3-using-row-reduction",
    "href": "la-22.html#example-3-using-row-reduction",
    "title": "Linear Algebra",
    "section": "Example 3: Using Row Reduction",
    "text": "Example 3: Using Row Reduction\nEvaluate \\(\\det(A)\\) where \\[\nA = \\left[ \\begin{array}{ccc}0 & 1 & 5 \\\\ 3 & -6 & 9 \\\\ 2 & 6 & 1 \\end{array} \\right]\n\\]\nSolution:\n\nSwap R1 and R2:\n\n\\[\n\\det(A) = - \\left| \\begin{array}{ccc}3 & -6 & 9 \\\\ 0 & 1 & 5 \\\\ 2 & 6 & 1 \\end{array} \\right|\n\\]\n(multiply by -1 for row swap)\n\nFactor 3 from R1:\n\n\\[\n= -3 \\left| \\begin{array}{ccc}1 & -2 & 3 \\\\ 0 & 1 & 5 \\\\ 2 & 6 & 1 \\end{array} \\right|\n\\]\n(factor out 3)"
  },
  {
    "objectID": "la-22.html#example-3-using-row-reduction-1",
    "href": "la-22.html#example-3-using-row-reduction-1",
    "title": "Linear Algebra",
    "section": "Example 3: Using Row Reduction",
    "text": "Example 3: Using Row Reduction\n\nR3 = R3 - 2*R1:\n\n\\[\n= -3 \\left| \\begin{array}{ccc}1 & -2 & 3 \\\\ 0 & 1 & 5 \\\\ 0 & 10 & -5 \\end{array} \\right|\n\\]\n(no change to determinant)\n\nR3 = R3 - 10*R2:\n\n\\[\n= -3 \\left| \\begin{array}{ccc}1 & -2 & 3 \\\\ 0 & 1 & 5 \\\\ 0 & 0 & -55 \\end{array} \\right|\n\\]\n(no change to determinant)"
  },
  {
    "objectID": "la-22.html#example-3-using-row-reduction-2",
    "href": "la-22.html#example-3-using-row-reduction-2",
    "title": "Linear Algebra",
    "section": "Example 3: Using Row Reduction",
    "text": "Example 3: Using Row Reduction\n\nFactor -55 from R3:\n\n\\[\n= (-3)(-55) \\left| \\begin{array}{ccc}1 & -2 & 3 \\\\ 0 & 1 & 5 \\\\ 0 & 0 & 1 \\end{array} \\right|\n\\]\n(factor out -55)\n\nCalculate determinant of triangular matrix: The final matrix is upper triangular.\n\n\\(\\operatorname *{det}(A) = (-3)(-55) \\cdot (1 \\cdot 1 \\cdot 1) = 165 \\cdot 1 = 165\\).\n\nNotice how each step using row operations is explicitly linked to its effect on the determinant. The final upper triangular matrix has a determinant that is simply the product of its diagonal entries. This process is much more manageable than repeatedly computing cofactors for a \\(3 \\times 3\\) matrix."
  },
  {
    "objectID": "la-22.html#interactive-verification-example-3",
    "href": "la-22.html#interactive-verification-example-3",
    "title": "Linear Algebra",
    "section": "Interactive Verification: Example 3",
    "text": "Interactive Verification: Example 3\nLet’s verify the result of Example 3 using Pyodide (Numpy linalg.det).\n\n\n\n\n\n\n\nThis interactive example serves to quickly confirm our manual calculation. It’s a valuable check, particularly in ECE applications where errors in matrix operations can lead to significant issues (e.g., control system instability, incorrect circuit behavior)."
  },
  {
    "objectID": "la-22.html#example-4-using-column-operations",
    "href": "la-22.html#example-4-using-column-operations",
    "title": "Linear Algebra",
    "section": "Example 4: Using Column Operations",
    "text": "Example 4: Using Column Operations\nRemember Theorem 2.2.2 stated \\(\\det(A) = \\det(A^T)\\), meaning rules for row operations also apply to column operations. We can strategically use column operations to simplify the matrix.\nCompute the determinant of \\[\nA = {\\left[ \\begin{array}{llll}{1} & 0 & 0 & 3\\\\ 2 & 7 & 0 & 6\\\\ 0 & 6 & 3 & 0\\\\ 7 & 3 & 1 & {-5} \\end{array} \\right]}\n\\]\nSolution: We can put \\(A\\) into a lower triangular form (another type of triangular matrix, for which Theorem 2.1.2 applies) in one step by a column operation."
  },
  {
    "objectID": "la-22.html#example-4-using-column-operations-1",
    "href": "la-22.html#example-4-using-column-operations-1",
    "title": "Linear Algebra",
    "section": "Example 4: Using Column Operations",
    "text": "Example 4: Using Column Operations\n\nC4 = C4 - 3*C1: This operation does not change the determinant according to Theorem 2.2.3(c) applied to columns. \\[\n\\operatorname *{det}(A) = \\operatorname *{det}{\\left[ \\begin{array}{llll}{1} & 0 & 0 & 0\\\\ 2 & 7 & 0 & 0\\\\ 0 & 6 & 3 & 0\\\\ 7 & 3 & 1 & {-26} \\end{array} \\right]}\n\\]\nCalculate determinant of lower triangular matrix: The resulting matrix is lower triangular. Its determinant is the product of its diagonal entries: \\(\\operatorname *{det}(A) = (1)(7)(3)(-26) = -546\\).\n\n\nThis example highlights that recognizing opportunities for column operations can also significantly simplify computations. Engineering problems often involve matrices with sparse (many zeros) or structured patterns, making these targeted operations very efficient. It pays to inspect the matrix before blindly applying a full row reduction."
  },
  {
    "objectID": "la-22.html#interactive-verification-example-4",
    "href": "la-22.html#interactive-verification-example-4",
    "title": "Linear Algebra",
    "section": "Interactive Verification: Example 4",
    "text": "Interactive Verification: Example 4\nLet’s quickly verify the result from Example 4."
  },
  {
    "objectID": "la-22.html#example-5-row-operations-and-cofactor-expansion-hybrid",
    "href": "la-22.html#example-5-row-operations-and-cofactor-expansion-hybrid",
    "title": "Linear Algebra",
    "section": "Example 5: Row Operations and Cofactor Expansion (Hybrid)",
    "text": "Example 5: Row Operations and Cofactor Expansion (Hybrid)\nFor larger matrices, a hybrid approach combining row operations with cofactor expansion can be highly effective:\n\nPerform EROs to create many zero entries in a a specific row or column.\nThen, use cofactor expansion along that simplified row/column.\n\nEvaluate \\(\\det(A)\\) where \\[\nA={\\left[\\begin{array}{llll}{3}&{5}&{-2}&{6}\\\\ {1}&{2}&{-1}&{1}\\\\ {2}&{4}&{1}&{5}\\\\ {3}&{7}&{5}&{3}\\end{array}\\right]}\n\\]\nSolution: We target row 2 to introduce zeros efficiently because its first entry is 1.\n\nR1 = R1 - 3*R2\nR3 = R3 - 2*R2\nR4 = R4 - 3*R2"
  },
  {
    "objectID": "la-22.html#example-5-row-operations-and-cofactor-expansion-hybrid-1",
    "href": "la-22.html#example-5-row-operations-and-cofactor-expansion-hybrid-1",
    "title": "Linear Algebra",
    "section": "Example 5: Row Operations and Cofactor Expansion (Hybrid)",
    "text": "Example 5: Row Operations and Cofactor Expansion (Hybrid)\nThese operations do not change the determinant: \\[\n\\operatorname *{det}(A) = {\\left| \\begin{array}{rrrr}{0} & {-1} & 1 & 3\\\\ 1 & 2 & {-1} & 1\\\\ {0} & {0} & 3 & 3\\\\ {0} & 1 & 8 & 0 \\end{array} \\right|}\n\\] Now, expand along the first column. Only \\(a_{21}=1\\) is non-zero. \\(\\operatorname *{det}(A) = a_{21}C_{21} = 1 \\cdot (-1)^{2+1}M_{21} = -M_{21}\\) \\[\n= - \\operatorname{det}\\left| \\begin{array}{ccc} -1 & 1 & 3 \\\\ 0 & 3 & 3 \\\\ 1 & 8 & 0 \\end{array} \\right|\n\\]"
  },
  {
    "objectID": "la-22.html#example-5-row-operations-and-cofactor-expansion-hybrid-2",
    "href": "la-22.html#example-5-row-operations-and-cofactor-expansion-hybrid-2",
    "title": "Linear Algebra",
    "section": "Example 5: Row Operations and Cofactor Expansion (Hybrid)",
    "text": "Example 5: Row Operations and Cofactor Expansion (Hybrid)\nNow, we can expand this \\(3 \\times 3\\) determinant, e.g., along its first column: \\[\n= - [ (-1) \\cdot \\operatorname{det}\\left| \\begin{array}{cc}3 & 3 \\\\ 8 & 0 \\end{array} \\right| - (0)C_{21} + (1) \\cdot \\operatorname{det}\\left| \\begin{array}{cc}1 & 3 \\\\ 3 & 3 \\end{array} \\right| ]\n\\] \\[\n= - [ (-1)((3)(0) - (3)(8)) + ((1)(3) - (3)(3)) ]\n\\] \\[\n= - [ (-1)(-24) + (3 - 9) ]\n\\] \\[\n= - [ 24 - 6 ] = - [18] = -18\n\\]\n\nThe visual from the prompt corresponds to the intermediate matrix after applying the row operations to create zeros in the first column. This mixed strategy is powerful for larger matrices, turning a complex problem into a simpler one, which can then be tackled by an easier \\(3 \\times 3\\) determinant calculation. This approach mirrors techniques in numerical analysis, where matrix sparsity and structure are exploited for computational gains."
  },
  {
    "objectID": "la-22.html#interactive-verification-example-5",
    "href": "la-22.html#interactive-verification-example-5",
    "title": "Linear Algebra",
    "section": "Interactive Verification: Example 5",
    "text": "Interactive Verification: Example 5\nConfirm the determinant of the matrix from Example 5."
  },
  {
    "objectID": "la-22.html#summary-and-key-takeaways",
    "href": "la-22.html#summary-and-key-takeaways",
    "title": "Linear Algebra",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\nKey Concepts\n\nTheorem 2.2.1 (Zero Rows/Columns): \\(\\det(A)=0\\) if \\(A\\) has a row or column of zeros.\nTheorem 2.2.2 (Transpose): \\(\\det(A) = \\det(A^T)\\).\nTheorem 2.2.3 (EROs Effects):\n\nMultiply row by \\(k \\implies \\det(B) = k \\det(A)\\)\nSwap two rows \\(\\implies \\det(B) = - \\det(A)\\)\nAdd multiple of row to another \\(\\implies \\det(B) = \\det(A)\\)\n\nTheorem 2.2.4 (Elementary Matrices): \\(\\det(E)\\) is \\(k\\), \\(-1\\), or \\(1\\) depending on the ERO.\nTheorem 2.2.5 (Proportional Rows/Columns): \\(\\det(A)=0\\) if \\(A\\) has two proportional rows or columns.\nRow Reduction Method: Efficiently compute determinants by reducing \\(A\\) to triangular form, tracking factors, and then multiplying diagonal entries.\nHybrid Approach: Combine EROs to create zeros, then use cofactor expansion."
  },
  {
    "objectID": "la-22.html#summary-and-key-takeaways-1",
    "href": "la-22.html#summary-and-key-takeaways-1",
    "title": "Linear Algebra",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\nECE Connections\n\nSystem Stability: Determinants of characteristic matrices (e.g., in control systems) are crucial. Efficient computation helps analyze larger, more complex systems.\nCircuit Analysis: Quick determination of singular matrices indicates issues like linear dependency or abnormal circuit behavior.\nNumerical Methods: Gaussian elimination and LU decomposition, fundamental to solving large linear systems in ECE, heavily rely on these determinant properties.\n\n\nThis section has equipped you with powerful tools for evaluating determinants efficiently. Understanding how elementary row operations affect determinants is fundamental, not just for matrix calculations, but for a deeper intuition of how linear systems behave and how their properties can be transformed or preserved. These principles underpin many numerical algorithms used in engineering software."
  },
  {
    "objectID": "la-31.html#introduction-to-vectors",
    "href": "la-31.html#introduction-to-vectors",
    "title": "Linear Algebra",
    "section": "Introduction to Vectors",
    "text": "Introduction to Vectors\nLinear algebra focuses on matrices and vectors.\nThis section extends your understanding of vectors from 2D/3D to \\(n\\)-space.\n\nToday, we’re diving into vectors, fundamental building blocks in linear algebra. We’ll start with familiar geometric concepts in 2D and 3D, then generalize them to higher dimensions, which are crucial in many ECE applications. Think about how vectors represent physical quantities like force, velocity, or even signals."
  },
  {
    "objectID": "la-31.html#geometric-vectors",
    "href": "la-31.html#geometric-vectors",
    "title": "Linear Algebra",
    "section": "Geometric Vectors",
    "text": "Geometric Vectors\nEngineers and physicists represent vectors using arrows.\n\nDirection: Specified by the arrowhead.\nMagnitude (Length): Specified by the length of the arrow.\nInitial Point: The tail of the arrow.\nTerminal Point: The tip of the arrow.\n\n\n\n\n\n\n\nGeometric vectors are our intuitive starting point. They’re visual representations that help us understand the abstract concepts. In ECE, these could represent anything from a current flow direction to an electric field’s strength and direction. The key characteristics are direction and magnitude."
  },
  {
    "objectID": "la-31.html#vector-notation",
    "href": "la-31.html#vector-notation",
    "title": "Linear Algebra",
    "section": "Vector Notation",
    "text": "Vector Notation\nA vector \\(\\mathbf{v}\\) with initial point \\(A\\) and terminal point \\(B\\) is written as:\n\\[\n\\mathbf{v} = \\overrightarrow{AB}\n\\]\n\n\n\n\n\nBoldface Notation: We denote vectors in boldface type, like \\(\\mathbf{a}, \\mathbf{b}, \\mathbf{v}, \\mathbf{w}, \\mathbf{x}\\). Scalars are in lowercase italic type, like \\(a, k, v, w, x\\).\n\nThe notation \\(\\overrightarrow{AB}\\) clearly indicates the start and end points. However, for general vectors, we use boldface letters. This distinction between vectors and scalars is important throughout linear algebra. Remember that a vector is defined by its magnitude and direction, not its specific location in space."
  },
  {
    "objectID": "la-31.html#equivalent-vectors-and-the-zero-vector",
    "href": "la-31.html#equivalent-vectors-and-the-zero-vector",
    "title": "Linear Algebra",
    "section": "Equivalent Vectors and the Zero Vector",
    "text": "Equivalent Vectors and the Zero Vector\n\nEquivalent Vectors: Vectors with the same length and direction are considered equivalent (or equal). \\[\n\\mathbf{v} = \\mathbf{w}\n\\] This means they are the “same vector” even if located differently.\nZero Vector (\\(\\mathbf{0}\\)): A vector whose initial and terminal points coincide, having zero length.\n\nIt has no natural direction, so we can assign any convenient direction.\n\n\n\n\n\n\n\n\nThe concept of equivalent vectors is crucial. It means we can move a vector around in space as long as we don’t change its length or direction. This is fundamental for operations like vector addition. The zero vector is like the number zero in arithmetic – it’s the additive identity."
  },
  {
    "objectID": "la-31.html#vector-addition-geometric-rules",
    "href": "la-31.html#vector-addition-geometric-rules",
    "title": "Linear Algebra",
    "section": "Vector Addition: Geometric Rules",
    "text": "Vector Addition: Geometric Rules\nParallelogram Rule: If \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) share an initial point, their sum \\(\\mathbf{v} + \\mathbf{w}\\) is the diagonal of the parallelogram formed by \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\).\nTriangle Rule (Tip-to-Tail): If the initial point of \\(\\mathbf{w}\\) is at the terminal point of \\(\\mathbf{v}\\), then \\(\\mathbf{v} + \\mathbf{w}\\) is the vector from the initial point of \\(\\mathbf{v}\\) to the terminal point of \\(\\mathbf{w}\\).\n\\[\n\\mathbf{v} + \\mathbf{w} = \\mathbf{w} + \\mathbf{v} \\tag{1}\n\\]\n\n\n\n\n\nParallelogram Rule\n\n\n\n\n\n\nTriangle Rule.\n\n\n\n\n\n\nCommutativity.\n\n\n\n\nThese two rules are geometrically equivalent ways to visualize vector addition. The parallelogram rule is great when vectors start from the same point, like forces acting on an object. The triangle rule is useful for sequential movements or displacements. The commutative property (\\(\\mathbf{v} + \\mathbf{w} = \\mathbf{w} + \\mathbf{v}\\)) is clear from both rules."
  },
  {
    "objectID": "la-31.html#interactive-vector-addition-2d",
    "href": "la-31.html#interactive-vector-addition-2d",
    "title": "Linear Algebra",
    "section": "Interactive Vector Addition (2D)",
    "text": "Interactive Vector Addition (2D)\nAdjust the components of \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) to see their sum.\n\nviewof v1 = Inputs.range([-5, 5], {step: 0.1, label: \"v1\"});\nviewof v2 = Inputs.range([-5, 5], {step: 0.1, label: \"v2\"});\nviewof w1 = Inputs.range([-5, 5], {step: 0.1, label: \"w1\"});\nviewof w2 = Inputs.range([-5, 5], {step: 0.1, label: \"w2\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive plot helps visualize the parallelogram rule. You can manipulate the components of vectors \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) using the sliders. Observe how the resultant sum vector \\(\\mathbf{v} + \\mathbf{w}\\) changes dynamically. This is a great way to build intuition about vector addition before moving to more abstract definitions."
  },
  {
    "objectID": "la-31.html#vector-addition-as-translation",
    "href": "la-31.html#vector-addition-as-translation",
    "title": "Linear Algebra",
    "section": "Vector Addition as Translation",
    "text": "Vector Addition as Translation\nVector addition can also be seen as translating points.\n\nThe terminal point of \\(\\mathbf{v} + \\mathbf{w}\\) is the point resulting from translating the terminal point of \\(\\mathbf{v}\\) in the direction of \\(\\mathbf{w}\\) by a distance equal to the length of \\(\\mathbf{w}\\).\n\nEquivalently: translating the terminal point of \\(\\mathbf{w}\\) in the direction of \\(\\mathbf{v}\\) by the length of \\(\\mathbf{v}\\).\n\n\n\n\n\nTranslation of \\(\\mathbf{v}\\) by \\(\\mathbf{w}\\).\n\n\n\nThis perspective is particularly useful in computer graphics and robotics, where vectors often represent displacements. Imagine moving an object. The first vector is the initial movement, and the second vector is an additional movement from that new position. The final position is the sum of the two vectors from the original starting point."
  },
  {
    "objectID": "la-31.html#vector-subtraction",
    "href": "la-31.html#vector-subtraction",
    "title": "Linear Algebra",
    "section": "Vector Subtraction",
    "text": "Vector Subtraction\nSubtraction is defined in terms of addition: \\(a - b = a + (-b)\\).\n\nNegative of a Vector (\\(\\mathbf{-v}\\)): Has the same length as \\(\\mathbf{v}\\) but is oppositely directed.\nVector Subtraction (\\(\\mathbf{w} - \\mathbf{v}\\)): Defined as the sum \\(\\mathbf{w} + (-\\mathbf{v})\\). \\[\n\\mathbf{w} - \\mathbf{v} = \\mathbf{w} + (-\\mathbf{v}) \\tag{2}\n\\]\n\nGeometrically, \\(\\mathbf{w} - \\mathbf{v}\\) is the vector from the terminal point of \\(\\mathbf{v}\\) to the terminal point of \\(\\mathbf{w}\\), when both start at the same initial point.\n\n\n\n\n\n\nVector subtraction is essentially adding the negative vector. The visual representation of \\(\\mathbf{w} - \\mathbf{v}\\) as the vector connecting the tips of \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) (when origins coincide) is very important for understanding relative positions or changes. In physics, if \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) are position vectors, \\(\\mathbf{w} - \\mathbf{v}\\) gives the displacement from the end of \\(\\mathbf{v}\\) to the end of \\(\\mathbf{w}\\)."
  },
  {
    "objectID": "la-31.html#interactive-vector-subtraction-2d",
    "href": "la-31.html#interactive-vector-subtraction-2d",
    "title": "Linear Algebra",
    "section": "Interactive Vector Subtraction (2D)",
    "text": "Interactive Vector Subtraction (2D)\nAdjust \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) to see their difference \\(\\mathbf{w} - \\mathbf{v}\\).\n\nviewof sv1 = Inputs.range([-5, 5], {step: 0.1, label: \"v1\"});\nviewof sv2 = Inputs.range([-5, 5], {step: 0.1, label: \"v2\"});\nviewof sw1 = Inputs.range([-5, 5], {step: 0.1, label: \"w1\"});\nviewof sw2 = Inputs.range([-5, 5], {step: 0.1, label: \"w2\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive plot demonstrates vector subtraction. Notice how \\(\\mathbf{w} - \\mathbf{v}\\) connects the terminal point of \\(\\mathbf{v}\\) to the terminal point of \\(\\mathbf{w}\\) when both vectors originate from the same initial point. This is a direct visual representation of the difference between two position vectors."
  },
  {
    "objectID": "la-31.html#scalar-multiplication",
    "href": "la-31.html#scalar-multiplication",
    "title": "Linear Algebra",
    "section": "Scalar Multiplication",
    "text": "Scalar Multiplication\nScalars change the length and/or reverse the direction of a vector.\n\nIf \\(k &gt; 0\\), \\(k\\mathbf{v}\\) has the same direction as \\(\\mathbf{v}\\) and length \\(|k|\\) times \\(\\mathbf{v}\\).\nIf \\(k &lt; 0\\), \\(k\\mathbf{v}\\) has the opposite direction of \\(\\mathbf{v}\\) and length \\(|k|\\) times \\(\\mathbf{v}\\).\nIf \\(k = 0\\) or \\(\\mathbf{v} = \\mathbf{0}\\), then \\(k\\mathbf{v} = \\mathbf{0}\\).\n\nFrom this, we see: \\[\n(-1)\\mathbf{v} = -\\mathbf{v} \\tag{3}\n\\]\n\n\nScalar multiplication is how we scale vectors. In ECE, scaling a voltage vector by a gain factor, or scaling a force vector, are common applications. The magnitude of the scalar \\(k\\) stretches or shrinks the vector, while its sign determines the direction."
  },
  {
    "objectID": "la-31.html#interactive-scalar-multiplication-2d",
    "href": "la-31.html#interactive-scalar-multiplication-2d",
    "title": "Linear Algebra",
    "section": "Interactive Scalar Multiplication (2D)",
    "text": "Interactive Scalar Multiplication (2D)\nAdjust the scalar \\(k\\) and vector \\(\\mathbf{v}\\) to see the result \\(k\\mathbf{v}\\).\n\nviewof k_scalar = Inputs.range([-2, 2], {step: 0.1, label: \"k\"});\nviewof sv_x = Inputs.range([-3, 3], {step: 0.1, label: \"v_x\"});\nviewof sv_y = Inputs.range([-3, 3], {step: 0.1, label: \"v_y\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, you can experiment with scalar multiplication. Change k and observe how the vector kv scales and potentially reverses direction. What happens when k is 0? What about k=1 or k=-1? This interactive element helps solidify the geometric interpretation of scalar multiplication."
  },
  {
    "objectID": "la-31.html#parallel-and-collinear-vectors",
    "href": "la-31.html#parallel-and-collinear-vectors",
    "title": "Linear Algebra",
    "section": "Parallel and Collinear Vectors",
    "text": "Parallel and Collinear Vectors\nIf one vector is a scalar multiple of another (\\(\\mathbf{w} = k\\mathbf{v}\\)), they are parallel.\n\nIf they share a common initial point, they are also collinear.\nTranslating a vector does not change it, so parallel and collinear mean the same thing for vectors.\nThe zero vector \\(\\mathbf{0}\\) is regarded as parallel to all vectors.\n\n\n\n\nParallel and collinear vectors.\n\n\n\nThe distinction between parallel and collinear can be confusing in everyday language. In linear algebra, when we say vectors are parallel, we mean they point in the same or opposite directions, regardless of their starting points. Collinear implies they lie on the same line. Since vectors can be moved, these terms become interchangeable."
  },
  {
    "objectID": "la-31.html#sums-of-three-or-more-vectors",
    "href": "la-31.html#sums-of-three-or-more-vectors",
    "title": "Linear Algebra",
    "section": "Sums of Three or More Vectors",
    "text": "Sums of Three or More Vectors\nVector addition is associative: \\[\n\\mathbf{u} + (\\mathbf{v} + \\mathbf{w}) = (\\mathbf{u} + \\mathbf{v}) + \\mathbf{w}\n\\] This means the order of grouping doesn’t matter.\nTip-to-Tail Method: Place vectors sequentially (tip-to-tail). The sum is from the initial point of the first to the terminal point of the last.\n\nSum of three vectors (tip-to-tail).\nThe associative property simplifies multi-vector additions. The tip-to-tail method is a powerful visualization for any number of vectors. Think of navigating a robot: each vector is a movement command. The total displacement is the sum. The image on the right (3.1.9c) also shows the sum of three vectors as the main diagonal of a parallelepiped, extending the parallelogram rule to 3D."
  },
  {
    "objectID": "la-31.html#vectors-in-coordinate-systems",
    "href": "la-31.html#vectors-in-coordinate-systems",
    "title": "Linear Algebra",
    "section": "Vectors in Coordinate Systems",
    "text": "Vectors in Coordinate Systems\nComputations are simpler with coordinate systems.\n\nIf a vector \\(\\mathbf{v}\\) has its initial point at the origin, its components are the coordinates of its terminal point.\n\n2-space: \\(\\mathbf{v} = (v_1, v_2)\\)\n3-space: \\(\\mathbf{v} = (v_1, v_2, v_3)\\)\n\nThe zero vector: \\(\\mathbf{0} = (0,0)\\) in 2-space, \\(\\mathbf{0} = (0,0,0)\\) in 3-space.\n\n\nVectors in coordinate systems.\nMoving from geometric intuition to coordinate systems is where linear algebra becomes computational. This allows us to perform operations numerically, which is essential for ECE applications like signal processing or control systems. The components essentially give us a unique “address” for each vector."
  },
  {
    "objectID": "la-31.html#equivalent-vectors-in-coordinates",
    "href": "la-31.html#equivalent-vectors-in-coordinates",
    "title": "Linear Algebra",
    "section": "Equivalent Vectors in Coordinates",
    "text": "Equivalent Vectors in Coordinates\nTwo vectors are equivalent if and only if their corresponding components are equal.\nFor \\(\\mathbf{v} = (v_1, v_2, v_3)\\) and \\(\\mathbf{w} = (w_1, w_2, w_3)\\) in 3-space: \\[\n\\mathbf{v} = \\mathbf{w} \\quad \\text{if and only if} \\quad v_1 = w_1, \\quad v_2 = w_2, \\quad v_3 = w_3\n\\]\n\n\n\n\n\n\nNote\n\n\nPoint vs. Vector: An ordered pair \\((v_1, v_2)\\) can represent a point (a location) or a vector (a displacement from the origin). The context determines the interpretation.\n\n\n\n\nThe ordered pair \\((v_1, v_2)\\) can represent a point or a vector.\nThis definition of equivalence is crucial for algebraic manipulation. It means we can compare vectors component by component. The callout highlights a common source of confusion: the dual interpretation of coordinates. In ECE, sometimes we care about the specific coordinates of a sensor, and sometimes about the vector representing its measurement."
  },
  {
    "objectID": "la-31.html#vectors-not-at-the-origin",
    "href": "la-31.html#vectors-not-at-the-origin",
    "title": "Linear Algebra",
    "section": "Vectors Not at the Origin",
    "text": "Vectors Not at the Origin\nIf a vector’s initial point is not the origin, its components are found by subtracting the initial point’s coordinates from the terminal point’s.\n\nFor \\(\\overrightarrow{P_1P_2}\\) with \\(P_1(x_1,y_1)\\) and \\(P_2(x_2,y_2)\\): \\[\n\\overrightarrow{P_1P_2} = (x_2 - x_1, y_2 - y_1) \\tag{4}\n\\]\nFor \\(\\overrightarrow{P_1P_2}\\) with \\(P_1(x_1,y_1,z_1)\\) and \\(P_2(x_2,y_2,z_2)\\): \\[\n\\overrightarrow{P_1P_2} = (x_2 - x_1, y_2 - y_1, z_2 - z_1) \\tag{5}\n\\]"
  },
  {
    "objectID": "la-31.html#vectors-not-at-the-origin-1",
    "href": "la-31.html#vectors-not-at-the-origin-1",
    "title": "Linear Algebra",
    "section": "Vectors Not at the Origin",
    "text": "Vectors Not at the Origin\n\nVector \\(\\overrightarrow{P_1P_2}\\) as difference of position vectors.\nThis formula effectively translates any vector so its tail is at the origin, allowing us to represent it by its terminal point’s coordinates. This is a standard way to represent displacement vectors in engineering. For example, the relative position of two components on a circuit board can be described this way."
  },
  {
    "objectID": "la-31.html#example-1-finding-the-components-of-a-vector",
    "href": "la-31.html#example-1-finding-the-components-of-a-vector",
    "title": "Linear Algebra",
    "section": "EXAMPLE 1: Finding the Components of a Vector",
    "text": "EXAMPLE 1: Finding the Components of a Vector\nFind the components of the vector \\(\\mathbf{v} = \\overrightarrow{P_1P_2}\\) with:\nInitial point \\(P_1(2, -1, 4)\\)\nTerminal point \\(P_2(7, 5, -8)\\)\n\\[\n\\mathbf{v} = (7 - 2, 5 - (-1), (-8) - 4) = (5, 6, -12)\n\\]\nInteractive Component Calculator\nEnter coordinates to find vector components.\n\n\n\n\n\n\n\nThis example shows a direct application of the formula. The interactive calculator allows students to practice this skill with different coordinate sets. It’s a straightforward but fundamental calculation."
  },
  {
    "objectID": "la-31.html#n-space-generalizing-dimensions",
    "href": "la-31.html#n-space-generalizing-dimensions",
    "title": "Linear Algebra",
    "section": "\\(n\\)-Space: Generalizing Dimensions",
    "text": "\\(n\\)-Space: Generalizing Dimensions\nWhat if we need more than three dimensions?\n\nOrdered \\(n\\)-tuple: A sequence of \\(n\\) real numbers \\((v_1, v_2, \\ldots, v_n)\\).\n\\(n\\)-Space (\\(R^n\\)): The set of all ordered \\(n\\)-tuples.\n\n\n\n\n\n\n\nTip\n\n\nThink of \\(n\\)-tuples as: - Coordinates of a generalized point. - Components of a generalized vector. The choice depends on the problem context.\n\n\n\n\nThis is where linear algebra truly generalizes beyond our visual intuition. While we can’t see 4D or 11D space, the algebraic rules extend perfectly. This abstraction is powerful for modeling complex systems in ECE. For example, in signal processing, a signal sampled at \\(N\\) points can be represented as a vector in \\(R^N\\)."
  },
  {
    "objectID": "la-31.html#applications-of-n-tuples-in-ece-and-beyond",
    "href": "la-31.html#applications-of-n-tuples-in-ece-and-beyond",
    "title": "Linear Algebra",
    "section": "Applications of \\(n\\)-tuples in ECE and Beyond",
    "text": "Applications of \\(n\\)-tuples in ECE and Beyond\n\n\n\nExperimental Data: \\(n\\) measurements form a vector \\(\\mathbf{y} = (y_1, \\ldots, y_n)\\) in \\(R^n\\).\n\nExample: Sensor readings from an array.\n\nStorage/Warehousing: Distribution of items across \\(n\\) locations: \\(\\mathbf{x} = (x_1, \\ldots, x_n)\\).\n\nExample: Inventory levels in a distributed system.\n\nElectrical Circuits: Input/output voltages for a chip.\n\nInput \\(\\mathbf{v} \\in R^4\\), Output \\(\\mathbf{w} \\in R^3\\). Chip maps \\(R^4 \\to R^3\\).\n\n\n\n\nGraphical Images: Pixel data (x, y, hue, saturation, brightness) as a 5-tuple.\n\n\\(\\mathbf{v} = (x, y, h, s, b)\\).\n\nEconomics: Economic output of \\(n\\) sectors as \\(\\mathbf{s} = (s_1, \\ldots, s_n)\\).\nMechanical Systems: State of particles (position, velocity, time).\n\n\\(\\mathbf{v} = (x_1, \\ldots, x_6, v_1, \\ldots, v_6, t)\\) in \\(R^{13}\\).\n\n\n\n\nThese examples illustrate the wide applicability of \\(n\\)-space. In ECE, think of state-space representations in control systems, feature vectors in machine learning, or even the parameters of a complex circuit model. The ability to represent diverse information as vectors in \\(R^n\\) is a cornerstone of modern engineering analysis."
  },
  {
    "objectID": "la-31.html#vector-equivalence-in-rn",
    "href": "la-31.html#vector-equivalence-in-rn",
    "title": "Linear Algebra",
    "section": "Vector Equivalence in \\(R^n\\)",
    "text": "Vector Equivalence in \\(R^n\\)\nVectors \\(\\mathbf{v} = (v_1, \\ldots, v_n)\\) and \\(\\mathbf{w} = (w_1, \\ldots, w_n)\\) in \\(R^n\\) are equivalent (equal) if:\n\\[\nv_1 = w_1, \\quad v_2 = w_2, \\quad \\ldots, \\quad v_n = w_n\n\\] We write this as \\(\\mathbf{v} = \\mathbf{w}\\).\nEXAMPLE 2: Equality of Vectors\n\\[\n(a, b, c, d) = (1, -4, 2, 7)\n\\] if and only if \\(a = 1, b = -4, c = 2,\\) and \\(d = 7\\).\n\nThis definition of equality is a direct extension from 2D and 3D. It means that for two vectors to be considered the same, every single one of their corresponding components must match. This is fundamental for solving vector equations and verifying identities."
  },
  {
    "objectID": "la-31.html#algebraic-operations-in-rn-component-wise",
    "href": "la-31.html#algebraic-operations-in-rn-component-wise",
    "title": "Linear Algebra",
    "section": "Algebraic Operations in \\(R^n\\) (Component-wise)",
    "text": "Algebraic Operations in \\(R^n\\) (Component-wise)\nOperations on vectors in \\(R^n\\) are natural extensions of those in \\(R^2\\) and \\(R^3\\).\nFor \\(\\mathbf{v} = (v_1, v_2)\\) and \\(\\mathbf{w} = (w_1, w_2)\\) in \\(R^2\\): \\[\n\\begin{array}{l}\n\\mathbf{v} + \\mathbf{w} = (v_1 + w_1, v_2 + w_2) \\\\\nk\\mathbf{v} = (k v_1, k v_2)\n\\end{array} \\tag{7}\n\\] And similarly for subtraction and negative vectors: \\[\n-\\mathbf{v} = (-v_1, -v_2) \\tag{8}\n\\] \\[\n\\mathbf{w} - \\mathbf{v} = (w_1 - v_1, w_2 - v_2) \\tag{9}\n\\]"
  },
  {
    "objectID": "la-31.html#algebraic-operations-in-rn-component-wise-1",
    "href": "la-31.html#algebraic-operations-in-rn-component-wise-1",
    "title": "Linear Algebra",
    "section": "Algebraic Operations in \\(R^n\\) (Component-wise)",
    "text": "Algebraic Operations in \\(R^n\\) (Component-wise)\n\nFigure 3.1.13: Geometric interpretation of component-wise operations.\nThe beauty of coordinate systems is that vector operations become simple arithmetic on their components. This ‘component-wise’ approach is the workhorse of computational linear algebra. This slide shows how these operations are defined for 2D vectors, setting the stage for \\(n\\)-space."
  },
  {
    "objectID": "la-31.html#formal-definitions-of-operations-in-rn",
    "href": "la-31.html#formal-definitions-of-operations-in-rn",
    "title": "Linear Algebra",
    "section": "Formal Definitions of Operations in \\(R^n\\)",
    "text": "Formal Definitions of Operations in \\(R^n\\)\nIf \\(\\mathbf{v} = (v_1, \\ldots, v_n)\\) and \\(\\mathbf{w} = (w_1, \\ldots, w_n)\\) are vectors in \\(R^n\\), and \\(k\\) is a scalar:\n\\[\n\\begin{array}{r l}\n\\mathbf{v} + \\mathbf{w} &= (v_1 + w_1, v_2 + w_2, \\ldots, v_n + w_n)\\\\\nk\\mathbf{v} &= (k v_1, k v_2, \\ldots, k v_n)\\\\\n-\\mathbf{v} &= (-v_1, -v_2, \\ldots, -v_n)\\\\\n\\mathbf{w} - \\mathbf{v} &= (w_1 - v_1, w_2 - v_2, \\ldots, w_n - v_n)\n\\end{array} \\tag{13}\n\\]\nEXAMPLE 3: Algebraic Operations Using Components\nIf \\(\\mathbf{v} = (1, -3, 2)\\) and \\(\\mathbf{w} = (4, 2, 1)\\), then: \\[\n\\begin{array}{r l}\n\\mathbf{v} + \\mathbf{w} &= (5, -1, 3) \\\\\n2\\mathbf{v} &= (2, -6, 4) \\\\\n-\\mathbf{w} &= (-4, -2, -1) \\\\\n\\mathbf{v} - \\mathbf{w} &= (-3, -5, 1)\n\\end{array}\n\\]\n\nThese are the formal definitions for \\(n\\)-space. It’s crucial to understand that all these operations are performed element-by-element on the vector components. The example illustrates these calculations with specific 3D vectors."
  },
  {
    "objectID": "la-31.html#interactive-vector-operations-in-r3",
    "href": "la-31.html#interactive-vector-operations-in-r3",
    "title": "Linear Algebra",
    "section": "Interactive Vector Operations in \\(R^3\\)",
    "text": "Interactive Vector Operations in \\(R^3\\)\nPerform vector addition, subtraction, and scalar multiplication.\n\n\n\n\n\n\n\nThis Pyodide block lets you directly compute vector operations using NumPy. NumPy is a standard library in Python for numerical computing and handles vector/matrix operations very efficiently. You can modify the vectors v, w, and the scalar k in the code block and re-run to see different results. This is how these operations are typically performed in engineering software."
  },
  {
    "objectID": "la-31.html#properties-of-vector-operations-theorem-3.1.1",
    "href": "la-31.html#properties-of-vector-operations-theorem-3.1.1",
    "title": "Linear Algebra",
    "section": "Properties of Vector Operations (Theorem 3.1.1)",
    "text": "Properties of Vector Operations (Theorem 3.1.1)\nIf \\(\\mathbf{u},\\mathbf{v},\\) and \\(\\mathbf{w}\\) are vectors in \\(R^n\\), and \\(k\\) and \\(m\\) are scalars:\n\n\\(\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\\) (Commutative Law for Addition)\n\\((\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})\\) (Associative Law for Addition)\n\\(\\mathbf{u} + \\mathbf{0} = \\mathbf{0} + \\mathbf{u} = \\mathbf{u}\\) (Additive Identity)\n\\(\\mathbf{u} + (-\\mathbf{u}) = \\mathbf{0}\\) (Additive Inverse)\n\\(k(\\mathbf{u} + \\mathbf{v}) = k\\mathbf{u} + k\\mathbf{v}\\) (Distributive Law)\n\\((k + m)\\mathbf{u} = k\\mathbf{u} + m\\mathbf{u}\\) (Distributive Law)\n\\(k(m\\mathbf{u}) = (km)\\mathbf{u}\\) (Associative Law for Scalar Multiplication)\n\\(1\\mathbf{u} = \\mathbf{u}\\) (Multiplicative Identity)\n\n\nThese properties are fundamental axioms that define a vector space. They are essential for manipulating vector equations and proving more complex theorems. They are direct extensions of properties you know from real number arithmetic."
  },
  {
    "objectID": "la-31.html#proof-of-associative-law-b",
    "href": "la-31.html#proof-of-associative-law-b",
    "title": "Linear Algebra",
    "section": "Proof of Associative Law (b)",
    "text": "Proof of Associative Law (b)\nLet \\(\\mathbf{u} = (u_1, \\ldots, u_n)\\), \\(\\mathbf{v} = (v_1, \\ldots, v_n)\\), \\(\\mathbf{w} = (w_1, \\ldots, w_n)\\).\n\\[\n\\begin{array}{r l}\n(\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} &= \\big((u_1,\\ldots,u_n) + (v_1,\\ldots,v_n)\\big) + (w_1,\\ldots,w_n)\\\\\n&= (u_1 + v_1, \\ldots, u_n + v_n) + (w_1,\\ldots,w_n) & \\text{[Vector addition]} \\\\\n&= \\big((u_1 + v_1) + w_1, \\ldots, (u_n + v_n) + w_n\\big) & \\text{[Vector addition]} \\\\\n&= \\big(u_1 + (v_1 + w_1), \\ldots, u_n + (v_n + w_n)\\big) & \\text{[Regroup real numbers]} \\\\\n&= (u_1,\\ldots,u_n) + (v_1 + w_1, \\ldots, v_n + w_n) & \\text{[Vector addition]} \\\\\n&= \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})\n\\end{array}\n\\]\n\nThis proof demonstrates how vector properties are derived directly from the properties of real numbers applied to their components. The key step is “Regroup real numbers,” where the associative property of scalar addition is used. This shows the consistency of our definitions."
  },
  {
    "objectID": "la-31.html#additional-vector-properties-theorem-3.1.2",
    "href": "la-31.html#additional-vector-properties-theorem-3.1.2",
    "title": "Linear Algebra",
    "section": "Additional Vector Properties (Theorem 3.1.2)",
    "text": "Additional Vector Properties (Theorem 3.1.2)\nIf \\(\\mathbf{v}\\) is a vector in \\(R^n\\) and \\(k\\) is a scalar:\n\n\\(0\\mathbf{v} = \\mathbf{0}\\)\n\\(k\\mathbf{0} = \\mathbf{0}\\)\n\\((-1)\\mathbf{v} = -\\mathbf{v}\\)\n\n\nThese properties are also straightforward to prove using component-wise operations. They highlight the behavior of the zero scalar and the zero vector, and reinforce the definition of the negative of a vector."
  },
  {
    "objectID": "la-31.html#calculating-without-components",
    "href": "la-31.html#calculating-without-components",
    "title": "Linear Algebra",
    "section": "Calculating Without Components",
    "text": "Calculating Without Components\nThese theorems allow algebraic manipulation of vector equations without explicit component calculations.\nExample: Solve \\(\\mathbf{x} + \\mathbf{a} = \\mathbf{b}\\) for \\(\\mathbf{x}\\).\n\\[\n\\begin{array}{r l r l}\n\\mathbf{x} + \\mathbf{a} &= \\mathbf{b} & & \\mathrm{[Given]}\\\\\n(\\mathbf{x} + \\mathbf{a}) + (-\\mathbf{a}) &= \\mathbf{b} + (-\\mathbf{a}) & & \\mathrm{[Add~negative~of~a~to~both~sides]}\\\\\n\\mathbf{x} + (\\mathbf{a} + (-\\mathbf{a})) &= \\mathbf{b} - \\mathbf{a} & & \\mathrm{[Part~}(b)\\mathrm{~of~Thm~}3.1.1]\\\\\n\\mathbf{x} + \\mathbf{0} &= \\mathbf{b} - \\mathbf{a} & & \\mathrm{[Part~}(d)\\mathrm{~of~Thm~}3.1.1]\\\\\n\\mathbf{x} &= \\mathbf{b} - \\mathbf{a} & & \\mathrm{[Part~}(c)\\mathrm{~of~Thm~}3.1.1]\n\\end{array}\n\\]\n\nWhile it seems more cumbersome for \\(R^n\\), this abstract method is crucial when dealing with more general vector spaces where components might not be as straightforward (e.g., function spaces). It shows that the algebraic structure of vector operations is consistent and powerful."
  },
  {
    "objectID": "la-31.html#linear-combinations",
    "href": "la-31.html#linear-combinations",
    "title": "Linear Algebra",
    "section": "Linear Combinations",
    "text": "Linear Combinations\nA vector \\(\\mathbf{w}\\) in \\(R^n\\) is a linear combination of vectors \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_r\\) in \\(R^n\\) if it can be expressed as:\n\\[\n\\mathbf{w} = k_1\\mathbf{v}_1 + k_2\\mathbf{v}_2 + \\dots + k_r\\mathbf{v}_r \\tag{14}\n\\] where \\(k_1, \\ldots, k_r\\) are scalars (coefficients).\nExample: If \\(\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\) are vectors:\n\\(\\mathbf{u} = 2\\mathbf{v}_1 + 3\\mathbf{v}_2 + \\mathbf{v}_3\\)\n\\(\\mathbf{w} = 7\\mathbf{v}_1 - 6\\mathbf{v}_2 + 8\\mathbf{v}_3\\)\n\n\n\n\n\n\nImportant\n\n\nLinear combinations are central to many concepts in linear algebra, including span, basis, and transformations.\n\n\n\n\nLinear combinations are one of the most important concepts in linear algebra. They allow us to build new vectors from existing ones. In ECE, think of superposition in circuits, where the total response is a linear combination of responses to individual inputs."
  },
  {
    "objectID": "la-31.html#interactive-linear-combination",
    "href": "la-31.html#interactive-linear-combination",
    "title": "Linear Algebra",
    "section": "Interactive Linear Combination",
    "text": "Interactive Linear Combination\nCalculate a linear combination of two 2D vectors.\n\nviewof lc_k1 = Inputs.range([-2, 2], {step: 0.1, label: \"k1\"});\nviewof lc_k2 = Inputs.range([-2, 2], {step: 0.1, label: \"k2\"});\nviewof lc_v1x = Inputs.range([-3, 3], {step: 0.1, label: \"v1_x\"});\nviewof lc_v1y = Inputs.range([-3, 3], {step: 0.1, label: \"v1_y\"});\nviewof lc_v2x = Inputs.range([-3, 3], {step: 0.1, label: \"v2_x\"});\nviewof lc_v2y = Inputs.range([-3, 3], {step: 0.1, label: \"v2_y\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive visualization demonstrates how a linear combination of two vectors creates a new vector. By adjusting the scalars k1, k2 and the vectors v1, v2, you can explore how the resultant vector k1*v1 + k2*v2 changes. Notice that the resultant vector always lies within the plane defined by v1 and v2 (if they are not collinear)."
  },
  {
    "objectID": "la-31.html#alternative-notations-for-vectors",
    "href": "la-31.html#alternative-notations-for-vectors",
    "title": "Linear Algebra",
    "section": "Alternative Notations for Vectors",
    "text": "Alternative Notations for Vectors\nVectors can be written in several forms, depending on context and convenience.\n\nComma-delimited form: \\[\n\\mathbf{v} = (v_1, v_2, \\ldots, v_n) \\tag{15}\n\\]\nRow-vector form: \\[\n\\mathbf{v} = [v_1 \\quad v_2 \\quad \\dots \\quad v_n] \\tag{16}\n\\]\nColumn-vector form: \\[\n\\mathbf{v} = \\left[ \\begin{array}{c}v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{array} \\right] \\tag{17}\n\\]\n\n\nWhile the comma-delimited form is common for general vector concepts, row and column vector forms are frequently used when vectors interact with matrices, like in matrix multiplication. In MATLAB or NumPy, column vectors are often preferred for certain linear algebra operations. It’s important to be comfortable with all notations."
  },
  {
    "objectID": "la-31.html#application-rgb-color-models",
    "href": "la-31.html#application-rgb-color-models",
    "title": "Linear Algebra",
    "section": "Application: RGB Color Models",
    "text": "Application: RGB Color Models\nRGB color model: colors on computer monitors are created by adding percentages of Red (R), Green (G), and Blue (B).\n\nPrimary colors as vectors in \\(R^3\\):\n\n\\(\\mathbf{r} = (1,0,0)\\) (pure red)\n\\(\\mathbf{g} = (0,1,0)\\) (pure green)\n\\(\\mathbf{b} = (0,0,1)\\) (pure blue)\n\nAny color \\(\\mathbf{c}\\) in the RGB color cube is a linear combination: \\[\n\\begin{array}{rl}\n\\mathbf{c} &= k_1\\mathbf{r} + k_2\\mathbf{g} + k_3\\mathbf{b}\\\\\n&= k_1(1,0,0) + k_2(0,1,0) + k_3(0,0,1)\\\\\n&= (k_1,k_2,k_3)\n\\end{array}\n\\] where \\(0 \\leq k_i \\leq 1\\)."
  },
  {
    "objectID": "la-31.html#application-rgb-color-models-1",
    "href": "la-31.html#application-rgb-color-models-1",
    "title": "Linear Algebra",
    "section": "Application: RGB Color Models",
    "text": "Application: RGB Color Models\n\nRGB Color Cube.\nThe RGB color model is a perfect real-world application of linear combinations and vectors in \\(R^3\\). Each color is a point (or vector) in a 3D space, where the axes represent red, green, and blue intensity. This is directly relevant to ECE students working with displays, image processing, or digital media."
  },
  {
    "objectID": "la-31.html#interactive-rgb-color-mixer",
    "href": "la-31.html#interactive-rgb-color-mixer",
    "title": "Linear Algebra",
    "section": "Interactive RGB Color Mixer",
    "text": "Interactive RGB Color Mixer\nAdjust the Red, Green, and Blue components (\\(k_1, k_2, k_3\\)) to mix colors.\n\nviewof red_comp = Inputs.range([0, 1], {step: 0.01, label: \"Red (k1)\"});\nviewof green_comp = Inputs.range([0, 1], {step: 0.01, label: \"Green (k2)\"});\nviewof blue_comp = Inputs.range([0, 1], {step: 0.01, label: \"Blue (k3)\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a highly interactive demonstration of linear combinations in a practical ECE context. Adjust the Red, Green, and Blue sliders. Observe how the “Mixed Color” point moves within the RGB color cube and how its color changes. You can create a wide spectrum of colors by varying these three components. This directly shows how any color within the cube is a linear combination of the primary basis vectors."
  },
  {
    "objectID": "la-31.html#summary-and-key-takeaways",
    "href": "la-31.html#summary-and-key-takeaways",
    "title": "Linear Algebra",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\n\nGeometric Vectors: Visualized as arrows, defined by magnitude and direction.\nVector Operations: Addition, subtraction, and scalar multiplication have clear geometric and algebraic interpretations.\nCoordinate Systems: Allow for algebraic manipulation of vectors via components.\n\\(n\\)-Space: Generalizes vector concepts to higher dimensions, crucial for complex data.\nLinear Combinations: Fundamental for building new vectors and understanding vector relationships.\nReal-world Applications: Vectors are ubiquitous in ECE, from signal processing to computer graphics.\n\n\nWe’ve covered the foundational concepts of vectors, moving from intuitive geometric representations to formal algebraic definitions in \\(n\\)-space. Understanding these basics is essential for the rest of your linear algebra journey. Remember, linear algebra is the language of modern engineering and data science. Feel free to ask any questions!"
  },
  {
    "objectID": "la-33.html#introduction-to-orthogonality",
    "href": "la-33.html#introduction-to-orthogonality",
    "title": "Linear Algebra",
    "section": "Introduction to Orthogonality",
    "text": "Introduction to Orthogonality\nIn the previous section, we defined the “angle” between vectors using the dot product. Now, we focus on perpendicularity, a special case of the angle between vectors. Perpendicular (orthogonal) vectors are crucial in many applications, especially in ECE.\nRecall the angle \\(\\theta\\) between nonzero vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) in \\(R^n\\): \\[\n\\theta = \\cos^{-1}\\left(\\frac{\\mathbf{u}\\cdot\\mathbf{v}}{\\|\\mathbf{u}\\|\\|\\mathbf{v}\\|}\\right)\n\\] From this, \\(\\theta = \\pi/2\\) (90 degrees) if and only if \\(\\mathbf{u}\\cdot \\mathbf{v} = 0\\).\n\nOrthogonality is a cornerstone concept in linear algebra. It’s the algebraic way of saying “perpendicular.” In ECE, orthogonal signals are uncorrelated, orthogonal basis vectors simplify coordinate transformations, and orthogonal transformations preserve length and angle."
  },
  {
    "objectID": "la-33.html#orthogonal-vectors-definition-1",
    "href": "la-33.html#orthogonal-vectors-definition-1",
    "title": "Linear Algebra",
    "section": "Orthogonal Vectors (Definition 1)",
    "text": "Orthogonal Vectors (Definition 1)\nTwo nonzero vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) in \\(R^n\\) are orthogonal (or perpendicular) if \\(\\mathbf{u}\\cdot \\mathbf{v} = 0\\).\nThe zero vector \\(\\mathbf{0}\\) is defined to be orthogonal to every vector in \\(R^n\\).\nEXAMPLE 1: Orthogonal Vectors\n\nShow that \\(\\mathbf{u} = (-2,3,1,4)\\) and \\(\\mathbf{v} = (1,2,0,-1)\\) are orthogonal in \\(R^4\\). \\[\n\\mathbf{u}\\cdot \\mathbf{v} = (-2)(1) + (3)(2) + (1)(0) + (4)(-1) = -2 + 6 + 0 - 4 = 0\n\\] Since \\(\\mathbf{u}\\cdot \\mathbf{v} = 0\\), they are orthogonal.\nShow that standard unit vectors in \\(R^3\\) are orthogonal.\n\n\\(\\mathbf{i} = (1,0,0), \\mathbf{j} = (0,1,0), \\mathbf{k} = (0,0,1)\\)\n\\(\\mathbf{i}\\cdot \\mathbf{j} = (1)(0) + (0)(1) + (0)(0) = 0\\)\n\\(\\mathbf{i}\\cdot \\mathbf{k} = (1)(0) + (0)(0) + (0)(1) = 0\\)\n\\(\\mathbf{j}\\cdot \\mathbf{k} = (0)(0) + (1)(0) + (0)(1) = 0\\)\nEach pair is orthogonal. This extends to \\(R^n\\).\n\nThe simple dot product check is the most efficient way to determine orthogonality. Standard unit vectors are a classic example of an orthogonal set, which forms the basis for Cartesian coordinate systems. This property is fundamental for simplifying many problems by rotating coordinate systems."
  },
  {
    "objectID": "la-33.html#interactive-orthogonality-checker",
    "href": "la-33.html#interactive-orthogonality-checker",
    "title": "Linear Algebra",
    "section": "Interactive Orthogonality Checker",
    "text": "Interactive Orthogonality Checker\nEnter two vectors in \\(R^n\\) to check if they are orthogonal.\n\n\n\n\n\n\n\nThis interactive tool allows you to quickly test for orthogonality. Input various pairs of vectors. Try some from the examples, and then create your own. What happens if one of the vectors is the zero vector?"
  },
  {
    "objectID": "la-33.html#lines-and-planes-determined-by-normals",
    "href": "la-33.html#lines-and-planes-determined-by-normals",
    "title": "Linear Algebra",
    "section": "Lines and Planes Determined by Normals",
    "text": "Lines and Planes Determined by Normals\nA line in \\(R^2\\) or a plane in \\(R^3\\) can be uniquely defined by a point \\(P_0\\) and a nonzero normal vector \\(\\mathbf{n}\\) that is orthogonal to the line/plane.\nThe vector equation for both cases is: \\[\n\\mathbf{n}\\cdot \\overrightarrow{P_0P} = 0 \\tag{1}\n\\] where \\(P\\) is an arbitrary point on the line/plane.\nFor a line in \\(R^2\\):\n\\(P_0(x_0,y_0)\\), \\(\\mathbf{n}=(a,b)\\), \\(P(x,y)\\)\n\\(\\overrightarrow{P_0P} = (x-x_0, y-y_0)\\)\nPoint-Normal Equation: \\[\na(x - x_0) + b(y - y_0) = 0 \\tag{2}\n\\]\n\n\n\n\n\n\nNote\n\n\nDefining Lines and Planes with Vectors. A line (in ℝ²) or a plane (in ℝ³) can be uniquely described using:\n\nA point that lies on the line or plane.\nA normal vector that is perpendicular (orthogonal) to the line or plane."
  },
  {
    "objectID": "la-33.html#lines-and-planes-determined-by-normals-1",
    "href": "la-33.html#lines-and-planes-determined-by-normals-1",
    "title": "Linear Algebra",
    "section": "Lines and Planes Determined by Normals",
    "text": "Lines and Planes Determined by Normals\nFor a plane in \\(R^3\\):\n\\(P_0(x_0,y_0,z_0)\\), \\(\\mathbf{n}=(a,b,c)\\), \\(P(x,y,z)\\)\n\\(\\overrightarrow{P_0P} = (x-x_0, y-y_0, z-z_0)\\)\nPoint-Normal Equation: \\[\na(x - x_0) + b(y - y_0) + c(z - z_0) = 0 \\tag{3}\n\\]\n\nFigure 3.3.1: Line in \\(R^2\\) and plane in \\(R^3\\) with normal vectors.\nThe normal vector is like a compass for the line or plane, indicating its orientation. This formulation is very powerful because it allows us to describe these geometric objects using vector algebra. In ECE, understanding normal vectors is crucial for fields like computer graphics (lighting calculations), electromagnetics (surface integrals), and control systems (defining constraint surfaces)."
  },
  {
    "objectID": "la-33.html#example-2-point-normal-equations",
    "href": "la-33.html#example-2-point-normal-equations",
    "title": "Linear Algebra",
    "section": "EXAMPLE 2: Point-Normal Equations",
    "text": "EXAMPLE 2: Point-Normal Equations\n\nIn \\(R^2\\), the equation \\(6(x - 3) + (y + 7) = 0\\) represents the line through \\(P_0(3, -7)\\) with normal \\(\\mathbf{n}=(6,1)\\).\nIn \\(R^3\\), the equation \\(4(x - 3) + 2y - 5(z - 7) = 0\\) represents the plane through \\(P_0(3,0,7)\\) with normal \\(\\mathbf{n}=(4,2,-5)\\).\n\nTHEOREM 3.3.1 (General Forms)\n\nA line in \\(R^2\\) with normal \\(\\mathbf{n}=(a,b)\\) has equation: \\[\nax + by + c = 0 \\tag{4}\n\\]\nA plane in \\(R^3\\) with normal \\(\\mathbf{n}=(a,b,c)\\) has equation: \\[\nax + by + cz + d = 0 \\tag{5}\n\\] (where \\(a,b\\) are not both zero, and \\(a,b,c\\) are not all zero, respectively)\n\n\nThese examples show how to interpret the point-normal form and how it simplifies to the general linear equation. The coefficients of \\(x, y, z\\) in the general form directly give you the normal vector. This is a quick way to find the orientation of a line or plane."
  },
  {
    "objectID": "la-33.html#example-3-vectors-orthogonal-to-lines-and-planes-through-the-origin",
    "href": "la-33.html#example-3-vectors-orthogonal-to-lines-and-planes-through-the-origin",
    "title": "Linear Algebra",
    "section": "EXAMPLE 3: Vectors Orthogonal to Lines and Planes Through the Origin",
    "text": "EXAMPLE 3: Vectors Orthogonal to Lines and Planes Through the Origin\n\nThe equation \\(ax + by = 0\\) represents a line through the origin in \\(R^2\\). Show that \\(\\mathbf{n}_1 = (a,b)\\) is orthogonal to every vector along the line.\nThe equation \\(ax + by + cz = 0\\) represents a plane through the origin in \\(R^3\\). Show that \\(\\mathbf{n}_2 = (a,b,c)\\) is orthogonal to every vector in the plane.\n\nSolution: Both equations can be written in the vector form: \\[\n\\mathbf{n}\\cdot \\mathbf{x} = 0 \\tag{6}\n\\] where \\(\\mathbf{n}\\) is the vector of coefficients (e.g., \\((a,b)\\) or \\((a,b,c)\\)) and \\(\\mathbf{x}\\) is the vector of unknowns (e.g., \\((x,y)\\) or \\((x,y,z)\\)). This equation directly states that \\(\\mathbf{n}\\) is orthogonal to any vector \\(\\mathbf{x}\\) that satisfies the equation (i.e., lies on the line or in the plane).\n\nThis example specifically addresses lines and planes passing through the origin. The vector form \\(\\mathbf{n}\\cdot \\mathbf{x} = 0\\) is a very compact and powerful way to express orthogonality. It shows that the set of all vectors orthogonal to a given vector forms a line (in 2D) or a plane (in 3D) through the origin."
  },
  {
    "objectID": "la-33.html#orthogonal-projections",
    "href": "la-33.html#orthogonal-projections",
    "title": "Linear Algebra",
    "section": "Orthogonal Projections",
    "text": "Orthogonal Projections\nOften, we need to decompose a vector \\(\\mathbf{u}\\) into two components:\n\nOne component (\\(\\mathbf{w}_1\\)) that is a scalar multiple of a specified nonzero vector \\(\\mathbf{a}\\).\nAnother component (\\(\\mathbf{w}_2\\)) that is orthogonal to \\(\\mathbf{a}\\).\n\nVisually: Drop a perpendicular from the tip of \\(\\mathbf{u}\\) to the line containing \\(\\mathbf{a}\\). \\(\\mathbf{w}_1\\) is the vector from the initial point to the foot of the perpendicular. \\(\\mathbf{w}_2 = \\mathbf{u} - \\mathbf{w}_1\\).\nThen \\(\\mathbf{u} = \\mathbf{w}_1 + \\mathbf{w}_2\\), where \\(\\mathbf{w}_1\\) is parallel to \\(\\mathbf{a}\\) and \\(\\mathbf{w}_2\\) is orthogonal to \\(\\mathbf{a}\\).\n  \n\nOrthogonal projection is a core concept with wide-ranging applications. Think of it as finding the “shadow” of one vector onto another. In ECE, this is used in signal processing (e.g., separating components of a signal), least squares approximation, and image compression."
  },
  {
    "objectID": "la-33.html#projection-theorem-theorem-3.3.2",
    "href": "la-33.html#projection-theorem-theorem-3.3.2",
    "title": "Linear Algebra",
    "section": "Projection Theorem (Theorem 3.3.2)",
    "text": "Projection Theorem (Theorem 3.3.2)\nIf \\(\\mathbf{u}\\) and \\(\\mathbf{a}\\) are vectors in \\(R^n\\), and \\(\\mathbf{a} \\neq \\mathbf{0}\\), then \\(\\mathbf{u}\\) can be expressed in exactly one way as: \\[\n\\mathbf{u} = \\mathbf{w}_1 + \\mathbf{w}_2\n\\] where \\(\\mathbf{w}_1\\) is a scalar multiple of \\(\\mathbf{a}\\) and \\(\\mathbf{w}_2\\) is orthogonal to \\(\\mathbf{a}\\).\nKey Formulas:\n\nThe vector \\(\\mathbf{w}_1\\) is called the orthogonal projection of \\(\\mathbf{u}\\) on \\(\\mathbf{a}\\), denoted \\(\\text{proj}_{\\mathbf{a}}\\mathbf{u}\\). \\[\n\\text{proj}_{\\mathbf{a}}\\mathbf{u} = \\frac{\\mathbf{u}\\cdot\\mathbf{a}}{\\|\\mathbf{a}\\|^2}\\mathbf{a} \\tag{10}\n\\]\nThe vector \\(\\mathbf{w}_2\\) is the vector component of \\(\\mathbf{u}\\) orthogonal to \\(\\mathbf{a}\\). \\[\n\\mathbf{w}_2 = \\mathbf{u} - \\text{proj}_{\\mathbf{a}}\\mathbf{u} \\tag{11}\n\\]\n\n\nThe proof (outlined in the text) shows how \\(k\\) in \\(\\mathbf{w}_1 = k\\mathbf{a}\\) is derived by ensuring \\(\\mathbf{w}_2\\) is orthogonal to \\(\\mathbf{a}\\). These formulas are fundamental. Memorize them! They allow us to mathematically perform the decomposition we visualized earlier."
  },
  {
    "objectID": "la-33.html#example-4-orthogonal-projection-on-a-line",
    "href": "la-33.html#example-4-orthogonal-projection-on-a-line",
    "title": "Linear Algebra",
    "section": "EXAMPLE 4: Orthogonal Projection on a Line",
    "text": "EXAMPLE 4: Orthogonal Projection on a Line\nFind the orthogonal projections of \\(\\mathbf{e}_1=(1,0)\\) and \\(\\mathbf{e}_2=(0,1)\\) on the line \\(L\\) that makes an angle \\(\\theta\\) with the positive x-axis in \\(R^2\\).\nLet \\(\\mathbf{a} = (\\cos \\theta, \\sin \\theta)\\) be a unit vector along line \\(L\\).\n\\(\\|\\mathbf{a}\\| = 1\\).\nFor \\(\\mathbf{e}_1=(1,0)\\):\n\\(\\mathbf{e}_1\\cdot \\mathbf{a} = (1,0)\\cdot (\\cos \\theta, \\sin \\theta) = \\cos \\theta\\) \\[\n\\text{proj}_{\\mathbf{a}}\\mathbf{e}_1 = \\frac{\\mathbf{e}_1\\cdot\\mathbf{a}}{\\|\\mathbf{a}\\|^2}\\mathbf{a} = (\\cos \\theta)(\\cos \\theta, \\sin \\theta) = (\\cos^2\\theta, \\sin \\theta \\cos \\theta)\n\\]\nFor \\(\\mathbf{e}_2=(0,1)\\):\n\\(\\mathbf{e}_2\\cdot \\mathbf{a} = (0,1)\\cdot (\\cos \\theta, \\sin \\theta) = \\sin \\theta\\) \\[\n\\text{proj}_{\\mathbf{a}}\\mathbf{e}_2 = \\frac{\\mathbf{e}_2\\cdot\\mathbf{a}}{\\|\\mathbf{a}\\|^2}\\mathbf{a} = (\\sin \\theta)(\\cos \\theta, \\sin \\theta) = (\\sin \\theta \\cos \\theta, \\sin^2\\theta)\n\\]\n\nThis example provides a concrete geometric interpretation of orthogonal projection in 2D. It shows how the projection depends on the angle of the line. Such projections are used in coordinate transformations and rotations in computer graphics and signal processing."
  },
  {
    "objectID": "la-33.html#example-5-vector-component-of-mathbfu-along-mathbfa",
    "href": "la-33.html#example-5-vector-component-of-mathbfu-along-mathbfa",
    "title": "Linear Algebra",
    "section": "EXAMPLE 5: Vector Component of \\(\\mathbf{u}\\) Along \\(\\mathbf{a}\\)",
    "text": "EXAMPLE 5: Vector Component of \\(\\mathbf{u}\\) Along \\(\\mathbf{a}\\)\nLet \\(\\mathbf{u} = (2, -1, 3)\\) and \\(\\mathbf{a} = (4, -1, 2)\\). Find \\(\\text{proj}_{\\mathbf{a}}\\mathbf{u}\\) and the vector component of \\(\\mathbf{u}\\) orthogonal to \\(\\mathbf{a}\\).\nSolution:\n\nCalculate \\(\\mathbf{u}\\cdot\\mathbf{a}\\): \\(\\mathbf{u}\\cdot\\mathbf{a} = (2)(4) + (-1)(-1) + (3)(2) = 8 + 1 + 6 = 15\\)\nCalculate \\(\\|\\mathbf{a}\\|^2\\): \\(\\|\\mathbf{a}\\|^2 = 4^2 + (-1)^2 + 2^2 = 16 + 1 + 4 = 21\\)\nCalculate \\(\\text{proj}_{\\mathbf{a}}\\mathbf{u}\\): \\[\n\\text{proj}_{\\mathbf{a}}\\mathbf{u} = \\frac{15}{21}(4, -1, 2) = \\frac{5}{7}(4, -1, 2) = \\left(\\frac{20}{7}, -\\frac{5}{7}, \\frac{10}{7}\\right)\n\\]\nCalculate \\(\\mathbf{u} - \\text{proj}_{\\mathbf{a}}\\mathbf{u}\\): \\[\n\\mathbf{u} - \\text{proj}_{\\mathbf{a}}\\mathbf{u} = (2, -1, 3) - \\left(\\frac{20}{7}, -\\frac{5}{7}, \\frac{10}{7}\\right) = \\left(\\frac{14-20}{7}, \\frac{-7+5}{7}, \\frac{21-10}{7}\\right) = \\left(-\\frac{6}{7}, -\\frac{2}{7}, \\frac{11}{7}\\right)\n\\] You can verify that \\(\\left(-\\frac{6}{7}, -\\frac{2}{7}, \\frac{11}{7}\\right)\\) is orthogonal to \\((4, -1, 2)\\) by checking their dot product.\n\n\nThis step-by-step example walks through the calculation of both components. The verification step is important to ensure the calculations are correct and reinforces the definition of orthogonality."
  },
  {
    "objectID": "la-33.html#interactive-orthogonal-projection-calculator",
    "href": "la-33.html#interactive-orthogonal-projection-calculator",
    "title": "Linear Algebra",
    "section": "Interactive Orthogonal Projection Calculator",
    "text": "Interactive Orthogonal Projection Calculator\nCalculate the orthogonal projection of \\(\\mathbf{u}\\) onto \\(\\mathbf{a}\\).\n\n\n\n\n\n\n\nThis interactive tool lets you compute orthogonal projections for any vectors \\(\\mathbf{u}\\) and \\(\\mathbf{a}\\) in \\(R^n\\). Experiment with different vectors, including those where \\(\\mathbf{u}\\) is already orthogonal to \\(\\mathbf{a}\\), or parallel to \\(\\mathbf{a}\\). Observe how the proj_a u and the orthogonal component change."
  },
  {
    "objectID": "la-33.html#norm-of-the-orthogonal-projection",
    "href": "la-33.html#norm-of-the-orthogonal-projection",
    "title": "Linear Algebra",
    "section": "Norm of the Orthogonal Projection",
    "text": "Norm of the Orthogonal Projection\nSometimes, we’re only interested in the length of the projected vector. \\[\n\\| \\text{proj}_{\\mathbf{a}}\\mathbf{u}\\| = \\left\\| \\frac{\\mathbf{u}\\cdot\\mathbf{a}}{\\|\\mathbf{a}\\|^2}\\mathbf{a}\\right\\| = \\left|\\frac{\\mathbf{u}\\cdot\\mathbf{a}}{\\|\\mathbf{a}\\|^2}\\right|\\|\\mathbf{a}\\| = \\frac{|\\mathbf{u}\\cdot\\mathbf{a}|}{\\|\\mathbf{a}\\|} \\tag{12}\n\\] If \\(\\theta\\) is the angle between \\(\\mathbf{u}\\) and \\(\\mathbf{a}\\), then \\(\\mathbf{u}\\cdot\\mathbf{a} = \\|\\mathbf{u}\\|\\|\\mathbf{a}\\|\\cos\\theta\\). Substituting this into (12): \\[\n\\| \\text{proj}_{\\mathbf{a}}\\mathbf{u}\\| = \\frac{|\\|\\mathbf{u}\\|\\|\\mathbf{a}\\|\\cos\\theta|}{\\|\\mathbf{a}\\|} = \\|\\mathbf{u}\\||\\cos\\theta| \\tag{13}\n\\]\n\nFigure 3.3.4: Geometric interpretation of \\(\\| \\text{proj}_{\\mathbf{a}}\\mathbf{u}\\|\\).\nThis formula tells us that the length of the projection is the length of \\(\\mathbf{u}\\) scaled by the absolute cosine of the angle between the vectors. This makes intuitive sense: if the vectors are aligned (\\(\\cos\\theta = 1\\)), the projection length is \\(\\|\\mathbf{u}\\|\\); if they are orthogonal (\\(\\cos\\theta = 0\\)), the projection length is 0."
  },
  {
    "objectID": "la-33.html#the-theorem-of-pythagoras-in-rn",
    "href": "la-33.html#the-theorem-of-pythagoras-in-rn",
    "title": "Linear Algebra",
    "section": "The Theorem of Pythagoras in \\(R^n\\)",
    "text": "The Theorem of Pythagoras in \\(R^n\\)\nThis theorem generalizes the familiar Pythagorean theorem to \\(n\\)-dimensional space.\nTheorem 3.3.3 (Theorem of Pythagoras in \\(R^n\\)): If \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are orthogonal vectors in \\(R^n\\), then: \\[\n\\| \\mathbf{u} + \\mathbf{v}\\|^2 = \\| \\mathbf{u}\\|^2 + \\| \\mathbf{v}\\|^2 \\tag{14}\n\\]\nProof: Since \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are orthogonal, \\(\\mathbf{u}\\cdot \\mathbf{v} = 0\\). \\[\n\\begin{aligned}\n\\| \\mathbf{u} + \\mathbf{v}\\|^2 &= (\\mathbf{u} + \\mathbf{v})\\cdot (\\mathbf{u} + \\mathbf{v}) \\\\\n&= \\mathbf{u}\\cdot \\mathbf{u} + 2(\\mathbf{u}\\cdot \\mathbf{v}) + \\mathbf{v}\\cdot \\mathbf{v} \\\\\n&= \\| \\mathbf{u}\\|^2 + 2(0) + \\| \\mathbf{v}\\|^2 \\\\\n&= \\| \\mathbf{u}\\|^2 + \\| \\mathbf{v}\\|^2\n\\end{aligned}\n\\]\n\n\n\nFigure 3.3.5: Pythagorean Theorem for orthogonal vectors.\n\n\n\nThis is a direct and elegant generalization. It shows that the algebraic properties of the dot product nicely extend fundamental geometric theorems. This is heavily used in signal processing (e.g., energy conservation in orthogonal decompositions), and in understanding variance components in statistics."
  },
  {
    "objectID": "la-33.html#example-6-theorem-of-pythagoras-in-r4",
    "href": "la-33.html#example-6-theorem-of-pythagoras-in-r4",
    "title": "Linear Algebra",
    "section": "EXAMPLE 6: Theorem of Pythagoras in \\(R^4\\)",
    "text": "EXAMPLE 6: Theorem of Pythagoras in \\(R^4\\)\nVectors \\(\\mathbf{u} = (-2, 3, 1, 4)\\) and \\(\\mathbf{v} = (1, 2, 0, -1)\\) are orthogonal (from Example 1). Verify the Theorem of Pythagoras for these vectors.\nSolution:\n\nCalculate \\(\\mathbf{u} + \\mathbf{v}\\):\n\\(\\mathbf{u} + \\mathbf{v} = (-2+1, 3+2, 1+0, 4-1) = (-1, 5, 1, 3)\\)\nCalculate \\(\\| \\mathbf{u} + \\mathbf{v}\\|^2\\):\n\\(\\| \\mathbf{u} + \\mathbf{v}\\|^2 = (-1)^2 + 5^2 + 1^2 + 3^2 = 1 + 25 + 1 + 9 = 36\\)\nCalculate \\(\\| \\mathbf{u}\\|^2\\):\n\\(\\| \\mathbf{u}\\|^2 = (-2)^2 + 3^2 + 1^2 + 4^2 = 4 + 9 + 1 + 16 = 30\\)\nCalculate \\(\\| \\mathbf{v}\\|^2\\):\n\\(\\| \\mathbf{v}\\|^2 = 1^2 + 2^2 + 0^2 + (-1)^2 = 1 + 4 + 0 + 1 = 6\\)\nCheck \\(\\| \\mathbf{u}\\|^2 + \\| \\mathbf{v}\\|^2\\):\n\\(\\| \\mathbf{u}\\|^2 + \\| \\mathbf{v}\\|^2 = 30 + 6 = 36\\)\n\nSince \\(36 = 36\\), the theorem holds.\n\nThis example provides a numerical verification of the Pythagorean theorem in \\(R^4\\). It’s important to first confirm the vectors are indeed orthogonal before applying the theorem."
  },
  {
    "objectID": "la-33.html#interactive-pythagoras-verifier",
    "href": "la-33.html#interactive-pythagoras-verifier",
    "title": "Linear Algebra",
    "section": "Interactive Pythagoras Verifier",
    "text": "Interactive Pythagoras Verifier\nEnter two vectors and verify the Theorem of Pythagoras if they are orthogonal.\n\n\n\n\n\n\n\nThis interactive tool allows you to test the Pythagorean theorem for any two vectors. First, it checks if the vectors are orthogonal. If they are, it proceeds to verify the theorem. Try changing the vectors. What happens if they are not orthogonal?"
  },
  {
    "objectID": "la-33.html#distance-problems-solved-with-orthogonal-projections",
    "href": "la-33.html#distance-problems-solved-with-orthogonal-projections",
    "title": "Linear Algebra",
    "section": "Distance Problems Solved with Orthogonal Projections",
    "text": "Distance Problems Solved with Orthogonal Projections\nOrthogonal projections can be used to solve distance problems:\n\nDistance between a point and a line in \\(R^2\\).\nDistance between a point and a plane in \\(R^3\\).\nDistance between two parallel planes in \\(R^3\\).\n\nTHEOREM 3.3.4 (Distance Formulas)\n\nIn \\(R^2\\), distance \\(D\\) between point \\(P_0(x_0,y_0)\\) and line \\(ax + by + c = 0\\):\n\n\\[\nD = \\frac{|ax_0 + by_0 + c|}{\\sqrt{a^2 + b^2}} \\tag{15}\n\\]\n\nIn \\(R^3\\), distance \\(D\\) between point \\(P_0(x_0,y_0,z_0)\\) and plane \\(ax + by + cz + d = 0\\):\n\n\\[\nD = \\frac{|ax_0 + by_0 + cz_0 + d|}{\\sqrt{a^2 + b^2 + c^2}} \\tag{16}\n\\]\n\nThese formulas are incredibly useful in practical geometry problems. They are derived directly from the concept of orthogonal projection, as the shortest distance is always along the normal vector. In ECE, these could be used for collision detection, path planning for robots, or optimizing sensor placement."
  },
  {
    "objectID": "la-33.html#proof-of-theorem-3.3.4b-point-plane-distance",
    "href": "la-33.html#proof-of-theorem-3.3.4b-point-plane-distance",
    "title": "Linear Algebra",
    "section": "Proof of Theorem 3.3.4(b) (Point-Plane Distance)",
    "text": "Proof of Theorem 3.3.4(b) (Point-Plane Distance)\nLet \\(Q(x_1,y_1,z_1)\\) be any point in the plane. Let \\(\\mathbf{n}=(a,b,c)\\) be the normal vector. The distance \\(D\\) is the length of the orthogonal projection of \\(\\overrightarrow{QP_0}\\) onto \\(\\mathbf{n}\\).\n\nFigure 3.3.6: Geometric idea behind point-plane distance."
  },
  {
    "objectID": "la-33.html#proof-of-theorem-3.3.4b-point-plane-distance-1",
    "href": "la-33.html#proof-of-theorem-3.3.4b-point-plane-distance-1",
    "title": "Linear Algebra",
    "section": "Proof of Theorem 3.3.4(b) (Point-Plane Distance)",
    "text": "Proof of Theorem 3.3.4(b) (Point-Plane Distance)\nUsing Formula (12) for the norm of projection: \\[\nD = \\| \\text{proj}_{\\mathbf{n}}\\overrightarrow{QP_0}\\| = \\frac{|\\overrightarrow{QP_0}\\cdot\\mathbf{n}|}{\\|\\mathbf{n}\\|}\n\\] Substitute \\(\\overrightarrow{QP_0} = (x_0 - x_1, y_0 - y_1, z_0 - z_1)\\) and \\(\\|\\mathbf{n}\\| = \\sqrt{a^2+b^2+c^2}\\).\nSince \\(Q\\) is in the plane, \\(ax_1 + by_1 + cz_1 + d = 0 \\implies d = -ax_1 - by_1 - cz_1\\).\nThe numerator becomes \\(|a(x_0 - x_1) + b(y_0 - y_1) + c(z_0 - z_1)| = |ax_0 + by_0 + cz_0 - (ax_1 + by_1 + cz_1)| = |ax_0 + by_0 + cz_0 + d|\\). This yields Formula (16).\n\nThis proof beautifully connects the geometric intuition of orthogonal projection with the algebraic distance formula. It demonstrates why the normal vector is central to finding the shortest distance."
  },
  {
    "objectID": "la-33.html#example-7-distance-between-a-point-and-a-plane",
    "href": "la-33.html#example-7-distance-between-a-point-and-a-plane",
    "title": "Linear Algebra",
    "section": "EXAMPLE 7: Distance Between a Point and a Plane",
    "text": "EXAMPLE 7: Distance Between a Point and a Plane\nFind the distance \\(D\\) between the point \\(P_0(1, -4, -3)\\) and the plane \\(2x - 3y + 6z = -1\\).\nSolution: First, rewrite the plane equation as \\(2x - 3y + 6z + 1 = 0\\).\nHere, \\((x_0,y_0,z_0) = (1,-4,-3)\\) and \\((a,b,c,d) = (2,-3,6,1)\\).\nUsing Formula (16): \\[\nD = \\frac{|2(1) + (-3)(-4) + 6(-3) + 1|}{\\sqrt{2^2 + (-3)^2 + 6^2}} = \\frac{|2 + 12 - 18 + 1|}{\\sqrt{4 + 9 + 36}} = \\frac{|-3|}{\\sqrt{49}} = \\frac{3}{7}\n\\]\n\nThis is a direct application of the distance formula. Remember to ensure the plane equation is in the form \\(ax+by+cz+d=0\\) before plugging into the formula."
  },
  {
    "objectID": "la-33.html#interactive-point-plane-distance-calculator",
    "href": "la-33.html#interactive-point-plane-distance-calculator",
    "title": "Linear Algebra",
    "section": "Interactive Point-Plane Distance Calculator",
    "text": "Interactive Point-Plane Distance Calculator\nCalculate the distance between a point and a plane in \\(R^3\\).\n\n\n\n\n\n\n\nUse this interactive tool to practice calculating point-plane distances. Modify the point coordinates and the plane’s coefficients. This is a quick way to check your understanding of the formula."
  },
  {
    "objectID": "la-33.html#distance-between-parallel-planes",
    "href": "la-33.html#distance-between-parallel-planes",
    "title": "Linear Algebra",
    "section": "Distance Between Parallel Planes",
    "text": "Distance Between Parallel Planes\nTo find the distance between two parallel planes in \\(R^3\\):\n\nFind any point \\(P_0\\) in one of the planes.\nCompute the distance between \\(P_0\\) and the other plane using Formula (16).\n\n\nFigure 3.3.7: Distance between parallel planes."
  },
  {
    "objectID": "la-33.html#distance-between-parallel-planes-1",
    "href": "la-33.html#distance-between-parallel-planes-1",
    "title": "Linear Algebra",
    "section": "Distance Between Parallel Planes",
    "text": "Distance Between Parallel Planes\nEXAMPLE 8: Distance Between Parallel Planes\nFind the distance between:\nPlane 1: \\(x + 2y - 2z = 3\\)\nPlane 2: \\(2x + 4y - 4z = 7\\)\nSolution: Normals are \\((1, 2, -2)\\) and \\((2, 4, -4)\\), which are parallel (one is twice the other).\n\nFind a point in Plane 1: Set \\(y=0, z=0 \\implies x=3\\). So, \\(P_0(3,0,0)\\) is in Plane 1.\nCompute distance from \\(P_0(3,0,0)\\) to Plane 2 (\\(2x + 4y - 4z - 7 = 0\\)).\nHere, \\((x_0,y_0,z_0) = (3,0,0)\\) and \\((a,b,c,d) = (2,4,-4,-7)\\).\n\\[\nD = \\frac{|2(3) + 4(0) + (-4)(0) - 7|}{\\sqrt{2^2 + 4^2 + (-4)^2}} = \\frac{|6 - 7|}{\\sqrt{4 + 16 + 16}} = \\frac{|-1|}{\\sqrt{36}} = \\frac{1}{6}\n\\]\n\n\nThis is a clever trick that simplifies the problem. The core idea is that the distance between parallel planes is constant, so finding it for any point in one plane to the other plane works."
  },
  {
    "objectID": "la-33.html#interactive-parallel-plane-distance-calculator",
    "href": "la-33.html#interactive-parallel-plane-distance-calculator",
    "title": "Linear Algebra",
    "section": "Interactive Parallel Plane Distance Calculator",
    "text": "Interactive Parallel Plane Distance Calculator\nCalculate the distance between two parallel planes in \\(R^3\\).\n\n\n\n\n\n\n\nThis interactive tool calculates the distance between two parallel planes. It first verifies that the planes are indeed parallel by checking their normal vectors. Observe how changing the coefficients d1 or d2 (while keeping a,b,c proportional) changes the distance. This is a more complex application, combining point finding and distance calculation."
  },
  {
    "objectID": "la-33.html#summary-and-key-takeaways",
    "href": "la-33.html#summary-and-key-takeaways",
    "title": "Linear Algebra",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\n\nOrthogonal Vectors: Vectors whose dot product is zero, representing perpendicularity.\nNormal Vectors: Define orientation of lines (in \\(R^2\\)) and planes (in \\(R^3\\)).\nOrthogonal Projection: Decomposing a vector into components parallel and orthogonal to another vector.\n\n\\(\\text{proj}_{\\mathbf{a}}\\mathbf{u} = \\frac{\\mathbf{u}\\cdot\\mathbf{a}}{\\|\\mathbf{a}\\|^2}\\mathbf{a}\\)\n\nPythagoras Theorem: Extends to \\(R^n\\) for orthogonal vectors: \\(\\| \\mathbf{u} + \\mathbf{v}\\|^2 = \\| \\mathbf{u}\\|^2 + \\| \\mathbf{v}\\|^2\\).\nDistance Formulas: Efficiently calculate distances between points, lines, and planes using normal vectors and projections."
  },
  {
    "objectID": "la-33.html#summary-and-key-takeaways-1",
    "href": "la-33.html#summary-and-key-takeaways-1",
    "title": "Linear Algebra",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\nApplications in ECE:\n\nSignal Processing: Orthogonal bases (e.g., Fourier, Wavelets) for signal decomposition and analysis.\nControl Systems: State-space analysis, defining constraint surfaces.\nComputer Graphics: Lighting, camera transformations, collision detection.\nMachine Learning: Principal Component Analysis (PCA) uses orthogonal projections to reduce dimensionality.\n\n\nOrthogonality is a pervasive concept in linear algebra and its applications. It simplifies calculations, provides geometric insights, and forms the basis for many powerful algorithms. Make sure you are comfortable with the definitions, formulas, and their geometric interpretations. Any final questions on orthogonality?"
  },
  {
    "objectID": "la-python.html#why-python-for-ece-linear-algebra",
    "href": "la-python.html#why-python-for-ece-linear-algebra",
    "title": "Linear Algebra",
    "section": "Why Python for ECE Linear Algebra?",
    "text": "Why Python for ECE Linear Algebra?\nPython is a versatile, high-level programming language widely used in engineering and science. It’s an indispensable tool for numerical computation, data analysis, and machine learning.\nKey Benefits\n\nFree & Open-Source: Accessible to everyone.\nRich Ecosystem: Powerful libraries like NumPy, SciPy, Matplotlib.\nVersatility: Used in data science, AI, web development, embedded systems.\nIndustry Standard: High demand for Python skills in ECE fields.\nPrototyping & Simulation: Quickly build and test models.\n\n\nPython’s popularity stems from its readability and vast community support. For ECE students, it’s particularly valuable for signal processing, control systems, machine learning, and circuit simulation, where linear algebra is paramount. NumPy provides the foundational array object that makes numerical operations incredibly efficient."
  },
  {
    "objectID": "la-python.html#getting-started-installation-alternative-1",
    "href": "la-python.html#getting-started-installation-alternative-1",
    "title": "Linear Algebra",
    "section": "Getting Started: Installation (Alternative 1)",
    "text": "Getting Started: Installation (Alternative 1)\nHere are two common ways to set up your Python environment for linear algebra.\nAlternative 1: Using Miniconda (Recommended)\nMiniconda is a lightweight version of Anaconda that includes just Python and the conda package manager. This is a clean and efficient way to manage project-specific environments.\n\nDownload Miniconda: Visit the Miniconda documentation and download the installer for your operating system.\nInstall Miniconda: Run the installer, accepting the default options. It’s recommended to allow the installer to initialize conda.\nCreate a Conda Environment: Open your terminal (or Anaconda Prompt on Windows) and run:\nconda create --name la_env python=3.9 numpy spyder -y\nThis command creates a new environment named la_env with Python 3.9, NumPy, and the Spyder IDE.\nActivate the Environment:\nconda activate la_env\nYou must activate this environment every time you want to work on your linear algebra projects.\nLaunch Spyder: With the environment active, simply type spyder in your terminal."
  },
  {
    "objectID": "la-python.html#getting-started-installation-alternative-2",
    "href": "la-python.html#getting-started-installation-alternative-2",
    "title": "Linear Algebra",
    "section": "Getting Started: Installation (Alternative 2)",
    "text": "Getting Started: Installation (Alternative 2)\nAlternative 2: Manual Installation (Python + pip)\nThis approach gives you full control over your Python installation without a special package manager.\n\nInstall Python:\n\nGo to python.org/downloads.\nDownload and install the latest stable version of Python (e.g., Python 3.9+).\nOn Windows: Make sure to check the box that says “Add Python to PATH” during installation.\n\nInstall Packages with pip: Open your terminal or command prompt and use pip (Python’s package installer) to install the necessary libraries:\npip install numpy\npip install spyder\nNote: Spyder will automatically pull in other required dependencies like scipy and matplotlib."
  },
  {
    "objectID": "la-python.html#getting-started-spyder",
    "href": "la-python.html#getting-started-spyder",
    "title": "Linear Algebra",
    "section": "Getting Started: Spyder",
    "text": "Getting Started: Spyder\nLaunch Spyder: Once installed, you can launch Spyder by typing spyder in your terminal.\nSpyder IDE Overview\n\nEditor Pane: Write your Python code (.py files).\nIPython Console: Interactive command line for executing code snippets.\nVariable Explorer: Inspect variables in memory.\nPlots Pane: View generated plots."
  },
  {
    "objectID": "la-python.html#python-basics-interpreter-variables",
    "href": "la-python.html#python-basics-interpreter-variables",
    "title": "Linear Algebra",
    "section": "Python Basics: Interpreter & Variables",
    "text": "Python Basics: Interpreter & Variables\nThe IPython Console in Spyder (or a standalone terminal) acts as your interactive Python interpreter.\n\n\n\n\n\n\n\nEncourage students to type these commands directly into the IPython console. Explain that Python is dynamically typed. Emphasize that the console is great for quick tests, but for larger programs, you’ll write scripts. # is used for comments in Python."
  },
  {
    "objectID": "la-python.html#numpy-the-powerhouse-for-linear-algebra",
    "href": "la-python.html#numpy-the-powerhouse-for-linear-algebra",
    "title": "Linear Algebra",
    "section": "NumPy: The Powerhouse for Linear Algebra",
    "text": "NumPy: The Powerhouse for Linear Algebra\nNumPy (Numerical Python) is the fundamental package for scientific computing with Python. It provides support for large, multi-dimensional arrays and matrices, along with a collection of high-level mathematical functions.\nThe ndarray Object\n\nNumPy’s core is the ndarray object, an N-dimensional array.\nIt’s significantly more efficient for numerical operations than Python’s built-in lists.\n\nImporting NumPy\n\n\n\n\n\n\n\nStress the importance of import numpy as np. This alias np is universally accepted. Explain that ndarray is specifically designed for numerical data and optimized for performance, unlike generic Python lists which can hold mixed types. The shape attribute is crucial for understanding array dimensions."
  },
  {
    "objectID": "la-python.html#working-with-vectors-in-numpy",
    "href": "la-python.html#working-with-vectors-in-numpy",
    "title": "Linear Algebra",
    "section": "Working with Vectors in NumPy",
    "text": "Working with Vectors in NumPy\nVectors are represented as 1D ndarray objects.\nCreating Vectors\n\nFrom List: v1 = np.array([1, 2, 3])\nRange: v2 = np.arange(1, 6) (1 to 5)\nlinspace: v3 = np.linspace(0, 10, 5) (5 points from 0 to 10)\n\nVector Operations\n\nAddition: v_a + v_b\nScalar Multiplication: 3 * v_a\nDot Product: np.dot(v_a, v_b) or v_a @ v_b (Python 3.5+)\nCross Product: np.cross(v_a, v_b) (for 3D vectors)\nMagnitude (Norm): np.linalg.norm(v_a)"
  },
  {
    "objectID": "la-python.html#working-with-vectors-in-numpy-1",
    "href": "la-python.html#working-with-vectors-in-numpy-1",
    "title": "Linear Algebra",
    "section": "Working with Vectors in NumPy",
    "text": "Working with Vectors in NumPy\nExample: Vector Addition\n\\[\n\\mathbf{u} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}, \\quad \\mathbf{v} = \\begin{bmatrix} -1 \\\\ 3 \\end{bmatrix} \\\\\n\\mathbf{u} + \\mathbf{v} = \\begin{bmatrix} 2 + (-1) \\\\ 1 + 3 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 4 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "la-python.html#working-with-vectors-in-numpy-2",
    "href": "la-python.html#working-with-vectors-in-numpy-2",
    "title": "Linear Algebra",
    "section": "Working with Vectors in NumPy",
    "text": "Working with Vectors in NumPy\nInteractive Vector Visualization\n\n\n\n\n\n\n\nExplain that NumPy handles element-wise operations by default when you use standard arithmetic operators (+, -, *, /). For dot products, use np.dot() or the @ operator. The interactive plot visually reinforces the parallelogram rule for vector addition."
  },
  {
    "objectID": "la-python.html#mastering-matrices-in-numpy",
    "href": "la-python.html#mastering-matrices-in-numpy",
    "title": "Linear Algebra",
    "section": "Mastering Matrices in NumPy",
    "text": "Mastering Matrices in NumPy\nMatrices are represented as 2D ndarray objects.\nCreating Matrices\n\nFrom nested lists: M = np.array([[1, 2], [3, 4]])\nSpecial Matrices:\n\nnp.zeros((2,3)): 2x3 matrix of zeros.\nnp.ones((3,3)): 3x3 matrix of ones.\nnp.eye(4): 4x4 identity matrix.\nnp.diag([1, 2, 3]): Diagonal matrix.\nnp.random.rand(2,2): Random matrix.\n\n\nAccessing Elements\n\nM[row, col]: M[0,1] (element at row 0, col 1)\nM[0,:]: First row.\nM[:,1]: Second column.\nSlicing: M[0:2, 1:3]"
  },
  {
    "objectID": "la-python.html#mastering-matrices-in-numpy-1",
    "href": "la-python.html#mastering-matrices-in-numpy-1",
    "title": "Linear Algebra",
    "section": "Mastering Matrices in NumPy",
    "text": "Mastering Matrices in NumPy\nMatrix Operations\n\nAddition/Subtraction: A + B, A - B (element-wise)\nScalar Multiplication: 5 * A\nMatrix Multiplication: A @ B (use @ or np.dot())\nElement-wise Multiplication: A * B (use *)\nTranspose: A.T or np.transpose(A)\nInverse: np.linalg.inv(A)\nDeterminant: np.linalg.det(A)"
  },
  {
    "objectID": "la-python.html#mastering-matrices-in-numpy-2",
    "href": "la-python.html#mastering-matrices-in-numpy-2",
    "title": "Linear Algebra",
    "section": "Mastering Matrices in NumPy",
    "text": "Mastering Matrices in NumPy\n\n\n\n\n\n\n\nCrucially distinguish between * (element-wise multiplication) and @ (matrix multiplication). This is a common pitfall. Explain that NumPy’s indexing is 0-based, like standard Python. Briefly touch upon why inv(A) and det(A) are important (e.g., checking singularity, solving systems)."
  },
  {
    "objectID": "la-python.html#solving-linear-systems-ax-b",
    "href": "la-python.html#solving-linear-systems-ax-b",
    "title": "Linear Algebra",
    "section": "Solving Linear Systems: \\(Ax = b\\)",
    "text": "Solving Linear Systems: \\(Ax = b\\)\nOne of the most fundamental applications of linear algebra in ECE is solving systems of linear equations.\n\\[\n\\mathbf{A}\\mathbf{x} = \\mathbf{b}\n\\]\nWhere:\n\n\\(\\mathbf{A}\\) is the coefficient matrix.\n\\(\\mathbf{x}\\) is the unknown vector.\n\\(\\mathbf{b}\\) is the constant vector.\n\nSolving in NumPy using np.linalg.solve() This function provides an efficient and numerically stable way to find \\(\\mathbf{x}\\)."
  },
  {
    "objectID": "la-python.html#solving-linear-systems-ax-b-1",
    "href": "la-python.html#solving-linear-systems-ax-b-1",
    "title": "Linear Algebra",
    "section": "Solving Linear Systems: \\(Ax = b\\)",
    "text": "Solving Linear Systems: \\(Ax = b\\)\n\n\n\n\n\n\n\nExplain that np.linalg.solve(A, b) is preferred over np.linalg.inv(A) @ b because it’s more robust and faster for large systems. Give a real-world ECE example, such as solving for currents or voltages in a circuit using Kirchhoff’s laws, which often leads to such systems."
  },
  {
    "objectID": "la-python.html#visualizing-linear-systems-interactive",
    "href": "la-python.html#visualizing-linear-systems-interactive",
    "title": "Linear Algebra",
    "section": "Visualizing Linear Systems (Interactive)",
    "text": "Visualizing Linear Systems (Interactive)\nFor a 2D system, each equation represents a line. The solution is their intersection point.\n\n\n\n\n\n\n\nThis interactive plot helps immensely in understanding the geometric meaning of solving Ax=b. Discuss what happens when lines are parallel (no solution) or coincident (infinite solutions) and how np.linalg.solve would handle such cases (it would raise an error for singular matrices)."
  },
  {
    "objectID": "la-python.html#scripting-in-python-.py-files",
    "href": "la-python.html#scripting-in-python-.py-files",
    "title": "Linear Algebra",
    "section": "Scripting in Python: .py Files",
    "text": "Scripting in Python: .py Files\nFor more complex tasks, you’ll write Python scripts (saved as .py files).\nCreating and Running a Script\n\nIn Spyder’s editor, create a new file.\nSave it as my_script.py.\nRun it from Spyder (green play button) or from terminal: python my_script.py."
  },
  {
    "objectID": "la-python.html#scripting-in-python-.py-files-1",
    "href": "la-python.html#scripting-in-python-.py-files-1",
    "title": "Linear Algebra",
    "section": "Scripting in Python: .py Files",
    "text": "Scripting in Python: .py Files\nExample: Vector Magnitude Script"
  },
  {
    "objectID": "la-python.html#scripting-in-python-.py-files-2",
    "href": "la-python.html#scripting-in-python-.py-files-2",
    "title": "Linear Algebra",
    "section": "Scripting in Python: .py Files",
    "text": "Scripting in Python: .py Files\nFunctions in Python\nFunctions allow you to organize, reuse, and modularize your code.\n\n\n\n\n\n\n\nExplain the importance of scripting for larger projects, reproducibility, and automation. Emphasize import statements at the top of scripts. Introduce def keyword for functions and docstrings for documentation. Explain if __name__ == \"__main__\": as a common Python idiom to ensure code runs only when the script is executed directly."
  },
  {
    "objectID": "la-python.html#conclusion-further-resources",
    "href": "la-python.html#conclusion-further-resources",
    "title": "Linear Algebra",
    "section": "Conclusion & Further Resources",
    "text": "Conclusion & Further Resources\nKey Takeaways\n\nPython with NumPy is a powerful, free tool for Linear Algebra.\nAnaconda/Spyder provides an excellent environment for ECE students.\nNumPy arrays (ndarray) are fundamental for vector and matrix operations.\nPython allows for efficient scripting and function development.\nThese tools are essential for solving systems and understanding transformations.\n\nNext Steps\n\nPractice: The best way to learn is by doing. Experiment with the code examples.\nNumPy Documentation: numpy.org/doc/ for comprehensive details.\nLinear Algebra Textbooks: Apply theoretical concepts using Python.\nECE Applications: Explore how these concepts and tools are used in your specific ECE courses and projects."
  },
  {
    "objectID": "la-python.html#thank-you",
    "href": "la-python.html#thank-you",
    "title": "Linear Algebra",
    "section": "Thank You!",
    "text": "Thank You!\nHappy Computing!\n\nEncourage active learning and highlight the practical utility of Python in their ECE curriculum and future careers. Offer to answer questions and provide contact information if appropriate."
  },
  {
    "objectID": "la_prompt/linear_algebra_prompt_2.html",
    "href": "la_prompt/linear_algebra_prompt_2.html",
    "title": "Linear Algebra",
    "section": "",
    "text": "Objective:\nCreate an engaging, interactive slide deck for an undergraduate Linear Algebra course in Electrical and Computer Engineering (ECE), blending theoretical concepts with practical, hands-on demonstrations using Quarto and reveal.js.\nTask:\nDevelop an interactive Quarto-based presentation using reveal.js tailored for undergraduate ECE students. Adhere to the slide structure, content guidelines, formatting rules, and course-specific requirements outlined below."
  },
  {
    "objectID": "la_prompt/linear_algebra_prompt_2.html#slide-structure",
    "href": "la_prompt/linear_algebra_prompt_2.html#slide-structure",
    "title": "Linear Algebra",
    "section": "1. Slide Structure",
    "text": "1. Slide Structure\n\nSlide Separator: Use --- to separate slides.\n\nTitle Slide: Use a first-level heading (#) and/or second-level heading (##).\n\nStandard Slides: Begin with a second-level heading (##) followed by content."
  },
  {
    "objectID": "la_prompt/linear_algebra_prompt_2.html#content-guidelines",
    "href": "la_prompt/linear_algebra_prompt_2.html#content-guidelines",
    "title": "Linear Algebra",
    "section": "2. Content Guidelines",
    "text": "2. Content Guidelines\n\nWrite text in markdown format, ending each sentence with double spaces.\n\nEnhance slides with:\n\nDiagrams (using Mermaid.js or Graphviz).\n\nExecutable Python code blocks (using Pyodide).\n\nInteractive visualizations (using Python, Pyodide, Observable.js, and Plotly).\n\nMath formulas (using LaTeX).\n\nMulti-column layouts.\n\nSpeaker notes for additional context.\nCallout for drawing extra attention."
  },
  {
    "objectID": "la_prompt/linear_algebra_prompt_2.html#diagrams",
    "href": "la_prompt/linear_algebra_prompt_2.html#diagrams",
    "title": "Linear Algebra",
    "section": "3. Diagrams",
    "text": "3. Diagrams\n\nCreate diagrams using Mermaid.js or Graphviz\nFormat for Mermaid.js:\n\n\\`\\`\\`\\{mermaid\\}\nMermaid code here\n\\`\\`\\`\\\n\nFormat for Graphviz:\n\n\\`\\`\\`\\{dot\\}\nGraphviz code here\n\\`\\`\\`\\"
  },
  {
    "objectID": "la_prompt/linear_algebra_prompt_2.html#executable-code-blocks",
    "href": "la_prompt/linear_algebra_prompt_2.html#executable-code-blocks",
    "title": "Linear Algebra",
    "section": "4. Executable Code Blocks",
    "text": "4. Executable Code Blocks\n\nInclude executable Python code blocks using Pyodide, formatted as:\n\n\\`\\`\\`\\{pyodide\\}\n#| max-lines: 10\nPython code here\n\\`\\`\\`\\"
  },
  {
    "objectID": "la_prompt/linear_algebra_prompt_2.html#interactive-code-blocks",
    "href": "la_prompt/linear_algebra_prompt_2.html#interactive-code-blocks",
    "title": "Linear Algebra",
    "section": "5. Interactive Code Blocks",
    "text": "5. Interactive Code Blocks\n\nCreate interactive visualizations using Python, Pyodide, Observable.js, and Plotly, formatted as:\n\n\\`\\`\\`\\{ojs\\}\nObservable.js code here\n```{pyodide} #| echo: false #| input: Python code here ```\n\n\n## 6. Math Formulas  \n- Use LaTeX for mathematical expressions:  \n  - **Inline:** \n    ```\n    $E=mc^2$\n    ```\n  - **Block:**  \n    ```\n    $$ E=mc^2 $$\n    ```\n\n## 7. Multi-Column Layout  \n- Use multi-column layouts for balanced content presentation, formatted as:  \n\n:::: {.columns} ::: {.column width=“50%”} Left Column\n- Item L1\n- Item L2\n::: ::: {.column width=“50%”} Right Column\n- Item R1\n- Item R2\n- Item R3\n::: :::: ```"
  },
  {
    "objectID": "la_prompt/linear_algebra_prompt_2.html#speaker-notes",
    "href": "la_prompt/linear_algebra_prompt_2.html#speaker-notes",
    "title": "Linear Algebra",
    "section": "8. Speaker Notes",
    "text": "8. Speaker Notes\n\nInclude speaker notes for additional explanations, formatted as:\n**Slide Content:**  \n- Point 1  \n- Point 2  \n\n::: {.notes}\nSpeaker notes here.\n:::"
  },
  {
    "objectID": "la_prompt/linear_algebra_prompt_2.html#callout",
    "href": "la_prompt/linear_algebra_prompt_2.html#callout",
    "title": "Linear Algebra",
    "section": "9. Callout",
    "text": "9. Callout\n\nThere are five different types of callouts available: callout-note, callout-warning, callout-important, callout-tip, and callout-caution.\nInclude callout for extra attention, formatted as:\n\n**Slide Content:**  \n- Point 1  \n- Point 2  \n\n::: {.callout-note}\nCallout notes here.\n:::"
  },
  {
    "objectID": "la_prompt/linear_algebra_prompt_2.html#course-specific-requirements",
    "href": "la_prompt/linear_algebra_prompt_2.html#course-specific-requirements",
    "title": "Linear Algebra",
    "section": "10. Course-Specific Requirements",
    "text": "10. Course-Specific Requirements\n\nContent Source: Use provided text and linked images as primary resources.\n\nAcademic Context: Ensure explanations, examples, and terminology align with ECE standards for Linear Algebra.\n\nInteractivity:\n\nIncorporate Python-based interactive elements (e.g., plots, vector and matrix operations, transformations).\n\nEmbed charts, plots, and simulations directly within slides.\n\n\nEnhancement: Include real-world engineering applications, analogies, and problem-solving examples to contextualize concepts.\n\nClarity & Engagement: Maintain a logical structure with engaging visuals and concise slide text.\n\nConciseness: Limit text on slides; elaborate in speaker notes. Split dense slides into multiple slides for clarity."
  },
  {
    "objectID": "la_prompt/linear_algebra_prompt_2.html#presentation-yaml",
    "href": "la_prompt/linear_algebra_prompt_2.html#presentation-yaml",
    "title": "Linear Algebra",
    "section": "11. Presentation YAML",
    "text": "11. Presentation YAML\nUse the following YAML header, replacing {Lecture Title} with the specific lecture subtitle:\n---\ntitle: \"Linear Algebra\"\nsubtitle: \"{Lecture Title}\"\nauthor: \"Imron Rosyadi\"\nformat:\n  live-revealjs:\n    logo: \"qrjs_assets/unsoed_logo.png\"\n    footer: \"[irosyadi-2025](https://imron-slide.vercel.app)\"\n    slide-number: true\n    chalkboard: true\n    scrollable: true\n    controls: true\n    progress: true\n    preview-links: false\n    transition: fade\n    incremental: false\n    smaller: false\n    theme: [default, qrjs_assets/ir_style.scss]\n    mermaid:\n        theme: neutral\npyodide:\n  packages:\n    - numpy\n    - plotly\n    - nbformat\n---"
  },
  {
    "objectID": "la-34.html#unveiling-vector-parametric-equations",
    "href": "la-34.html#unveiling-vector-parametric-equations",
    "title": "Linear Algebra",
    "section": "Unveiling Vector & Parametric Equations",
    "text": "Unveiling Vector & Parametric Equations"
  },
  {
    "objectID": "la-34.html#introduction-visualizing-linear-algebra",
    "href": "la-34.html#introduction-visualizing-linear-algebra",
    "title": "Linear Algebra",
    "section": "Introduction: Visualizing Linear Algebra",
    "text": "Introduction: Visualizing Linear Algebra\nIn ECE, understanding linear systems isn’t just about solving equations; it’s about visualizing their geometric interpretations.\nThis lecture explores how to represent solution sets of linear systems as geometric objects like points, lines, and planes.\n\nWelcome to this session on the geometry of linear systems. For ECE students, abstract mathematical concepts become much more tangible and useful when we can visualize them. Today, we’ll bridge the gap between algebraic equations and their geometric counterparts, which is crucial for fields like signal processing, control theory, and computer graphics."
  },
  {
    "objectID": "la-34.html#lines-in-r2-and-r3-vector-form",
    "href": "la-34.html#lines-in-r2-and-r3-vector-form",
    "title": "Linear Algebra",
    "section": "Lines in \\(R^2\\) and \\(R^3\\): Vector Form",
    "text": "Lines in \\(R^2\\) and \\(R^3\\): Vector Form\n\n\nA unique line L is determined by a point \\(\\mathbf{x}_0\\) on the line and a non-zero vector \\(\\mathbf{v}\\) parallel to the line.\nThe vector \\(\\mathbf{x} - \\mathbf{x}_0\\) is a scalar multiple of \\(\\mathbf{v}\\). \\[ \\mathbf{x} - \\mathbf{x}_0 = t\\mathbf{v} \\] This leads to the vector equation of a line: \\[ \\mathbf{x} = \\mathbf{x}_0 + t\\mathbf{v} \\quad \\text{(1)} \\] Where \\(t\\) is the parameter, varying from \\(-\\infty\\) to \\(\\infty\\).\nIf \\(\\mathbf{x}_0 = \\mathbf{0}\\), the line passes through the origin: \\[ \\mathbf{x} = t\\mathbf{v} \\quad \\text{(2)} \\]\n\n Figure 3.4.2\n\n\n\n\n\n\n\nNote\n\n\nTo define a line in either 2D (ℝ²) or 3D (ℝ³), you need:\n\nA point 𝑥0 on the line.\nA direction vector 𝑣 that is parallel to the line.\n\nAny point 𝑥 on the line satisfies: 𝑥−𝑥0=𝑡𝑣\nWhich rearranges to the vector equation of a line: 𝑥 =𝑥0+𝑡𝑣\n\n\n\n\nImagine a point x0 in space. Now, imagine a direction vector v. Any point x on the line starting from x0 and moving in direction v can be reached by adding a scaled version of v to x0. The scalar t dictates how far along v we move, and its variation covers the entire line. This concept is fundamental in robotics for path planning or in control systems for describing system trajectories."
  },
  {
    "objectID": "la-34.html#interactive-line-plot-in-r2",
    "href": "la-34.html#interactive-line-plot-in-r2",
    "title": "Linear Algebra",
    "section": "Interactive Line Plot in \\(R^2\\)",
    "text": "Interactive Line Plot in \\(R^2\\)\nAdjust the starting point \\(\\mathbf{x}_0\\) and direction vector \\(\\mathbf{v}\\) to see how the line changes.\n\n\n\n\n\n\nTip\n\n\nTry changing the values of x0_val, y0_val, vx_val, and vy_val below. Observe how the line shifts and rotates.\n\n\n\n\n\n\n\n\n\n\nThis interactive plot allows you to directly manipulate the parameters x0 and v. Think about how changing the components of v affects the slope and direction of the line. Changing x0 simply translates the entire line without altering its orientation. This is a perfect example of how vector addition works geometrically. In signal processing, a line can represent a signal’s trajectory over time, where x0 is the initial state and v is the rate of change."
  },
  {
    "objectID": "la-34.html#planes-in-r3-vector-form",
    "href": "la-34.html#planes-in-r3-vector-form",
    "title": "Linear Algebra",
    "section": "Planes in \\(R^3\\): Vector Form",
    "text": "Planes in \\(R^3\\): Vector Form\n\n\nA unique plane W is determined by a point \\(\\mathbf{x}_0\\) in the plane and two non-collinear (not lie on the same line) vectors \\(\\mathbf{v}_1, \\mathbf{v}_2\\) parallel to the plane.\nThe vector \\(\\mathbf{x} - \\mathbf{x}_0\\) can be expressed as a linear combination of \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\). \\[ \\mathbf{x} - \\mathbf{x}_0 = t_1\\mathbf{v}_1 + t_2\\mathbf{v}_2 \\] This leads to the vector equation of a plane: \\[ \\mathbf{x} = \\mathbf{x}_0 + t_1\\mathbf{v}_1 + t_2\\mathbf{v}_2 \\quad \\text{(3)} \\] Where \\(t_1, t_2\\) are parameters, varying from \\(-\\infty\\) to \\(\\infty\\).\nIf \\(\\mathbf{x}_0 = \\mathbf{0}\\), the plane passes through the origin: \\[ \\mathbf{x} = t_1\\mathbf{v}_1 + t_2\\mathbf{v}_2 \\quad \\text{(4)} \\]\n\n Figure 3.4.3\n\n\nFor a plane, instead of one direction, we need two independent directions. These vectors, v1 and v2, span the plane when added to x0. The parameters t1 and t2 allow us to reach any point within that plane. Think of this as defining a 2D surface within a 3D space. This is relevant for defining surfaces in CAD software, or understanding the solution space of certain optimization problems in ECE."
  },
  {
    "objectID": "la-34.html#interactive-plane-plot-in-r3",
    "href": "la-34.html#interactive-plane-plot-in-r3",
    "title": "Linear Algebra",
    "section": "Interactive Plane Plot in \\(R^3\\)",
    "text": "Interactive Plane Plot in \\(R^3\\)\nVisualize a plane defined by a point and two direction vectors.\n\n\n\n\n\n\nTip\n\n\nModify the components of x0, v1, and v2 to explore different planes. Notice how v1 and v2 must be non-collinear to define a plane.\n\n\n\n\n\n\n\n\n\n\nThis interactive plot lets us visualize a plane in 3D space. Notice how the two vectors v1 and v2 define the orientation of the plane. If v1 and v2 were collinear, they would only define a line, not a plane. This is an important condition: the vectors must be linearly independent. This kind of visualization is critical for understanding concepts like subspaces and basis vectors in higher dimensions."
  },
  {
    "objectID": "la-34.html#generalizing-to-rn",
    "href": "la-34.html#generalizing-to-rn",
    "title": "Linear Algebra",
    "section": "Generalizing to \\(R^n\\)",
    "text": "Generalizing to \\(R^n\\)\nThe concepts of lines and planes extend naturally to higher dimensions.\n\n\nDefinition 1: Line in \\(R^n\\)\nIf \\(\\mathbf{x}_0\\) and \\(\\mathbf{v}\\) are vectors in \\(R^n\\), and if \\(\\mathbf{v}\\) is nonzero, then the equation \\[ \\mathbf{x} = \\mathbf{x}_0 + t\\mathbf{v} \\quad \\text{(5)} \\] defines the line through \\(\\mathbf{x}_0\\) that is parallel to \\(\\mathbf{v}\\).\nIn the special case where \\(\\mathbf{x}_0 = \\mathbf{0}\\), the line is said to pass through the origin.\n\nDefinition 2: Plane in \\(R^n\\)\nIf \\(\\mathbf{x}_0\\), \\(\\mathbf{v}_1\\), and \\(\\mathbf{v}_2\\) are vectors in \\(R^n\\), and if \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\) are not collinear, then the equation \\[ \\mathbf{x} = \\mathbf{x}_0 + t_1\\mathbf{v}_1 + t_2\\mathbf{v}_2 \\quad \\text{(6)} \\] defines the plane through \\(\\mathbf{x}_0\\) that is parallel to \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\).\nIn the special case where \\(\\mathbf{x}_0 = \\mathbf{0}\\), the plane is said to pass through the origin.\n\n\nWhile we can’t visualize \\(R^n\\) directly for \\(n &gt; 3\\), the algebraic forms remain the same. These vector forms are powerful because they generalize easily. In ECE, working with high-dimensional data is common, for example, in machine learning or large-scale control systems, where x could represent a state vector with many components."
  },
  {
    "objectID": "la-34.html#example-1-lines-in-r2-and-r3",
    "href": "la-34.html#example-1-lines-in-r2-and-r3",
    "title": "Linear Algebra",
    "section": "Example 1: Lines in \\(R^2\\) and \\(R^3\\)",
    "text": "Example 1: Lines in \\(R^2\\) and \\(R^3\\)\nPart (a): Line in \\(R^2\\) through origin, parallel to \\(\\mathbf{v} = (-2,3)\\)\nVector equation: \\((x,y) = t(-2,3)\\)\nParametric equations: \\(x = -2t, \\quad y = 3t\\)\nPart (b): Line in \\(R^3\\) through \\(P_0(1,2,-3)\\), parallel to \\(\\mathbf{v} = (4,-5,1)\\)\nVector equation: \\((x, y, z) = (1, 2, -3) + t(4, -5, 1)\\)\nParametric equations: \\(x = 1 + 4t, \\quad y = 2 - 5t, \\quad z = -3 + t\\)\n\n\n\n\n\n\nNote\n\n\nThe parameter \\(t\\) allows us to trace any point on the line. Different values of \\(t\\) correspond to different points.\n\n\n\n\nThese examples illustrate how to translate between vector and parametric forms. The parametric form is particularly useful when you need to substitute coordinates into other equations or perform calculations component-wise. For instance, in trajectory planning for a drone, you might need to know its exact coordinates \\((x, y, z)\\) at a given time \\(t\\)."
  },
  {
    "objectID": "la-34.html#interactive-example-1c-points-on-a-line",
    "href": "la-34.html#interactive-example-1c-points-on-a-line",
    "title": "Linear Algebra",
    "section": "Interactive Example 1(c): Points on a Line",
    "text": "Interactive Example 1(c): Points on a Line\nUsing the vector equation from Example 1(b): \\[ (x, y, z) = (1, 2, -3) + t(4, -5, 1) \\] Let’s find points for different values of \\(t\\).\n\n\n\n\n\n\n\nAs you can see, for each value of t, we get a distinct point on the line. t=0 yields the starting point P0. t=1 moves us one unit along the v vector from P0. t=-1 moves us in the opposite direction. This is a direct application of vector addition and scalar multiplication."
  },
  {
    "objectID": "la-34.html#example-2-plane-from-rectangular-equation",
    "href": "la-34.html#example-2-plane-from-rectangular-equation",
    "title": "Linear Algebra",
    "section": "Example 2: Plane from Rectangular Equation",
    "text": "Example 2: Plane from Rectangular Equation\nFind vector and parametric equations for the plane \\(x - y + 2z = 5\\).\n\nSolve for one variable: Solve for \\(x\\): \\(x = 5 + y - 2z\\)\nIntroduce parameters: Let \\(y = t_1\\) and \\(z = t_2\\).\nParametric equations:\n\\(x = 5 + t_1 - 2t_2\\)\n\\(y = t_1\\)\n\\(z = t_2\\)\nVector Equation: Rewrite in vector form:\n\\((x, y, z) = (5 + t_1 - 2t_2, t_1, t_2)\\)\nSplit into components:\n\\((x, y, z) = (5, 0, 0) + t_1(1, 1, 0) + t_2(-2, 0, 1)\\)\n\n\n\n\n\n\n\nImportant\n\n\nNote that we could have chosen to solve for \\(y\\) or \\(z\\) instead, leading to different parametric forms, but representing the same geometric plane.\n\n\n\n\nThis example demonstrates how to convert a standard Cartesian equation of a plane into its vector and parametric forms. The choice of which variable to solve for is arbitrary, but it affects the specific vectors v1 and v2 you obtain. However, the plane itself, the geometric object, remains the same. This transformation is useful in computer graphics, for example, where you might need to define a surface for rendering from an implicit equation."
  },
  {
    "objectID": "la-34.html#interactive-example-2-plane-visualization",
    "href": "la-34.html#interactive-example-2-plane-visualization",
    "title": "Linear Algebra",
    "section": "Interactive Example 2: Plane Visualization",
    "text": "Interactive Example 2: Plane Visualization\nVisualize the plane \\(x - y + 2z = 5\\).\n\n\n\n\n\n\n\nThis visualization allows you to rotate and explore the plane defined by x - y + 2z = 5. Notice how it extends infinitely in all directions. In control systems, the solution space of certain linear equations can represent a constraint surface within which system states must operate."
  },
  {
    "objectID": "la-34.html#lines-through-two-points-in-rn",
    "href": "la-34.html#lines-through-two-points-in-rn",
    "title": "Linear Algebra",
    "section": "Lines Through Two Points in \\(R^n\\)",
    "text": "Lines Through Two Points in \\(R^n\\)\n\n\nA line can also be defined by two distinct points \\(\\mathbf{x}_0\\) and \\(\\mathbf{x}_1\\).\nThe line determined by \\(\\mathbf{x}_0\\) and \\(\\mathbf{x}_1\\) is parallel to the vector \\(\\mathbf{v} = \\mathbf{x}_1 - \\mathbf{x}_0\\).\nUsing the general line equation \\(\\mathbf{x} = \\mathbf{x}_0 + t\\mathbf{v}\\): \\[ \\mathbf{x} = \\mathbf{x}_0 + t(\\mathbf{x}_1 - \\mathbf{x}_0) \\quad \\text{(9)} \\]\nThis can be rewritten as: \\[ \\mathbf{x} = (1 - t)\\mathbf{x}_0 + t\\mathbf{x}_1 \\quad \\text{(10)} \\]\nThese are the two-point vector equations of a line.\n\n Figure 3.4.5\n\n\nThis formulation is very intuitive. If you have two points, you can find the direction vector between them. Then, any point on the line can be seen as starting at one point and moving along that direction. The form (1-t)x0 + t*x1 is particularly elegant as it clearly shows the weighted average of the two points, which is useful in interpolation."
  },
  {
    "objectID": "la-34.html#interactive-line-through-two-points",
    "href": "la-34.html#interactive-line-through-two-points",
    "title": "Linear Algebra",
    "section": "Interactive: Line Through Two Points",
    "text": "Interactive: Line Through Two Points\nFind the vector and parametric equations for a line passing through two given points.\n\n\n\n\n\n\nTip\n\n\nEnter coordinates for \\(\\mathbf{x}_0\\) and \\(\\mathbf{x}_1\\) to generate the line equations and visualize it.\n\n\n\n\n\n\n\n\n\n\nThis interactive tool calculates both the vector and parametric forms and plots the line. You can easily change the input points x0_point and x1_point to see how the line adjusts. This is highly practical in engineering, for example, when defining paths for robots or data interpolation."
  },
  {
    "objectID": "la-34.html#line-segments",
    "href": "la-34.html#line-segments",
    "title": "Linear Algebra",
    "section": "Line Segments",
    "text": "Line Segments\nA line segment is a portion of a line between two specific points.\nIf \\(\\mathbf{x}_0\\) and \\(\\mathbf{x}_1\\) are vectors in \\(R^n\\), the equation: \\[ \\mathbf{x} = \\mathbf{x}_0 + t(\\mathbf{x}_1 - \\mathbf{x}_0) \\quad (0 \\leq t \\leq 1) \\quad \\text{(13)} \\] defines the line segment from \\(\\mathbf{x}_0\\) to \\(\\mathbf{x}_1\\).\nEquivalently: \\[ \\mathbf{x} = (1 - t)\\mathbf{x}_0 + t\\mathbf{x}_1 \\quad (0 \\leq t \\leq 1) \\quad \\text{(14)} \\]\n\nWhen \\(t=0\\), \\(\\mathbf{x} = \\mathbf{x}_0\\).\nWhen \\(t=1\\), \\(\\mathbf{x} = \\mathbf{x}_1\\).\nFor \\(0 &lt; t &lt; 1\\), \\(\\mathbf{x}\\) lies between \\(\\mathbf{x}_0\\) and \\(\\mathbf{x}_1\\).\n\n\nRestricting the parameter \\(t\\) to the interval [0, 1] transforms a full line into a line segment. This is extremely important in computer graphics for drawing edges, in robotics for defining specific movement trajectories, or in signal processing for windowing functions. The form (1-t)x0 + t*x1 is often called a linear interpolation formula."
  },
  {
    "objectID": "la-34.html#interactive-line-segment",
    "href": "la-34.html#interactive-line-segment",
    "title": "Linear Algebra",
    "section": "Interactive Line Segment",
    "text": "Interactive Line Segment\nVisualize a line segment between two points and explore intermediate points.\n\n\n\n\n\n\nTip\n\n\nAdjust the value of t_slider to see how a point moves along the segment. Change x0_coords and x1_coords to define a new segment.\n\n\n\n\nviewof x0_x = Inputs.number([-10, 10], {label: \"x0 (x-coor):\", step: 1, value: 1})\nviewof x0_y = Inputs.number([-10, 10], {label: \"x0 (y-coordinate):\", step: 1, value: -3})\nviewof x1_x = Inputs.number([-10, 10], {label: \"x1 (x-coordinate):\", step: 1, value: 5})\nviewof x1_y = Inputs.number([-10, 10], {label: \"x1 (y-coordinate):\", step: 1, value: 6})\n\n// Recombine into coordinate arrays\nx0_coords = [x0_x, x0_y]\nx1_coords = [x1_x, x1_y]\n\n// Create a slider for parameter t\nviewof t_slider = Inputs.range([0, 1], {step: 0.01, value: 0.5, label: \"Parameter t\"})\n\n// Calculate the point on the segment\ncurrent_point = [\n  (1 - t_slider) * x0_coords[0] + t_slider * x1_coords[0],\n  (1 - t_slider) * x0_coords[1] + t_slider * x1_coords[1]\n]\n\n// Create the plot using Observable Plot\nPlot.plot({\n  title: \"Line Segment Visualization\",\n  x: {domain: [-6, 10], label: \"X-axis\"},\n  y: {domain: [-6, 10], label: \"Y-axis\"},\n  grid: true,\n  marks: [\n    Plot.line([[x0_coords[0], x1_coords[0]], [x0_coords[1], x1_coords[1]]], {stroke: \"lightgray\", strokeWidth: 3}),\n    Plot.dot([x0_coords], {r: 8, fill: \"red\", title: `x0: (${x0_coords[0]}, ${x0_coords[1]})`}),\n    Plot.dot([x1_coords], {r: 8, fill: \"blue\", title: `x1: (${x1_coords[0]}, ${x1_coords[1]})`}),\n    Plot.dot([current_point], {r: 10, fill: \"green\", title: `Current Point (t=${t_slider.toFixed(2)}): (${current_point[0].toFixed(2)}, ${current_point[1].toFixed(2)})`})\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis Observable.js visualization allows direct manipulation of the t parameter. You can drag the slider to see the point x move from x0 to x1. This is a visual representation of linear interpolation, a core concept in various ECE applications, such as digital signal processing for creating intermediate samples or in computer graphics for blending colors or positions."
  },
  {
    "objectID": "la-34.html#dot-product-form-of-a-linear-system",
    "href": "la-34.html#dot-product-form-of-a-linear-system",
    "title": "Linear Algebra",
    "section": "Dot Product Form of a Linear System",
    "text": "Dot Product Form of a Linear System\nLinear equations can be expressed concisely using dot products.\nA linear equation \\(a_1x_1 + \\dots + a_nx_n = b\\) can be written as: \\[ \\mathbf{a} \\cdot \\mathbf{x} = b \\quad \\text{(17)} \\] where \\(\\mathbf{a} = (a_1, \\dots, a_n)\\) and \\(\\mathbf{x} = (x_1, \\dots, x_n)\\).\nFor a homogeneous equation (\\(\\mathbf{b}=0\\)): \\[ \\mathbf{a} \\cdot \\mathbf{x} = 0 \\quad \\text{(18)} \\] This implies that every solution vector \\(\\mathbf{x}\\) is orthogonal to the coefficient vector \\(\\mathbf{a}\\)."
  },
  {
    "objectID": "la-34.html#interactive-orthogonality-check",
    "href": "la-34.html#interactive-orthogonality-check",
    "title": "Linear Algebra",
    "section": "Interactive Orthogonality Check",
    "text": "Interactive Orthogonality Check\nVerify the orthogonality between a solution vector and a row vector.\n\n\n\n\n\n\nTip\n\n\nChange the solution_vector or row_vector to test different scenarios. A dot product of zero indicates orthogonality.\n\n\n\n\n\n\n\n\n\n\nThis interactive code verifies the geometric property of orthogonality. It’s a direct numerical check of Theorem 3.4.3. Notice how the first calculation, using a derived solution vector, correctly yields a dot product of zero. This has significant implications in signal processing for designing filters that eliminate certain signal components, or in error correction codes where orthogonal vectors represent distinct codewords."
  },
  {
    "objectID": "la-34.html#relationship-between-amathbfxmathbf0-and-amathbfxmathbfb",
    "href": "la-34.html#relationship-between-amathbfxmathbf0-and-amathbfxmathbfb",
    "title": "Linear Algebra",
    "section": "Relationship Between \\(A\\mathbf{x}=\\mathbf{0}\\) and \\(A\\mathbf{x}=\\mathbf{b}\\)",
    "text": "Relationship Between \\(A\\mathbf{x}=\\mathbf{0}\\) and \\(A\\mathbf{x}=\\mathbf{b}\\)\n\n\nThe solutions of a nonhomogeneous system are a translation of the solutions of its corresponding homogeneous system.\nConsider a consistent linear system \\(A\\mathbf{x} = \\mathbf{b}\\).\nLet \\(\\mathbf{x}_p\\) be any particular solution to \\(A\\mathbf{x} = \\mathbf{b}\\).\nLet \\(\\mathbf{x}_h\\) be the general solution to the homogeneous system \\(A\\mathbf{x} = \\mathbf{0}\\).\nTheorem 3.4.4: The general solution of \\(A\\mathbf{x} = \\mathbf{b}\\) is given by: \\[ \\mathbf{x} = \\mathbf{x}_p + \\mathbf{x}_h \\]\nThis means the solution set of \\(A\\mathbf{x}=\\mathbf{b}\\) is a translation of the solution space of \\(A\\mathbf{x}=\\mathbf{0}\\) by the particular solution \\(\\mathbf{x}_p\\).\n\n Figure 3.4.7\n\n\nThis theorem is incredibly powerful. It tells us that to find all solutions to a nonhomogeneous system, we just need one specific solution and then add all possible solutions to the homogeneous system. Geometrically, if the homogeneous solution set is a plane through the origin, the nonhomogeneous solution set is the same plane, just shifted to pass through xp. This concept is vital in solving differential equations, control system responses (forced vs. natural response), and understanding the structure of solution spaces in general."
  },
  {
    "objectID": "la-34.html#interactive-solution-space-translation",
    "href": "la-34.html#interactive-solution-space-translation",
    "title": "Linear Algebra",
    "section": "Interactive: Solution Space Translation",
    "text": "Interactive: Solution Space Translation\nVisualize how the solution set of \\(A\\mathbf{x} = \\mathbf{b}\\) is a translated version of the solution space of \\(A\\mathbf{x} = \\mathbf{0}\\).\n\n\n\n\n\n\nNote\n\n\nHere, we simulate a 1D solution space (a line) in 2D for simplicity. The homogeneous solution is a line through the origin. The nonhomogeneous solution is the same line, translated by \\(\\mathbf{x}_p\\).\n\n\n\n\n\n\n\n\n\n\nThis plot vividly demonstrates the translation property. The blue line represents the solution space of Ax=0, always passing through the origin. The dashed green line represents the solution space of Ax=b, which is simply the blue line shifted by the particular solution xp (the red point). This visualization reinforces the idea that the structure of the homogeneous solution space determines the “shape” of the general solution, while xp merely positions it in space. This is analogous to the concept of steady-state and transient responses in circuit analysis."
  },
  {
    "objectID": "la-34.html#conclusion-key-takeaways",
    "href": "la-34.html#conclusion-key-takeaways",
    "title": "Linear Algebra",
    "section": "Conclusion: Key Takeaways",
    "text": "Conclusion: Key Takeaways\n\nVector & Parametric Equations: Powerful tools to describe lines and planes using points and direction vectors.\nGeneralization to \\(R^n\\): These geometric concepts extend directly to higher dimensions, crucial for ECE applications.\nOrthogonality in Linear Systems: Solution vectors of \\(A\\mathbf{x}=\\mathbf{0}\\) are orthogonal to the row vectors of \\(A\\).\nSolution Space Translation: The solution set of \\(A\\mathbf{x}=\\mathbf{b}\\) is a translation of the solution space of \\(A\\mathbf{x}=\\mathbf{0}\\).\n\n\n\n\n\n\n\nImportant\n\n\nUnderstanding the geometry behind linear systems is essential for intuitive problem-solving and deeper insights in fields like signal processing, control systems, and machine learning.\n\n\n\n\nTo wrap up, remember that linear algebra isn’t just about symbols and numbers; it’s about geometric relationships. These geometric interpretations provide intuition, help visualize complex data, and are directly applicable in many ECE domains. From designing filters to understanding system stability, these geometric insights are invaluable. Keep practicing these concepts and visualizing them!"
  },
  {
    "objectID": "la-34.html#dot-product-form-of-a-linear-system-1",
    "href": "la-34.html#dot-product-form-of-a-linear-system-1",
    "title": "Linear Algebra",
    "section": "Dot Product Form of a Linear System",
    "text": "Dot Product Form of a Linear System\nHomogeneous System \\(A\\mathbf{x} = \\mathbf{0}\\)\nIf \\(A\\) has row vectors \\(\\mathbf{r}_1, \\dots, \\mathbf{r}_m\\), the system becomes: \\[\n\\begin{array}{r}{\\mathbf{r}_{1}\\cdot \\mathbf{x} = \\mathbf{0}}\\\\ {\\mathbf{r}_{2}\\cdot \\mathbf{x} = \\mathbf{0}}\\\\ \\vdots \\quad \\vdots \\\\ {\\mathbf{r}_{m}\\cdot \\mathbf{x} = \\mathbf{0}} \\end{array} \\quad \\text{(19)}\n\\] Theorem 3.4.3: The solution set of \\(A\\mathbf{x}=\\mathbf{0}\\) consists of all vectors in \\(R^n\\) that are orthogonal to every row vector of \\(A\\).\n\nThe dot product notation simplifies the representation of linear equations and systems. The geometric interpretation of a . x = 0 is crucial: x lies in the space orthogonal to a. When we extend this to a system Ax = 0, the solution space is orthogonal to all row vectors of A. This concept is fundamental in understanding null spaces, which are vital in signal processing for noise reduction and in control theory for system stability analysis."
  },
  {
    "objectID": "la-35.html#introduction-to-the-cross-product",
    "href": "la-35.html#introduction-to-the-cross-product",
    "title": "Linear Algebra",
    "section": "Introduction to the Cross Product",
    "text": "Introduction to the Cross Product\nThe cross product is a vector operation unique to 3-dimensional space. Unlike the dot product, which results in a scalar, the cross product produces a vector. It’s crucial for understanding concepts like torque, angular momentum, and magnetic forces in physics and engineering.\n\nIn ECE, particularly in electromagnetism, mechanics, and computer graphics, the cross product is indispensable. It allows us to find a vector perpendicular to two given vectors, which is fundamental for defining normal vectors to surfaces, calculating torque, or determining the direction of magnetic fields. While the dot product measures the “alignment” of two vectors, the cross product measures their “perpendicularity” and gives a direction."
  },
  {
    "objectID": "la-35.html#definition-of-the-cross-product",
    "href": "la-35.html#definition-of-the-cross-product",
    "title": "Linear Algebra",
    "section": "Definition of the Cross Product",
    "text": "Definition of the Cross Product\nIf \\(\\mathbf{u} = (u_{1}, u_{2}, u_{3})\\) and \\(\\mathbf{v} = (v_{1}, v_{2}, v_{3})\\) are vectors in 3-space, then the cross product \\(\\mathbf{u} \\times \\mathbf{v}\\) is defined as:\n\\[\n\\mathbf{u} \\times \\mathbf{v} = (u_{2}v_{3} - u_{3}v_{2}, u_{3}v_{1} - u_{1}v_{3}, u_{1}v_{2} - u_{2}v_{1})\n\\]\nDeterminant Notation\nThis can be remembered using a determinant format:\n\\[\n\\mathbf{u} \\times \\mathbf{v}\n= \\biggl(\n    \\left| \\begin{array}{cc} u_{2} & u_{3} \\\\ v_{2} & v_{3} \\end{array} \\right|,\n    - \\left| \\begin{array}{cc} u_{1} & u_{3} \\\\ v_{1} & v_{3} \\end{array} \\right|,\n    \\left| \\begin{array}{cc} u_{1} & u_{2} \\\\ v_{1} & v_{2} \\end{array} \\right|\n\\biggr) \\quad \\text{(1)}\n\\]\n\n\n\n\n\n\nTip\n\n\nMnemonic for Calculation: Form a \\(2 \\times 3\\) matrix: \\(\\left[ \\begin{array}{lll}u_{1} & u_{2} & u_{3} \\\\ v_{1} & v_{2} & v_{3} \\end{array} \\right]\\)\n\n1st Component: Delete 1st column, take determinant.\n2nd Component: Delete 2nd column, take negative of determinant.\n3rd Component: Delete 3rd column, take determinant.\n\n\n\n\n\nThe cross product is only defined for vectors in 3-space. This definition might look complicated, but the determinant mnemonic makes it much easier to recall. The resulting vector is perpendicular to both u and v, a property we will explore further. In computer graphics, if u and v define two edges of a polygon, their cross product gives the normal vector to that polygon, which is essential for lighting calculations and surface rendering."
  },
  {
    "objectID": "la-35.html#example-1-calculating-a-cross-product",
    "href": "la-35.html#example-1-calculating-a-cross-product",
    "title": "Linear Algebra",
    "section": "Example 1: Calculating a Cross Product",
    "text": "Example 1: Calculating a Cross Product\nFind \\(\\mathbf{u} \\times \\mathbf{v}\\), where \\(\\mathbf{u} = (1, 2, -2)\\) and \\(\\mathbf{v} = (3, 0, 1)\\).\nSolution:\nUsing the determinant notation or mnemonic:\n\\[\n{\\begin{array}{r l}&{\\mathbf{u}\\times\\mathbf{v}={\\bigg(}{\\bigg|}{\\begin{array}{l l}{2}&{-2}\\\\ {0}&{1}\\end{array}}{\\bigg|},-{\\bigg|}{\\begin{array}{l l}{1}&{-2}\\\\ {3}&{1}\\end{array}}{\\bigg|},{\\bigg|}{\\begin{array}{l l}{1}&{2}\\\\ {3}&{0}\\end{array}}{\\bigg|}{\\bigg)}}\\\\ &{\\qquad=(2,-7,-6)}\\end{array}}\n\\]"
  },
  {
    "objectID": "la-35.html#properties-of-cross-product-theorem-3.5.1",
    "href": "la-35.html#properties-of-cross-product-theorem-3.5.1",
    "title": "Linear Algebra",
    "section": "Properties of Cross Product: Theorem 3.5.1",
    "text": "Properties of Cross Product: Theorem 3.5.1\nThe cross product has fundamental relationships with the dot product and important geometric implications.\nIf \\(\\mathbf{u}, \\mathbf{v},\\) and \\(\\mathbf{w}\\) are vectors in 3-space:\n\n\\(\\mathbf{u} \\cdot (\\mathbf{u} \\times \\mathbf{v}) = 0\\) \\([\\mathbf{u} \\times \\mathbf{v}\\) is orthogonal to \\(\\mathbf{u}]\\)\n\\(\\mathbf{v} \\cdot (\\mathbf{u} \\times \\mathbf{v}) = 0\\) \\([\\mathbf{u} \\times \\mathbf{v}\\) is orthogonal to \\(\\mathbf{v}]\\)\n\\(\\| \\mathbf{u} \\times \\mathbf{v}\\| ^2 = \\| \\mathbf{u}\\| ^2 \\| \\mathbf{v}\\| ^2 - (\\mathbf{u} \\cdot \\mathbf{v})^2\\) [Lagrange’s identity]\n\\(\\mathbf{u} \\times (\\mathbf{v} \\times \\mathbf{w}) = (\\mathbf{u} \\cdot \\mathbf{w}) \\mathbf{v} - (\\mathbf{u} \\cdot \\mathbf{v}) \\mathbf{w}\\) [Vector Triple Product]\n\\((\\mathbf{u} \\times \\mathbf{v}) \\times \\mathbf{w} = (\\mathbf{u} \\cdot \\mathbf{w}) \\mathbf{v} - (\\mathbf{v} \\cdot \\mathbf{w}) \\mathbf{u}\\) [Vector Triple Product]\n\n\n\n\n\n\n\nImportant\n\n\nProperties (a) and (b) are key: The cross product of two vectors is always orthogonal (perpendicular) to both original vectors. This is its defining geometric characteristic.\n\n\n\n\nThe orthogonality property is incredibly useful. For example, in electromagnetism, the force on a charge moving through a magnetic field (Lorentz force) is given by a cross product, and the force is always perpendicular to both the velocity and the magnetic field. Lagrange’s identity (c) provides a link between the magnitudes of the cross product and dot product, which we’ll use for geometric interpretations. The vector triple product formulas (d) and (e) are important for simplifying complex vector expressions, especially in advanced mechanics or field theory."
  },
  {
    "objectID": "la-35.html#interactive-orthogonality-verification",
    "href": "la-35.html#interactive-orthogonality-verification",
    "title": "Linear Algebra",
    "section": "Interactive: Orthogonality Verification",
    "text": "Interactive: Orthogonality Verification\nLet’s verify properties (a) and (b) using the vectors from Example 1.\n\\(\\mathbf{u} = (1, 2, -2)\\), \\(\\mathbf{v} = (3, 0, 1)\\), and \\(\\mathbf{u} \\times \\mathbf{v} = (2, -7, -6)\\).\n\n\n\n\n\n\n\nThis interactive check confirms that the dot product of the cross product with each of the original vectors is indeed zero, demonstrating their orthogonality. This property is fundamental in physics, for instance, when calculating the normal to a surface or the direction of magnetic fields. In computer graphics, surface normals, derived from cross products, are used to determine how light reflects off objects."
  },
  {
    "objectID": "la-35.html#properties-of-cross-product-theorem-3.5.2",
    "href": "la-35.html#properties-of-cross-product-theorem-3.5.2",
    "title": "Linear Algebra",
    "section": "Properties of Cross Product: Theorem 3.5.2",
    "text": "Properties of Cross Product: Theorem 3.5.2\nThe cross product also has several algebraic properties.\nIf \\(\\mathbf{u}\\), \\(\\mathbf{v}\\), and \\(\\mathbf{w}\\) are any vectors in 3-space and \\(k\\) is any scalar:\n\n\\(\\mathbf{u}\\times \\mathbf{v} = -(\\mathbf{v}\\times \\mathbf{u})\\) (Anti-commutative)\n\\(\\mathbf{u}\\times (\\mathbf{v} + \\mathbf{w}) = (\\mathbf{u}\\times \\mathbf{v}) + (\\mathbf{u}\\times \\mathbf{w})\\) (Distributive over vector addition)\n\\((\\mathbf{u} + \\mathbf{v})\\times \\mathbf{w} = (\\mathbf{u}\\times \\mathbf{w}) + (\\mathbf{v}\\times \\mathbf{w})\\) (Distributive over vector addition)\n\\(k(\\mathbf{u}\\times \\mathbf{v}) = (k\\mathbf{u})\\times \\mathbf{v} = \\mathbf{u}\\times (k\\mathbf{v})\\) (Scalar multiplication)\n\\(\\mathbf{u}\\times \\mathbf{0} = \\mathbf{0}\\times \\mathbf{u} = \\mathbf{0}\\) (Zero vector property)\n\\(\\mathbf{u}\\times \\mathbf{u} = \\mathbf{0}\\) (Self cross product)\n\n\nThe anti-commutative property (a) is important: the order of vectors matters, and reversing it flips the direction of the resulting vector. This is unlike scalar multiplication or dot product. Properties (b), (c), and (d) show that the cross product interacts well with vector addition and scalar multiplication, making it a linear operation in each argument. Property (f) implies that the cross product of a vector with itself (or with a parallel vector) is always the zero vector, as the vectors do not define a unique perpendicular direction."
  },
  {
    "objectID": "la-35.html#cross-products-of-standard-unit-vectors",
    "href": "la-35.html#cross-products-of-standard-unit-vectors",
    "title": "Linear Algebra",
    "section": "Cross Products of Standard Unit Vectors",
    "text": "Cross Products of Standard Unit Vectors\nThe standard unit vectors are \\(\\mathbf{i} = (1,0,0)\\), \\(\\mathbf{j} = (0,1,0)\\), \\(\\mathbf{k} = (0,0,1)\\).\n\n\nCalculations show:\n\n\\(\\mathbf{i}\\times \\mathbf{j} = \\mathbf{k}\\)\n\\(\\mathbf{j}\\times \\mathbf{k} = \\mathbf{i}\\)\n\\(\\mathbf{k}\\times \\mathbf{i} = \\mathbf{j}\\)\n\nAnd due to anti-commutativity:\n\n\\(\\mathbf{j}\\times \\mathbf{i} = -\\mathbf{k}\\)\n\\(\\mathbf{k}\\times \\mathbf{j} = -\\mathbf{i}\\)\n\\(\\mathbf{i}\\times \\mathbf{k} = -\\mathbf{j}\\)\n\nAlso, for self-cross products: - \\(\\mathbf{i}\\times \\mathbf{i} = \\mathbf{0}\\) - \\(\\mathbf{j}\\times \\mathbf{j} = \\mathbf{0}\\) - \\(\\mathbf{k}\\times \\mathbf{k} = \\mathbf{0}\\)\n\n\nFigure 3.5.2: Mnemonic for unit vector cross products.\n\n\nThis diagram is a fantastic mnemonic! Moving clockwise (i to j to k to i) gives the next vector. Moving counter-clockwise gives the negative of the next vector. This pattern directly reflects the right-hand rule, which is a convention for determining the direction of the cross product, vital in electromagnetism for current, magnetic fields, and forces."
  },
  {
    "objectID": "la-35.html#determinant-form-of-cross-product",
    "href": "la-35.html#determinant-form-of-cross-product",
    "title": "Linear Algebra",
    "section": "Determinant Form of Cross Product",
    "text": "Determinant Form of Cross Product\nThe cross product can also be expressed symbolically using a \\(3 \\times 3\\) determinant.\n\\[\n\\mathbf{u}\\times \\mathbf{v} = \\left| \\begin{array}{lll}\\mathbf{i} & \\mathbf{j} & \\mathbf{k}\\\\ u_{1} & u_{2} & u_{3}\\\\ v_{1} & v_{2} & v_{3} \\end{array} \\right| = \\left| \\begin{array}{lll}u_{2} & u_{3}\\\\ v_{2} & v_{3} \\end{array} \\right|\\mathbf{i} - \\left| \\begin{array}{lll}u_{1} & u_{3}\\\\ v_{1} & v_{3} \\end{array} \\right|\\mathbf{j} + \\left| \\begin{array}{lll}u_{1} & u_{2}\\\\ v_{1} & v_{2} \\end{array} \\right|\\mathbf{k} \\quad \\text{(4)}\n\\]\nExample: \\(\\mathbf{u} = (1,2,-2)\\), \\(\\mathbf{v} = (3,0,1)\\)\n\\[\n\\mathbf{u}\\times \\mathbf{v} = \\left| \\begin{array}{lll}\\mathbf{i} & \\mathbf{j} & \\mathbf{k}\\\\ 1 & 2 & -2\\\\ 3 & 0 & 1 \\end{array} \\right| = (2)(1) - (-2)(0)\\mathbf{i} - ((1)(1) - (-2)(3))\\mathbf{j} + ((1)(0) - (2)(3))\\mathbf{k} = 2\\mathbf{i} - 7\\mathbf{j} - 6\\mathbf{k}\n\\]\n\n\n\n\n\n\nWarning\n\n\nThe cross product is not associative! \\(\\mathbf{u}\\times (\\mathbf{v}\\times \\mathbf{w}) \\neq (\\mathbf{u}\\times \\mathbf{v})\\times \\mathbf{w}\\) For example, \\(\\mathbf{i}\\times (\\mathbf{j}\\times \\mathbf{j}) = \\mathbf{0}\\), but \\((\\mathbf{i}\\times \\mathbf{j})\\times \\mathbf{j} = -\\mathbf{i}\\).\n\n\n\n\nThis \\(3 \\times 3\\) determinant form is perhaps the most common way to calculate a cross product by hand, as it naturally produces the components in terms of i, j, and k. It’s essentially a compact way of writing the component-wise definition. The non-associativity warning is critical! This is a common pitfall. The order of operations matters significantly, just as it does for matrix multiplication."
  },
  {
    "objectID": "la-35.html#geometric-interpretation-the-right-hand-rule",
    "href": "la-35.html#geometric-interpretation-the-right-hand-rule",
    "title": "Linear Algebra",
    "section": "Geometric Interpretation: The Right-Hand Rule",
    "text": "Geometric Interpretation: The Right-Hand Rule\nThe direction of \\(\\mathbf{u} \\times \\mathbf{v}\\) for non-zero vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) is given by the right-hand rule.\n\n\n\nCurl the fingers of your right hand from \\(\\mathbf{u}\\) towards \\(\\mathbf{v}\\) through the smaller angle between them.\nYour thumb will point in the direction of \\(\\mathbf{u} \\times \\mathbf{v}\\).\n\nThis rule is a convention that establishes the orientation of the resulting vector in 3-space.\n\n Figure 3.5.3: The Right-Hand Rule.\n\n\nThe right-hand rule is more than just a convention; it’s deeply embedded in physics and engineering. It’s used to determine the direction of magnetic fields around currents, the direction of torque, or the direction of propagation of electromagnetic waves. It’s a visual and intuitive way to understand the vector direction that results from a cross product. Practice with i x j = k to get a feel for it."
  },
  {
    "objectID": "la-35.html#geometric-interpretation-area-of-a-parallelogram",
    "href": "la-35.html#geometric-interpretation-area-of-a-parallelogram",
    "title": "Linear Algebra",
    "section": "Geometric Interpretation: Area of a Parallelogram",
    "text": "Geometric Interpretation: Area of a Parallelogram\nThe magnitude of the cross product has a direct geometric meaning.\n\n\nFrom Lagrange’s identity: \\(\\| \\mathbf{u}\\times \\mathbf{v}\\|^{2} = \\| \\mathbf{u}\\|^{2}\\| \\mathbf{v}\\|^{2} - (\\mathbf{u}\\cdot \\mathbf{v})^{2}\\). Using \\(\\mathbf{u}\\cdot \\mathbf{v} = \\| \\mathbf{u}\\| \\| \\mathbf{v}\\| \\cos \\theta\\): \\[\n\\| \\mathbf{u}\\times \\mathbf{v}\\| = \\| \\mathbf{u}\\| \\| \\mathbf{v}\\| \\sin \\theta \\quad \\text{(6)}\n\\] This formula is precisely the area of the parallelogram determined by vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\).\nTheorem 3.5.3: If \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are vectors in 3-space, then \\(\\| \\mathbf{u}\\times \\mathbf{v}\\|\\) is equal to the area of the parallelogram determined by \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\).\n\n Figure 3.5.4: Area of parallelogram.\n\n\nThis is a beautiful geometric interpretation. The sin(theta) factor naturally accounts for how “spread out” the vectors are. If they are parallel (\\(\\theta=0\\) or \\(\\theta=\\pi\\)), then \\(\\sin\\theta=0\\), and the area (and cross product magnitude) is zero, which makes sense as parallel vectors don’t form a parallelogram with positive area. This is very useful in computer graphics for calculating surface areas or in physics for magnetic flux calculations."
  },
  {
    "objectID": "la-35.html#example-4-area-of-a-triangle",
    "href": "la-35.html#example-4-area-of-a-triangle",
    "title": "Linear Algebra",
    "section": "Example 4: Area of a Triangle",
    "text": "Example 4: Area of a Triangle\nFind the area of the triangle determined by points \\(P_1(2,2,0)\\), \\(P_2(-1,0,2)\\), and \\(P_3(0,4,3)\\).\n\nForm vectors: \\(\\overrightarrow{P_1P_2} = (-1-2, 0-2, 2-0) = (-3, -2, 2)\\) \\(\\overrightarrow{P_1P_3} = (0-2, 4-2, 3-0) = (-2, 2, 3)\\)\nCalculate cross product: \\(\\overrightarrow{P_1P_2} \\times \\overrightarrow{P_1P_3} = ((-2)(3) - (2)(2), (2)(-2) - (-3)(3), (-3)(2) - (-2)(-2))\\) \\(= (-6-4, -4+9, -6-4) = (-10, 5, -10)\\)\nCalculate magnitude: \\(\\| \\overrightarrow{P_1P_2} \\times \\overrightarrow{P_1P_3}\\| = \\sqrt{(-10)^2 + 5^2 + (-10)^2} = \\sqrt{100 + 25 + 100} = \\sqrt{225} = 15\\)\nArea of triangle: \\(A = \\frac{1}{2} \\| \\overrightarrow{P_1P_2} \\times \\overrightarrow{P_1P_3}\\| = \\frac{1}{2} (15) = \\frac{15}{2}\\)\n\n Figure 3.5.5: Triangle formed by three points.\n\nThe area of a triangle formed by three points is simply half the area of the parallelogram formed by two vectors originating from one of those points. This is a practical application of the cross product. In computer graphics, this is used to calculate the area of triangular facets, which are the basic building blocks of 3D models."
  },
  {
    "objectID": "la-35.html#interactive-area-of-triangle-calculator",
    "href": "la-35.html#interactive-area-of-triangle-calculator",
    "title": "Linear Algebra",
    "section": "Interactive: Area of Triangle Calculator",
    "text": "Interactive: Area of Triangle Calculator\nEnter the coordinates of three points to calculate the area of the triangle they form.\n\n\n\n\n\n\n\nThis interactive tool automates the process of finding the area of a triangle in 3D space. It reinforces the understanding that the cross product’s magnitude is geometrically significant. This can be used in surveying, architectural design, or in robotics for path planning where areas of regions need to be computed."
  },
  {
    "objectID": "la-35.html#scalar-triple-product",
    "href": "la-35.html#scalar-triple-product",
    "title": "Linear Algebra",
    "section": "Scalar Triple Product",
    "text": "Scalar Triple Product\nThe scalar triple product involves three vectors and results in a scalar.\nDefinition 2: If \\(\\mathbf{u}, \\mathbf{v}, \\mathbf{w}\\) are vectors in 3-space, then \\(\\mathbf{u} \\cdot (\\mathbf{v} \\times \\mathbf{w})\\) is called the scalar triple product.\nIt can be calculated using a \\(3 \\times 3\\) determinant: \\[\n\\mathbf{u} \\cdot (\\mathbf{v} \\times \\mathbf{w}) = \\left| \\begin{array}{ccc}u_{1} & u_{2} & u_{3} \\\\ v_{1} & v_{2} & v_{3} \\\\ w_{1} & w_{2} & w_{3} \\end{array} \\right| \\quad \\text{(7)}\n\\]\nExample 5: Calculation\nFor \\(\\mathbf{u} = (3, -2, -5)\\), \\(\\mathbf{v} = (1, 4, -4)\\), \\(\\mathbf{w} = (0, 3, 2)\\):\n\\[\n\\begin{array}{r l} & {\\mathbf{u}\\cdot (\\mathbf{v}\\times \\mathbf{w}) = \\left| \\begin{array}{l l l}{3} & {-2} & {-5}\\\\ {1} & {4} & {-4}\\\\ {0} & {3} & {2} \\end{array} \\right|}\\\\ & {\\qquad = 3(4 \\cdot 2 - (-4) \\cdot 3) - (-2)(1 \\cdot 2 - (-4) \\cdot 0) + (-5)(1 \\cdot 3 - 4 \\cdot 0)}\\\\ & {\\qquad = 3(8 + 12) + 2(2) - 5(3)}\\\\ & {\\qquad = 3(20) + 4 - 15 = 60 + 4 - 15 = 49} \\end{array}\n\\]\n\nThe scalar triple product is a very efficient way to compute this specific combination of dot and cross products. The order of operations is fixed by the parentheses: v x w is calculated first, resulting in a vector, then dotted with u. The determinant form simplifies the calculation significantly. Note that writing \\(\\mathbf{u} \\cdot \\mathbf{v} \\times \\mathbf{w}\\) without parentheses is usually understood to mean \\(\\mathbf{u} \\cdot (\\mathbf{v} \\times \\mathbf{w})\\) because \\((\\mathbf{u} \\cdot \\mathbf{v}) \\times \\mathbf{w}\\) is not a valid operation (you can’t cross a scalar with a vector)."
  },
  {
    "objectID": "la-35.html#geometric-interpretation-of-determinants",
    "href": "la-35.html#geometric-interpretation-of-determinants",
    "title": "Linear Algebra",
    "section": "Geometric Interpretation of Determinants",
    "text": "Geometric Interpretation of Determinants\nThe absolute value of determinants has profound geometric meaning.\nTheorem 3.5.4 (a): The absolute value of \\(\\operatorname{det}\\left[ \\begin{smallmatrix}u_{1} & u_{2} \\\\ v_{1} & v_{2} \\end{smallmatrix} \\right]\\) is the area of the parallelogram in 2-space determined by \\(\\mathbf{u}=(u_1, u_2)\\) and \\(\\mathbf{v}=(v_1, v_2)\\).\nTheorem 3.5.4 (b): The absolute value of \\(\\operatorname{det}\\left[ \\begin{smallmatrix}u_{1} & u_{2} & u_{3}\\\\ v_{1} & v_{2} & v_{3}\\\\ w_{1} & w_{2} & w_{3} \\end{smallmatrix} \\right]\\) is the volume of the parallelepiped in 3-space determined by \\(\\mathbf{u}, \\mathbf{v}, \\mathbf{w}\\). \\[\nV = |\\mathbf{u}\\cdot (\\mathbf{v}\\times \\mathbf{w})| \\quad \\text{(9)}\n\\]\n\nFigure 3.5.7: (a) Area of 2D parallelogram, (b) Volume of 3D parallelepiped.\n\nThis theorem connects determinants, a purely algebraic concept, to fundamental geometric quantities: area and volume. The 2D case can be seen as a special case of the cross product’s magnitude by embedding the vectors in the XY-plane of 3D space. The 3D case directly relates to the scalar triple product. This is incredibly important in fields like numerical analysis and computational geometry, where areas and volumes need to be calculated efficiently, for example, in finite element analysis or fluid dynamics simulations."
  },
  {
    "objectID": "la-35.html#interactive-volume-of-parallelepiped-calculator",
    "href": "la-35.html#interactive-volume-of-parallelepiped-calculator",
    "title": "Linear Algebra",
    "section": "Interactive: Volume of Parallelepiped Calculator",
    "text": "Interactive: Volume of Parallelepiped Calculator\nEnter the components of three vectors \\(\\mathbf{u}, \\mathbf{v}, \\mathbf{w}\\) to calculate the volume of the parallelepiped they determine.\n\n\n\n\n\n\n\nThis tool computes the volume using the scalar triple product. Try making the vectors coplanar (e.g., w = u + v or a linear combination of u and v). What do you expect the volume to be? If the vectors are coplanar, they will not form a 3D parallelepiped, and the volume should be zero. This leads directly into our next concept: the test for coplanarity."
  },
  {
    "objectID": "la-35.html#coplanarity-test-theorem-3.5.5",
    "href": "la-35.html#coplanarity-test-theorem-3.5.5",
    "title": "Linear Algebra",
    "section": "Coplanarity Test: Theorem 3.5.5",
    "text": "Coplanarity Test: Theorem 3.5.5\nThe scalar triple product provides a simple test for whether three vectors lie in the same plane.\nIf the vectors \\(\\mathbf{u}, \\mathbf{v}, \\mathbf{w}\\) have the same initial point, then they lie in the same plane if and only if:\n\\[\n\\mathbf{u}\\cdot (\\mathbf{v}\\times \\mathbf{w}) = \\left| \\begin{array}{lll}u_{1} & u_{2} & u_{3}\\\\ v_{1} & v_{2} & v_{3}\\\\ w_{1} & w_{2} & w_{3} \\end{array} \\right| = 0\n\\]\n\n\n\n\n\n\nNote\n\n\nCoplanar points: A set of points is coplanar if you can draw a single flat surface (a plane) that contains all of them.\nCoplanar vectors: Vectors are coplanar if their linear combinations lie within the same plane.\nGeometrically, if three vectors are coplanar, they cannot form a parallelepiped with a positive volume. Hence, their scalar triple product (which is the signed volume) must be zero.\n\n\n\n\nThis theorem is extremely useful in geometry and physics. For example, in structural engineering, determining if three forces are coplanar is crucial for analyzing equilibrium. In computer graphics, it can be used to check if three points lie on the same plane, which is important for rendering and collision detection. It’s a powerful and elegant consequence of the geometric interpretation of the scalar triple product."
  },
  {
    "objectID": "la-35.html#conclusion-key-takeaways",
    "href": "la-35.html#conclusion-key-takeaways",
    "title": "Linear Algebra",
    "section": "Conclusion: Key Takeaways",
    "text": "Conclusion: Key Takeaways\n\nDefinition: The cross product \\(\\mathbf{u} \\times \\mathbf{v}\\) yields a vector in 3-space orthogonal to both \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\).\nProperties: It’s anti-commutative, distributive, but NOT associative.\nGeometric Magnitude: \\(\\| \\mathbf{u} \\times \\mathbf{v}\\|\\) represents the area of the parallelogram formed by \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\).\nDirection: Determined by the right-hand rule.\nScalar Triple Product: \\(\\mathbf{u} \\cdot (\\mathbf{v} \\times \\mathbf{w})\\) represents the signed volume of the parallelepiped formed by \\(\\mathbf{u}, \\mathbf{v}, \\mathbf{w}\\).\nCoplanarity Test: Three vectors are coplanar if their scalar triple product is zero.\n\n\n\n\n\n\n\nImportant\n\n\nThe cross product and scalar triple product are fundamental tools in ECE for understanding 3D geometry, forces, fields, and spatial relationships.\n\n\n\n\nToday, we’ve explored the cross product, a unique and powerful operation in 3D vector algebra. Its ability to generate a perpendicular vector, its connection to areas and volumes, and its role in determining coplanarity are all incredibly valuable. These concepts are not just theoretical; they are the backbone of many practical applications in electromagnetism, robotics, computer graphics, and mechanics. Keep these geometric interpretations in mind as you encounter more complex problems in your ECE studies."
  },
  {
    "objectID": "la-35.html#example-1-calculating-a-cross-product-1",
    "href": "la-35.html#example-1-calculating-a-cross-product-1",
    "title": "Linear Algebra",
    "section": "Example 1: Calculating a Cross Product",
    "text": "Example 1: Calculating a Cross Product\nInteractive Cross Product Calculator\nEnter the components of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) to calculate their cross product.\n\n\n\n\n\n\n\nThis interactive tool allows you to quickly compute cross products. Try changing the input vectors. For instance, what happens if u and v are parallel? (e.g., u = [1,2,3], v = [2,4,6]). The result should be the zero vector, as parallel vectors do not define a unique perpendicular direction. This is a direct application of numpy.cross, a highly optimized function in Python’s numerical library."
  },
  {
    "objectID": "la-41.html#what-is-a-vector-space",
    "href": "la-41.html#what-is-a-vector-space",
    "title": "Linear Algebra",
    "section": "What is a Vector Space?",
    "text": "What is a Vector Space?\nA vector space is a nonempty set \\(V\\) of objects, called vectors, on which two operations are defined:\n\nAddition: For any \\(\\mathbf{u}, \\mathbf{v}\\) in \\(V\\), their sum \\(\\mathbf{u} + \\mathbf{v}\\) is also in \\(V\\).\nScalar Multiplication: For any scalar \\(k\\) and \\(\\mathbf{u}\\) in \\(V\\), the scalar multiple \\(k\\mathbf{u}\\) is also in \\(V\\).\n\nIf these operations satisfy ten axioms, then \\(V\\) is called a vector space. Scalars will primarily be real numbers, defining a real vector space.\n\n\n\n\n\n\nNote\n\n\nIn ECE, scalars often represent physical quantities like gain, amplitude, or time constants. The “objects” can be diverse: voltages, currents, signals, images, or even matrices.\n\n\n\n\nEmphasize that the definition is abstract. The “vectors” don’t have to be arrows in space. They could be functions, matrices, or sequences. The key is that they obey these ten rules. We’ll focus on real vector spaces for now, as they are most common in introductory ECE applications."
  },
  {
    "objectID": "la-41.html#the-ten-vector-space-axioms-part-1",
    "href": "la-41.html#the-ten-vector-space-axioms-part-1",
    "title": "Linear Algebra",
    "section": "The Ten Vector Space Axioms (Part 1)",
    "text": "The Ten Vector Space Axioms (Part 1)\nLet \\(\\mathbf{u}\\), \\(\\mathbf{v}\\), \\(\\mathbf{w}\\) be objects in \\(V\\), and \\(k\\), \\(m\\) be scalars.\n\n\nClosure & Commutativity\n\nClosure under Addition: If \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are in \\(V\\), then \\(\\mathbf{u} + \\mathbf{v}\\) is in \\(V\\).\nCommutativity: \\(\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\\).\nAssociativity: \\(\\mathbf{u} + (\\mathbf{v} + \\mathbf{w}) = (\\mathbf{u} + \\mathbf{v}) + \\mathbf{w}\\).\n\n\nIdentity & Inverse\n\nZero Vector: There is a zero vector \\(\\mathbf{0}\\) in \\(V\\) such that \\(\\mathbf{0} + \\mathbf{u} = \\mathbf{u} + \\mathbf{0} = \\mathbf{u}\\) for all \\(\\mathbf{u}\\) in \\(V\\).\nNegative Vector: For each \\(\\mathbf{u}\\) in \\(V\\), there is a negative vector \\(-\\mathbf{u}\\) in \\(V\\) such that \\(\\mathbf{u} + (-\\mathbf{u}) = (-\\mathbf{u}) + \\mathbf{u} = \\mathbf{0}\\).\n\n\n\nThese first five axioms deal with the addition operation. Closure means that adding two vectors together always results in another vector within the same set. Commutativity means order doesn’t matter for addition. Associativity means grouping doesn’t matter. The zero vector is like the additive identity, and the negative vector is the additive inverse. These might seem obvious for \\(R^n\\), but for other types of vectors, they need explicit verification."
  },
  {
    "objectID": "la-41.html#the-ten-vector-space-axioms-part-2",
    "href": "la-41.html#the-ten-vector-space-axioms-part-2",
    "title": "Linear Algebra",
    "section": "The Ten Vector Space Axioms (Part 2)",
    "text": "The Ten Vector Space Axioms (Part 2)\nLet \\(\\mathbf{u}\\), \\(\\mathbf{v}\\), \\(\\mathbf{w}\\) be objects in \\(V\\), and \\(k\\), \\(m\\) be scalars.\n\n\nScalar Multiplication Properties\n\nClosure under Scalar Multiplication: If \\(k\\) is any scalar and \\(\\mathbf{u}\\) is in \\(V\\), then \\(k\\mathbf{u}\\) is in \\(V\\).\nDistributivity (Scalar over Vector Sum): \\(k(\\mathbf{u} + \\mathbf{v}) = k\\mathbf{u} + k\\mathbf{v}\\).\nDistributivity (Scalar Sum over Vector): \\((k + m)\\mathbf{u} = k\\mathbf{u} + m\\mathbf{u}\\).\n\n\nCompatibility & Identity\n\nAssociativity of Scalar Multiplication: \\(k(m\\mathbf{u}) = (km)(\\mathbf{u})\\).\nScalar Identity: \\(1\\mathbf{u} = \\mathbf{u}\\).\n\n\n\n\n\n\n\n\nImportant\n\n\nAxioms 1 and 6 (closure) are crucial. If these fail, the set cannot be a vector space, as the operations take us outside the defined set \\(V\\).\n\n\n\n\nThe remaining five axioms deal with scalar multiplication and how it interacts with vector addition. Again, these are intuitive for \\(R^n\\), but need to be proven for other vector candidates. Axiom 10, the scalar identity, ensures that multiplying by 1 leaves the vector unchanged, which is a fundamental requirement for the “scalar” concept."
  },
  {
    "objectID": "la-41.html#how-to-verify-a-vector-space",
    "href": "la-41.html#how-to-verify-a-vector-space",
    "title": "Linear Algebra",
    "section": "How to Verify a Vector Space",
    "text": "How to Verify a Vector Space\nTo show that a set with two operations is a vector space, follow these four steps:\n\nIdentify \\(V\\): Clearly define the set of objects that will be called “vectors”.\nIdentify Operations: Clearly define the addition and scalar multiplication operations on \\(V\\).\nVerify Closure (Axioms 1 & 6):\n\nAdding two vectors in \\(V\\) must result in a vector in \\(V\\).\nMultiplying a vector in \\(V\\) by a scalar must result in a vector in \\(V\\).\n\nConfirm Remaining Axioms (Axioms 2, 3, 4, 5, 7, 8, 9, 10):\n\nProve that the other eight axioms hold true for the defined operations.\n\n\n\nThis systematic approach is essential. Don’t jump straight to proving axiom 10 without first defining the set and its operations. Closure axioms are often the easiest to check first and can quickly rule out a set from being a vector space."
  },
  {
    "objectID": "la-41.html#illustration-of-vector-space-properties",
    "href": "la-41.html#illustration-of-vector-space-properties",
    "title": "Linear Algebra",
    "section": "Illustration of Vector Space Properties",
    "text": "Illustration of Vector Space Properties\nLet’s visualize the structure of a vector space.\n\n\n\n\n\ngraph TD\n    A[Set V of Objects] --&gt; B{Operations Defined?};\n    B -- Yes --&gt; C(Vector Addition: +);\n    B -- Yes --&gt; D(Scalar Multiplication: .);\n\n    C --&gt; E{Axioms 1-5 Satisfied?};\n    D --&gt; F{Axioms 6-10 Satisfied?};\n\n    E -- Yes --&gt; G(Addition Properties Hold);\n    F -- Yes --&gt; H(Scalar Mult. Properties Hold);\n\n    G & H --&gt; I{All 10 Axioms Satisfied?};\n    I -- Yes --&gt; J[V is a Vector Space];\n    I -- No --&gt; K[V is NOT a Vector Space];\n\n    subgraph Axioms\n        E;F;\n    end\n    style J fill:#cef,stroke:#3c3,stroke-width:2px;\n    style K fill:#fcc,stroke:#c33,stroke-width:2px;\n\n\n\n\n\n\n\nThis diagram visually represents the logical flow for determining if a given set and its operations form a vector space. It emphasizes the critical role of the ten axioms."
  },
  {
    "objectID": "la-41.html#example-1-the-zero-vector-space",
    "href": "la-41.html#example-1-the-zero-vector-space",
    "title": "Linear Algebra",
    "section": "Example 1: The Zero Vector Space",
    "text": "Example 1: The Zero Vector Space\nLet \\(V\\) consist of a single object, denoted by \\(\\mathbf{0}\\). Define operations:\n\\[\n\\mathbf{0} + \\mathbf{0} = \\mathbf{0} \\text{ and } k\\mathbf{0} = \\mathbf{0}\n\\]\nfor all scalars \\(k\\).\nVerification:\n\nAxiom 1 (Closure under Addition): \\(\\mathbf{0} + \\mathbf{0} = \\mathbf{0}\\), which is in \\(V\\). (Holds)\nAxiom 6 (Closure under Scalar Multiplication): \\(k\\mathbf{0} = \\mathbf{0}\\), which is in \\(V\\). (Holds)\nAll other axioms are trivially satisfied because there’s only one object, the zero vector itself.\n\nThis is called the zero vector space.\n\nThis is the simplest possible vector space. It might seem trivial, but it’s a valid example and confirms that the axioms can be satisfied even in the most basic scenarios. It highlights that the zero vector is a fundamental requirement."
  },
  {
    "objectID": "la-41.html#example-2-rn-is-a-vector-space",
    "href": "la-41.html#example-2-rn-is-a-vector-space",
    "title": "Linear Algebra",
    "section": "Example 2: \\(R^n\\) is a Vector Space",
    "text": "Example 2: \\(R^n\\) is a Vector Space\nLet \\(V = R^n\\), the set of all \\(n\\)-tuples of real numbers. Define vector space operations as the usual operations of addition and scalar multiplication for \\(n\\)-tuples:\n\\[\n\\mathbf{u} + \\mathbf{v} = (u_1, u_2, \\ldots, u_n) + (v_1, v_2, \\ldots, v_n) = (u_1+v_1, u_2+v_2, \\ldots, u_n+v_n)\n\\]\n\\[\nk\\mathbf{u} = (k u_1, k u_2, \\ldots, k u_n)\n\\]\nVerification:\n\nClosure (Axioms 1 & 6): The operations produce \\(n\\)-tuples, so they are closed. (Holds)\nRemaining Axioms: These operations are precisely what the axioms were based on. They all hold true.\n\n\n\n\n\n\n\nTip\n\n\nIn ECE, \\(R^n\\) is ubiquitous! It represents signal samples, state vectors in control systems, pixel intensities in images, or coefficients of polynomials.\n\n\n\n\nThis is our foundational example. The axioms were literally derived from the properties of \\(R^n\\). So, it’s not surprising that \\(R^n\\) is indeed a vector space. This gives us confidence in the definition."
  },
  {
    "objectID": "la-41.html#interactive-example-vector-addition-in-r2",
    "href": "la-41.html#interactive-example-vector-addition-in-r2",
    "title": "Linear Algebra",
    "section": "Interactive Example: Vector Addition in \\(R^2\\)",
    "text": "Interactive Example: Vector Addition in \\(R^2\\)\nAdjust the components of vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) to see their sum.\n\nviewof u_x = Inputs.range([-5, 5], {label: \"u_x\", step: 0.1, value: 2})\nviewof u_y = Inputs.range([-5, 5], {label: \"u_y\", step: 0.1, value: 3})\nviewof v_x = Inputs.range([-5, 5], {label: \"v_x\", step: 0.1, value: 1})\nviewof v_y = Inputs.range([-5, 5], {label: \"v_y\", step: 0.1, value: -2})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive plot helps students visualize vector addition in \\(R^2\\). They can manipulate the components and see how the resultant vector changes, reinforcing the geometric interpretation of vector addition."
  },
  {
    "objectID": "la-41.html#example-3-the-vector-space-of-infinite-sequences",
    "href": "la-41.html#example-3-the-vector-space-of-infinite-sequences",
    "title": "Linear Algebra",
    "section": "Example 3: The Vector Space of Infinite Sequences",
    "text": "Example 3: The Vector Space of Infinite Sequences\nLet \\(V\\) be the set of objects of the form \\(\\mathbf{u} = (u_1, u_2, \\ldots, u_n, \\ldots)\\), an infinite sequence of real numbers. Define addition and scalar multiplication componentwise:\n\\[\n\\mathbf{u} + \\mathbf{v} = (u_1+v_1, u_2+v_2, \\ldots, u_n+v_n, \\ldots)\n\\]\n\\[\nk\\mathbf{u} = (k u_1, k u_2, \\ldots, k u_n, \\ldots)\n\\]\nThis vector space is denoted by \\(R^\\infty\\).\n\n\n\n\n\n\nTip\n\n\nECE Application: In signal processing, a discrete-time signal of infinite duration can be represented as an infinite sequence. Adding signals or scaling them (e.g., amplification) are common operations.\n\n\n\n\nThis example shows that vectors can have infinitely many components. This is very relevant in ECE, especially in digital signal processing where signals are often represented as sequences of values. The image illustrates how a continuous signal is converted into a discrete sequence, which can then be treated as a vector in \\(R^\\infty\\)."
  },
  {
    "objectID": "la-41.html#example-4-the-vector-space-of-2-times-2-matrices",
    "href": "la-41.html#example-4-the-vector-space-of-2-times-2-matrices",
    "title": "Linear Algebra",
    "section": "Example 4: The Vector Space of \\(2 \\times 2\\) Matrices",
    "text": "Example 4: The Vector Space of \\(2 \\times 2\\) Matrices\nLet \\(V\\) be the set of \\(2 \\times 2\\) matrices with real entries. We treat these matrices as our “vectors”. The operations are standard matrix addition and scalar multiplication.\n\\[\n\\mathbf{u} = \\left[ \\begin{array}{ll}u_{11} & u_{12} \\\\ u_{21} & u_{22} \\end{array} \\right], \\quad \\mathbf{v} = \\left[ \\begin{array}{ll}v_{11} & v_{12} \\\\ v_{21} & v_{22} \\end{array} \\right]\n\\]\n\n\n\n\n\n\nTip\n\n\nECE Application: Matrices are fundamental in ECE for representing linear transformations, systems of equations (e.g., circuit analysis), and state-space models of dynamic systems.\n\n\n\n\nThis might be confusing at first because rows and columns of matrices are themselves vectors. However, here we consider the entire matrix as a single vector in the vector space \\(M_{22}\\). This is a crucial distinction. We are looking at the properties of the matrix operations as they apply to the matrix as a whole."
  },
  {
    "objectID": "la-41.html#example-4-2-times-2-matrices---operations",
    "href": "la-41.html#example-4-2-times-2-matrices---operations",
    "title": "Linear Algebra",
    "section": "Example 4: \\(2 \\times 2\\) Matrices - Operations",
    "text": "Example 4: \\(2 \\times 2\\) Matrices - Operations\nAddition:\n\\[\n\\mathbf{u} + \\mathbf{v} = \\left[ \\begin{array}{ll}u_{11} & u_{12} \\\\ u_{21} & u_{22} \\end{array} \\right] + \\left[ \\begin{array}{ll}v_{11} & v_{12} \\\\ v_{21} & v_{22} \\end{array} \\right] = \\left[ \\begin{array}{ll}u_{11} + v_{11} & u_{12} + v_{12} \\\\ u_{21} + v_{21} & u_{22} + v_{22} \\end{array} \\right] \\tag{1}\n\\]\nScalar Multiplication:\n\\[\nk\\mathbf{u} = k\\left[ \\begin{array}{ll}u_{11} & u_{12} \\\\ u_{21} & u_{22} \\end{array} \\right] = \\left[ \\begin{array}{ll}k u_{11} & k u_{12} \\\\ k u_{21} & k u_{22} \\end{array} \\right]\n\\]\n\nNotice that Equation (1) involves three different addition operations: vector addition on the left, matrix addition in the middle, and numerical addition on the right. This highlights the abstraction."
  },
  {
    "objectID": "la-41.html#example-4-2-times-2-matrices---axiom-verification",
    "href": "la-41.html#example-4-2-times-2-matrices---axiom-verification",
    "title": "Linear Algebra",
    "section": "Example 4: \\(2 \\times 2\\) Matrices - Axiom Verification",
    "text": "Example 4: \\(2 \\times 2\\) Matrices - Axiom Verification\nThe set \\(V\\) is closed under addition and scalar multiplication because the operations produce \\(2 \\times 2\\) matrices.\nAxiom 4 (Zero Vector): The zero vector for \\(V\\) is the \\(2 \\times 2\\) zero matrix:\n\\[\n\\mathbf{0} = \\left[ \\begin{array}{ll}0 & 0 \\\\ 0 & 0 \\end{array} \\right]\n\\]\nThen \\(\\mathbf{0} + \\mathbf{u} = \\mathbf{u}\\).\nAxiom 5 (Negative Vector): The negative of \\(\\mathbf{u}\\) is:\n\\[\n-\\mathbf{u} = \\left[ \\begin{array}{ll} - u_{11} & -u_{12} \\\\ -u_{21} & -u_{22} \\end{array} \\right]\n\\]\nThen \\(\\mathbf{u} + (-\\mathbf{u}) = \\mathbf{0}\\).\nAxiom 10 (Scalar Identity):\n\\[\n1\\mathbf{u} = 1\\left[ \\begin{array}{ll}u_{11} & u_{12} \\\\ u_{21} & u_{22} \\end{array} \\right] = \\left[ \\begin{array}{ll}u_{11} & u_{12} \\\\ u_{21} & u_{22} \\end{array} \\right] = \\mathbf{u}\n\\]\nThe other axioms follow from properties of real numbers and matrix arithmetic.\n\nThis verification demonstrates how to formally check the axioms. It’s important to explicitly state what the zero vector and negative vector are in this specific context."
  },
  {
    "objectID": "la-41.html#interactive-example-matrix-operations",
    "href": "la-41.html#interactive-example-matrix-operations",
    "title": "Linear Algebra",
    "section": "Interactive Example: Matrix Operations",
    "text": "Interactive Example: Matrix Operations\nExperiment with matrix addition and scalar multiplication.\n\n\n\nviewof a11 = Inputs.range([-5, 5], {label: \"A[0,0]\", step: 0.1, value: 1})\nviewof a12 = Inputs.range([-5, 5], {label: \"A[0,1]\", step: 0.1, value: 2})\nviewof a21 = Inputs.range([-5, 5], {label: \"A[1,0]\", step: 0.1, value: 3})\nviewof a22 = Inputs.range([-5, 5], {label: \"A[1,1]\", step: 0.1, value: 4})\n\nviewof b11 = Inputs.range([-5, 5], {label: \"B[0,0]\", step: 0.1, value: -1})\nviewof b12 = Inputs.range([-5, 5], {label: \"B[0,1]\", step: 0.1, value: 0})\nviewof b21 = Inputs.range([-5, 5], {label: \"B[1,0]\", step: 0.1, value: 2})\nviewof b22 = Inputs.range([-5, 5], {label: \"B[1,1]\", step: 0.1, value: -3})\n\nviewof scalar_k = Inputs.range([-3, 3], {label: \"Scalar k\", step: 0.1, value: 2})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive example allows students to directly manipulate matrix elements and a scalar, then observe the results of addition and scalar multiplication. This hands-on experience can help solidify their understanding of these operations within the context of matrices as vectors."
  },
  {
    "objectID": "la-41.html#example-5-the-vector-space-of-m-times-n-matrices",
    "href": "la-41.html#example-5-the-vector-space-of-m-times-n-matrices",
    "title": "Linear Algebra",
    "section": "Example 5: The Vector Space of \\(m \\times n\\) Matrices",
    "text": "Example 5: The Vector Space of \\(m \\times n\\) Matrices\nThe set \\(V\\) of all \\(m \\times n\\) matrices with real entries, with the usual matrix operations of addition and scalar multiplication, is a vector space. This vector space is denoted by \\(M_{mn}\\).\n\nExample 4 (\\(M_{22}\\)) is a special case.\nThe proof follows the same logic: closure, existence of zero matrix (all zeros), negative matrix (all elements negated), and other axioms derived from real number properties.\n\n\nThis generalizes the previous example. It’s important for students to recognize that the principles apply universally to matrices of any compatible dimensions. This is especially relevant in ECE for systems with multiple inputs/outputs or for representing complex data structures."
  },
  {
    "objectID": "la-41.html#example-6-the-vector-space-of-real-valued-functions",
    "href": "la-41.html#example-6-the-vector-space-of-real-valued-functions",
    "title": "Linear Algebra",
    "section": "Example 6: The Vector Space of Real-Valued Functions",
    "text": "Example 6: The Vector Space of Real-Valued Functions\nLet \\(V\\) be the set of real-valued functions defined at each \\(x\\) in the interval \\((-\\infty, \\infty)\\). If \\(\\mathbf{f} = f(x)\\) and \\(\\mathbf{g} = g(x)\\) are two functions in \\(V\\), and \\(k\\) is any scalar:\nAddition:\n\\[\n(\\mathbf{f}+\\mathbf{g})(x) = f(x) + g(x)\n\\]\nScalar Multiplication:\n\\[\n(k\\mathbf{f})(x) = k f(x)\n\\]\nThis vector space is denoted by \\(F(-\\infty, \\infty)\\).\n\n\n\n\n\n\nTip\n\n\nECE Application: Signals and systems theory heavily relies on functions as vectors. Superposition (linear combination) of signals is a direct application of vector space properties.\n\n\n\n\nHere, the “components” of the functions are their values at each point \\(x\\). So, adding functions means adding their values at each point, and scalar multiplying a function means scaling its value at each point. This is analogous to component-wise operations in \\(R^n\\) or \\(R^\\infty\\)."
  },
  {
    "objectID": "la-41.html#example-6-real-valued-functions---verification",
    "href": "la-41.html#example-6-real-valued-functions---verification",
    "title": "Linear Algebra",
    "section": "Example 6: Real-Valued Functions - Verification",
    "text": "Example 6: Real-Valued Functions - Verification\nAxioms 1 & 6 (Closure):\nIf \\(f(x)\\) and \\(g(x)\\) are defined for all \\(x\\), then \\(f(x)+g(x)\\) and \\(k f(x)\\) are also defined for all \\(x\\). (Holds)\nAxiom 4 (Zero Vector):\nThe zero vector is the function \\(\\mathbf{0}(x) = 0\\) for all \\(x\\). \\(\\mathbf{0}(x) + f(x) = 0 + f(x) = f(x)\\).\nAxiom 5 (Negative Vector):\nThe negative of \\(f(x)\\) is the function \\((-\\mathbf{f})(x) = -f(x)\\). \\(f(x) + (-f(x)) = 0 = \\mathbf{0}(x)\\).\nRemaining Axioms:\nFollow from properties of real numbers. For example, for Axiom 2: \\((\\mathbf{f}+\\mathbf{g})(x) = f(x)+g(x) = g(x)+f(x) = (\\mathbf{g}+\\mathbf{f})(x)\\).\n\nThe graph visually reinforces the concepts of function addition, scalar multiplication, and the negative of a function. This is highly relevant in ECE for understanding signal manipulation, such as combining multiple sensor inputs or amplifying a signal."
  },
  {
    "objectID": "la-41.html#interactive-example-function-operations",
    "href": "la-41.html#interactive-example-function-operations",
    "title": "Linear Algebra",
    "section": "Interactive Example: Function Operations",
    "text": "Interactive Example: Function Operations\nAdjust functions and scalar to visualize their sum and scaled version.\n\n\n\nviewof func1_expr = Inputs.text({label: \"Function f(x)\", value: \"Math.sin(x)\"})\nviewof func2_expr = Inputs.text({label: \"Function g(x)\", value: \"Math.cos(x/2)\"})\nviewof scalar_k_func = Inputs.range([-2, 2], {label: \"Scalar k\", step: 0.1, value: 0.5})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive plot allows students to define their own functions (e.g., Math.sin(x), x**2, Math.exp(-x)) and a scalar, then see the graphical results of their sum and scalar multiplication. This is a powerful tool for understanding how linear combinations of functions work, which is central to Fourier series, Laplace transforms, and many other ECE topics."
  },
  {
    "objectID": "la-41.html#when-a-set-is-not-a-vector-space",
    "href": "la-41.html#when-a-set-is-not-a-vector-space",
    "title": "Linear Algebra",
    "section": "When a Set is NOT a Vector Space",
    "text": "When a Set is NOT a Vector Space\nIt’s crucial to understand that not every set with defined addition and scalar multiplication operations forms a vector space. One or more axioms might fail.\n\n\n\n\n\n\nWarning\n\n\nEven if operations seem “normal,” careful verification of ALL ten axioms is required. A common pitfall is assuming closure if the operations are defined.\n\n\n\nExample: The set of \\(n\\)-tuples with positive components, using standard \\(R^n\\) operations, is NOT a vector space.\n\nIf \\(\\mathbf{u} = (1, 2)\\) (positive components), and \\(k = -1\\).\nThen \\(k\\mathbf{u} = (-1, -2)\\), which has negative components.\nThis result is not in the set of \\(n\\)-tuples with positive components.\nAxiom 6 (Closure under Scalar Multiplication) fails!\n\n\nThis example highlights that closure axioms (1 and 6) are often the first to fail. If the operations take you out of the set, it’s immediately not a vector space. This is a simple but effective way to weed out non-vector spaces."
  },
  {
    "objectID": "la-41.html#example-7-r2-with-modified-scalar-multiplication",
    "href": "la-41.html#example-7-r2-with-modified-scalar-multiplication",
    "title": "Linear Algebra",
    "section": "Example 7: \\(R^2\\) with Modified Scalar Multiplication",
    "text": "Example 7: \\(R^2\\) with Modified Scalar Multiplication\nLet \\(V = R^2\\).\nDefine addition as standard: \\(\\mathbf{u} + \\mathbf{v} = (u_1+v_1, u_2+v_2)\\).\nDefine scalar multiplication as unusual: \\(k\\mathbf{u} = (k u_1, 0)\\).\nLet’s check Axiom 10: \\(1\\mathbf{u} = \\mathbf{u}\\).\nIf \\(\\mathbf{u} = (u_1, u_2)\\) with \\(u_2 \\neq 0\\):\n\\[\n1\\mathbf{u} = 1(u_1, u_2) = (1 \\cdot u_1, 0) = (u_1, 0)\n\\]\nThis result \\((u_1, 0)\\) is not equal to \\(\\mathbf{u} = (u_1, u_2)\\) if \\(u_2 \\neq 0\\).\nTherefore, Axiom 10 fails, and \\(V\\) is not a vector space with these operations.\n\nThis example demonstrates that even if 9 out of 10 axioms hold, if just one fails, it’s not a vector space. The modified scalar multiplication forces the second component to zero, breaking the identity property for any vector not already on the x-axis."
  },
  {
    "objectID": "la-41.html#example-8-an-unusual-vector-space",
    "href": "la-41.html#example-8-an-unusual-vector-space",
    "title": "Linear Algebra",
    "section": "Example 8: An Unusual Vector Space",
    "text": "Example 8: An Unusual Vector Space\nLet \\(V\\) be the set of positive real numbers.\nLet \\(\\mathbf{u} = u\\) and \\(\\mathbf{v} = v\\) be any “vectors” (positive real numbers) in \\(V\\).\nLet \\(k\\) be any scalar.\nDefine the operations on \\(V\\) as:\n\nVector Addition: \\(\\mathbf{u} + \\mathbf{v} = u \\cdot v\\) (numerical multiplication).\nScalar Multiplication: \\(k\\mathbf{u} = u^k\\) (numerical exponentiation).\n\nExample: \\(1 + 1 = 1 \\cdot 1 = 1\\).\n\\((2)(1) = 1^2 = 1\\).\nThis is indeed a vector space!\n\nThis is a truly “unusual” vector space designed to challenge intuition. It shows how abstract the definition is. The ordinary meaning of addition and multiplication is replaced by new definitions, yet the ten axioms can still be satisfied. This is a great example to emphasize the formal nature of the definition, rather than relying on intuition from \\(R^n\\)."
  },
  {
    "objectID": "la-41.html#example-8-unusual-vector-space---axiom-verification",
    "href": "la-41.html#example-8-unusual-vector-space---axiom-verification",
    "title": "Linear Algebra",
    "section": "Example 8: Unusual Vector Space - Axiom Verification",
    "text": "Example 8: Unusual Vector Space - Axiom Verification\nLet’s confirm a few axioms:\nAxiom 4 (Zero Vector):\nThe zero vector \\(\\mathbf{0}\\) in this space is the number \\(1\\).\n\\(u + \\mathbf{0} = u \\cdot 1 = u\\). (Holds)\nAxiom 5 (Negative Vector):\nThe negative of a vector \\(u\\) is its reciprocal, \\(1/u\\).\n\\(u + (-u) = u \\cdot (1/u) = 1 \\, (= \\mathbf{0})\\). (Holds)\nAxiom 7 (Distributivity): \\(k(\\mathbf{u} + \\mathbf{v}) = k(u \\cdot v) = (u \\cdot v)^k = u^k \\cdot v^k\\).\nAlso, \\(k\\mathbf{u} + k\\mathbf{v} = u^k + v^k = u^k \\cdot v^k\\).\nSo, \\(k(\\mathbf{u} + \\mathbf{v}) = k\\mathbf{u} + k\\mathbf{v}\\). (Holds)\nThe other axioms can also be verified.\n\nThis detailed verification of specific axioms for the “unusual” vector space further demonstrates the process. It’s important to remember that the symbols + and k u here represent the defined operations (\\(uv\\) and \\(u^k\\)), not standard numerical addition and multiplication."
  },
  {
    "objectID": "la-41.html#some-properties-of-vectors",
    "href": "la-41.html#some-properties-of-vectors",
    "title": "Linear Algebra",
    "section": "Some Properties of Vectors",
    "text": "Some Properties of Vectors\nOnce a set is confirmed to be a vector space, we can derive properties that hold for all vector spaces.\nTheorem 4.1.1: Let \\(V\\) be a vector space, \\(\\mathbf{u}\\) a vector in \\(V\\), and \\(k\\) a scalar; then:\n\n\\(0\\mathbf{u} = \\mathbf{0}\\)\n\\(k\\mathbf{0} = \\mathbf{0}\\)\n\\((-1)\\mathbf{u} = -\\mathbf{u}\\)\nIf \\(k\\mathbf{u} = \\mathbf{0}\\), then \\(k = 0\\) or \\(\\mathbf{u} = \\mathbf{0}\\).\n\nThese properties, familiar from \\(R^n\\), are consequences of the ten axioms.\n\nThis theorem shows the power of abstraction. By proving these properties once for general vector spaces, they apply to all specific examples we’ve seen (R^n, matrices, functions, infinite sequences, even the unusual positive real numbers example). This is a core concept in mathematics and engineering: generalize once, apply everywhere."
  },
  {
    "objectID": "la-41.html#proof-of-theorem-4.1.1-part-a",
    "href": "la-41.html#proof-of-theorem-4.1.1-part-a",
    "title": "Linear Algebra",
    "section": "Proof of Theorem 4.1.1 (Part a)",
    "text": "Proof of Theorem 4.1.1 (Part a)\nTo prove: \\(0\\mathbf{u} = \\mathbf{0}\\).\nWe start with properties of scalars and axioms:\n\\[\n\\begin{array}{r l}\n\\mathbf{0}\\mathbf{u} + \\mathbf{0}\\mathbf{u} &= (0 + 0)\\mathbf{u} & \\text{[Axiom 8]} \\\\\n&= \\mathbf{0}\\mathbf{u} & \\text{[Property of the number 0]}\n\\end{array}\n\\]\nNow, add the negative of \\(\\mathbf{0}\\mathbf{u}\\) (which exists by Axiom 5) to both sides:\n\\[\n[\\mathbf{0}\\mathbf{u} + \\mathbf{0}\\mathbf{u}] + (-\\mathbf{0}\\mathbf{u}) = \\mathbf{0}\\mathbf{u} + (-\\mathbf{0}\\mathbf{u})\n\\]\nApplying Axioms:\n\\[\n\\begin{array}{r l}\n\\mathbf{0}\\mathbf{u} + [\\mathbf{0}\\mathbf{u} + (-\\mathbf{0}\\mathbf{u})] &= \\mathbf{0}\\mathbf{u} + (-\\mathbf{0}\\mathbf{u}) & \\text{[Axiom 3]} \\\\\n\\mathbf{0}\\mathbf{u} + \\mathbf{0} &= \\mathbf{0} & \\text{[Axiom 5]} \\\\\n\\mathbf{0}\\mathbf{u} &= \\mathbf{0} & \\text{[Axiom 4]}\n\\end{array}\n\\]\n\nThis step-by-step proof, explicitly citing each axiom, reinforces the formal nature of vector space theory. It shows how familiar results are rigorously derived from the fundamental axioms."
  },
  {
    "objectID": "la-41.html#proof-of-theorem-4.1.1-part-c",
    "href": "la-41.html#proof-of-theorem-4.1.1-part-c",
    "title": "Linear Algebra",
    "section": "Proof of Theorem 4.1.1 (Part c)",
    "text": "Proof of Theorem 4.1.1 (Part c)\nTo prove: \\((-1)\\mathbf{u} = -\\mathbf{u}\\).\nWe must show that \\(\\mathbf{u} + (-1)\\mathbf{u} = \\mathbf{0}\\) (by definition of \\(-\\mathbf{u}\\) from Axiom 5).\n\\[\n\\begin{array}{r l}\n\\mathbf{u} + (-1)\\mathbf{u} &= 1\\mathbf{u} + (-1)\\mathbf{u} & \\text{[Axiom 10]} \\\\\n&= (1 + (-1))\\mathbf{u} & \\text{[Axiom 8]} \\\\\n&= 0\\mathbf{u} & \\text{[Property of numbers]} \\\\\n&= \\mathbf{0} & \\text{[Part (a) of this theorem]}\n\\end{array}\n\\]\nSince \\(\\mathbf{u} + (-1)\\mathbf{u} = \\mathbf{0}\\), it follows that \\((-1)\\mathbf{u}\\) is the unique negative of \\(\\mathbf{u}\\), so \\((-1)\\mathbf{u} = -\\mathbf{u}\\).\n\nThis proof demonstrates how previously proven parts of a theorem (like part ‘a’) can be used to prove subsequent parts. It also reinforces the definition of a negative vector."
  },
  {
    "objectID": "la-41.html#closing-observation-engineering-relevance",
    "href": "la-41.html#closing-observation-engineering-relevance",
    "title": "Linear Algebra",
    "section": "Closing Observation & Engineering Relevance",
    "text": "Closing Observation & Engineering Relevance\nThe concept of a vector space unifies diverse mathematical objects:\n\nGeometric vectors (\\(R^2, R^3\\))\nVectors in \\(R^n\\)\nInfinite sequences (\\(R^\\infty\\))\nMatrices (\\(M_{mn}\\))\nReal-valued functions (\\(F(-\\infty, \\infty)\\))\nEven abstract constructions like the “unusual” vector space.\n\nWhenever we prove a theorem about general vector spaces, it automatically applies to all these specific examples.\n\n\n\n\n\n\nImportant\n\n\nImpact for ECE: This abstraction allows us to develop powerful tools and theories (like linear transformations, eigenvalues, eigenvectors) that are applicable across various ECE domains: signal processing, control systems, machine learning, circuit analysis, and more.\n\n\n\n\nThis final slide is crucial for tying everything together and highlighting the practical significance of this abstract concept for ECE students. It emphasizes that the effort put into understanding general vector spaces pays off by providing a unified framework for solving problems in many different areas of engineering."
  },
  {
    "objectID": "la-41.html#real-vector-spaces-beyond-rn",
    "href": "la-41.html#real-vector-spaces-beyond-rn",
    "title": "Linear Algebra",
    "section": "Real Vector Spaces: Beyond \\(R^n\\)",
    "text": "Real Vector Spaces: Beyond \\(R^n\\)\nIntroduction: Extending the Vector Concept\nIn this section, we extend the familiar concept of a vector beyond \\(R^n\\). We use the fundamental properties of vectors in \\(R^n\\) as axioms. These axioms define what it means for a set of objects to “behave like” vectors.\n\nWe’re moving from concrete examples like 2D or 3D vectors to a more abstract, yet powerful, definition. This abstraction allows us to apply linear algebra tools to a much wider range of mathematical objects encountered in engineering. Think about signals, images, or even control system states as vectors."
  },
  {
    "objectID": "la-42.html#introduction-to-subspaces",
    "href": "la-42.html#introduction-to-subspaces",
    "title": "Linear Algebra",
    "section": "Introduction to Subspaces",
    "text": "Introduction to Subspaces\nIt is often the case that some vector space of interest is contained within a larger vector space whose properties are known. In this section, we learn how to recognize these ‘smaller’ vector spaces and use the properties of the larger space."
  },
  {
    "objectID": "la-42.html#definition-of-a-subspace",
    "href": "la-42.html#definition-of-a-subspace",
    "title": "Linear Algebra",
    "section": "Definition of a Subspace",
    "text": "Definition of a Subspace\nA subset \\(W\\) of a vector space \\(V\\) is called a subspace of \\(V\\) if \\(W\\) is itself a vector space under the addition and scalar multiplication defined on \\(V\\).\n\nTo prove a set is a vector space, one usually verifies all ten axioms. However, if a set \\(W\\) is a subset of a known vector space \\(V\\), many axioms are “inherited” from \\(V\\). We only need to check closure under addition and scalar multiplication, as shown in the next theorem."
  },
  {
    "objectID": "la-42.html#subspace-verification-theorem",
    "href": "la-42.html#subspace-verification-theorem",
    "title": "Linear Algebra",
    "section": "Subspace Verification Theorem",
    "text": "Subspace Verification Theorem\n\n\n\n\n\n\nTHEOREM 4.2.1\n\n\nIf \\(W\\) is a set of one or more vectors in a vector space \\(V\\), then \\(W\\) is a subspace of \\(V\\) if and only if the following conditions are satisfied:\n\nIf \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are vectors in \\(W\\), then \\(\\mathbf{u} + \\mathbf{v}\\) is in \\(W\\). (Closure under addition)\nIf \\(k\\) is a scalar and \\(\\mathbf{u}\\) is a vector in \\(W\\), then \\(k\\mathbf{u}\\) is in \\(W\\). (Closure under scalar multiplication)\n\n\n\n\n\nProof Idea:\nIf \\(W\\) is a subspace, conditions (a) and (b) hold by definition (Axioms 1 and 6).\nConversely, if (a) and (b) hold, then Axioms 1 and 6 are satisfied. Axioms 2, 3, 7, 8, 9, 10 are inherited from \\(V\\).\nWe only need to show Axioms 4 (zero vector) and 5 (negative vector) hold.\nFrom (b), for any \\(\\mathbf{u} \\in W\\), \\(0\\mathbf{u} = \\mathbf{0}\\) is in \\(W\\), satisfying Axiom 4.\nAlso from (b), for any \\(\\mathbf{u} \\in W\\), \\((-1)\\mathbf{u} = -\\mathbf{u}\\) is in \\(W\\), satisfying Axiom 5.\nThus, all axioms are satisfied, and \\(W\\) is a subspace."
  },
  {
    "objectID": "la-42.html#visualizing-non-closure",
    "href": "la-42.html#visualizing-non-closure",
    "title": "Linear Algebra",
    "section": "Visualizing Non-Closure",
    "text": "Visualizing Non-Closure\nSometimes, adding two vectors in \\(W\\) or multiplying a vector in \\(W\\) by a scalar produces a vector in \\(V\\) that is outside of \\(W\\).\n\n\n\n\nFigure 4.2.1:\nThe vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are in \\(W\\), but the vectors \\(\\mathbf{u} + \\mathbf{v}\\) and \\(k\\mathbf{u}\\) are not.\nThis illustrates why closure under addition and scalar multiplication are critical to verify for a subset to be a subspace."
  },
  {
    "objectID": "la-42.html#example-1-the-zero-subspace",
    "href": "la-42.html#example-1-the-zero-subspace",
    "title": "Linear Algebra",
    "section": "Example 1: The Zero Subspace",
    "text": "Example 1: The Zero Subspace\nEvery vector space has at least two subspaces: itself and its zero subspace.\nIf \\(V\\) is any vector space, and if \\(W = \\{\\mathbf{0}\\}\\) is the subset of \\(V\\) that consists of the zero vector only, then \\(W\\) is closed under addition and scalar multiplication since\n\\[\n\\mathbf{0} + \\mathbf{0} = \\mathbf{0} \\text{and} k\\mathbf{0} = \\mathbf{0}\n\\]\nfor any scalar \\(k\\). We call \\(W\\) the zero subspace of \\(V\\)."
  },
  {
    "objectID": "la-42.html#example-2-lines-through-the-origin",
    "href": "la-42.html#example-2-lines-through-the-origin",
    "title": "Linear Algebra",
    "section": "Example 2: Lines Through the Origin",
    "text": "Example 2: Lines Through the Origin\nIf \\(W\\) is a line through the origin of either \\(R^2\\) or \\(R^3\\), then \\(W\\) is a subspace.\nAdding two vectors on the line or multiplying a vector on the line by a scalar produces another vector on the line.\n\n\n\n\n\nFigure 4.2.2 (a): \\(W\\) is closed under addition.\nFigure 4.2.2 (b): \\(W\\) is closed under scalar multiplication.\nBoth operations keep the resultant vector on the same line through the origin."
  },
  {
    "objectID": "la-42.html#example-3-planes-through-the-origin",
    "href": "la-42.html#example-3-planes-through-the-origin",
    "title": "Linear Algebra",
    "section": "Example 3: Planes Through the Origin",
    "text": "Example 3: Planes Through the Origin\nIf \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are vectors in a plane \\(W\\) through the origin of \\(R^3\\), then \\(\\mathbf{u} + \\mathbf{v}\\) and \\(k\\mathbf{u}\\) also lie in the same plane \\(W\\).\n\nFigure 4.2.3:\nThe vectors \\(\\mathbf{u} + \\mathbf{v}\\) and \\(k\\mathbf{u}\\) both lie in the same plane as \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\).\nThus \\(W\\) is closed under addition and scalar multiplication."
  },
  {
    "objectID": "la-42.html#common-subspaces-of-r2-and-r3",
    "href": "la-42.html#common-subspaces-of-r2-and-r3",
    "title": "Linear Algebra",
    "section": "Common Subspaces of \\(R^2\\) and \\(R^3\\)",
    "text": "Common Subspaces of \\(R^2\\) and \\(R^3\\)\nThese are the only subspaces of \\(R^2\\) and \\(R^3\\).\n\n\nSubspaces of \\(R^2\\)\n\n\\(\\{\\mathbf{0}\\}\\) (the zero vector)\nLines through the origin\n\\(R^2\\) itself\n\n\nSubspaces of \\(R^3\\)\n\n\\(\\{\\mathbf{0}\\}\\) (the zero vector)\nLines through the origin\nPlanes through the origin\n\\(R^3\\) itself"
  },
  {
    "objectID": "la-42.html#example-4-a-subset-of-r2-that-is-not-a-subspace",
    "href": "la-42.html#example-4-a-subset-of-r2-that-is-not-a-subspace",
    "title": "Linear Algebra",
    "section": "Example 4: A Subset of \\(R^2\\) That Is Not a Subspace",
    "text": "Example 4: A Subset of \\(R^2\\) That Is Not a Subspace\nLet \\(W\\) be the set of all points \\((x,y)\\) in \\(R^2\\) for which \\(x \\geq 0\\) and \\(y \\geq 0\\) (the first quadrant).\nThis set is not a subspace of \\(R^2\\) because it is not closed under scalar multiplication.\n\nFigure 4.2.4:\n\\(W\\) is not closed under scalar multiplication.\nHere, \\(\\mathbf{v} = (1,1)\\) is in \\(W\\), but \\((-1)\\mathbf{v} = (-1,-1)\\) is not.\nLet’s test this."
  },
  {
    "objectID": "la-42.html#example-5-subspaces-of-m_nn",
    "href": "la-42.html#example-5-subspaces-of-m_nn",
    "title": "Linear Algebra",
    "section": "Example 5: Subspaces of \\(M_{nn}\\)",
    "text": "Example 5: Subspaces of \\(M_{nn}\\)\nThe set of symmetric \\(n \\times n\\) matrices is a subspace of \\(M_{nn}\\).\n\nSum of two symmetric matrices is symmetric.\nScalar multiple of a symmetric matrix is symmetric.\n\nSimilarly, the sets of:\n\nUpper triangular matrices\nLower triangular matrices\nDiagonal matrices\n\nare all subspaces of \\(M_{nn}\\)."
  },
  {
    "objectID": "la-42.html#example-6-a-subset-of-m_nn-that-is-not-a-subspace",
    "href": "la-42.html#example-6-a-subset-of-m_nn-that-is-not-a-subspace",
    "title": "Linear Algebra",
    "section": "Example 6: A Subset of \\(M_{nn}\\) That Is Not a Subspace",
    "text": "Example 6: A Subset of \\(M_{nn}\\) That Is Not a Subspace\nThe set \\(W\\) of invertible \\(n \\times n\\) matrices is not a subspace of \\(M_{nn}\\).\nIt fails on two counts:\n\nNot closed under addition.\nNot closed under scalar multiplication.\n\nConsider these \\(2 \\times 2\\) matrices: \\[\nU = \\left[ \\begin{array}{ll}1 & 2 \\\\ 2 & 5 \\end{array} \\right] \\quad \\text{and} \\quad V = \\left[ \\begin{array}{ll} - 1 & 2 \\\\ -2 & 5 \\end{array} \\right]\n\\]\n\nAn invertible matrix must have a non-zero determinant. \\(U\\) is invertible: \\(\\det(U) = 1 \\cdot 5 - 2 \\cdot 2 = 1 \\neq 0\\). \\(V\\) is invertible: \\(\\det(V) = (-1) \\cdot 5 - 2 \\cdot (-2) = -5 + 4 = -1 \\neq 0\\)."
  },
  {
    "objectID": "la-42.html#example-6-invertible-matrices-continued",
    "href": "la-42.html#example-6-invertible-matrices-continued",
    "title": "Linear Algebra",
    "section": "Example 6: Invertible Matrices (Continued)",
    "text": "Example 6: Invertible Matrices (Continued)\nLet’s test closure for invertible matrices."
  },
  {
    "objectID": "la-42.html#subspaces-in-function-spaces-calculus-required",
    "href": "la-42.html#subspaces-in-function-spaces-calculus-required",
    "title": "Linear Algebra",
    "section": "Subspaces in Function Spaces (Calculus Required)",
    "text": "Subspaces in Function Spaces (Calculus Required)\nExample 7: The Subspace \\(C(-\\infty, \\infty)\\)\nThe set of continuous functions on \\((-\\infty, \\infty)\\) is a subspace of \\(F(-\\infty, \\infty)\\) (the set of all real-valued functions). This subspace is denoted by \\(C(-\\infty, \\infty)\\).\nExample 8: Functions with Continuous Derivatives\nThe set of functions with \\(m\\) continuous derivatives on \\((-\\infty, \\infty)\\) is a subspace of \\(F(-\\infty, \\infty)\\), denoted by \\(C^m(-\\infty, \\infty)\\). Similarly, functions with derivatives of all orders form \\(C^\\infty(-\\infty, \\infty)\\)."
  },
  {
    "objectID": "la-42.html#subspaces-in-function-spaces-continued",
    "href": "la-42.html#subspaces-in-function-spaces-continued",
    "title": "Linear Algebra",
    "section": "Subspaces in Function Spaces (Continued)",
    "text": "Subspaces in Function Spaces (Continued)\nExample 9: The Subspace of All Polynomials\nA polynomial can be expressed as \\(p(x) = a_0 + a_1x + \\dots + a_nx^n\\).\nThe sum of two polynomials is a polynomial.\nA constant times a polynomial is a polynomial.\nThus, the set \\(W\\) of all polynomials is a subspace of \\(F(-\\infty, \\infty)\\), denoted by \\(P_\\infty\\).\n\nNote: Some authors do not assign a degree to the constant 0, but in this text, all constants are polynomials of degree zero."
  },
  {
    "objectID": "la-42.html#example-10-the-subspace-of-polynomials-of-degree-leq-n",
    "href": "la-42.html#example-10-the-subspace-of-polynomials-of-degree-leq-n",
    "title": "Linear Algebra",
    "section": "Example 10: The Subspace of Polynomials of Degree \\(\\leq n\\)",
    "text": "Example 10: The Subspace of Polynomials of Degree \\(\\leq n\\)\nFor each nonnegative integer \\(n\\), the polynomials of degree \\(n\\) or less form a subspace of \\(F(-\\infty, \\infty)\\).\nThis space is denoted by \\(P_n\\).\n\n\n\n\n\n\nWarning\n\n\nThe set of polynomials with exact degree \\(n\\) is not a subspace. For example, the sum of two degree-2 polynomials can result in a degree-1 polynomial."
  },
  {
    "objectID": "la-42.html#example-10-polynomials-of-exact-degree-continued",
    "href": "la-42.html#example-10-polynomials-of-exact-degree-continued",
    "title": "Linear Algebra",
    "section": "Example 10: Polynomials of Exact Degree (Continued)",
    "text": "Example 10: Polynomials of Exact Degree (Continued)\nConsider these two degree-2 polynomials: \\(p_1(x) = 1 + 2x + 3x^2\\) \\(p_2(x) = 5 + 7x - 3x^2\\)\nLet’s find their sum and degree."
  },
  {
    "objectID": "la-42.html#the-hierarchy-of-function-spaces",
    "href": "la-42.html#the-hierarchy-of-function-spaces",
    "title": "Linear Algebra",
    "section": "The Hierarchy of Function Spaces",
    "text": "The Hierarchy of Function Spaces\nPolynomials are continuous functions and have continuous derivatives of all orders.\nThis creates a “nested” structure of function spaces.\n\n\n\n\n\ngraph LR\n    A[\"F(-&#8734;, &#8734;)\"] --&gt; B[\"C(-&#8734;, &#8734;)\"];\n    B --&gt; C[\"C^1(-&#8734;, &#8734;)\"];\n    C --&gt; D[\"C^m(-&#8734;, &#8734;)\"];\n    D --&gt; E[\"C^&#8734;(-&#8734;, &#8734;)\"];\n    E --&gt; F[\"P_&#8734;\"];\n    F --&gt; G[P_n];\n\n    style A fill:#e0f7fa,stroke:#00bcd4,stroke-width:2px;\n    style B fill:#fffde7,stroke:#ffeb3b,stroke-width:2px;\n    style C fill:#c8e6c9,stroke:#4caf50,stroke-width:1px;\n    style D fill:#a7ffeb,stroke:#00bfa5,stroke-width:1px;\n    style E fill:#bbdefb,stroke:#2196f3,stroke-width:1px;\n    style F fill:#e1bee7,stroke:#9c27b0,stroke-width:1px;\n    style G fill:#ffccbc,stroke:#ff5722,stroke-width:1px;\n\n    linkStyle 0 stroke-width:2px,stroke:blue;\n    linkStyle 1 stroke-width:2px,stroke:blue;\n    linkStyle 2 stroke-width:2px,stroke:blue;\n    linkStyle 3 stroke-width:2px,stroke:blue;\n    linkStyle 4 stroke-width:2px,stroke:blue;\n    linkStyle 5 stroke-width:2px,stroke:blue;\n\n\n\n\n\n\nFigure 4.2.5: \\(P_n \\subset P_\\infty \\subset C^\\infty(-\\infty, \\infty) \\subset C^m(-\\infty, \\infty) \\subset C^1(-\\infty, \\infty) \\subset C(-\\infty, \\infty) \\subset F(-\\infty, \\infty)\\)."
  },
  {
    "objectID": "la-42.html#building-subspaces-intersection",
    "href": "la-42.html#building-subspaces-intersection",
    "title": "Linear Algebra",
    "section": "Building Subspaces: Intersection",
    "text": "Building Subspaces: Intersection\n\n\n\n\n\n\nTHEOREM 4.2.2\n\n\nIf \\(W_1, W_2, \\ldots, W_r\\) are subspaces of a vector space \\(V\\), then the intersection of these subspaces is also a subspace of \\(V\\).\n\n\n\n\nProof Idea:\n\nNon-empty: Each subspace contains the zero vector \\(\\mathbf{0}\\), so their intersection contains \\(\\mathbf{0}\\) and is thus non-empty.\nClosure under Addition: Let \\(\\mathbf{u}, \\mathbf{v}\\) be in the intersection \\(W\\). This means \\(\\mathbf{u}, \\mathbf{v}\\) are in every \\(W_i\\). Since each \\(W_i\\) is a subspace, \\(\\mathbf{u} + \\mathbf{v}\\) is in every \\(W_i\\). Therefore, \\(\\mathbf{u} + \\mathbf{v}\\) is in their intersection \\(W\\).\nClosure under Scalar Multiplication: Let \\(k\\) be a scalar and \\(\\mathbf{u}\\) be in \\(W\\). This means \\(\\mathbf{u}\\) is in every \\(W_i\\). Since each \\(W_i\\) is a subspace, \\(k\\mathbf{u}\\) is in every \\(W_i\\). Therefore, \\(k\\mathbf{u}\\) is in their intersection \\(W\\). Since all conditions of Theorem 4.2.1 are met, the intersection \\(W\\) is a subspace."
  },
  {
    "objectID": "la-42.html#linear-combinations",
    "href": "la-42.html#linear-combinations",
    "title": "Linear Algebra",
    "section": "Linear Combinations",
    "text": "Linear Combinations\n\n\n\n\n\n\nDEFINITION 2\n\n\nIf \\(\\mathbf{w}\\) is a vector in a vector space \\(V\\), then \\(\\mathbf{w}\\) is said to be a linear combination of the vectors \\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_r\\) in \\(V\\) if \\(\\mathbf{w}\\) can be expressed in the form\n\\[\n\\mathbf{w} = k_1\\mathbf{v}_1 + k_2\\mathbf{v}_2 + \\dots + k_r\\mathbf{v}_r \\tag{2}\n\\]\nwhere \\(k_1, k_2, \\ldots, k_r\\) are scalars. These scalars are called the coefficients of the linear combination.\n\n\n\n\nIf \\(r=1\\), the linear combination is simply a scalar multiple of \\(\\mathbf{v}_1\\). Linear combinations are fundamental to understanding spanning sets and bases."
  },
  {
    "objectID": "la-42.html#building-subspaces-spanning-sets",
    "href": "la-42.html#building-subspaces-spanning-sets",
    "title": "Linear Algebra",
    "section": "Building Subspaces: Spanning Sets",
    "text": "Building Subspaces: Spanning Sets\n\n\n\n\n\n\nTHEOREM 4.2.3\n\n\nIf \\(S = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_r\\}\\) is a nonempty set of vectors in a vector space \\(V\\), then:\n\nThe set \\(W\\) of all possible linear combinations of the vectors in \\(S\\) is a subspace of \\(V\\).\nThe set \\(W\\) in part (a) is the “smallest” subspace of \\(V\\) that contains all of the vectors in \\(S\\).\n\n\n\n\n\nProof Idea for (a):\n\nNon-empty: \\(0\\mathbf{w}_1 + \\dots + 0\\mathbf{w}_r = \\mathbf{0}\\) is in \\(W\\).\nClosure under Addition: Let \\(\\mathbf{u}, \\mathbf{v} \\in W\\). Then \\(\\mathbf{u} = \\sum c_i \\mathbf{w}_i\\) and \\(\\mathbf{v} = \\sum k_i \\mathbf{w}_i\\). Their sum \\(\\mathbf{u} + \\mathbf{v} = \\sum (c_i + k_i) \\mathbf{w}_i\\), which is also a linear combination of vectors in \\(S\\), so \\(\\mathbf{u} + \\mathbf{v} \\in W\\).\nClosure under Scalar Multiplication: Let \\(c\\) be a scalar and \\(\\mathbf{u} \\in W\\). Then \\(c\\mathbf{u} = c(\\sum k_i \\mathbf{w}_i) = \\sum (ck_i) \\mathbf{w}_i\\), which is also a linear combination of vectors in \\(S\\), so \\(c\\mathbf{u} \\in W\\). Proof Idea for (b): Any subspace containing \\(S\\) must contain all linear combinations of vectors in \\(S\\) (by closure properties), hence it must contain \\(W\\)."
  },
  {
    "objectID": "la-42.html#definition-of-span",
    "href": "la-42.html#definition-of-span",
    "title": "Linear Algebra",
    "section": "Definition of Span",
    "text": "Definition of Span\n\n\n\n\n\n\nDEFINITION 3\n\n\nIf \\(S = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_r\\}\\) is a nonempty set of vectors in a vector space \\(V\\), then the subspace \\(W\\) of \\(V\\) that consists of all possible linear combinations of the vectors in \\(S\\) is called the subspace of \\(V\\) generated by \\(S\\).\nWe say that the vectors \\(\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_r\\) span \\(W\\). We denote this subspace as:\n\\[\nW = \\operatorname{span}\\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_r\\} \\quad \\text{or} \\quad W = \\operatorname{span}(S)\n\\]\n\n\n\n\nBy convention, \\(\\operatorname{span}(\\varnothing) = \\{\\mathbf{0}\\}\\)."
  },
  {
    "objectID": "la-42.html#example-11-the-standard-unit-vectors-span-rn",
    "href": "la-42.html#example-11-the-standard-unit-vectors-span-rn",
    "title": "Linear Algebra",
    "section": "Example 11: The Standard Unit Vectors Span \\(R^n\\)",
    "text": "Example 11: The Standard Unit Vectors Span \\(R^n\\)\nThe standard unit vectors in \\(R^n\\) are: \\[\n\\mathbf{e}_1 = (1,0,0,\\ldots,0), \\quad \\mathbf{e}_2 = (0,1,0,\\ldots,0), \\ldots, \\quad \\mathbf{e}_n = (0,0,0,\\ldots,1)\n\\] These vectors span \\(R^n\\) because every vector \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_n)\\) in \\(R^n\\) can be expressed as: \\[\n\\mathbf{v} = v_1\\mathbf{e}_1 + v_2\\mathbf{e}_2 + \\dots + v_n\\mathbf{e}_n\n\\] This is a linear combination of \\(\\mathbf{e}_1, \\mathbf{e}_2, \\ldots, \\mathbf{e}_n\\).\nFor example, in \\(R^3\\), \\(\\mathbf{i}=(1,0,0)\\), \\(\\mathbf{j}=(0,1,0)\\), \\(\\mathbf{k}=(0,0,1)\\) span \\(R^3\\)."
  },
  {
    "objectID": "la-42.html#example-11-standard-unit-vectors-continued",
    "href": "la-42.html#example-11-standard-unit-vectors-continued",
    "title": "Linear Algebra",
    "section": "Example 11: Standard Unit Vectors (Continued)",
    "text": "Example 11: Standard Unit Vectors (Continued)\nLet’s express a vector in \\(R^3\\) as a linear combination of standard unit vectors."
  },
  {
    "objectID": "la-42.html#example-12-a-geometric-view-of-spanning-in-r2",
    "href": "la-42.html#example-12-a-geometric-view-of-spanning-in-r2",
    "title": "Linear Algebra",
    "section": "Example 12: A Geometric View of Spanning in \\(R^2\\)",
    "text": "Example 12: A Geometric View of Spanning in \\(R^2\\)\nIf \\(\\mathbf{v}\\) is a nonzero vector in \\(R^2\\) or \\(R^3\\) with its initial point at the origin, then \\(\\operatorname{span}\\{\\mathbf{v}\\}\\) is the line through the origin determined by \\(\\mathbf{v}\\).\n\n\nThe tip of \\(k\\mathbf{v}\\) can fall at any point on the line by adjusting \\(k\\).\n\nviewof k = Inputs.range([-2, 2], {step: 0.1, label: \"Scalar k\", value: 1.5})\nv_vec = [2, 1]"
  },
  {
    "objectID": "la-42.html#example-13-a-geometric-view-of-spanning-in-r3",
    "href": "la-42.html#example-13-a-geometric-view-of-spanning-in-r3",
    "title": "Linear Algebra",
    "section": "Example 13: A Geometric View of Spanning in \\(R^3\\)",
    "text": "Example 13: A Geometric View of Spanning in \\(R^3\\)\nIf \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\) are two non-collinear vectors in \\(R^3\\), then \\(\\operatorname{span}\\{\\mathbf{v}_1, \\mathbf{v}_2\\}\\) is the plane through the origin determined by these two vectors.\n\n\nThe tip of \\(k_1\\mathbf{v}_1 + k_2\\mathbf{v}_2\\) can fall at any point on the plane by adjusting \\(k_1\\) and \\(k_2\\).\n\nviewof k1_slider_3d = Inputs.range([-1.5, 1.5], {step: 0.1, label: \"Scalar k1\", value: 0.5})\nviewof k2_slider_3d = Inputs.range([-1.5, 1.5], {step: 0.1, label: \"Scalar k2\", value: 1.0})\n\nv1_vec_3d = [1, 1, 0];\nv2_vec_3d = [0, 1, 1];"
  },
  {
    "objectID": "la-42.html#example-13-a-spanning-set-for-p_n",
    "href": "la-42.html#example-13-a-spanning-set-for-p_n",
    "title": "Linear Algebra",
    "section": "Example 13: A Spanning Set for \\(P_n\\)",
    "text": "Example 13: A Spanning Set for \\(P_n\\)\nThe polynomials \\(1, x, x^2, \\ldots, x^n\\) span the vector space \\(P_n\\). Any polynomial \\(\\mathbf{p}\\) in \\(P_n\\) can be written as: \\[\n\\mathbf{p} = a_0 + a_1x + \\dots + a_nx^n\n\\] This is a linear combination of \\(1, x, x^2, \\ldots, x^n\\).\nWe denote this by writing: \\[\nP_n = \\operatorname{span}\\{1,x,x^2,\\ldots,x^n\\}\n\\]"
  },
  {
    "objectID": "la-42.html#problem-types-linear-combinations-spanning",
    "href": "la-42.html#problem-types-linear-combinations-spanning",
    "title": "Linear Algebra",
    "section": "Problem Types: Linear Combinations & Spanning",
    "text": "Problem Types: Linear Combinations & Spanning\nWe often encounter two important problems:\n\nLinear Combination Check: Given a nonempty set \\(S\\) of vectors in \\(R^n\\) and a vector \\(\\mathbf{v}\\) in \\(R^n\\), determine whether \\(\\mathbf{v}\\) is a linear combination of the vectors in \\(S\\).\nSpanning Check: Given a nonempty set \\(S\\) of vectors in \\(R^n\\), determine whether the vectors span \\(R^n\\)."
  },
  {
    "objectID": "la-42.html#example-14-linear-combinations",
    "href": "la-42.html#example-14-linear-combinations",
    "title": "Linear Algebra",
    "section": "Example 14: Linear Combinations",
    "text": "Example 14: Linear Combinations\nConsider \\(\\mathbf{u} = (1,2, -1)\\) and \\(\\mathbf{v} = (6,4,2)\\) in \\(R^3\\).\n\n\nPart 1: Is \\(\\mathbf{w} = (9,2,7)\\) a linear combination of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\)?\nWe need scalars \\(k_1, k_2\\) such that \\(\\mathbf{w} = k_1\\mathbf{u} + k_2\\mathbf{v}\\).\nThis leads to the system: \\[\n\\begin{align*} k_1 + 6k_2 &= 9 \\\\ 2k_1 + 4k_2 &= 2 \\\\ -k_1 + 2k_2 &= 7 \\end{align*}\n\\]\nSolving this system yields \\(k_1 = -3, k_2 = 2\\).\nSo, \\(\\mathbf{w} = -3\\mathbf{u} + 2\\mathbf{v}\\).\n\nPart 2: Is \\(\\mathbf{w}' = (4,-1,8)\\) a linear combination of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\)?\nWe need scalars \\(k_1, k_2\\) such that \\(\\mathbf{w}' = k_1\\mathbf{u} + k_2\\mathbf{v}\\).\nThis leads to the system: \\[\n\\begin{align*} k_1 + 6k_2 &= 4 \\\\ 2k_1 + 4k_2 &= -1 \\\\ -k_1 + 2k_2 &= 8 \\end{align*}\n\\]\nThis system is inconsistent (no solution).\nThus, \\(\\mathbf{w}'\\) is not a linear combination of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\)."
  },
  {
    "objectID": "la-42.html#example-14-linear-combinations-pyodide-demonstration",
    "href": "la-42.html#example-14-linear-combinations-pyodide-demonstration",
    "title": "Linear Algebra",
    "section": "Example 14: Linear Combinations (Pyodide Demonstration)",
    "text": "Example 14: Linear Combinations (Pyodide Demonstration)\nLet’s use Pyodide to verify the consistency of the systems."
  },
  {
    "objectID": "la-42.html#example-15-testing-for-spanning",
    "href": "la-42.html#example-15-testing-for-spanning",
    "title": "Linear Algebra",
    "section": "Example 15: Testing for Spanning",
    "text": "Example 15: Testing for Spanning\nDetermine whether the vectors \\(\\mathbf{v}_1 = (1,1,2)\\), \\(\\mathbf{v}_2 = (1,0,1)\\), and \\(\\mathbf{v}_3 = (2,1,3)\\) span the vector space \\(R^3\\).\nTo span \\(R^3\\), any arbitrary vector \\(\\mathbf{b} = (b_1,b_2,b_3)\\) in \\(R^3\\) must be expressible as a linear combination: \\[\n\\mathbf{b} = k_1\\mathbf{v}_1 + k_2\\mathbf{v}_2 + k_3\\mathbf{v}_3\n\\] This forms a system of linear equations: \\[\n\\begin{align*} k_1 + k_2 + 2k_3 &= b_1 \\\\ k_1 + \\quad 0k_2 + k_3 &= b_2 \\\\ 2k_1 + k_2 + 3k_3 &= b_3 \\end{align*}\n\\] This system is consistent for all \\(b_1, b_2, b_3\\) if and only if the coefficient matrix \\(A\\) has a non-zero determinant. \\[\nA = \\left[ \\begin{array}{lll}1 & 1 & 2 \\\\ 1 & 0 & 1 \\\\ 2 & 1 & 3 \\end{array} \\right]\n\\]"
  },
  {
    "objectID": "la-42.html#example-15-testing-for-spanning-pyodide-demonstration",
    "href": "la-42.html#example-15-testing-for-spanning-pyodide-demonstration",
    "title": "Linear Algebra",
    "section": "Example 15: Testing for Spanning (Pyodide Demonstration)",
    "text": "Example 15: Testing for Spanning (Pyodide Demonstration)\nLet’s calculate the determinant of matrix \\(A\\) using Pyodide."
  },
  {
    "objectID": "la-42.html#solution-spaces-of-homogeneous-systems",
    "href": "la-42.html#solution-spaces-of-homogeneous-systems",
    "title": "Linear Algebra",
    "section": "Solution Spaces of Homogeneous Systems",
    "text": "Solution Spaces of Homogeneous Systems\nThe solutions of a homogeneous linear system \\(A\\mathbf{x} = \\mathbf{0}\\) of \\(m\\) equations in \\(n\\) unknowns can be viewed as vectors in \\(R^n\\).\n\n\n\n\n\n\nTHEOREM 4.2.4\n\n\nThe solution set of a homogeneous linear system \\(A\\mathbf{x} = \\mathbf{0}\\) of \\(m\\) equations in \\(n\\) unknowns is a subspace of \\(R^n\\).\n\n\n\n\nProof Idea:\n\nNon-empty: The trivial solution \\(\\mathbf{x} = \\mathbf{0}\\) always satisfies \\(A\\mathbf{0} = \\mathbf{0}\\), so the solution set is non-empty.\nClosure under Addition: Let \\(\\mathbf{x}_1, \\mathbf{x}_2\\) be solutions. Then \\(A\\mathbf{x}_1 = \\mathbf{0}\\) and \\(A\\mathbf{x}_2 = \\mathbf{0}\\). So, \\(A(\\mathbf{x}_1 + \\mathbf{x}_2) = A\\mathbf{x}_1 + A\\mathbf{x}_2 = \\mathbf{0} + \\mathbf{0} = \\mathbf{0}\\). Thus, \\(\\mathbf{x}_1 + \\mathbf{x}_2\\) is a solution.\nClosure under Scalar Multiplication: Let \\(k\\) be a scalar and \\(\\mathbf{x}_1\\) be a solution. Then \\(A\\mathbf{x}_1 = \\mathbf{0}\\). So, \\(A(k\\mathbf{x}_1) = k(A\\mathbf{x}_1) = k\\mathbf{0} = \\mathbf{0}\\). Thus, \\(k\\mathbf{x}_1\\) is a solution. All conditions are met, so the solution set is a subspace, often called the solution space."
  },
  {
    "objectID": "la-42.html#example-16-solution-spaces-of-homogeneous-systems",
    "href": "la-42.html#example-16-solution-spaces-of-homogeneous-systems",
    "title": "Linear Algebra",
    "section": "Example 16: Solution Spaces of Homogeneous Systems",
    "text": "Example 16: Solution Spaces of Homogeneous Systems\nDetermine the solution space for each system. Each system is \\(A\\mathbf{x} = \\mathbf{0}\\).\n\n\\(\\left[\\begin{array}{lll}{1}&{- 2}&{3}\\\\ {2}&{- 4}&{6}\\\\ {3}&{- 6}&{9}\\end{array}\\right]{\\left[\\begin{array}{l}{x}\\\\ {y}\\\\ {z}\\end{array}\\right]}={\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {0}\\end{array}\\right]}\\)\n\nSolution: \\(x = 2s - 3t, y=s, z=t\\). This describes a plane through the origin (\\(x - 2y + 3z = 0\\)).\n\n\\(\\left[\\begin{array}{lll}{1}&{- 2}&{3}\\\\ {- 3}&{7}&{- 8}\\\\ {- 2}&{4}&{- 6}\\end{array}\\right]{\\left[\\begin{array}{l}{x}\\\\ {y}\\\\ {z}\\end{array}\\right]}={\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {0}\\end{array}\\right]}\\)\n\nSolution: \\(x = -5t, y = -t, z = t\\). This describes a line through the origin parallel to \\(\\mathbf{v} = (-5, -1, 1)\\)."
  },
  {
    "objectID": "la-42.html#example-16-solution-spaces-continued",
    "href": "la-42.html#example-16-solution-spaces-continued",
    "title": "Linear Algebra",
    "section": "Example 16: Solution Spaces (Continued)",
    "text": "Example 16: Solution Spaces (Continued)\n\n\\(\\left[\\begin{array}{lll}{1}&{- 2}&{3}\\\\ {- 3}&{7}&{8}\\\\ {4}&{1}&{2}\\end{array}\\right]{\\left[\\begin{array}{l}{x}\\\\ {y}\\\\ {z}\\end{array}\\right]}={\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {0}\\end{array}\\right]}\\)\n\nSolution: The only solution is \\(x=0, y=0, z=0\\). The solution space is the single point \\(\\{\\mathbf{0}\\}\\).\n\n\\(\\left[\\begin{array}{lll}{0}&{0}&{0}\\\\ {0}&{0}&{0}\\\\ {0}&{0}&{0}\\end{array}\\right]{\\left[\\begin{array}{l}{x}\\\\ {y}\\\\ {z}\\end{array}\\right]}={\\left[\\begin{array}{l}{0}\\\\ {0}\\\\ {0}\\end{array}\\right]}\\)\n\nSolution: This system is satisfied by all real values of \\(x,y,z\\). The solution space is all of \\(R^3\\).\n\n\n\n\n\n\nWarning\n\n\nThe solution set of a nonhomogeneous system (\\(A\\mathbf{x} = \\mathbf{b}\\) where \\(\\mathbf{b} \\neq \\mathbf{0}\\)) is never a subspace of \\(R^n\\). It is not closed under addition or scalar multiplication."
  },
  {
    "objectID": "la-42.html#the-linear-transformation-viewpoint",
    "href": "la-42.html#the-linear-transformation-viewpoint",
    "title": "Linear Algebra",
    "section": "The Linear Transformation Viewpoint",
    "text": "The Linear Transformation Viewpoint\nWe can view Theorem 4.2.4 in terms of matrix transformations.\nLet \\(T_A: R^n \\to R^m\\) be the matrix transformation defined by multiplication by the coefficient matrix \\(A\\).\nThe solution space of \\(A\\mathbf{x} = \\mathbf{0}\\) is the set of vectors in \\(R^n\\) that \\(T_A\\) maps into the zero vector in \\(R^m\\).\nThis set is sometimes called the kernel of the transformation.\n\n\n\n\n\n\nTHEOREM 4.2.5\n\n\nIf \\(A\\) is an \\(m \\times n\\) matrix, then the kernel of the matrix transformation \\(T_A: R^n \\to R^m\\) is a subspace of \\(R^n\\)."
  },
  {
    "objectID": "la-42.html#a-concluding-observation-non-unique-spanning-sets",
    "href": "la-42.html#a-concluding-observation-non-unique-spanning-sets",
    "title": "Linear Algebra",
    "section": "A Concluding Observation: Non-Unique Spanning Sets",
    "text": "A Concluding Observation: Non-Unique Spanning Sets\nIt is important to recognize that spanning sets are not unique.\nFor example, any nonzero vector on a line will span that line.\nAny two non-collinear vectors in a plane will span that plane.\n\n\n\n\n\n\nTHEOREM 4.2.6\n\n\nIf \\(S = \\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_r\\}\\) and \\(S' = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_k\\}\\) are nonempty sets of vectors in a vector space \\(V\\), then\n\\[\n\\operatorname{span}\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_r\\} = \\operatorname{span}\\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_k\\}\n\\]\nif and only if each vector in \\(S\\) is a linear combination of those in \\(S'\\), and each vector in \\(S'\\) is a linear combination of those in \\(S\\)."
  },
  {
    "objectID": "la_43.html#introduction-to-linear-independence",
    "href": "la_43.html#introduction-to-linear-independence",
    "title": "Linear Algebra",
    "section": "Introduction to Linear Independence",
    "text": "Introduction to Linear Independence\nIn applications, knowing if vectors are interrelated is crucial. A “superfluous” vector can complicate analysis.\nConsider \\(\\mathbf{i} = (1,0)\\), \\(\\mathbf{j} = (0,1)\\) in \\(R^2\\).\nAny vector \\((x,y)\\) is uniquely \\(x\\mathbf{i} + y\\mathbf{j}\\).\nExample: \\((3,2) = 3\\mathbf{i} + 2\\mathbf{j}\\).\n\n\n\n\n\n\nNote\n\n\nThis is the only way to express \\((3,2)\\) using \\(\\mathbf{i}\\) and \\(\\mathbf{j}\\).\n\n\n\nNow, add \\(\\mathbf{w} = \\left(\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}\\right)\\) (unit vector at \\(45^\\circ\\)).\n\\((3,2)\\) can be expressed in infinitely many ways:\n\n\\(3\\mathbf{i} + 2\\mathbf{j} + 0\\mathbf{w}\\)\n\\(2\\mathbf{i} + \\mathbf{j} + \\sqrt{2}\\mathbf{w}\\)\n\\(4\\mathbf{i} + 3\\mathbf{j} - \\sqrt{2}\\mathbf{w}\\)\n\nThe vector \\(\\mathbf{w}\\) is unnecessary because: \\(\\mathbf{w} = \\frac{1}{\\sqrt{2}} \\mathbf{i} + \\frac{1}{\\sqrt{2}} \\mathbf{j}\\)\n\nStart by introducing the core idea of linear independence: whether a set of vectors contains redundant information. Use the simple 2D coordinate system as an analogy. Emphasize how adding a vector that can be formed by others creates ambiguity in representation, which is generally undesirable in systems. The “superfluous” concept is key here."
  },
  {
    "objectID": "la_43.html#visualizing-superfluous-vectors",
    "href": "la_43.html#visualizing-superfluous-vectors",
    "title": "Linear Algebra",
    "section": "Visualizing Superfluous Vectors",
    "text": "Visualizing Superfluous Vectors\nThe vector \\(\\mathbf{w}\\) can be seen as a linear combination of \\(\\mathbf{i}\\) and \\(\\mathbf{j}\\).\n\n\n\n\n\ngraph TD\n    subgraph R^2 Plane\n        O(Origin) --- i_vec --&gt; A(1,0)\n        O --- j_vec --&gt; B(0,1)\n        O --- w_vec --&gt; C(0.707,0.707)\n    end\n    i_vec[Vector i]\n    j_vec[Vector j]\n    w_vec[Vector w]\n  \n    style i_vec fill:#f9f,stroke:#333,stroke-width:2px;\n    style j_vec fill:#bbf,stroke:#333,stroke-width:2px;\n    style w_vec fill:#ffc,stroke:#333,stroke-width:2px;\n\n    subgraph Linear Dependence\n        A -- \"i\" --&gt; Combine\n        B -- \"j\" --&gt; Combine\n        Combine(\"w = (1/√2)i + (1/√2)j\") --&gt; Dependent[Linearly Dependent]\n    end\n\n\n\n\n\n\n\nThis Mermaid diagram visually reinforces the concept introduced on the previous slide. Show how vector w can be constructed from i and j, illustrating its dependency."
  },
  {
    "objectID": "la_43.html#defining-linear-independence",
    "href": "la_43.html#defining-linear-independence",
    "title": "Linear Algebra",
    "section": "Defining Linear Independence",
    "text": "Defining Linear Independence\nDEFINITION 1:\nA set \\(S = \\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots , \\mathbf{v}_{r}\\}\\) of two or more vectors in a vector space \\(V\\) is linearly independent if no vector in \\(S\\) can be expressed as a linear combination of the others.\nA set that is not linearly independent is linearly dependent.\nFor a single vector set \\(\\{\\mathbf{v}\\}\\), it is linearly independent if and only if \\(\\mathbf{v} \\neq \\mathbf{0}\\).\n\n\n\n\n\n\nImportant\n\n\nKey Theorem 4.3.1: A nonempty set \\(S = \\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots , \\mathbf{v}_{r}\\}\\) in a vector space \\(V\\) is linearly independent if and only if the only coefficients satisfying the vector equation: \\[ k_{1} \\mathbf{v}_{1} + k_{2} \\mathbf{v}_{2} + \\dots + k_{r} \\mathbf{v}_{r} = \\mathbf{0} \\] are \\(k_{1} = 0, k_{2} = 0, \\ldots , k_{r} = 0\\).\n\n\n\n\nThis slide formally defines linear independence and dependence. The most crucial part is Theorem 4.3.1, which provides a practical method for checking independence. Emphasize that finding any non-zero coefficients means the set is dependent. This theorem transforms the problem into solving a homogeneous linear system."
  },
  {
    "objectID": "la_43.html#example-standard-unit-vectors-in-rn",
    "href": "la_43.html#example-standard-unit-vectors-in-rn",
    "title": "Linear Algebra",
    "section": "Example: Standard Unit Vectors in \\(R^n\\)",
    "text": "Example: Standard Unit Vectors in \\(R^n\\)\nThe standard unit vectors \\(\\mathbf{e}_{1}, \\mathbf{e}_{2}, \\ldots , \\mathbf{e}_{n}\\) are linearly independent.\nFor \\(R^3\\): \\(\\mathbf{i} = (1,0,0)\\), \\(\\mathbf{j} = (0,1,0)\\), \\(\\mathbf{k} = (0,0,1)\\)\nConsider the equation: \\(k_{1}\\mathbf{i} + k_{2}\\mathbf{j} + k_{3}\\mathbf{k} = \\mathbf{0}\\)\nIn component form: \\(k_{1}(1,0,0) + k_{2}(0,1,0) + k_{3}(0,0,1) = (0,0,0)\\) \\((k_{1}, k_{2}, k_{3}) = (0, 0, 0)\\)\nThis directly implies \\(k_{1} = 0, k_{2} = 0, k_{3} = 0\\). Thus, they are linearly independent.\nLet’s verify this in Python.\n\n\n\n\n\n\n\nThis example applies Theorem 4.3.1 to a familiar set of vectors. The Python code reinforces the idea that if the determinant of the matrix formed by the vectors is non-zero, the homogeneous system has only the trivial solution, confirming linear independence. This is a common method in ECE, especially when dealing with system matrices."
  },
  {
    "objectID": "la_43.html#example-linear-independence-in-r3",
    "href": "la_43.html#example-linear-independence-in-r3",
    "title": "Linear Algebra",
    "section": "Example: Linear Independence in \\(R^3\\)",
    "text": "Example: Linear Independence in \\(R^3\\)\nDetermine if \\(\\mathbf{v}_{1} = (1, -2, 3)\\), \\(\\mathbf{v}_{2} = (5, 6, -1)\\), \\(\\mathbf{v}_{3} = (3, 2, 1)\\) are linearly independent.\nWe set up the equation: \\(k_{1} \\mathbf{v}_{1} + k_{2} \\mathbf{v}_{2} + k_{3} \\mathbf{v}_{3} = \\mathbf{0}\\)\nThis translates to the homogeneous system: \\[\n\\begin{array}{r}\nk_{1} + 5k_{2} + 3k_{3} = 0 \\\\\n-2k_{1} + 6k_{2} + 2k_{3} = 0 \\\\\n3k_{1} - k_{2} + k_{3} = 0\n\\end{array}\n\\]\nThe coefficient matrix is: \\[\nA = \\left[ \\begin{array}{ccc}\n1 & 5 & 3 \\\\\n-2 & 6 & 2 \\\\\n3 & -1 & 1\n\\end{array} \\right]\n\\] We need to check if this system has non-trivial solutions.\n\n\n\n\n\n\nTip\n\n\nFor a square matrix, if \\(\\operatorname{det}(A) = 0\\), then \\(A\\mathbf{x} = \\mathbf{0}\\) has non-trivial solutions, implying linear dependence. If \\(\\operatorname{det}(A) \\neq 0\\), only the trivial solution exists, implying linear independence.\n\n\n\n\nThis slide sets up the problem for a more complex R^3 example. It explicitly shows how the vector equation translates into a system of linear equations. The callout highlights the determinant test, a powerful tool for square matrices."
  },
  {
    "objectID": "la_43.html#example-r3-cont.---python-solution",
    "href": "la_43.html#example-r3-cont.---python-solution",
    "title": "Linear Algebra",
    "section": "Example: \\(R^3\\) (Cont.) - Python Solution",
    "text": "Example: \\(R^3\\) (Cont.) - Python Solution\nLet’s use Python to solve the system and find the determinant.\n\n\n\n\n\n\n\nThis interactive slide directly solves the system from the previous slide using numpy. It calculates the determinant and then, because it’s zero, demonstrates a non-trivial solution, confirming linear dependence. This is a practical application of computational tools in linear algebra."
  },
  {
    "objectID": "la_43.html#example-linear-independence-in-r4",
    "href": "la_43.html#example-linear-independence-in-r4",
    "title": "Linear Algebra",
    "section": "Example: Linear Independence in \\(R^4\\)",
    "text": "Example: Linear Independence in \\(R^4\\)\nDetermine if \\(\\mathbf{v}_{1} = (1,2,2, -1)\\), \\(\\mathbf{v}_{2} = (4,9,9, -4)\\), \\(\\mathbf{v}_{3} = (5,8,9, -5)\\) are linearly independent in \\(R^4\\).\nWe need to solve \\(k_{1}\\mathbf{v}_{1} + k_{2}\\mathbf{v}_{2} + k_{3}\\mathbf{v}_{3} = \\mathbf{0}\\). This leads to the system: \\[\n\\begin{array}{r}\nk_{1} + 4k_{2} + 5k_{3} = 0 \\\\\n2k_{1} + 9k_{2} + 8k_{3} = 0 \\\\\n2k_{1} + 9k_{2} + 9k_{3} = 0 \\\\\n-k_{1} - 4k_{2} - 5k_{3} = 0\n\\end{array}\n\\]\n\n\n\n\n\n\n\nThis example demonstrates a case of linear independence for a non-square system. It introduces the concept of checking the rank of the matrix. If the rank equals the number of columns (variables), it implies only the trivial solution exists, thus linear independence. This is a common technique in numerical linear algebra."
  },
  {
    "objectID": "la_43.html#linear-independence-of-polynomials",
    "href": "la_43.html#linear-independence-of-polynomials",
    "title": "Linear Algebra",
    "section": "Linear Independence of Polynomials",
    "text": "Linear Independence of Polynomials\nThe set of polynomials \\(\\{1, x, x^2, \\ldots, x^n\\}\\) forms a linearly independent set in \\(P_n\\).\nConsider the equation: \\(a_{0}(1) + a_{1}(x) + a_{2}(x^{2}) + \\dots +a_{n}(x^{n}) = \\mathbf{0}\\)\nThis means \\(a_{0} + a_{1}x + a_{2}x^{2} + \\dots +a_{n}x^{n} = 0\\) for all \\(x \\in (-\\infty, \\infty)\\).\n\n\n\n\n\n\nTip\n\n\nA non-zero polynomial of degree \\(n\\) has at most \\(n\\) distinct roots. If a polynomial is zero for all \\(x\\), it must be the zero polynomial, meaning all its coefficients are zero. Therefore, \\(a_{0} = a_{1} = a_{2} = \\dots = a_{n} = 0\\).\n\n\n\nThis confirms the linear independence of the set \\(\\{1, x, x^2, \\ldots, x^n\\}\\).\n\nThis slide covers the linear independence of the standard basis for polynomial spaces. The key insight here is the property of polynomials regarding their roots. If a polynomial equation holds for all x, then all its coefficients must be zero. This is fundamental for understanding polynomial spaces."
  },
  {
    "objectID": "la_43.html#example-linear-independence-of-specific-polynomials",
    "href": "la_43.html#example-linear-independence-of-specific-polynomials",
    "title": "Linear Algebra",
    "section": "Example: Linear Independence of Specific Polynomials",
    "text": "Example: Linear Independence of Specific Polynomials\nDetermine if \\(\\mathbf{p}_{1} = 1 - x\\), \\(\\mathbf{p}_{2} = 5 + 3x - 2x^{2}\\), \\(\\mathbf{p}_{3} = 1 + 3x - x^{2}\\) are linearly independent in \\(P_2\\).\nSet up the equation:\n\\(k_{1}\\mathbf{p}_{1} + k_{2}\\mathbf{p}_{2} + k_{3}\\mathbf{p}_{3} = \\mathbf{0}\\)\nSubstitute the polynomials:\n\\(k_{1}(1 - x) + k_{2}(5 + 3x - 2x^{2}) + k_{3}(1 + 3x - x^{2}) = 0\\)\nGroup terms by powers of \\(x\\):\n\\((k_{1} + 5k_{2} + k_{3}) + (-k_{1} + 3k_{2} + 3k_{3})x + (-2k_{2} - k_{3})x^{2} = 0\\)\nFor this to be true for all \\(x\\), each coefficient must be zero: \\[\n\\begin{array}{c}\nk_{1}+5k_{2}+ k_{3}=0 \\\\\n-k_{1}+3k_{2}+3k_{3}=0 \\\\\n-2k_{2}- k_{3}=0\n\\end{array}\n\\]\nLet’s solve this system using Python.\n\n\n\n\n\n\n\nThis example shows how to reduce the problem of polynomial linear independence to solving a system of linear equations. The interactive Python code confirms the determinant is zero, thus leading to linear dependence. This is a practical method for checking independence in function spaces using matrix methods."
  },
  {
    "objectID": "la_43.html#special-cases-and-geometric-interpretation",
    "href": "la_43.html#special-cases-and-geometric-interpretation",
    "title": "Linear Algebra",
    "section": "Special Cases and Geometric Interpretation",
    "text": "Special Cases and Geometric Interpretation\nTHEOREM 4.3.2:\n\nA finite set that contains \\(\\mathbf{0}\\) is linearly dependent.\nA set with exactly one vector \\(\\{\\mathbf{v}\\}\\) is linearly independent iff \\(\\mathbf{v} \\neq \\mathbf{0}\\).\nA set with exactly two vectors \\(\\{\\mathbf{v}_1, \\mathbf{v}_2\\}\\) is linearly independent iff neither vector is a scalar multiple of the other.\n\n\n\n\n\n\n\nNote\n\n\nProof of (a):\nFor any vectors \\(\\mathbf{v}_{1},\\ldots ,\\mathbf{v}_{r}\\), the set \\(S = \\{\\mathbf{v}_{1},\\ldots ,\\mathbf{v}_{r},\\mathbf{0}\\}\\) is linearly dependent since \\(0\\mathbf{v}_{1} + \\dots +0\\mathbf{v}_{r} + 1(\\mathbf{0}) = \\mathbf{0}\\) uses non-zero coefficient \\(1\\) for \\(\\mathbf{0}\\).\n\n\n\nGeometric Interpretation:\n\nTwo vectors in \\(R^2\\) or \\(R^3\\) are linearly independent if and only if they do not lie on the same line (when originating from the origin). Otherwise, one is a scalar multiple of the other.\n\n\n\nLinearly dependent\n\n\n\nLinearly independent\n\n\nThis slide consolidates simple cases and offers valuable geometric intuition. For ECE students, visualizing these concepts can greatly aid understanding. Emphasize that linear dependence implies collinearity for two vectors."
  },
  {
    "objectID": "la_43.html#geometric-interpretation-cont.",
    "href": "la_43.html#geometric-interpretation-cont.",
    "title": "Linear Algebra",
    "section": "Geometric Interpretation (Cont.)",
    "text": "Geometric Interpretation (Cont.)\n\nThree vectors in \\(R^3\\) are linearly independent if and only if they do not lie in the same plane (when originating from the origin). Otherwise, at least one would be a linear combination of the other two.\n\n\n\nLinearly dependent\n\n\n\nLinearly independent\n\n\nContinuing the geometric interpretation, this slide illustrates linear dependence for three vectors in \\(R^3\\). If they lie in the same plane, one can be expressed as a combination of the others. This is a crucial visual for understanding the concept in 3D space."
  },
  {
    "objectID": "la_43.html#maximum-number-of-linearly-independent-vectors",
    "href": "la_43.html#maximum-number-of-linearly-independent-vectors",
    "title": "Linear Algebra",
    "section": "Maximum Number of Linearly Independent Vectors",
    "text": "Maximum Number of Linearly Independent Vectors\nTHEOREM 4.3.3: Let \\(S = \\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots , \\mathbf{v}_{r}\\}\\) be a set of vectors in \\(R^{n}\\). If \\(r &gt; n\\), then \\(S\\) is linearly dependent.\n\n\n\n\n\n\nImportant\n\n\nThis means in \\(R^n\\), you can have at most \\(n\\) linearly independent vectors.\n\nIn \\(R^2\\), at most 2 linearly independent vectors.\nIn \\(R^3\\), at most 3 linearly independent vectors.\n\n\n\n\nProof Sketch:\n\nForm the homogeneous system \\(k_{1}\\mathbf{v}_{1} + \\dots + k_{r}\\mathbf{v}_{r} = \\mathbf{0}\\).\nThis system has \\(n\\) equations and \\(r\\) unknowns (\\(k_1, \\ldots, k_r\\)).\nIf \\(r &gt; n\\) (more unknowns than equations), the system must have non-trivial solutions.\nTherefore, \\(S\\) is linearly dependent.\n\n\nTheorem 4.3.3 is fundamental for understanding the dimensionality of vector spaces. Explain that having more vectors than the dimension of the space guarantees redundancy. Connect this to real-world ECE scenarios like sensor networks: if you have more sensors than the degrees of freedom in your system, some sensor readings must be linearly dependent on others. This implies redundancy and potentially unnecessary complexity."
  },
  {
    "objectID": "la_43.html#linear-independence-of-functions-the-wronskian",
    "href": "la_43.html#linear-independence-of-functions-the-wronskian",
    "title": "Linear Algebra",
    "section": "Linear Independence of Functions: The Wronskian",
    "text": "Linear Independence of Functions: The Wronskian\nDetermining linear independence for functions can be complex.\nDEFINITION 2: The Wronskian\nIf \\(\\mathbf{f}_{1}(x),\\ldots ,\\mathbf{f}_{n}(x)\\) are functions that are \\(n-1\\) times differentiable on an interval, then the determinant: \\[\nW(x) = \\begin{vmatrix}\nf_{1}(x) & f_{2}(x) & \\dots & f_{n}(x) \\\\\nf_{1}^{\\prime}(x) & f_{2}^{\\prime}(x) & \\dots & f_{n}^{\\prime}(x) \\\\\n\\vdots & \\vdots & & \\vdots \\\\\nf_{1}^{(n - 1)}(x) & f_{2}^{(n - 1)}(x) & \\dots & f_{n}^{(n - 1)}(x)\n\\end{vmatrix}\n\\] is called the Wronskian of \\(f_{1},f_{2},\\ldots ,f_{n}\\).\nTHEOREM 4.3.4:\nIf \\(\\mathbf{f}_{1},\\ldots ,\\mathbf{f}_{n}\\) have \\(n-1\\) continuous derivatives on \\((-\\infty, \\infty)\\), and if \\(W(x)\\) is not identically zero on \\((-\\infty, \\infty)\\), then these functions form a linearly independent set.\n\n\n\n\n\n\nWarning\n\n\nWARNING: If \\(W(x)\\) is identically zero, no conclusion can be reached about linear independence. The functions may be linearly independent or dependent.\n\n\n\n\nIntroduce the Wronskian as a tool, particularly relevant in differential equations and control theory (ECE applications). Explain its definition and the key theorem. Crucially, highlight the warning about the converse not being true, as this is a common point of confusion."
  },
  {
    "objectID": "la_43.html#example-wronskian-for-x-and-sin-x",
    "href": "la_43.html#example-wronskian-for-x-and-sin-x",
    "title": "Linear Algebra",
    "section": "Example: Wronskian for \\(x\\) and \\(\\sin x\\)",
    "text": "Example: Wronskian for \\(x\\) and \\(\\sin x\\)\nShow that \\(\\mathbf{f}_{1} = x\\) and \\(\\mathbf{f}_{2} = \\sin x\\) are linearly independent using the Wronskian.\nHere, \\(n=2\\). We need \\(n-1=1\\) derivative.\n\\(f_1(x) = x\\), \\(f_1'(x) = 1\\)\n\\(f_2(x) = \\sin x\\), \\(f_2'(x) = \\cos x\\)\nThe Wronskian is: \\[\nW(x) = \\begin{vmatrix}\nx & \\sin x \\\\\n1 & \\cos x\n\\end{vmatrix} = x \\cos x - \\sin x\n\\]\nThis function is not identically zero. For example, \\(W\\left(\\frac{\\pi}{2}\\right) = \\frac{\\pi}{2} \\cos \\left(\\frac{\\pi}{2}\\right) - \\sin \\left(\\frac{\\pi}{2}\\right) = \\frac{\\pi}{2}(0) - 1 = -1\\).\nSince \\(W(x)\\) is not identically zero, \\(\\mathbf{f}_{1}\\) and \\(\\mathbf{f}_{2}\\) are linearly independent.\n\nThis slide provides a concrete example of calculating and interpreting the Wronskian. The calculation is straightforward, and the conclusion is drawn based on Theorem 4.3.4."
  },
  {
    "objectID": "la_43.html#interactive-wronskian-plot-x-cos-x---sin-x",
    "href": "la_43.html#interactive-wronskian-plot-x-cos-x---sin-x",
    "title": "Linear Algebra",
    "section": "Interactive Wronskian Plot: \\(x \\cos x - \\sin x\\)",
    "text": "Interactive Wronskian Plot: \\(x \\cos x - \\sin x\\)\nLet’s visualize the Wronskian \\(W(x) = x \\cos x - \\sin x\\).\n\n\n\n\n\n\n\nThis interactive plot visually demonstrates that the Wronskian \\(W(x) = x \\cos x - \\sin x\\) is not identically zero. Students can see that the function has many non-zero values, reinforcing the concept of linear independence. This is a great way to engage ECE students who appreciate visual data."
  },
  {
    "objectID": "la_43.html#example-wronskian-for-1-ex-e2x",
    "href": "la_43.html#example-wronskian-for-1-ex-e2x",
    "title": "Linear Algebra",
    "section": "Example: Wronskian for \\(1, e^x, e^{2x}\\)",
    "text": "Example: Wronskian for \\(1, e^x, e^{2x}\\)\nShow that \\(\\mathbf{f}_{1} = 1\\), \\(\\mathbf{f}_{2} = e^{x}\\), and \\(\\mathbf{f}_{3} = e^{2x}\\) are linearly independent.\nHere, \\(n=3\\). We need \\(n-1=2\\) derivatives.\n\\(f_1(x) = 1\\), \\(f_1'(x) = 0\\), \\(f_1''(x) = 0\\)\n\\(f_2(x) = e^x\\), \\(f_2'(x) = e^x\\), \\(f_2''(x) = e^x\\)\n\\(f_3(x) = e^{2x}\\), \\(f_3'(x) = 2e^{2x}\\), \\(f_3''(x) = 4e^{2x}\\)\nThe Wronskian is: \\[\nW(x) = \\begin{vmatrix}\n1 & e^{x} & e^{2x} \\\\\n0 & e^{x} & 2e^{2x} \\\\\n0 & e^{x} & 4e^{2x}\n\\end{vmatrix}\n\\] Expand along the first column:\n\\(W(x) = 1 \\cdot \\begin{vmatrix} e^{x} & 2e^{2x} \\\\ e^{x} & 4e^{2x} \\end{vmatrix} - 0 + 0\\)\n\\(W(x) = e^{x}(4e^{2x}) - 2e^{2x}(e^{x}) = 4e^{3x} - 2e^{3x} = 2e^{3x}\\)\nSince \\(W(x) = 2e^{3x}\\) is never zero for any \\(x\\), these functions are linearly independent.\n\nAnother Wronskian example, this time with exponential functions. The calculation is more involved but still manageable. Emphasize that \\(2e^{3x}\\) is always positive and thus never zero, which is the key to concluding linear independence."
  },
  {
    "objectID": "la_43.html#interactive-wronskian-plot-2e3x",
    "href": "la_43.html#interactive-wronskian-plot-2e3x",
    "title": "Linear Algebra",
    "section": "Interactive Wronskian Plot: \\(2e^{3x}\\)",
    "text": "Interactive Wronskian Plot: \\(2e^{3x}\\)\nLet’s visualize the Wronskian \\(W(x) = 2e^{3x}\\).\n\n\n\n\n\n\n\nThis interactive plot for \\(W(x) = 2e^{3x}\\) clearly shows that the Wronskian is always positive and never crosses the x-axis, thus confirming it is not identically zero. This further strengthens the understanding of the Wronskian criterion for linear independence."
  },
  {
    "objectID": "la_43.html#proof-of-theorem-4.3.1-outline",
    "href": "la_43.html#proof-of-theorem-4.3.1-outline",
    "title": "Linear Algebra",
    "section": "Proof of Theorem 4.3.1 (Outline)",
    "text": "Proof of Theorem 4.3.1 (Outline)\nTheorem 4.3.1: \\(S = \\{\\mathbf{v}_{1}, \\ldots , \\mathbf{v}_{r}\\}\\) is linearly independent \\(\\iff\\) the only solution to \\(k_{1} \\mathbf{v}_{1} + \\dots + k_{r} \\mathbf{v}_{r} = \\mathbf{0}\\) is \\(k_1 = \\dots = k_r = 0\\).\nPart 1: \\((\\implies)\\) If \\(S\\) is linearly independent, then \\(k_1 = \\dots = k_r = 0\\).\n\nAssume \\(S\\) is linearly independent.\nSuppose there is a solution with at least one \\(k_i \\neq 0\\).\nWithout loss of generality, let \\(k_1 \\neq 0\\).\nThen \\(\\mathbf{v}_{1} = \\left(-\\frac{k_{2}}{k_{1}}\\right)\\mathbf{v}_{2} + \\dots + \\left(-\\frac{k_{r}}{k_{1}}\\right)\\mathbf{v}_{r}\\).\nThis expresses \\(\\mathbf{v}_1\\) as a linear combination of others, contradicting linear independence.\nTherefore, all \\(k_i\\) must be zero."
  },
  {
    "objectID": "la_43.html#proof-of-theorem-4.3.1-outline-1",
    "href": "la_43.html#proof-of-theorem-4.3.1-outline-1",
    "title": "Linear Algebra",
    "section": "Proof of Theorem 4.3.1 (Outline)",
    "text": "Proof of Theorem 4.3.1 (Outline)\nPart 2: \\((\\impliedby)\\) If the only solution is \\(k_1 = \\dots = k_r = 0\\), then \\(S\\) is linearly independent.\n\nAssume the only solution to \\(k_{1} \\mathbf{v}_{1} + \\dots + k_{r} \\mathbf{v}_{r} = \\mathbf{0}\\) is \\(k_1 = \\dots = k_r = 0\\).\nSuppose \\(S\\) is linearly dependent.\nBy definition, some vector (say \\(\\mathbf{v}_1\\)) can be written as \\(\\mathbf{v}_{1} = c_{2}\\mathbf{v}_{2} + \\dots + c_{r}\\mathbf{v}_{r}\\).\nRearranging gives \\(\\mathbf{v}_{1} + (-c_{2})\\mathbf{v}_{2} + \\dots + (-c_{r})\\mathbf{v}_{r} = \\mathbf{0}\\).\nThis is a non-trivial solution (coefficient of \\(\\mathbf{v}_1\\) is \\(1 \\neq 0\\)).\nThis contradicts our initial assumption.\nTherefore, \\(S\\) must be linearly independent.\n\n\nThis slide provides a concise outline of the proof for Theorem 4.3.1. While the full proof might not be required for every ECE student, understanding the logic behind it is important. Emphasize the proof by contradiction approach for both directions."
  },
  {
    "objectID": "la-42.html#example-14-linear-combinations-1",
    "href": "la-42.html#example-14-linear-combinations-1",
    "title": "Linear Algebra",
    "section": "Example 14: Linear Combinations",
    "text": "Example 14: Linear Combinations\nLet’s verify the consistency of the systems."
  },
  {
    "objectID": "la-42.html#example-15-testing-for-spanning-1",
    "href": "la-42.html#example-15-testing-for-spanning-1",
    "title": "Linear Algebra",
    "section": "Example 15: Testing for Spanning",
    "text": "Example 15: Testing for Spanning\nLet’s calculate the determinant of matrix \\(A\\)."
  },
  {
    "objectID": "index.html#week-7",
    "href": "index.html#week-7",
    "title": "Linear Algebra",
    "section": "Week 7",
    "text": "Week 7\n\nLinear Algebra 4.1: Real Vector Spaces\nLinear Algebra 4.2: Subspaces"
  },
  {
    "objectID": "la-43.html#introduction-to-linear-independence",
    "href": "la-43.html#introduction-to-linear-independence",
    "title": "Linear Algebra",
    "section": "Introduction to Linear Independence",
    "text": "Introduction to Linear Independence\nIn applications, knowing if vectors are interrelated is crucial. A “superfluous” vector can complicate analysis.\nConsider \\(\\mathbf{i} = (1,0)\\), \\(\\mathbf{j} = (0,1)\\) in \\(R^2\\).\nAny vector \\((x,y)\\) is uniquely \\(x\\mathbf{i} + y\\mathbf{j}\\).\nExample: \\((3,2) = 3\\mathbf{i} + 2\\mathbf{j}\\).\n\n\n\n\n\n\nNote\n\n\nThis is the only way to express \\((3,2)\\) using \\(\\mathbf{i}\\) and \\(\\mathbf{j}\\).\n\n\n\nNow, add \\(\\mathbf{w} = \\left(\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}\\right)\\) (unit vector at \\(45^\\circ\\)).\n\\((3,2)\\) can be expressed in infinitely many ways:\n\n\\(3\\mathbf{i} + 2\\mathbf{j} + 0\\mathbf{w}\\)\n\\(2\\mathbf{i} + \\mathbf{j} + \\sqrt{2}\\mathbf{w}\\)\n\\(4\\mathbf{i} + 3\\mathbf{j} - \\sqrt{2}\\mathbf{w}\\)\n\nThe vector \\(\\mathbf{w}\\) is unnecessary because: \\(\\mathbf{w} = \\frac{1}{\\sqrt{2}} \\mathbf{i} + \\frac{1}{\\sqrt{2}} \\mathbf{j}\\)\n\nStart by introducing the core idea of linear independence: whether a set of vectors contains redundant information. Use the simple 2D coordinate system as an analogy. Emphasize how adding a vector that can be formed by others creates ambiguity in representation, which is generally undesirable in systems. The “superfluous” concept is key here."
  },
  {
    "objectID": "la-43.html#defining-linear-independence",
    "href": "la-43.html#defining-linear-independence",
    "title": "Linear Algebra",
    "section": "Defining Linear Independence",
    "text": "Defining Linear Independence\nDEFINITION 1:\nA set \\(S = \\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots , \\mathbf{v}_{r}\\}\\) of two or more vectors in a vector space \\(V\\) is linearly independent if no vector in \\(S\\) can be expressed as a linear combination of the others.\nA set that is not linearly independent is linearly dependent.\nFor a single vector set \\(\\{\\mathbf{v}\\}\\), it is linearly independent if and only if \\(\\mathbf{v} \\neq \\mathbf{0}\\).\n\n\n\n\n\n\nImportant\n\n\nKey Theorem 4.3.1: A nonempty set \\(S = \\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots , \\mathbf{v}_{r}\\}\\) in a vector space \\(V\\) is linearly independent if and only if the only coefficients satisfying the vector equation: \\[ k_{1} \\mathbf{v}_{1} + k_{2} \\mathbf{v}_{2} + \\dots + k_{r} \\mathbf{v}_{r} = \\mathbf{0} \\] are \\(k_{1} = 0, k_{2} = 0, \\ldots , k_{r} = 0\\).\n\n\n\n\nThis slide formally defines linear independence and dependence. The most crucial part is Theorem 4.3.1, which provides a practical method for checking independence. Emphasize that finding any non-zero coefficients means the set is dependent. This theorem transforms the problem into solving a homogeneous linear system."
  },
  {
    "objectID": "la-43.html#example-standard-unit-vectors-in-rn",
    "href": "la-43.html#example-standard-unit-vectors-in-rn",
    "title": "Linear Algebra",
    "section": "Example: Standard Unit Vectors in \\(R^n\\)",
    "text": "Example: Standard Unit Vectors in \\(R^n\\)\nThe standard unit vectors \\(\\mathbf{e}_{1}, \\mathbf{e}_{2}, \\ldots , \\mathbf{e}_{n}\\) are linearly independent.\nFor \\(R^3\\): \\(\\mathbf{i} = (1,0,0)\\), \\(\\mathbf{j} = (0,1,0)\\), \\(\\mathbf{k} = (0,0,1)\\)\nConsider the equation: \\(k_{1}\\mathbf{i} + k_{2}\\mathbf{j} + k_{3}\\mathbf{k} = \\mathbf{0}\\)\nIn component form: \\(k_{1}(1,0,0) + k_{2}(0,1,0) + k_{3}(0,0,1) = (0,0,0)\\) \\((k_{1}, k_{2}, k_{3}) = (0, 0, 0)\\)\nThis directly implies \\(k_{1} = 0, k_{2} = 0, k_{3} = 0\\). Thus, they are linearly independent.\nLet’s verify this in Python.\n\n\n\n\n\n\n\nThis example applies Theorem 4.3.1 to a familiar set of vectors. The Python code reinforces the idea that if the determinant of the matrix formed by the vectors is non-zero, the homogeneous system has only the trivial solution, confirming linear independence. This is a common method in ECE, especially when dealing with system matrices."
  },
  {
    "objectID": "la-43.html#example-linear-independence-in-r3",
    "href": "la-43.html#example-linear-independence-in-r3",
    "title": "Linear Algebra",
    "section": "Example: Linear Independence in \\(R^3\\)",
    "text": "Example: Linear Independence in \\(R^3\\)\nDetermine if \\(\\mathbf{v}_{1} = (1, -2, 3)\\), \\(\\mathbf{v}_{2} = (5, 6, -1)\\), \\(\\mathbf{v}_{3} = (3, 2, 1)\\) are linearly independent.\nWe set up the equation: \\(k_{1} \\mathbf{v}_{1} + k_{2} \\mathbf{v}_{2} + k_{3} \\mathbf{v}_{3} = \\mathbf{0}\\)\nThis translates to the homogeneous system: \\[\n\\begin{array}{r}\nk_{1} + 5k_{2} + 3k_{3} = 0 \\\\\n-2k_{1} + 6k_{2} + 2k_{3} = 0 \\\\\n3k_{1} - k_{2} + k_{3} = 0\n\\end{array}\n\\]\nThe coefficient matrix is: \\[\nA = \\left[ \\begin{array}{ccc}\n1 & 5 & 3 \\\\\n-2 & 6 & 2 \\\\\n3 & -1 & 1\n\\end{array} \\right]\n\\] We need to check if this system has non-trivial solutions.\n\n\n\n\n\n\nTip\n\n\nFor a square matrix, if \\(\\operatorname{det}(A) = 0\\), then \\(A\\mathbf{x} = \\mathbf{0}\\) has non-trivial solutions, implying linear dependence. If \\(\\operatorname{det}(A) \\neq 0\\), only the trivial solution exists, implying linear independence.\n\n\n\n\nThis slide sets up the problem for a more complex R^3 example. It explicitly shows how the vector equation translates into a system of linear equations. The callout highlights the determinant test, a powerful tool for square matrices."
  },
  {
    "objectID": "la-43.html#example-r3-cont.---python-solution",
    "href": "la-43.html#example-r3-cont.---python-solution",
    "title": "Linear Algebra",
    "section": "Example: \\(R^3\\) (Cont.) - Python Solution",
    "text": "Example: \\(R^3\\) (Cont.) - Python Solution\nLet’s use Python to solve the system and find the determinant.\n\n\n\n\n\n\n\nThis interactive slide directly solves the system from the previous slide using numpy. It calculates the determinant and then, because it’s zero, demonstrates a non-trivial solution, confirming linear dependence. This is a practical application of computational tools in linear algebra."
  },
  {
    "objectID": "la-43.html#example-linear-independence-in-r4",
    "href": "la-43.html#example-linear-independence-in-r4",
    "title": "Linear Algebra",
    "section": "Example: Linear Independence in \\(R^4\\)",
    "text": "Example: Linear Independence in \\(R^4\\)\nDetermine if \\(\\mathbf{v}_{1} = (1,2,2, -1)\\), \\(\\mathbf{v}_{2} = (4,9,9, -4)\\), \\(\\mathbf{v}_{3} = (5,8,9, -5)\\) are linearly independent in \\(R^4\\).\nWe need to solve \\(k_{1}\\mathbf{v}_{1} + k_{2}\\mathbf{v}_{2} + k_{3}\\mathbf{v}_{3} = \\mathbf{0}\\). This leads to the system: \\[\n\\begin{array}{r}\nk_{1} + 4k_{2} + 5k_{3} = 0 \\\\\n2k_{1} + 9k_{2} + 8k_{3} = 0 \\\\\n2k_{1} + 9k_{2} + 9k_{3} = 0 \\\\\n-k_{1} - 4k_{2} - 5k_{3} = 0\n\\end{array}\n\\]\n\n\n\n\n\n\n\nThis example demonstrates a case of linear independence for a non-square system. It introduces the concept of checking the rank of the matrix. If the rank equals the number of columns (variables), it implies only the trivial solution exists, thus linear independence. This is a common technique in numerical linear algebra."
  },
  {
    "objectID": "la-43.html#linear-independence-of-polynomials",
    "href": "la-43.html#linear-independence-of-polynomials",
    "title": "Linear Algebra",
    "section": "Linear Independence of Polynomials",
    "text": "Linear Independence of Polynomials\nThe set of polynomials \\(\\{1, x, x^2, \\ldots, x^n\\}\\) forms a linearly independent set in \\(P_n\\).\nConsider the equation: \\(a_{0}(1) + a_{1}(x) + a_{2}(x^{2}) + \\dots +a_{n}(x^{n}) = \\mathbf{0}\\)\nThis means \\(a_{0} + a_{1}x + a_{2}x^{2} + \\dots +a_{n}x^{n} = 0\\) for all \\(x \\in (-\\infty, \\infty)\\).\n\n\n\n\n\n\nTip\n\n\nA non-zero polynomial of degree \\(n\\) has at most \\(n\\) distinct roots. If a polynomial is zero for all \\(x\\), it must be the zero polynomial, meaning all its coefficients are zero. Therefore, \\(a_{0} = a_{1} = a_{2} = \\dots = a_{n} = 0\\).\n\n\n\nThis confirms the linear independence of the set \\(\\{1, x, x^2, \\ldots, x^n\\}\\).\n\nThis slide covers the linear independence of the standard basis for polynomial spaces. The key insight here is the property of polynomials regarding their roots. If a polynomial equation holds for all x, then all its coefficients must be zero. This is fundamental for understanding polynomial spaces."
  },
  {
    "objectID": "la-43.html#example-linear-independence-of-specific-polynomials",
    "href": "la-43.html#example-linear-independence-of-specific-polynomials",
    "title": "Linear Algebra",
    "section": "Example: Linear Independence of Specific Polynomials",
    "text": "Example: Linear Independence of Specific Polynomials\nDetermine if \\(\\mathbf{p}_{1} = 1 - x\\), \\(\\mathbf{p}_{2} = 5 + 3x - 2x^{2}\\), \\(\\mathbf{p}_{3} = 1 + 3x - x^{2}\\) are linearly independent in \\(P_2\\).\nSet up the equation:\n\\(k_{1}\\mathbf{p}_{1} + k_{2}\\mathbf{p}_{2} + k_{3}\\mathbf{p}_{3} = \\mathbf{0}\\)\nSubstitute the polynomials:\n\\(k_{1}(1 - x) + k_{2}(5 + 3x - 2x^{2}) + k_{3}(1 + 3x - x^{2}) = 0\\)\nGroup terms by powers of \\(x\\):\n\\((k_{1} + 5k_{2} + k_{3}) + (-k_{1} + 3k_{2} + 3k_{3})x + (-2k_{2} - k_{3})x^{2} = 0\\)\nFor this to be true for all \\(x\\), each coefficient must be zero: \\[\n\\begin{array}{c}\nk_{1}+5k_{2}+ k_{3}=0 \\\\\n-k_{1}+3k_{2}+3k_{3}=0 \\\\\n-2k_{2}- k_{3}=0\n\\end{array}\n\\]\nLet’s solve this system using Python.\n\n\n\n\n\n\n\nThis example shows how to reduce the problem of polynomial linear independence to solving a system of linear equations. The interactive Python code confirms the determinant is zero, thus leading to linear dependence. This is a practical method for checking independence in function spaces using matrix methods."
  },
  {
    "objectID": "la-43.html#special-cases-and-geometric-interpretation",
    "href": "la-43.html#special-cases-and-geometric-interpretation",
    "title": "Linear Algebra",
    "section": "Special Cases and Geometric Interpretation",
    "text": "Special Cases and Geometric Interpretation\nTHEOREM 4.3.2:\n\nA finite set that contains \\(\\mathbf{0}\\) is linearly dependent.\nA set with exactly one vector \\(\\{\\mathbf{v}\\}\\) is linearly independent iff \\(\\mathbf{v} \\neq \\mathbf{0}\\).\nA set with exactly two vectors \\(\\{\\mathbf{v}_1, \\mathbf{v}_2\\}\\) is linearly independent iff neither vector is a scalar multiple of the other.\n\n\n\n\n\n\n\nNote\n\n\nProof of (a):\nFor any vectors \\(\\mathbf{v}_{1},\\ldots ,\\mathbf{v}_{r}\\), the set \\(S = \\{\\mathbf{v}_{1},\\ldots ,\\mathbf{v}_{r},\\mathbf{0}\\}\\) is linearly dependent since \\(0\\mathbf{v}_{1} + \\dots +0\\mathbf{v}_{r} + 1(\\mathbf{0}) = \\mathbf{0}\\) uses non-zero coefficient \\(1\\) for \\(\\mathbf{0}\\)."
  },
  {
    "objectID": "la-43.html#geometric-interpretation-cont.",
    "href": "la-43.html#geometric-interpretation-cont.",
    "title": "Linear Algebra",
    "section": "Geometric Interpretation (Cont.)",
    "text": "Geometric Interpretation (Cont.)\n\nThree vectors in \\(R^3\\) are linearly independent if and only if they do not lie in the same plane (when originating from the origin). Otherwise, at least one would be a linear combination of the other two.\n\n\n\nLinearly dependent\n\n\n\nLinearly independent\n\n\nContinuing the geometric interpretation, this slide illustrates linear dependence for three vectors in \\(R^3\\). If they lie in the same plane, one can be expressed as a combination of the others. This is a crucial visual for understanding the concept in 3D space."
  },
  {
    "objectID": "la-43.html#maximum-number-of-linearly-independent-vectors",
    "href": "la-43.html#maximum-number-of-linearly-independent-vectors",
    "title": "Linear Algebra",
    "section": "Maximum Number of Linearly Independent Vectors",
    "text": "Maximum Number of Linearly Independent Vectors\nTHEOREM 4.3.3: Let \\(S = \\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots , \\mathbf{v}_{r}\\}\\) be a set of vectors in \\(R^{n}\\). If \\(r &gt; n\\), then \\(S\\) is linearly dependent.\n\n\n\n\n\n\nImportant\n\n\nThis means in \\(R^n\\), you can have at most \\(n\\) linearly independent vectors.\n\nIn \\(R^2\\), at most 2 linearly independent vectors.\nIn \\(R^3\\), at most 3 linearly independent vectors.\n\n\n\n\nProof Sketch:\n\nForm the homogeneous system \\(k_{1}\\mathbf{v}_{1} + \\dots + k_{r}\\mathbf{v}_{r} = \\mathbf{0}\\).\nThis system has \\(n\\) equations and \\(r\\) unknowns (\\(k_1, \\ldots, k_r\\)).\nIf \\(r &gt; n\\) (more unknowns than equations), the system must have non-trivial solutions.\nTherefore, \\(S\\) is linearly dependent.\n\n\nTheorem 4.3.3 is fundamental for understanding the dimensionality of vector spaces. Explain that having more vectors than the dimension of the space guarantees redundancy. Connect this to real-world ECE scenarios like sensor networks: if you have more sensors than the degrees of freedom in your system, some sensor readings must be linearly dependent on others. This implies redundancy and potentially unnecessary complexity."
  },
  {
    "objectID": "la-43.html#linear-independence-of-functions-the-wronskian",
    "href": "la-43.html#linear-independence-of-functions-the-wronskian",
    "title": "Linear Algebra",
    "section": "Linear Independence of Functions: The Wronskian",
    "text": "Linear Independence of Functions: The Wronskian\nDetermining linear independence for functions can be complex.\nDEFINITION 2: The Wronskian\nIf \\(\\mathbf{f}_{1}(x),\\ldots ,\\mathbf{f}_{n}(x)\\) are functions that are \\(n-1\\) times differentiable on an interval, then the determinant: \\[\nW(x) = \\begin{vmatrix}\nf_{1}(x) & f_{2}(x) & \\dots & f_{n}(x) \\\\\nf_{1}^{\\prime}(x) & f_{2}^{\\prime}(x) & \\dots & f_{n}^{\\prime}(x) \\\\\n\\vdots & \\vdots & & \\vdots \\\\\nf_{1}^{(n - 1)}(x) & f_{2}^{(n - 1)}(x) & \\dots & f_{n}^{(n - 1)}(x)\n\\end{vmatrix}\n\\] is called the Wronskian of \\(f_{1},f_{2},\\ldots ,f_{n}\\).\nTHEOREM 4.3.4:\nIf \\(\\mathbf{f}_{1},\\ldots ,\\mathbf{f}_{n}\\) have \\(n-1\\) continuous derivatives on \\((-\\infty, \\infty)\\), and if \\(W(x)\\) is not identically zero on \\((-\\infty, \\infty)\\), then these functions form a linearly independent set.\n\n\n\n\n\n\nWarning\n\n\nWARNING: If \\(W(x)\\) is identically zero, no conclusion can be reached about linear independence. The functions may be linearly independent or dependent.\n\n\n\n\nIntroduce the Wronskian as a tool, particularly relevant in differential equations and control theory (ECE applications). Explain its definition and the key theorem. Crucially, highlight the warning about the converse not being true, as this is a common point of confusion."
  },
  {
    "objectID": "la-43.html#example-wronskian-for-x-and-sin-x",
    "href": "la-43.html#example-wronskian-for-x-and-sin-x",
    "title": "Linear Algebra",
    "section": "Example: Wronskian for \\(x\\) and \\(\\sin x\\)",
    "text": "Example: Wronskian for \\(x\\) and \\(\\sin x\\)\nShow that \\(\\mathbf{f}_{1} = x\\) and \\(\\mathbf{f}_{2} = \\sin x\\) are linearly independent using the Wronskian.\nHere, \\(n=2\\). We need \\(n-1=1\\) derivative.\n\\(f_1(x) = x\\), \\(f_1'(x) = 1\\)\n\\(f_2(x) = \\sin x\\), \\(f_2'(x) = \\cos x\\)\nThe Wronskian is: \\[\nW(x) = \\begin{vmatrix}\nx & \\sin x \\\\\n1 & \\cos x\n\\end{vmatrix} = x \\cos x - \\sin x\n\\]\nThis function is not identically zero. For example, \\(W\\left(\\frac{\\pi}{2}\\right) = \\frac{\\pi}{2} \\cos \\left(\\frac{\\pi}{2}\\right) - \\sin \\left(\\frac{\\pi}{2}\\right) = \\frac{\\pi}{2}(0) - 1 = -1\\).\nSince \\(W(x)\\) is not identically zero, \\(\\mathbf{f}_{1}\\) and \\(\\mathbf{f}_{2}\\) are linearly independent.\n\nThis slide provides a concrete example of calculating and interpreting the Wronskian. The calculation is straightforward, and the conclusion is drawn based on Theorem 4.3.4."
  },
  {
    "objectID": "la-43.html#interactive-wronskian-plot-x-cos-x---sin-x",
    "href": "la-43.html#interactive-wronskian-plot-x-cos-x---sin-x",
    "title": "Linear Algebra",
    "section": "Interactive Wronskian Plot: \\(x \\cos x - \\sin x\\)",
    "text": "Interactive Wronskian Plot: \\(x \\cos x - \\sin x\\)\nLet’s visualize the Wronskian \\(W(x) = x \\cos x - \\sin x\\).\n\n\n\n\n\n\n\nThis interactive plot visually demonstrates that the Wronskian \\(W(x) = x \\cos x - \\sin x\\) is not identically zero. Students can see that the function has many non-zero values, reinforcing the concept of linear independence. This is a great way to engage ECE students who appreciate visual data."
  },
  {
    "objectID": "la-43.html#example-wronskian-for-1-ex-e2x",
    "href": "la-43.html#example-wronskian-for-1-ex-e2x",
    "title": "Linear Algebra",
    "section": "Example: Wronskian for \\(1, e^x, e^{2x}\\)",
    "text": "Example: Wronskian for \\(1, e^x, e^{2x}\\)\nShow that \\(\\mathbf{f}_{1} = 1\\), \\(\\mathbf{f}_{2} = e^{x}\\), and \\(\\mathbf{f}_{3} = e^{2x}\\) are linearly independent.\nHere, \\(n=3\\). We need \\(n-1=2\\) derivatives.\n\\(f_1(x) = 1\\), \\(f_1'(x) = 0\\), \\(f_1''(x) = 0\\)\n\\(f_2(x) = e^x\\), \\(f_2'(x) = e^x\\), \\(f_2''(x) = e^x\\)\n\\(f_3(x) = e^{2x}\\), \\(f_3'(x) = 2e^{2x}\\), \\(f_3''(x) = 4e^{2x}\\)\nThe Wronskian is: \\[\nW(x) = \\begin{vmatrix}\n1 & e^{x} & e^{2x} \\\\\n0 & e^{x} & 2e^{2x} \\\\\n0 & e^{x} & 4e^{2x}\n\\end{vmatrix}\n\\] Expand along the first column:\n\\(W(x) = 1 \\cdot \\begin{vmatrix} e^{x} & 2e^{2x} \\\\ e^{x} & 4e^{2x} \\end{vmatrix} - 0 + 0\\)\n\\(W(x) = e^{x}(4e^{2x}) - 2e^{2x}(e^{x}) = 4e^{3x} - 2e^{3x} = 2e^{3x}\\)\nSince \\(W(x) = 2e^{3x}\\) is never zero for any \\(x\\), these functions are linearly independent.\n\nAnother Wronskian example, this time with exponential functions. The calculation is more involved but still manageable. Emphasize that \\(2e^{3x}\\) is always positive and thus never zero, which is the key to concluding linear independence."
  },
  {
    "objectID": "la-43.html#interactive-wronskian-plot-2e3x",
    "href": "la-43.html#interactive-wronskian-plot-2e3x",
    "title": "Linear Algebra",
    "section": "Interactive Wronskian Plot: \\(2e^{3x}\\)",
    "text": "Interactive Wronskian Plot: \\(2e^{3x}\\)\nLet’s visualize the Wronskian \\(W(x) = 2e^{3x}\\).\n\n\n\n\n\n\n\nThis interactive plot for \\(W(x) = 2e^{3x}\\) clearly shows that the Wronskian is always positive and never crosses the x-axis, thus confirming it is not identically zero. This further strengthens the understanding of the Wronskian criterion for linear independence."
  },
  {
    "objectID": "la-43.html#proof-of-theorem-4.3.1-outline",
    "href": "la-43.html#proof-of-theorem-4.3.1-outline",
    "title": "Linear Algebra",
    "section": "Proof of Theorem 4.3.1 (Outline)",
    "text": "Proof of Theorem 4.3.1 (Outline)\nTheorem 4.3.1: \\(S = \\{\\mathbf{v}_{1}, \\ldots , \\mathbf{v}_{r}\\}\\) is linearly independent \\(\\iff\\) the only solution to \\(k_{1} \\mathbf{v}_{1} + \\dots + k_{r} \\mathbf{v}_{r} = \\mathbf{0}\\) is \\(k_1 = \\dots = k_r = 0\\).\nPart 1: \\((\\implies)\\) If \\(S\\) is linearly independent, then \\(k_1 = \\dots = k_r = 0\\).\n\nAssume \\(S\\) is linearly independent.\nSuppose there is a solution with at least one \\(k_i \\neq 0\\).\nWithout loss of generality, let \\(k_1 \\neq 0\\).\nThen \\(\\mathbf{v}_{1} = \\left(-\\frac{k_{2}}{k_{1}}\\right)\\mathbf{v}_{2} + \\dots + \\left(-\\frac{k_{r}}{k_{1}}\\right)\\mathbf{v}_{r}\\).\nThis expresses \\(\\mathbf{v}_1\\) as a linear combination of others, contradicting linear independence.\nTherefore, all \\(k_i\\) must be zero."
  },
  {
    "objectID": "la-43.html#proof-of-theorem-4.3.1-outline-1",
    "href": "la-43.html#proof-of-theorem-4.3.1-outline-1",
    "title": "Linear Algebra",
    "section": "Proof of Theorem 4.3.1 (Outline)",
    "text": "Proof of Theorem 4.3.1 (Outline)\nPart 2: \\((\\impliedby)\\) If the only solution is \\(k_1 = \\dots = k_r = 0\\), then \\(S\\) is linearly independent.\n\nAssume the only solution to \\(k_{1} \\mathbf{v}_{1} + \\dots + k_{r} \\mathbf{v}_{r} = \\mathbf{0}\\) is \\(k_1 = \\dots = k_r = 0\\).\nSuppose \\(S\\) is linearly dependent.\nBy definition, some vector (say \\(\\mathbf{v}_1\\)) can be written as \\(\\mathbf{v}_{1} = c_{2}\\mathbf{v}_{2} + \\dots + c_{r}\\mathbf{v}_{r}\\).\nRearranging gives \\(\\mathbf{v}_{1} + (-c_{2})\\mathbf{v}_{2} + \\dots + (-c_{r})\\mathbf{v}_{r} = \\mathbf{0}\\).\nThis is a non-trivial solution (coefficient of \\(\\mathbf{v}_1\\) is \\(1 \\neq 0\\)).\nThis contradicts our initial assumption.\nTherefore, \\(S\\) must be linearly independent.\n\n\nThis slide provides a concise outline of the proof for Theorem 4.3.1. While the full proof might not be required for every ECE student, understanding the logic behind it is important. Emphasize the proof by contradiction approach for both directions."
  },
  {
    "objectID": "la-43.html#special-cases-and-geometric-interpretation-cont.",
    "href": "la-43.html#special-cases-and-geometric-interpretation-cont.",
    "title": "Linear Algebra",
    "section": "Special Cases and Geometric Interpretation (Cont.)",
    "text": "Special Cases and Geometric Interpretation (Cont.)\nGeometric Interpretation:\n\nTwo vectors in \\(R^2\\) or \\(R^3\\) are linearly independent if and only if they do not lie on the same line (when originating from the origin).\nOtherwise, one is a scalar multiple of the other.\n\n\n\n\n\nLinearly dependent\n\n\n\n\nLinearly independent\n\n\n\nThis slide consolidates simple cases and offers valuable geometric intuition. For ECE students, visualizing these concepts can greatly aid understanding. Emphasize that linear dependence implies collinearity for two vectors."
  },
  {
    "objectID": "la-44.html#the-essence-of-coordinate-systems",
    "href": "la-44.html#the-essence-of-coordinate-systems",
    "title": "Linear Algebra",
    "section": "The Essence of Coordinate Systems",
    "text": "The Essence of Coordinate Systems\nCoordinate systems allow us to describe points and vectors numerically. While rectangular systems are common, they are not the only option.\n\n\nRectangular Coordinates\n\nFamiliar \\(x, y, z\\) axes.\nMutually perpendicular axes.\nCommon in basic geometry.\n\n\nNon-Rectangular Coordinates\n\nAxes not necessarily perpendicular.\nStill define unique positions.\nMore flexible for various applications.\n\n\n\nWe’re all familiar with rectangular coordinate systems from calculus and physics. They use mutually perpendicular axes, like the x, y, and z axes, to assign unique coordinates to points. However, in linear algebra, we generalize this idea. We can define coordinate systems where the axes are not necessarily perpendicular, as shown in the figures you might have seen in textbooks. This flexibility is crucial for understanding more abstract vector spaces."
  },
  {
    "objectID": "la-44.html#coordinate-systems-from-geometry-to-vectors",
    "href": "la-44.html#coordinate-systems-from-geometry-to-vectors",
    "title": "Linear Algebra",
    "section": "Coordinate Systems: From Geometry to Vectors",
    "text": "Coordinate Systems: From Geometry to Vectors\nIn linear algebra, coordinate systems are often defined using vectors. These vectors define the “directions” and “units” for each axis.\n\n\n\nFigure 4.4.1: Rectangular coordinates.\n\n\nFigure 4.4.2: Non-rectangular coordinates.\n\n\nHere, you see the visual representation of both rectangular and non-rectangular coordinate systems. Notice how in the non-rectangular case, the axes are still straight lines passing through the origin, but they are not at 90-degree angles to each other. This is perfectly valid and useful in many scenarios."
  },
  {
    "objectID": "la-44.html#defining-coordinates-with-vectors",
    "href": "la-44.html#defining-coordinates-with-vectors",
    "title": "Linear Algebra",
    "section": "Defining Coordinates with Vectors",
    "text": "Defining Coordinates with Vectors\nWe use vectors, often unit vectors, to identify positive directions. A point \\(P\\) is then represented by scalar coefficients.\n\\[\n\\overrightarrow{OP} = a\\mathbf{u}_1 + b\\mathbf{u}_2 \\quad \\mathrm{and} \\quad \\overrightarrow{OP} = a\\mathbf{u}_1 + b\\mathbf{u}_2 + c\\mathbf{u}_3\n\\]\n\nFigure 4.4.3: Coordinate systems defined by unit vectors.\n\nIn linear algebra, we formalize this by using vectors, \\(\\mathbf{u}_1, \\mathbf{u}_2\\), and so on, to define the coordinate axes. Any point or vector \\(\\overrightarrow{OP}\\) can then be expressed as a linear combination of these defining vectors. The coefficients, \\(a, b, c\\), become the coordinates of the point relative to this vector-defined system. Initially, these vectors might be unit vectors like \\(\\mathbf{i}, \\mathbf{j}, \\mathbf{k}\\)."
  },
  {
    "objectID": "la-44.html#basis-vectors-generalizing-coordinate-axes",
    "href": "la-44.html#basis-vectors-generalizing-coordinate-axes",
    "title": "Linear Algebra",
    "section": "Basis Vectors: Generalizing Coordinate Axes",
    "text": "Basis Vectors: Generalizing Coordinate Axes\nTo allow for more generality, we relax the unit vector requirement. The defining vectors, now called basis vectors, only need to be linearly independent.\n\nDirections: Established by the basis vectors.\nSpacing: Determined by the lengths of the basis vectors.\nRequirement: Basis vectors must be linearly independent. \n\nFigure 4.4.4: Basis vectors determining directions and spacing.\n\nThe key insight here is that the vectors defining our coordinate system don’t necessarily have to be unit vectors, nor do they need to be orthogonal. The crucial property is that these “basis vectors” must be linearly independent. This ensures that each vector in the space has a unique representation. The directions of these vectors define the positive directions, and their lengths determine the scaling or spacing along those “axes.” This concept is powerful because it extends to abstract vector spaces beyond just geometric R^2 or R^3."
  },
  {
    "objectID": "la-44.html#basis-for-a-vector-space-definition",
    "href": "la-44.html#basis-for-a-vector-space-definition",
    "title": "Linear Algebra",
    "section": "Basis for a Vector Space: Definition",
    "text": "Basis for a Vector Space: Definition\nA basis provides a fundamental set of building blocks for a vector space. Vector spaces can be finite-dimensional or infinite-dimensional.\n\n\n\n\n\n\nDEFINITION 1\n\n\nIf \\(S = \\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots , \\mathbf{v}_{n}\\}\\) is a set of vectors in a finite-dimensional vector space \\(V\\), then \\(S\\) is called a basis for \\(V\\) if:\n\n\\(S\\) spans \\(V\\).\n\\(S\\) is linearly independent.\n\n\n\n\n\nNow, let’s formalize what a “basis” means in the context of general vector spaces. A basis is a special set of vectors that satisfies two critical conditions: first, it must span the entire vector space, meaning every vector in the space can be formed by a linear combination of the basis vectors. Second, the vectors in the basis must be linearly independent, meaning no vector in the set can be expressed as a linear combination of the others. These two conditions together ensure that we have just enough vectors to describe the entire space, without any redundancy. This definition applies to finite-dimensional vector spaces, which are our primary focus."
  },
  {
    "objectID": "la-44.html#finite-vs.-infinite-dimensional-spaces",
    "href": "la-44.html#finite-vs.-infinite-dimensional-spaces",
    "title": "Linear Algebra",
    "section": "Finite vs. Infinite-Dimensional Spaces",
    "text": "Finite vs. Infinite-Dimensional Spaces\nVector spaces are categorized by whether they have a finite spanning set.\n\n\nFinite-Dimensional\n\nThere exists a finite set of vectors that spans \\(V\\).\nExamples: \\(R^n\\), \\(P_n\\) (polynomials of degree \\(\\le n\\)), \\(M_{mn}\\) (matrices).\n\n\nInfinite-Dimensional\n\nNo finite set of vectors can span \\(V\\).\nExamples: \\(P_\\infty\\) (all polynomials), \\(R^\\infty\\) (infinite sequences), \\(C(-\\infty, \\infty)\\) (continuous functions).\n\n\n\nThe concept of a basis naturally leads to the idea of dimension. A vector space is finite-dimensional if we can find a finite set of vectors that spans it. Most of the spaces we deal with in ECE, like \\(R^n\\) for signals or state vectors, are finite-dimensional. However, some spaces, like the space of all continuous functions or infinite sequences, are infinite-dimensional because no finite set of vectors can possibly span them. For these, the concept of a basis still exists, but it involves infinite sets of vectors, which is usually covered in more advanced courses."
  },
  {
    "objectID": "la-44.html#example-1-the-standard-basis-for-rn",
    "href": "la-44.html#example-1-the-standard-basis-for-rn",
    "title": "Linear Algebra",
    "section": "EXAMPLE 1: The Standard Basis for \\(R^{n}\\)",
    "text": "EXAMPLE 1: The Standard Basis for \\(R^{n}\\)\nThe most familiar basis for \\(R^n\\) is the standard basis.\nThe standard unit vectors: \\[\n\\mathbf{e}_{1} = (1,0,0,\\ldots ,0), \\quad \\mathbf{e}_{2} = (0,1,0,\\ldots ,0), \\ldots , \\quad \\mathbf{e}_{n} = (0,0,0,\\ldots ,1)\n\\]\nFor \\(R^3\\), the standard basis is: \\[\n\\mathbf{i} = (1,0,0), \\quad \\mathbf{j} = (0,1,0), \\quad \\mathbf{k} = (0,0,1)\n\\]\nThese vectors are linearly independent and span \\(R^n\\).\n\nLet’s look at some concrete examples of bases. The standard basis for \\(R^n\\) is probably the one you’re most familiar with. For \\(R^3\\), these are the \\(\\mathbf{i}\\), \\(\\mathbf{j}\\), and \\(\\mathbf{k}\\) vectors. They are clearly linearly independent, as you can’t form one from the others, and any vector in \\(R^3\\) can be uniquely expressed as a linear combination of them. This makes them a perfect basis."
  },
  {
    "objectID": "la-44.html#interactive-example-standard-basis-in-r3",
    "href": "la-44.html#interactive-example-standard-basis-in-r3",
    "title": "Linear Algebra",
    "section": "Interactive Example: Standard Basis in \\(R^3\\)",
    "text": "Interactive Example: Standard Basis in \\(R^3\\)\nVisualize the standard basis vectors and a vector formed by their linear combination.\n\n\n\n\n\n\n\nHere’s an interactive Python example using matplotlib to visualize the standard basis vectors in \\(R^3\\). The red, green, and blue arrows represent \\(\\mathbf{e}_1\\), \\(\\mathbf{e}_2\\), and \\(\\mathbf{e}_3\\) respectively. The black arrow shows a vector \\(\\mathbf{v}\\) that is a linear combination of these basis vectors. You can see how \\(\\mathbf{v}\\) is constructed by scaling and summing the basis vectors."
  },
  {
    "objectID": "la-44.html#example-2-the-standard-basis-for-p_n",
    "href": "la-44.html#example-2-the-standard-basis-for-p_n",
    "title": "Linear Algebra",
    "section": "EXAMPLE 2: The Standard Basis for \\(P_{n}\\)",
    "text": "EXAMPLE 2: The Standard Basis for \\(P_{n}\\)\nThe vector space \\(P_n\\) consists of polynomials of degree \\(n\\) or less. The standard basis for \\(P_n\\) is \\(S = \\{1, x, x^{2}, \\ldots , x^{n}\\}\\).\nLet’s denote these polynomials as: \\[\n\\mathbf{p}_{0} = 1, \\quad \\mathbf{p}_{1} = x, \\quad \\mathbf{p}_{2} = x^{2}, \\ldots , \\quad \\mathbf{p}_{n} = x^{n}\n\\]\nThese polynomials span \\(P_n\\) and are linearly independent, forming a basis.\n\nNot all vector spaces are about geometric vectors. Consider \\(P_n\\), the space of polynomials of degree \\(n\\) or less. A basis for this space is simply the set of monomials \\(\\{1, x, x^2, \\ldots, x^n\\}\\). Any polynomial of degree \\(n\\) or less can be written as a unique linear combination of these. For example, \\(3x^2 - 2x + 1\\) is a linear combination of \\(1, x, x^2\\). These are also linearly independent; you can’t create \\(x^2\\) from \\(1\\) and \\(x\\)."
  },
  {
    "objectID": "la-44.html#example-3-another-basis-for-r3",
    "href": "la-44.html#example-3-another-basis-for-r3",
    "title": "Linear Algebra",
    "section": "EXAMPLE 3: Another Basis for \\(R^{3}\\)",
    "text": "EXAMPLE 3: Another Basis for \\(R^{3}\\)\nA vector space can have multiple bases. Show that \\(\\mathbf{v}_{1} = (1,2,1)\\), \\(\\mathbf{v}_{2} = (2,9,0)\\), and \\(\\mathbf{v}_{3} = (3,3,4)\\) form a basis for \\(R^{3}\\).\nTo prove this, we need to show:\n\nLinear Independence: The equation \\(c_{1}\\mathbf{v}_{1} + c_{2}\\mathbf{v}_{2} + c_{3}\\mathbf{v}_{3} = \\mathbf{0}\\) has only the trivial solution (\\(c_1=c_2=c_3=0\\)).\nSpanning: Any vector \\(\\mathbf{b} = (b_{1},b_{2},b_{3})\\) in \\(R^{3}\\) can be expressed as \\(c_{1}\\mathbf{v}_{1} + c_{2}\\mathbf{v}_{2} + c_{3}\\mathbf{v}_{3} = \\mathbf{b}\\).\n\nBoth conditions rely on the invertibility of the coefficient matrix \\(A\\).\n\\[\nA={\\left[\\begin{array}{l l l}{1}&{2}&{3}\\\\ {2}&{9}&{3}\\\\ {1}&{0}&{4}\\end{array}\\right]}\n\\]\nIf \\(\\det(A) \\neq 0\\), then \\(A\\) is invertible, implying both conditions are met.\n\nIt’s important to understand that a vector space can have many different bases. Here’s an example of a non-standard basis for \\(R^3\\). To prove that these three vectors form a basis, we need to verify two things: they are linearly independent, and they span \\(R^3\\). Both of these conditions can be checked by examining the determinant of the matrix formed by these vectors. If the determinant is non-zero, the matrix is invertible, which guarantees both linear independence and spanning for \\(R^3\\)."
  },
  {
    "objectID": "la-44.html#interactive-example-checking-basis-for-r3",
    "href": "la-44.html#interactive-example-checking-basis-for-r3",
    "title": "Linear Algebra",
    "section": "Interactive Example: Checking Basis for \\(R^3\\)",
    "text": "Interactive Example: Checking Basis for \\(R^3\\)\nLet’s use Python to calculate the determinant of matrix \\(A\\).\n\n\n\n\n\n\n\nHere, we use numpy to construct the matrix \\(A\\) from our given vectors and then calculate its determinant. As you can see, the determinant is -1, which is non-zero. This confirms that the vectors \\(\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\) are indeed linearly independent and span \\(R^3\\), thus forming a valid basis. This is a powerful computational check."
  },
  {
    "objectID": "la-44.html#example-4-the-standard-basis-for-m_mn",
    "href": "la-44.html#example-4-the-standard-basis-for-m_mn",
    "title": "Linear Algebra",
    "section": "EXAMPLE 4: The Standard Basis for \\(M_{mn}\\)",
    "text": "EXAMPLE 4: The Standard Basis for \\(M_{mn}\\)\nThe vector space \\(M_{mn}\\) consists of \\(m \\times n\\) matrices. For \\(M_{22}\\) ( \\(2 \\times 2\\) matrices), the standard basis is:\n\\[\nM_{1}={\\left[\\begin{array}{l l}{1}&{0}\\\\ {0}&{0}\\end{array}\\right]},\\quad M_{2}={\\left[\\begin{array}{l l}{0}&{1}\\\\ {0}&{0}\\end{array}\\right]},\\quad M_{3}={\\left[\\begin{array}{l l}{0}&{0}\\\\ {1}&{0}\\end{array}\\right]},\\quad M_{4}={\\left[\\begin{array}{l l}{0}&{0}\\\\ {0}&{1}\\end{array}\\right]}\n\\]\nAny \\(2 \\times 2\\) matrix \\(B = \\left[ \\begin{array}{ll}a & b \\\\ c & d \\end{array} \\right]\\) can be uniquely written as:\n\\[\nB = aM_1 + bM_2 + cM_3 + dM_4\n\\]\nThese matrices are linearly independent and span \\(M_{22}\\).\n\nEven matrices can form vector spaces, and similarly, they have bases. For \\(M_{22}\\), the space of \\(2 \\times 2\\) matrices, the standard basis consists of four matrices, each with a single ‘1’ and zeros elsewhere. Any \\(2 \\times 2\\) matrix can be expressed as a unique linear combination of these four basis matrices. This concept extends to \\(M_{mn}\\) for any \\(m\\) and \\(n\\)."
  },
  {
    "objectID": "la-44.html#the-zero-vector-space",
    "href": "la-44.html#the-zero-vector-space",
    "title": "Linear Algebra",
    "section": "The Zero Vector Space",
    "text": "The Zero Vector Space\nThe simplest vector space is \\(V = \\{\\mathbf{0}\\}\\).\nIt is finite-dimensional, spanned by \\(\\mathbf{0}\\).\nHowever, \\(\\{\\mathbf{0}\\}\\) is not linearly independent.\n\n\n\n\n\n\nTip\n\n\nFor convenience, the empty set \\(\\varnothing\\) is defined as a basis for the zero vector space.\n\n\n\n\nA special case is the zero vector space, containing only the zero vector. It’s finite-dimensional because it’s spanned by the zero vector itself. However, a set containing only the zero vector is not linearly independent because you can always write \\(c \\cdot \\mathbf{0} = \\mathbf{0}\\) for any non-zero scalar \\(c\\). To maintain consistency with the definition of a basis, we conventionally define the empty set as the basis for the zero vector space."
  },
  {
    "objectID": "la-44.html#example-5-6-infinite-dimensional-vector-space",
    "href": "la-44.html#example-5-6-infinite-dimensional-vector-space",
    "title": "Linear Algebra",
    "section": "EXAMPLE 5 & 6: Infinite-Dimensional Vector Space",
    "text": "EXAMPLE 5 & 6: Infinite-Dimensional Vector Space\nThe vector space of all polynomials, \\(P_{\\infty}\\), is infinite-dimensional.\nNo finite set of polynomials can span \\(P_{\\infty}\\).\n\n\nWhy?\n\nIf \\(S = \\{\\mathbf{p}_{1}, \\ldots , \\mathbf{p}_{r}\\}\\) spanned \\(P_{\\infty}\\), there would be a maximum degree, say \\(n\\).\nThen \\(x^{n+1}\\) could not be expressed as a linear combination of polynomials in \\(S\\).\nThis contradicts \\(S\\) spanning \\(P_{\\infty}\\).\n\n\nOther infinite-dimensional spaces:\n\n\\(R^{\\infty}\\) (sequences)\n\\(F(- \\infty , \\infty)\\) (all real-valued functions)\n\\(C(- \\infty , \\infty)\\) (continuous functions)\n\\(C^{m}(- \\infty , \\infty)\\) (functions with \\(m\\) continuous derivatives)\n\n\n\nLet’s quickly revisit infinite-dimensional spaces. The space of all polynomials, \\(P_{\\infty}\\), is a classic example. If you had a finite set of polynomials, say up to degree \\(n\\), you could never form a polynomial of degree \\(n+1\\) from their linear combinations. This shows that no finite set can span \\(P_{\\infty}\\). Similarly, spaces of functions or infinite sequences are also infinite-dimensional, playing crucial roles in advanced engineering topics like Fourier analysis and signal processing."
  },
  {
    "objectID": "la-44.html#coordinates-relative-to-a-basis",
    "href": "la-44.html#coordinates-relative-to-a-basis",
    "title": "Linear Algebra",
    "section": "Coordinates Relative to a Basis",
    "text": "Coordinates Relative to a Basis\nA basis allows us to assign unique coordinates to every vector in a space. This formalizes the idea of a “coordinate system” in abstract vector spaces.\n\n\n\n\n\n\nTHEOREM 4.4.1 Uniqueness of Basis Representation\n\n\nIf \\(S = \\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots , \\mathbf{v}_{n}\\}\\) is a basis for a vector space \\(V\\), then every vector \\(\\mathbf{v}\\) in \\(V\\) can be expressed in the form \\(\\mathbf{v} = c_{1} \\mathbf{v}_{1} + c_{2} \\mathbf{v}_{2} + \\dots + c_{n} \\mathbf{v}_{n}\\) in exactly one way.\n\n\n\n\nThe most important consequence of having a basis is that every vector in the space can be uniquely represented as a linear combination of the basis vectors. This is Theorem 4.4.1, and it’s fundamental. The uniqueness comes directly from the linear independence of the basis vectors. If there were two different ways to write a vector, their difference would be a non-trivial linear combination of basis vectors equal to zero, which contradicts linear independence."
  },
  {
    "objectID": "la-44.html#defining-coordinate-vectors",
    "href": "la-44.html#defining-coordinate-vectors",
    "title": "Linear Algebra",
    "section": "Defining Coordinate Vectors",
    "text": "Defining Coordinate Vectors\nThe coefficients in the unique linear combination are the coordinates.\n\n\n\n\n\n\nDEFINITION 2\n\n\nIf \\(S = \\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots , \\mathbf{v}_{n}\\}\\) is a basis for a vector space \\(V\\), and \\[\n\\mathbf{v} = c_{1}\\mathbf{v}_{1} + c_{2}\\mathbf{v}_{2} + \\dots + c_{n}\\mathbf{v}_{n}\n\\] is the expression for a vector \\(\\mathbf{v}\\) in terms of the basis \\(S\\), then the scalars \\(c_{1}, c_{2}, \\ldots , c_{n}\\) are called the coordinates of \\(\\mathbf{v}\\) relative to the basis \\(S\\). The vector \\((c_{1}, c_{2}, \\ldots , c_{n})\\) in \\(R^{n}\\) is called the coordinate vector of \\(\\mathbf{v}\\) relative to \\(S\\); it is denoted by \\[\n(\\mathbf{v})_{S} = (c_{1}, c_{2}, \\ldots , c_{n}) \\tag{6}\n\\]\n\n\n\n\nThis leads us to the formal definition of coordinates relative to a basis. Once we have a basis, the scalar coefficients \\(c_1, \\ldots, c_n\\) that uniquely express a vector \\(\\mathbf{v}\\) are called its coordinates relative to that basis. We can then collect these coefficients into a coordinate vector, denoted \\((\\mathbf{v})_S\\). This coordinate vector is an element of \\(R^n\\), effectively translating vectors from any \\(n\\)-dimensional vector space into the familiar \\(R^n\\)."
  },
  {
    "objectID": "la-44.html#ordered-basis",
    "href": "la-44.html#ordered-basis",
    "title": "Linear Algebra",
    "section": "Ordered Basis",
    "text": "Ordered Basis\nThe order of basis vectors matters for coordinate vectors.\n\n\n\n\n\n\nNote\n\n\nWhen we talk about coordinate vectors, we always assume an ordered basis.\nChanging the order of basis vectors changes the coordinate vector.\nFor example, in \\(R^2\\), \\((1,2)\\) is not the same as \\((2,1)\\).\n\n\n\n\nFigure 4.4.6: Correspondence between vectors in \\(V\\) and \\(R^n\\).\n\nA crucial point about coordinate vectors is that the order of the basis vectors is critical. While a set of basis vectors is mathematically a set (where order doesn’t matter), when we form a coordinate vector, we implicitly assume an ordered basis. If you change the order of the basis vectors, the coordinate vector will change, even though the underlying vector in \\(V\\) remains the same. This one-to-one correspondence between vectors in \\(V\\) and coordinate vectors in \\(R^n\\) is what makes coordinate systems so powerful."
  },
  {
    "objectID": "la-44.html#example-7-coordinates-relative-to-the-standard-basis-for-rn",
    "href": "la-44.html#example-7-coordinates-relative-to-the-standard-basis-for-rn",
    "title": "Linear Algebra",
    "section": "EXAMPLE 7: Coordinates Relative to the Standard Basis for \\(R^{n}\\)",
    "text": "EXAMPLE 7: Coordinates Relative to the Standard Basis for \\(R^{n}\\)\nFor the standard basis \\(S\\) in \\(R^n\\), the coordinate vector is identical to the vector itself.\nFor \\(\\mathbf{v} = (a,b,c)\\) in \\(R^3\\) and standard basis \\(S = \\{\\mathbf{i},\\mathbf{j},\\mathbf{k}\\}\\): \\[\n\\mathbf{v} = a\\mathbf{i} + b\\mathbf{j} + c\\mathbf{k}\n\\] The coordinate vector relative to \\(S\\) is: \\[\n(\\mathbf{v})_{S} = (a,b,c)\n\\] This means \\((\\mathbf{v})_{S} = \\mathbf{v}\\).\n\nLet’s quickly re-examine the standard basis. For \\(R^n\\) with its standard basis, the coordinates of a vector are simply its components. So, the coordinate vector \\((\\mathbf{v})_S\\) is identical to the vector \\(\\mathbf{v}\\) itself. This is why we often don’t explicitly distinguish between a vector and its coordinates in \\(R^n\\) when working with the standard basis."
  },
  {
    "objectID": "la-44.html#example-8-coordinate-vectors-relative-to-standard-bases",
    "href": "la-44.html#example-8-coordinate-vectors-relative-to-standard-bases",
    "title": "Linear Algebra",
    "section": "EXAMPLE 8: Coordinate Vectors Relative to Standard Bases",
    "text": "EXAMPLE 8: Coordinate Vectors Relative to Standard Bases\nCoordinate vectors for polynomials and matrices are also straightforward with standard bases.\n\n\n(a) Polynomial \\(P_n\\)\nFor \\(\\mathbf{p}(x) = c_{0} + c_{1}x + \\dots +c_{n}x^{n}\\) and standard basis \\(S = \\{1,x,\\ldots ,x^{n}\\}\\), \\[\n(\\mathbf{p})_{S} = (c_{0},c_{1},c_{2},\\ldots ,c_{n})\n\\]\n\n(b) Matrix \\(M_{22}\\)\nFor \\(B = \\left[ \\begin{array}{ll}a & b \\\\ c & d \\end{array} \\right]\\) and standard basis \\(S = \\{M_1, M_2, M_3, M_4\\}\\), \\[\n(B)_{S} = (a,b,c,d)\n\\]\n\n\nSimilarly, for polynomials and matrices, when using their respective standard bases, the coordinate vector is simply the vector of coefficients or entries. For a polynomial, it’s the coefficients from constant term to highest degree. For a matrix, it’s the entries read in a specific order (e.g., row by row). These examples highlight how the abstract definition of a coordinate vector translates to concrete representations in different vector spaces."
  },
  {
    "objectID": "la-44.html#example-9-coordinates-in-r3-with-a-non-standard-basis",
    "href": "la-44.html#example-9-coordinates-in-r3-with-a-non-standard-basis",
    "title": "Linear Algebra",
    "section": "EXAMPLE 9: Coordinates in \\(R^{3}\\) with a Non-Standard Basis",
    "text": "EXAMPLE 9: Coordinates in \\(R^{3}\\) with a Non-Standard Basis\nUsing the basis \\(S = \\{\\mathbf{v}_{1},\\mathbf{v}_{2},\\mathbf{v}_{3}\\}\\) from Example 3: \\(\\mathbf{v}_{1} = (1,2,1)\\), \\(\\mathbf{v}_{2} = (2,9,0)\\), \\(\\mathbf{v}_{3} = (3,3,4)\\).\n(a) Find \\((\\mathbf{v})_{S}\\) for \\(\\mathbf{v} = (5, - 1,9)\\).\nWe need \\(c_{1},c_{2},c_{3}\\) such that \\(\\mathbf{v} = c_{1}\\mathbf{v}_{1} + c_{2}\\mathbf{v}_{2} + c_{3}\\mathbf{v}_{3}\\).\nThis leads to the system: \\[\n\\begin{array}{r l} & {c_{1} + 2c_{2} + 3c_{3} = 5}\\\\ & {2c_{1} + 9c_{2} + 3c_{3} = -1}\\\\ & {c_{1}\\qquad +4c_{3} = 9} \\end{array}\n\\]\n\nNow for a more challenging example, let’s find the coordinates of a vector relative to the non-standard basis we saw earlier. Given \\(\\mathbf{v} = (5, -1, 9)\\), we want to find the scalars \\(c_1, c_2, c_3\\) such that \\(\\mathbf{v}\\) is a linear combination of \\(\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\). This translates into solving a system of linear equations, where the unknowns are our coordinates."
  },
  {
    "objectID": "la-44.html#interactive-example-finding-coordinates",
    "href": "la-44.html#interactive-example-finding-coordinates",
    "title": "Linear Algebra",
    "section": "Interactive Example: Finding Coordinates",
    "text": "Interactive Example: Finding Coordinates\nLet’s solve the system to find the coordinates \\((\\mathbf{v})_{S}\\).\n\n\n\n\n\n\nThe solution is \\(c_{1} = 1, c_{2} = - 1, c_{3} = 2\\). So, \\((\\mathbf{v})_{S} = (1, -1,2)\\).\n\nUsing numpy.linalg.solve, we can efficiently find the coefficients \\(c_1, c_2, c_3\\). The output shows that \\(c_1=1, c_2=-1, c_3=2\\). This means the coordinate vector of \\(\\mathbf{v}\\) relative to basis \\(S\\) is \\((1, -1, 2)\\). We can also verify this by reconstructing \\(\\mathbf{v}\\) from these coordinates and the basis vectors to ensure it matches the original vector."
  },
  {
    "objectID": "la-44.html#example-9-coordinates-in-r3-continued",
    "href": "la-44.html#example-9-coordinates-in-r3-continued",
    "title": "Linear Algebra",
    "section": "EXAMPLE 9: Coordinates in \\(R^{3}\\) (Continued)",
    "text": "EXAMPLE 9: Coordinates in \\(R^{3}\\) (Continued)\n(b) Find the vector \\(\\mathbf{v}\\) in \\(R^{3}\\) whose coordinate vector relative to \\(S\\) is \\((\\mathbf{v})_{S} = (-1,3,2)\\).\nHere, we are given the coordinates and need to find the actual vector. Using the definition \\(\\mathbf{v} = c_{1}\\mathbf{v}_{1} + c_{2}\\mathbf{v}_{2} + c_{3}\\mathbf{v}_{3}\\):\n\\[\n\\begin{array}{r l} & {\\mathbf{v} = (-1)\\mathbf{v}_{1} + 3\\mathbf{v}_{2} + 2\\mathbf{v}_{3}}\\\\ & {\\quad = (-1)(1,2,1) + 3(2,9,0) + 2(3,3,4)} \\end{array}\n\\]\n\nNow, let’s reverse the process. If we’re given the coordinate vector, say \\((-1, 3, 2)\\) relative to basis \\(S\\), we can find the actual vector \\(\\mathbf{v}\\) in \\(R^3\\). This is simpler; we just perform the linear combination using the given coefficients and the basis vectors."
  },
  {
    "objectID": "la-44.html#interactive-example-reconstructing-vector-from-coordinates",
    "href": "la-44.html#interactive-example-reconstructing-vector-from-coordinates",
    "title": "Linear Algebra",
    "section": "Interactive Example: Reconstructing Vector from Coordinates",
    "text": "Interactive Example: Reconstructing Vector from Coordinates\nLet’s compute \\(\\mathbf{v}\\) from its coordinate vector.\n\n\n\n\n\n\nThe resulting vector is \\(\\mathbf{v} = (11,31,7)\\).\n\nBy plugging in the coordinates and the basis vectors, we compute the linear combination. The output shows that the reconstructed vector \\(\\mathbf{v}\\) is \\((11, 31, 7)\\). This demonstrates the direct relationship between a vector and its coordinates once a basis is defined. This is a fundamental operation in many engineering applications, such as changing coordinate frames in robotics or signal transformations."
  },
  {
    "objectID": "la-44.html#summary",
    "href": "la-44.html#summary",
    "title": "Linear Algebra",
    "section": "Summary",
    "text": "Summary\n\nCoordinate Systems: Generalize beyond rectangular axes using vectors.\nBasis: A set of linearly independent vectors that spans a vector space.\nUniqueness: Every vector has a unique representation relative to a given basis.\nCoordinate Vector: The ordered set of scalars representing a vector in terms of a basis.\nApplication: Crucial for understanding transformations, solving systems, and representing data in ECE.\n\n\nTo summarize, we’ve explored how coordinate systems are generalized in linear algebra through the concept of a basis. A basis is a fundamental set of building blocks that allows for the unique representation of any vector in a vector space. The coordinate vector provides a numerical “address” for a vector relative to a specific basis. Understanding these concepts is paramount for ECE, as it forms the backbone for topics like change of basis, linear transformations, and efficient data representation in various engineering fields."
  },
  {
    "objectID": "la-45.html#the-concept-of-dimension",
    "href": "la-45.html#the-concept-of-dimension",
    "title": "Linear Algebra",
    "section": "The Concept of Dimension",
    "text": "The Concept of Dimension\nThe number of vectors in a basis is intrinsically linked to the “dimension” of a vector space.\nOur goal is to make this intuitive notion precise.\n\n\n\n\n\n\nTHEOREM 4.5.1\n\n\nAll bases for a finite-dimensional vector space have the same number of vectors.\n\n\n\nThis theorem is foundational; it means the “number of vectors in a basis” is a well-defined property of the vector space itself, not just a property of a particular basis.\n\nWe start with a very important theorem: Theorem 4.5.1. It states that no matter which basis you choose for a given finite-dimensional vector space, they will all contain the same number of vectors. This is a powerful statement because it allows us to define the dimension of a vector space unambiguously. Before we formally define dimension, let’s look at a supporting theorem that helps prove this idea."
  },
  {
    "objectID": "la-45.html#supporting-theorem-properties-of-sets-in-n-dimensional-space",
    "href": "la-45.html#supporting-theorem-properties-of-sets-in-n-dimensional-space",
    "title": "Linear Algebra",
    "section": "Supporting Theorem: Properties of Sets in \\(n\\)-Dimensional Space",
    "text": "Supporting Theorem: Properties of Sets in \\(n\\)-Dimensional Space\nTo prove Theorem 4.5.1, we rely on properties of sets of vectors relative to a basis.\n\n\n\n\n\n\nTHEOREM 4.5.2\n\n\nLet \\(V\\) be an \\(n\\)-dimensional vector space, and let \\(S = \\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots , \\mathbf{v}_{n}\\}\\) be any basis.\n\nIf a set in \\(V\\) has more than \\(n\\) vectors, then it is linearly dependent.\n\nIf a set in \\(V\\) has fewer than \\(n\\) vectors, then it does not span \\(V\\).\n\n\n\n\nProof of Theorem 4.5.1: If \\(S_1\\) and \\(S_2\\) are two bases for \\(V\\) with \\(n_1\\) and \\(n_2\\) vectors respectively. If \\(n_1 &gt; n_2\\), then by Theorem 4.5.2(a), \\(S_1\\) would be linearly dependent, which contradicts it being a basis. If \\(n_1 &lt; n_2\\), then by Theorem 4.5.2(b), \\(S_1\\) would not span \\(V\\), which also contradicts it being a basis. Therefore, \\(n_1\\) must equal \\(n_2\\).\n\nTheorem 4.5.2 provides the logical stepping stones for proving Theorem 4.5.1. Part (a) says you can’t have “too many” linearly independent vectors in a space; if you exceed the dimension, your set must be dependent. Part (b) says you can’t have “too few” vectors to span the space; if you’re below the dimension, you won’t cover the whole space. Together, these two parts imply that any basis must have exactly \\(n\\) vectors, thus proving Theorem 4.5.1. The formal proof details can be found in your textbook."
  },
  {
    "objectID": "la-45.html#definition-of-dimension",
    "href": "la-45.html#definition-of-dimension",
    "title": "Linear Algebra",
    "section": "Definition of Dimension",
    "text": "Definition of Dimension\nWith Theorem 4.5.1 established, we can now formally define dimension.\n\n\n\n\n\n\nDEFINITION 1\n\n\nThe dimension of a finite-dimensional vector space \\(V\\) is denoted by \\(\\dim (V)\\) and is defined to be the number of vectors in a basis for \\(V\\). In addition, the zero vector space is defined to have dimension zero.\n\n\n\nThis definition aligns with our intuitive understanding of dimension.\n\nThis is the moment we’ve been building towards. The dimension of a finite-dimensional vector space is simply the number of vectors in any of its bases. Because all bases have the same number of vectors, this definition is robust. For completeness, the zero vector space, containing only the zero vector, is defined to have dimension zero, as its basis is the empty set."
  },
  {
    "objectID": "la-45.html#example-1-dimensions-of-some-familiar-vector-spaces",
    "href": "la-45.html#example-1-dimensions-of-some-familiar-vector-spaces",
    "title": "Linear Algebra",
    "section": "EXAMPLE 1: Dimensions of Some Familiar Vector Spaces",
    "text": "EXAMPLE 1: Dimensions of Some Familiar Vector Spaces\nLet’s apply the definition to some common vector spaces.\n\n\n\n\\(\\dim (R^{n}) = n\\) The standard basis \\(\\{\\mathbf{e}_{1}, \\ldots, \\mathbf{e}_{n}\\}\\) has \\(n\\) vectors. Example: \\(\\dim (R^3) = 3\\).\n\\(\\dim (P_{n}) = n + 1\\) The standard basis \\(\\{1, x, x^{2}, \\ldots , x^{n}\\}\\) has \\(n + 1\\) vectors. Example: \\(\\dim (P_2) = 3\\) (basis \\(\\{1, x, x^2\\}\\)).\n\n\n\n\\(\\dim (M_{mn}) = mn\\) The standard basis (matrices with a single ‘1’ and rest ‘0’) has \\(mn\\) vectors. Example: \\(\\dim (M_{22}) = 4\\) (basis \\(M_1, M_2, M_3, M_4\\)).\n\\(\\dim (\\{\\mathbf{0}\\}) = 0\\) The zero vector space has no non-zero vectors, so its basis is the empty set.\n\n\n\nHere are some concrete examples of dimensions for spaces we’ve previously discussed. \\(R^n\\) has dimension \\(n\\), which makes perfect sense. The space of polynomials of degree \\(n\\) or less, \\(P_n\\), has dimension \\(n+1\\) because its standard basis includes the constant term and powers of x up to \\(x^n\\). Similarly, the space of \\(m \\times n\\) matrices, \\(M_{mn}\\), has dimension \\(mn\\). These examples reinforce how the number of basis vectors directly translates to our concept of dimension."
  },
  {
    "objectID": "la-45.html#example-2-dimension-of-operatornamespans",
    "href": "la-45.html#example-2-dimension-of-operatornamespans",
    "title": "Linear Algebra",
    "section": "EXAMPLE 2: Dimension of \\(\\operatorname{span}(S)\\)",
    "text": "EXAMPLE 2: Dimension of \\(\\operatorname{span}(S)\\)\nIf a set of vectors \\(S\\) is linearly independent, it forms a basis for the space it spans.\nIf \\(S = \\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots , \\mathbf{v}_{r}\\}\\) is a linearly independent set, then: \\[\n\\dim [\\operatorname {span}\\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots , \\mathbf{v}_{r}\\} ] = r\n\\]\nIn words: The dimension of the space spanned by a linearly independent set of vectors is equal to the number of vectors in that set.\n\nThis is a very useful property. If you have a set of vectors that you know are linearly independent, then the dimension of the subspace they span is simply the number of vectors in that set. This simplifies finding the dimension of a subspace once you’ve identified a linearly independent spanning set for it."
  },
  {
    "objectID": "la-45.html#example-3-dimension-of-a-solution-space",
    "href": "la-45.html#example-3-dimension-of-a-solution-space",
    "title": "Linear Algebra",
    "section": "EXAMPLE 3: Dimension of a Solution Space",
    "text": "EXAMPLE 3: Dimension of a Solution Space\nFind a basis for and the dimension of the solution space of the homogeneous system:\n\\[\n\\begin{array}{r l r} & {} & {x_{1} + 3x_{2} - 2x_{3}\\qquad +2x_{5}\\qquad = 0}\\\\ & {} & {2x_{1} + 6x_{2} - 5x_{3} - 2x_{4} + 4x_{5} - 3x_{6} = 0}\\\\ & {} & {5x_{3} + 10x_{4}\\qquad +15x_{6} = 0}\\\\ & {} & {2x_{1} + 6x_{2}\\qquad +8x_{4} + 4x_{5} + 18x_{6} = 0} \\end{array}\n\\]\n\nLet’s tackle a practical problem: finding the dimension of the solution space for a homogeneous system of linear equations. This is a common task in control systems and circuit analysis. We first need to find the general solution and express it in vector form."
  },
  {
    "objectID": "la-45.html#example-3-solution-space-solution",
    "href": "la-45.html#example-3-solution-space-solution",
    "title": "Linear Algebra",
    "section": "EXAMPLE 3: Solution Space (Solution)",
    "text": "EXAMPLE 3: Solution Space (Solution)\nThe solution to the system is given by: \\(x_{1} = -3r - 4s - 2t\\), \\(x_{2} = r\\), \\(x_{3} = -2s\\), \\(x_{4} = s\\), \\(x_{5} = t\\), \\(x_{6} = 0\\)\nThis can be written in vector form as: \\[\n(x_{1},x_{2},x_{3},x_{4},x_{5},x_{6}) = r(-3,1,0,0,0,0) + s(-4,0, - 2,1,0,0) + t(-2,0,0,0,1,0)\n\\]\nThe vectors that span the solution space are: \\[\n\\mathbf{v}_{1} = (-3,1,0,0,0,0),\\quad \\mathbf{v}_{2} = (-4,0, - 2,1,0,0),\\quad \\mathbf{v}_{3} = (-2,0,0,0,1,0)\n\\]\nThese vectors are linearly independent.\nThus, the solution space has dimension 3.\n\nBy parameterizing the free variables (\\(r, s, t\\)), we can express any solution vector as a linear combination of these three specific vectors: \\(\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\). These vectors inherently span the solution space. A quick inspection shows they are linearly independent because each vector has a ‘1’ in a position where the others have ‘0’ (e.g., \\(x_2\\) for \\(\\mathbf{v}_1\\), \\(x_4\\) for \\(\\mathbf{v}_2\\), \\(x_5\\) for \\(\\mathbf{v}_3\\)). Since there are three linearly independent vectors that span the solution space, the dimension of the solution space is 3."
  },
  {
    "objectID": "la-45.html#interactive-example-solution-space-basis",
    "href": "la-45.html#interactive-example-solution-space-basis",
    "title": "Linear Algebra",
    "section": "Interactive Example: Solution Space Basis",
    "text": "Interactive Example: Solution Space Basis\nLet’s verify the linear independence of the spanning vectors using Python.\nWe’ll form a matrix with these vectors and check its rank.\n\n\n\n\n\n\n\nHere’s a quick Python check. We form a matrix with our candidate basis vectors as rows. The rank of this matrix tells us the maximum number of linearly independent rows (or columns). Since the rank is 3, which is equal to the number of vectors, it confirms that these vectors are linearly independent. Thus, they form a basis for the solution space, and its dimension is 3. This method is generally applicable for checking linear independence."
  },
  {
    "objectID": "la-45.html#some-fundamental-theorems",
    "href": "la-45.html#some-fundamental-theorems",
    "title": "Linear Algebra",
    "section": "Some Fundamental Theorems",
    "text": "Some Fundamental Theorems\nWe will now look at theorems that reveal interrelationships among linear independence, spanning sets, basis, and dimension.\n\n\n\n\n\n\nNote\n\n\nInformal Idea:\n- Add a vector not in the span of a linearly independent set, and the set remains independent. - Remove a redundant vector from a spanning set, and it still spans the same space.\n\n\n\n\nFigure 4.5.1: Illustrating adding/removing vectors.\n\nThese theorems are not just theoretical exercises; they are essential for understanding how to construct and manipulate bases, and how linear independence and spanning are connected. The “Plus/Minus Theorem” is particularly intuitive: if you have a set of vectors, you can either add a new, independent direction to make the set larger and still independent, or remove a redundant vector without shrinking the space it spans. This process is like fine-tuning a set of tools to be just right – not too many, not too few."
  },
  {
    "objectID": "la-45.html#theorem-4.5.3-plusminus-theorem",
    "href": "la-45.html#theorem-4.5.3-plusminus-theorem",
    "title": "Linear Algebra",
    "section": "THEOREM 4.5.3: Plus/Minus Theorem",
    "text": "THEOREM 4.5.3: Plus/Minus Theorem\nThis theorem formally describes how adding or removing vectors affects linear independence and spanning.\n\n\n\n\n\n\nPlus/Minus Theorem\n\n\nLet \\(S\\) be a nonempty set of vectors in a vector space \\(V\\).\n\nIf \\(S\\) is a linearly independent set, and if \\(\\mathbf{v}\\) is a vector in \\(V\\) that is outside of \\(\\operatorname{span}(S)\\), then the set \\(S\\cup \\{\\mathbf{v}\\}\\) that results by inserting \\(\\mathbf{v}\\) into \\(S\\) is still linearly independent.\nIf \\(\\mathbf{v}\\) is a vector in \\(S\\) that is expressible as a linear combination of other vectors in \\(S\\), and if \\(S - \\{\\mathbf{v}\\}\\) denotes the set obtained by removing \\(\\mathbf{v}\\) from \\(S\\), then \\(S\\) and \\(S - \\{\\mathbf{v}\\}\\) span the same space; that is, \\(\\operatorname{span}(S) = \\operatorname{span}(S - \\{\\mathbf{v}\\})\\).\n\n\n\n\n\nPart (a) is about growing a linearly independent set. If you pick a vector that cannot be formed by your current independent set, adding it will keep the expanded set linearly independent. Part (b) is about shrinking a spanning set without losing coverage. If one vector in your set is redundant (can be formed by the others), you can remove it, and the remaining vectors will still span the same space. These are powerful tools for adjusting sets of vectors to meet basis criteria."
  },
  {
    "objectID": "la-45.html#example-4-applying-the-plusminus-theorem",
    "href": "la-45.html#example-4-applying-the-plusminus-theorem",
    "title": "Linear Algebra",
    "section": "EXAMPLE 4: Applying the Plus/Minus Theorem",
    "text": "EXAMPLE 4: Applying the Plus/Minus Theorem\nShow that \\(\\mathbf{p}_{1} = 1 - x^{2}\\), \\(\\mathbf{p}_{2} = 2 - x^{2}\\), and \\(\\mathbf{p}_{3} = x^{3}\\) are linearly independent vectors.\nSolution:\n\nConsider \\(S_1 = \\{\\mathbf{p}_{1}, \\mathbf{p}_{2}\\}\\). \\(\\mathbf{p}_{1} = 1 - x^2\\) \\(\\mathbf{p}_{2} = 2 - x^2\\) Neither is a scalar multiple of the other, so \\(S_1\\) is linearly independent.\nConsider \\(\\mathbf{p}_{3} = x^{3}\\). Can \\(x^3\\) be expressed as \\(c_1(1-x^2) + c_2(2-x^2)\\)? No, because the linear combination of \\(\\mathbf{p}_{1}\\) and \\(\\mathbf{p}_{2}\\) will only produce polynomials of degree at most 2, while \\(\\mathbf{p}_{3}\\) is degree 3. Thus, \\(\\mathbf{p}_{3}\\) is outside of \\(\\operatorname{span}(S_1)\\).\n\nConclusion:\nBy Theorem 4.5.3(a), since \\(S_1\\) is linearly independent and \\(\\mathbf{p}_{3}\\) is outside of \\(\\operatorname{span}(S_1)\\), the set \\(S_1 \\cup \\{\\mathbf{p}_{3}\\} = \\{\\mathbf{p}_{1}, \\mathbf{p}_{2}, \\mathbf{p}_{3}\\}\\) is also linearly independent.\nThis method allows us to build up linearly independent sets step-by-step.\n\nLet’s apply the Plus/Minus Theorem to polynomials. We start with \\(\\mathbf{p}_1\\) and \\(\\mathbf{p}_2\\). They are clearly linearly independent as one is not a scalar multiple of the other. Now, we consider \\(\\mathbf{p}_3 = x^3\\). The span of \\(\\mathbf{p}_1\\) and \\(\\mathbf{p}_2\\) only contains polynomials of degree 2 or less. Since \\(\\mathbf{p}_3\\) is a polynomial of degree 3, it cannot be in the span of \\(\\{\\mathbf{p}_1, \\mathbf{p}_2\\}\\). Therefore, by part (a) of the Plus/Minus Theorem, adding \\(\\mathbf{p}_3\\) to the set keeps the entire set linearly independent. This is a neat way to establish linear independence."
  },
  {
    "objectID": "la-45.html#theorem-4.5.4-basis-by-inspection",
    "href": "la-45.html#theorem-4.5.4-basis-by-inspection",
    "title": "Linear Algebra",
    "section": "THEOREM 4.5.4: Basis by Inspection",
    "text": "THEOREM 4.5.4: Basis by Inspection\nIf you know the dimension of a vector space, you only need to check one condition for a set of vectors to be a basis.\n\n\n\n\n\n\nTHEOREM 4.5.4\n\n\nLet \\(V\\) be an \\(n\\)-dimensional vector space, and let \\(S\\) be a set in \\(V\\) with exactly \\(n\\) vectors. Then \\(S\\) is a basis for \\(V\\) if and only if \\(S\\) spans \\(V\\) or \\(S\\) is linearly independent.\n\n\n\nProof Logic: This theorem is powerful because it uses Theorem 4.5.2 and 4.5.3. If \\(S\\) spans \\(V\\) but isn’t a basis, it must be linearly dependent. Then, by 4.5.3(b), you could remove a vector, leaving \\(n-1\\) vectors that still span \\(V\\), which contradicts 4.5.2(b). A similar argument holds if \\(S\\) is linearly independent but doesn’t span \\(V\\).\n\nTheorem 4.5.4 is a huge time-saver. If you already know the dimension of a vector space, and you have a set with exactly that many vectors, you only need to check one of the two basis conditions (either spanning or linear independence). The other condition will automatically be satisfied. This is because violating one condition would lead to a contradiction with our earlier theorems, particularly Theorem 4.5.2."
  },
  {
    "objectID": "la-45.html#example-5-bases-by-inspection-for-r2",
    "href": "la-45.html#example-5-bases-by-inspection-for-r2",
    "title": "Linear Algebra",
    "section": "EXAMPLE 5: Bases by Inspection for \\(R^{2}\\)",
    "text": "EXAMPLE 5: Bases by Inspection for \\(R^{2}\\)\nExplain why the vectors \\(\\mathbf{v}_{1} = (-3,7)\\) and \\(\\mathbf{v}_{2} = (5,5)\\) form a basis for \\(R^{2}\\).\nSolution (a):\n\nWe know \\(\\dim (R^{2}) = 2\\).\nThe set \\(S = \\{\\mathbf{v}_{1}, \\mathbf{v}_{2}\\}\\) has exactly 2 vectors.\nWe check for linear independence: Neither \\(\\mathbf{v}_{1}\\) nor \\(\\mathbf{v}_{2}\\) is a scalar multiple of the other. \\(k(-3,7) = (5,5)\\) implies \\(-3k=5\\) (\\(k=-5/3\\)) and \\(7k=5\\) (\\(k=5/7\\)), which is a contradiction. Therefore, \\(S\\) is linearly independent.\nBy Theorem 4.5.4, since \\(S\\) is a linearly independent set with \\(n=2\\) vectors in an \\(n=2\\) dimensional space, \\(S\\) must also span \\(R^{2}\\) and thus forms a basis for \\(R^{2}\\).\n\n\nLet’s apply Theorem 4.5.4. We know that \\(R^2\\) is 2-dimensional. We are given two vectors. All we need to do is check if they are linearly independent (or if they span \\(R^2\\)). A quick check reveals that \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\) are not scalar multiples of each other, meaning they are linearly independent. Since we have exactly 2 linearly independent vectors in a 2-dimensional space, Theorem 4.5.4 guarantees they form a basis."
  },
  {
    "objectID": "la-45.html#interactive-example-basis-by-inspection-in-r2",
    "href": "la-45.html#interactive-example-basis-by-inspection-in-r2",
    "title": "Linear Algebra",
    "section": "Interactive Example: Basis by Inspection in \\(R^2\\)",
    "text": "Interactive Example: Basis by Inspection in \\(R^2\\)\nLet’s visualize \\(\\mathbf{v}_{1} = (-3,7)\\) and \\(\\mathbf{v}_{2} = (5,5)\\) and confirm linear independence.\n\n\n\n\n\n\n\nHere’s a visual and computational check. We plot \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\). Visually, they don’t lie on the same line, suggesting linear independence. Computationally, we form a matrix with these vectors and calculate its determinant. A non-zero determinant confirms linear independence. Since the determinant is 50, which is non-zero, the vectors are linearly independent. Given that \\(R^2\\) is 2-dimensional, these two vectors form a basis."
  },
  {
    "objectID": "la-45.html#example-5-bases-by-inspection-for-r3",
    "href": "la-45.html#example-5-bases-by-inspection-for-r3",
    "title": "Linear Algebra",
    "section": "EXAMPLE 5: Bases by Inspection for \\(R^{3}\\)",
    "text": "EXAMPLE 5: Bases by Inspection for \\(R^{3}\\)\nExplain why \\(\\mathbf{v}_{1} = (2,0, - 1)\\), \\(\\mathbf{v}_{2} = (4,0,7)\\), and \\(\\mathbf{v}_{3} = (-1,1,4)\\) form a basis for \\(R^{3}\\).\nSolution (b):\n\nWe know \\(\\dim (R^{3}) = 3\\).\nThe set \\(S = \\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\}\\) has exactly 3 vectors.\nWe check for linear independence:\n\n\\(\\mathbf{v}_{1}\\) and \\(\\mathbf{v}_{2}\\) are in the \\(xz\\)-plane (their \\(y\\)-components are 0). They are linearly independent since neither is a scalar multiple of the other.\n\\(\\mathbf{v}_{3}\\) has a non-zero \\(y\\)-component (1). This means \\(\\mathbf{v}_{3}\\) is outside the \\(xz\\)-plane, and thus outside the \\(\\operatorname{span}(\\{\\mathbf{v}_{1}, \\mathbf{v}_{2}\\})\\).\nBy the Plus/Minus Theorem (4.5.3a), adding \\(\\mathbf{v}_{3}\\) to the linearly independent set \\(\\{\\mathbf{v}_{1}, \\mathbf{v}_{2}\\}\\) results in a linearly independent set \\(\\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}\\}\\).\n\nBy Theorem 4.5.4, since \\(S\\) is a linearly independent set with \\(n=3\\) vectors in an \\(n=3\\) dimensional space, \\(S\\) must also span \\(R^{3}\\) and thus forms a basis for \\(R^{3}\\).\n\n\nFor \\(R^3\\), which is 3-dimensional, we have three vectors. We can use a similar logic. First, \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\) are linearly independent and lie in the xz-plane. Now, \\(\\mathbf{v}_3\\) has a non-zero y-component, meaning it’s not in the plane spanned by \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\). By the Plus/Minus Theorem, adding \\(\\mathbf{v}_3\\) to the set makes the full set \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\}\\) linearly independent. Since we have 3 linearly independent vectors in a 3-dimensional space, they form a basis."
  },
  {
    "objectID": "la-45.html#theorem-4.5.5-building-and-reducing-bases",
    "href": "la-45.html#theorem-4.5.5-building-and-reducing-bases",
    "title": "Linear Algebra",
    "section": "THEOREM 4.5.5: Building and Reducing Bases",
    "text": "THEOREM 4.5.5: Building and Reducing Bases\nThis theorem provides methods for constructing bases from spanning sets or linearly independent sets.\n\n\n\n\n\n\nTHEOREM 4.5.5\n\n\nLet \\(S\\) be a finite set of vectors in a finite-dimensional vector space \\(V\\).\n\nIf \\(S\\) spans \\(V\\) but is not a basis for \\(V\\), then \\(S\\) can be reduced to a basis for \\(V\\) by removing appropriate vectors from \\(S\\).\nIf \\(S\\) is a linearly independent set that is not already a basis for \\(V\\), then \\(S\\) can be enlarged to a basis for \\(V\\) by inserting appropriate vectors into \\(S\\).\n\n\n\n\nThese processes are fundamental in many algorithms, such as Gram-Schmidt orthogonalization.\n\nTheorem 4.5.5 is a constructive theorem. Part (a) tells us that if you have a set that spans the entire space but is redundant (not a basis), you can trim the fat by removing linearly dependent vectors until you arrive at a basis. Part (b) says if you have a linearly independent set that doesn’t quite span the whole space (not a basis), you can add vectors that are outside its span until you complete it into a basis. These principles are used in various algorithms, for example, in finding a basis for a subspace or in orthogonalization processes important for signal processing."
  },
  {
    "objectID": "la-45.html#theorem-4.5.6-dimension-of-subspaces",
    "href": "la-45.html#theorem-4.5.6-dimension-of-subspaces",
    "title": "Linear Algebra",
    "section": "THEOREM 4.5.6: Dimension of Subspaces",
    "text": "THEOREM 4.5.6: Dimension of Subspaces\nThe dimension of a subspace is always less than or equal to the dimension of the parent space.\n\n\n\n\n\n\nTHEOREM 4.5.6\n\n\nIf \\(W\\) is a subspace of a finite-dimensional vector space \\(V\\), then:\n\n\\(W\\) is finite-dimensional.\n\\(\\dim (W) \\leq \\dim (V)\\).\n\\(W = V\\) if and only if \\(\\dim (W) = \\dim (V)\\).\n\n\n\n\n\nFigure 4.5.2: Geometric relationship between subspaces of \\(R^3\\).\n\nFinally, Theorem 4.5.6 provides important properties about subspaces. Part (a) states that any subspace of a finite-dimensional space is itself finite-dimensional. Part (b) confirms our intuition that a subspace cannot be “larger” than the space it’s contained within; its dimension must be less than or equal to the parent space’s dimension. And part (c) is particularly insightful: if a subspace has the same dimension as its parent space, then it must actually be the parent space. This is very useful for proving equality of spaces. For instance, in \\(R^3\\), a 2-dimensional subspace is a plane through the origin, and a 1-dimensional subspace is a line through the origin, as illustrated in the figure."
  },
  {
    "objectID": "la-45.html#summary",
    "href": "la-45.html#summary",
    "title": "Linear Algebra",
    "section": "Summary",
    "text": "Summary\n\nDimension Definition: The number of vectors in any basis for a finite-dimensional vector space.\nKey Property (Theorem 4.5.1): All bases for a given vector space have the same number of vectors.\nRelationship (Theorem 4.5.2): Sets with too many vectors are dependent; sets with too few cannot span the space.\nConstructing Bases (Theorem 4.5.3 & 4.5.5): Methods for adding vectors to linearly independent sets or removing vectors from spanning sets to form a basis.\nBasis by Inspection (Theorem 4.5.4): If dimension is known, only one condition (spanning or independence) needs to be checked.\nSubspace Dimensions (Theorem 4.5.6): Subspaces are finite-dimensional, and their dimension is less than or equal to the parent space’s dimension.\n\n\nIn this section, we’ve formalized the concept of dimension, moving beyond intuition to a precise mathematical definition based on the number of vectors in a basis. We explored several fundamental theorems that govern the relationships between linear independence, spanning sets, and dimension. These concepts are absolutely critical for understanding the structure of vector spaces and are applied extensively in various ECE fields, from understanding signal spaces to solving complex system equations."
  },
  {
    "objectID": "la-46.html#the-need-for-changing-bases",
    "href": "la-46.html#the-need-for-changing-bases",
    "title": "Linear Algebra",
    "section": "The Need for Changing Bases",
    "text": "The Need for Changing Bases\nA suitable basis for one problem may not be suitable for another. Changing bases is analogous to changing coordinate axes in \\(R^{2}\\) and \\(R^{3}\\).\nCoordinate Maps\nIf \\(S = \\{\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\ldots , \\mathbf{v}_{n}\\}\\) is a basis for \\(V\\), and \\((\\mathbf{v})_{S} = (c_{1}, c_{2}, \\ldots , c_{n})\\) is the coordinate vector of \\(\\mathbf{v}\\) relative to \\(S\\), then the mapping: \\[\n\\mathbf{v} \\to (\\mathbf{v})_{S} \\tag{1}\n\\] creates a one-to-one correspondence between vectors in \\(V\\) and vectors in \\(R^{n}\\). We call this the coordinate map relative to \\(S\\) from \\(V\\) to \\(R^{n}\\).\nFor convenience, coordinate vectors are often expressed in matrix form: \\[\n[\\mathbf{v}]_{S} = \\left[ \\begin{array}{c}c_{1} \\\\ c_{2} \\\\ \\vdots \\\\ c_{n} \\end{array} \\right] \\tag{2}\n\\] This matrix notation emphasizes its role in linear transformations.\n\nFigure 4.6.1: Coordinate map from \\(V\\) to \\(R^n\\).\n\nThink about how you might rotate your coordinate system to align with an object’s principal axes in physics or engineering mechanics. That’s a change of basis. In abstract vector spaces, we use “coordinate maps” to translate between a vector \\(\\mathbf{v}\\) in a general space \\(V\\) and its numerical representation as a coordinate vector in \\(R^n\\). For calculations, we typically write these coordinate vectors as column matrices, denoted with square brackets like \\([\\mathbf{v}]_S\\)."
  },
  {
    "objectID": "la-46.html#the-change-of-basis-problem",
    "href": "la-46.html#the-change-of-basis-problem",
    "title": "Linear Algebra",
    "section": "The Change of Basis Problem",
    "text": "The Change of Basis Problem\nA common problem arises when we have a vector \\(\\mathbf{v}\\) and want to express its coordinates in a different basis.\n\n\n\n\n\n\nThe Change of Basis Problem\n\n\nIf \\(\\mathbf{v}\\) is a vector in a finite-dimensional vector space \\(V\\), and if we change the basis for \\(V\\) from a basis \\(B\\) to a basis \\(B'\\), how are the coordinate vectors \\([\\mathbf{v}]_{B}\\) and \\([\\mathbf{v}]_{B'}\\) related?\n\n\n\nRemark: We will refer to \\(B\\) as the “old basis” and \\(B'\\) as the “new basis.” Our objective is to find a relationship between the old and new coordinates of a fixed vector \\(\\mathbf{v}\\) in \\(V\\).\n\nThe “Change of Basis Problem” is precisely about finding this relationship. If we have a vector \\(\\mathbf{v}\\) expressed in terms of an “old basis” \\(B\\), and we want to know its coordinates in a “new basis” \\(B'\\), how do we convert \\([\\mathbf{v}]_B\\) to \\([\\mathbf{v}]_{B'}\\) or vice-versa? This is crucial for consistency when different parts of a system or different algorithms use different reference frames."
  },
  {
    "objectID": "la-46.html#deriving-the-relationship-2d-case",
    "href": "la-46.html#deriving-the-relationship-2d-case",
    "title": "Linear Algebra",
    "section": "Deriving the Relationship (2D Case)",
    "text": "Deriving the Relationship (2D Case)\nLet’s derive the relationship for a 2-dimensional space first.\nLet \\(B = \\{\\mathbf{u}_{1}, \\mathbf{u}_{2}\\}\\) be the old basis and \\(B' = \\{\\mathbf{u}_{1}', \\mathbf{u}_{2}'\\}\\) be the new basis.\n\nExpress new basis vectors in terms of the old basis: Suppose the coordinate vectors of the new basis vectors relative to the old basis are: \\[\n[\\mathbf{u}_{1}']_{B} = \\begin{bmatrix} a \\\\ b \\end{bmatrix} \\quad \\text{and} \\quad [\\mathbf{u}_{2}']_{B} = \\begin{bmatrix} c \\\\ d \\end{bmatrix} \\tag{3}\n\\] This means: \\[\n\\begin{array}{r}\\mathbf{u}_{1}' = a\\mathbf{u}_{1} + b\\mathbf{u}_{2} \\\\ \\mathbf{u}_{2}' = c\\mathbf{u}_{1} + d\\mathbf{u}_{2} \\end{array} \\tag{4}\n\\]\n\n\nTo understand the relationship, let’s consider a 2D space. We have an old basis \\(B\\) and a new basis \\(B'\\). The key step is to express each vector of the new basis as a linear combination of the vectors in the old basis. The coefficients of these linear combinations form the coordinate vectors of the new basis vectors relative to the old basis."
  },
  {
    "objectID": "la-46.html#deriving-the-relationship-2d-case-continued",
    "href": "la-46.html#deriving-the-relationship-2d-case-continued",
    "title": "Linear Algebra",
    "section": "Deriving the Relationship (2D Case Continued)",
    "text": "Deriving the Relationship (2D Case Continued)\n\nExpress \\(\\mathbf{v}\\) in terms of the new basis: Let \\([\\mathbf{v}]_{B'} = \\begin{bmatrix} k_{1} \\\\ k_{2} \\end{bmatrix}\\) be the new coordinate vector, so: \\[\n\\mathbf{v} = k_{1}\\mathbf{u}_{1}^{\\prime} + k_{2}\\mathbf{u}_{2}^{\\prime} \\tag{6}\n\\]\nSubstitute and find old coordinates: Substitute (4) into (6) to express \\(\\mathbf{v}\\) in terms of the old basis: \\[\n\\mathbf{v} = k_{1}(a\\mathbf{u}_{1} + b\\mathbf{u}_{2}) + k_{2}(c\\mathbf{u}_{1} + d\\mathbf{u}_{2})\n\\] Rearranging terms: \\[\n\\mathbf{v} = (k_{1}a + k_{2}c)\\mathbf{u}_{1} + (k_{1}b + k_{2}d)\\mathbf{u}_{2}\n\\] Thus, the old coordinate vector for \\(\\mathbf{v}\\) is: \\[\n[\\mathbf{v}]_{B} = \\begin{bmatrix} k_{1}a + k_{2}c \\\\ k_{1}b + k_{2}d \\end{bmatrix}\n\\]\n\n\nNext, we take an arbitrary vector \\(\\mathbf{v}\\) and express it using its coordinates in the new basis \\(B'\\). Then, we substitute the expressions for \\(\\mathbf{u}_1'\\) and \\(\\mathbf{u}_2'\\) (from the previous slide) into this equation. After regrouping the terms by \\(\\mathbf{u}_1\\) and \\(\\mathbf{u}_2\\), we get the coefficients that represent \\(\\mathbf{v}\\) in the old basis \\(B\\). These coefficients form the old coordinate vector \\([\\mathbf{v}]_B\\)."
  },
  {
    "objectID": "la-46.html#the-transition-matrix",
    "href": "la-46.html#the-transition-matrix",
    "title": "Linear Algebra",
    "section": "The Transition Matrix",
    "text": "The Transition Matrix\nThe relationship found can be expressed as a matrix multiplication.\nUsing the new coordinate vector \\([\\mathbf{v}]_{B'} = \\begin{bmatrix} k_{1} \\\\ k_{2} \\end{bmatrix}\\), the old coordinate vector can be written as: \\[\n[\\mathbf{v}]_{B} = \\left[ \\begin{array}{ll}a & c \\\\ b & d \\end{array} \\right]\\left[ \\begin{array}{l}k_{1} \\\\ k_{2} \\end{array} \\right] = \\left[ \\begin{array}{ll}a & c \\\\ b & d \\end{array} \\right][\\mathbf{v}]_{B^{\\prime}}\n\\]\nThe matrix \\(P = \\left[ \\begin{array}{ll}a & c \\\\ b & d \\end{array} \\right]\\) is called the transition matrix. Notice that the columns of \\(P\\) are \\([\\mathbf{u}_{1}']_{B}\\) and \\([\\mathbf{u}_{2}']_{B}\\).\n\nThe crucial observation is that the relationship between the old and new coordinate vectors can be written as a matrix-vector product. The matrix, which we call \\(P\\), has its columns composed of the coordinate vectors of the new basis vectors, expressed in terms of the old basis. This matrix \\(P\\) is our “transition matrix,” and it’s the key to changing bases."
  },
  {
    "objectID": "la-46.html#solution-of-the-change-of-basis-problem-general-case",
    "href": "la-46.html#solution-of-the-change-of-basis-problem-general-case",
    "title": "Linear Algebra",
    "section": "Solution of the Change of Basis Problem (General Case)",
    "text": "Solution of the Change of Basis Problem (General Case)\nThis generalizes to \\(n\\)-dimensional spaces.\n\n\n\n\n\n\nSolution of the Change of Basis Problem\n\n\nIf we change the basis for a vector space \\(V\\) from an old basis \\(B = \\{\\mathbf{u}_{1}, \\ldots , \\mathbf{u}_{n}\\}\\) to a new basis \\(B^{\\prime} = \\{\\mathbf{u}_{1}^{\\prime}, \\ldots , \\mathbf{u}_{n}^{\\prime}\\}\\), then for each vector \\(\\mathbf{v}\\) in \\(V\\), the old coordinate vector \\([\\mathbf{v}]_{B}\\) is related to the new coordinate vector \\([\\mathbf{v}]_{B^{\\prime}}\\) by the equation: \\[\n[\\mathbf{v}]_{B} = P[\\mathbf{v}]_{B^{\\prime}} \\tag{7}\n\\] where the columns of \\(P\\) are the coordinate vectors of the new basis vectors relative to the old basis; that is, the column vectors of \\(P\\) are: \\[\n[\\mathbf{u}_{1}^{\\prime}]_{B}, \\quad [\\mathbf{u}_{2}^{\\prime}]_{B}, \\ldots , \\quad [\\mathbf{u}_{n}^{\\prime}]_{B} \\tag{8}\n\\]\n\n\n\n\nSo, in general, to convert coordinate vectors from a new basis \\(B'\\) to an old basis \\(B\\), we multiply by a transition matrix \\(P\\). The columns of this matrix \\(P\\) are formed by taking each vector from the new basis \\(B'\\) and expressing it as a coordinate vector with respect to the old basis \\(B\\). This is a fundamental formula in linear algebra."
  },
  {
    "objectID": "la-46.html#transition-matrices-notation",
    "href": "la-46.html#transition-matrices-notation",
    "title": "Linear Algebra",
    "section": "Transition Matrices Notation",
    "text": "Transition Matrices Notation\nWe often use specific notation to clarify the direction of the transition.\nThe matrix \\(P\\) in Equation (7) is called the transition matrix from \\(B^{\\prime}\\) to \\(B\\), denoted by \\(P_{B^{\\prime} \\rightarrow B}\\). It can be expressed as: \\[\nP_{B^{\\prime} \\rightarrow B} = \\left[ [\\mathbf{u}_{1}^{\\prime}]_{B} \\mid [\\mathbf{u}_{2}^{\\prime}]_{B} \\mid \\dots \\mid [\\mathbf{u}_{n}^{\\prime}]_{B} \\right] \\tag{9}\n\\]\nSimilarly, the transition matrix from \\(B\\) to \\(B^{\\prime}\\) is \\(P_{B \\rightarrow B^{\\prime}}\\), and its columns are: \\[\nP_{B \\rightarrow B^{\\prime}} = \\left[ [\\mathbf{u}_{1}]_{B^{\\prime}} \\mid [\\mathbf{u}_{2}]_{B^{\\prime}} \\mid \\dots \\mid [\\mathbf{u}_{n}]_{B^{\\prime}} \\right] \\tag{10}\n\\]\n\n\n\n\n\n\nTip\n\n\nMemory Aid: The columns of the transition matrix from an old basis to a new basis are the coordinate vectors of the old basis relative to the new basis. (This statement in the text seems to have a typo. It should be: “The columns of the transition matrix from a new basis to an old basis are the coordinate vectors of the new basis relative to the old basis.”) Let’s stick to the definition: \\(P_{B' \\to B}\\) takes coordinates in \\(B'\\) to \\(B\\). Its columns are the new basis vectors expressed in the old basis.\n\n\n\n\nTo avoid confusion, we use subscripts to denote the direction of the transition. \\(P_{B' \\to B}\\) takes coordinates from \\(B'\\) and gives you coordinates in \\(B\\). Its columns are the vectors of \\(B'\\) expressed in terms of \\(B\\). Conversely, \\(P_{B \\to B'}\\) takes coordinates from \\(B\\) and gives you coordinates in \\(B'\\), and its columns are the vectors of \\(B\\) expressed in terms of \\(B'\\). The memory aid in the textbook can be a bit confusing, so let’s rely on the formal definitions and the direction of the arrow."
  },
  {
    "objectID": "la-46.html#example-1-finding-transition-matrices",
    "href": "la-46.html#example-1-finding-transition-matrices",
    "title": "Linear Algebra",
    "section": "EXAMPLE 1: Finding Transition Matrices",
    "text": "EXAMPLE 1: Finding Transition Matrices\nConsider bases \\(B = \\{\\mathbf{u}_{1}, \\mathbf{u}_{2}\\}\\) and \\(B^{\\prime} = \\{\\mathbf{u}_{1}^{\\prime}, \\mathbf{u}_{2}^{\\prime}\\}\\) for \\(R^{2}\\), where:\n\\(\\mathbf{u}_{1} = (1,0)\\), \\(\\mathbf{u}_{2} = (0,1)\\)\n\\(\\mathbf{u}_{1}^{\\prime} = (1,1)\\), \\(\\mathbf{u}_{2}^{\\prime} = (2,1)\\)\n(a) Find \\(P_{B^{\\prime} \\rightarrow B}\\) (from \\(B'\\) to \\(B\\)).\nNew basis vectors in terms of old basis \\(B\\):\n\\(\\mathbf{u}_{1}^{\\prime} = 1\\mathbf{u}_{1} + 1\\mathbf{u}_{2} \\implies [\\mathbf{u}_{1}^{\\prime}]_{B} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\)\n\\(\\mathbf{u}_{2}^{\\prime} = 2\\mathbf{u}_{1} + 1\\mathbf{u}_{2} \\implies [\\mathbf{u}_{2}^{\\prime}]_{B} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\\)\nSo, \\(P_{B^{\\prime} \\rightarrow B} = \\begin{bmatrix} 1 & 2 \\\\ 1 & 1 \\end{bmatrix}\\).\n\nLet’s work through an example. We have two bases for \\(R^2\\). For part (a), we want to go from \\(B'\\) to \\(B\\). This means \\(B'\\) is our “new” basis, and \\(B\\) is our “old” basis. We need to express \\(\\mathbf{u}_1'\\) and \\(\\mathbf{u}_2'\\) in terms of \\(\\mathbf{u}_1\\) and \\(\\mathbf{u}_2\\). Since \\(\\mathbf{u}_1\\) and \\(\\mathbf{u}_2\\) are the standard basis vectors, this is straightforward. The coefficients become the columns of our transition matrix."
  },
  {
    "objectID": "la-46.html#example-1-finding-transition-matrices-continued",
    "href": "la-46.html#example-1-finding-transition-matrices-continued",
    "title": "Linear Algebra",
    "section": "EXAMPLE 1: Finding Transition Matrices (Continued)",
    "text": "EXAMPLE 1: Finding Transition Matrices (Continued)\n(b) Find \\(P_{B \\rightarrow B^{\\prime}}\\) (from \\(B\\) to \\(B'\\)).\nOld basis vectors in terms of new basis \\(B'\\):\nWe need to find \\(c_1, c_2\\) such that \\(\\mathbf{u}_{1} = c_{1}\\mathbf{u}_{1}^{\\prime} + c_{2}\\mathbf{u}_{2}^{\\prime}\\).\n\\((1,0) = c_{1}(1,1) + c_{2}(2,1) \\implies \\begin{cases} c_1 + 2c_2 = 1 \\\\ c_1 + c_2 = 0 \\end{cases}\\)\nSolving gives \\(c_1 = -1, c_2 = 1\\). So, \\([\\mathbf{u}_{1}]_{B^{\\prime}} = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\\).\nSimilarly for \\(\\mathbf{u}_{2}\\):\n\\((0,1) = d_{1}(1,1) + d_{2}(2,1) \\implies \\begin{cases} d_1 + 2d_2 = 0 \\\\ d_1 + d_2 = 1 \\end{cases}\\)\nSolving gives \\(d_1 = 2, d_2 = -1\\). So, \\([\\mathbf{u}_{2}]_{B^{\\prime}} = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}\\).\nThus, \\(P_{B \\rightarrow B^{\\prime}} = \\begin{bmatrix} -1 & 2 \\\\ 1 & -1 \\end{bmatrix}\\).\n\nFor part (b), we want to go from \\(B\\) to \\(B'\\). Now, \\(B\\) is the “new” basis, and \\(B'\\) is the “old” basis. This means we need to express \\(\\mathbf{u}_1\\) and \\(\\mathbf{u}_2\\) in terms of \\(\\mathbf{u}_1'\\) and \\(\\mathbf{u}_2'\\). This typically involves solving a system of linear equations for each basis vector. Once we find the coefficients, they form the columns of \\(P_{B \\to B'}\\)."
  },
  {
    "objectID": "la-46.html#interactive-example-verify-transition-matrix-p_b-rightarrow-b",
    "href": "la-46.html#interactive-example-verify-transition-matrix-p_b-rightarrow-b",
    "title": "Linear Algebra",
    "section": "Interactive Example: Verify Transition Matrix \\(P_{B \\rightarrow B'}\\)",
    "text": "Interactive Example: Verify Transition Matrix \\(P_{B \\rightarrow B'}\\)\nLet’s use Python to solve the systems and construct \\(P_{B \\rightarrow B'}\\).\n\n\n\n\n\n\n\nUsing numpy, we can automate the process of solving the linear systems. We form a matrix A whose columns are the vectors of \\(B'\\). Then, we solve \\(A\\mathbf{c} = \\mathbf{u}_1\\) and \\(A\\mathbf{d} = \\mathbf{u}_2\\) to find the coordinates. The output confirms our manual calculations for \\(P_{B \\to B'}\\), demonstrating the power of computational tools for these tasks."
  },
  {
    "objectID": "la-46.html#example-2-computing-coordinate-vectors",
    "href": "la-46.html#example-2-computing-coordinate-vectors",
    "title": "Linear Algebra",
    "section": "EXAMPLE 2: Computing Coordinate Vectors",
    "text": "EXAMPLE 2: Computing Coordinate Vectors\nLet \\(B\\) and \\(B^{\\prime}\\) be the bases from Example 1. Use an appropriate formula to find \\([\\mathbf{v}]_{B}\\) given that \\([\\mathbf{v}]_{B^{\\prime}} = \\begin{bmatrix} -3 \\\\ 5 \\end{bmatrix}\\).\nSolution: We want to find \\([\\mathbf{v}]_{B}\\) from \\([\\mathbf{v}]_{B'}\\). This means we need the transition matrix \\(P_{B^{\\prime} \\rightarrow B}\\). From Example 1(a), \\(P_{B^{\\prime} \\rightarrow B} = \\begin{bmatrix} 1 & 2 \\\\ 1 & 1 \\end{bmatrix}\\).\nUsing the formula \\([\\mathbf{v}]_{B} = P_{B^{\\prime} \\rightarrow B}[\\mathbf{v}]_{B^{\\prime}}\\): \\[\n[\\mathbf{v}]_{B} = \\begin{bmatrix} 1 & 2 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} -3 \\\\ 5 \\end{bmatrix} = \\begin{bmatrix} (1)(-3) + (2)(5) \\\\ (1)(-3) + (1)(5) \\end{bmatrix} = \\begin{bmatrix} 7 \\\\ 2 \\end{bmatrix}\n\\]\nSo, the coordinate vector of \\(\\mathbf{v}\\) relative to basis \\(B\\) is \\(\\begin{bmatrix} 7 \\\\ 2 \\end{bmatrix}\\).\n\nNow, let’s see how to use these transition matrices. If we’re given the coordinates of a vector \\(\\mathbf{v}\\) in the \\(B'\\) basis, and we want its coordinates in the \\(B\\) basis, we simply multiply \\([\\mathbf{v}]_{B'}\\) by the transition matrix \\(P_{B' \\to B}\\). As you can see, a simple matrix multiplication gives us the required coordinate vector."
  },
  {
    "objectID": "la-46.html#interactive-example-compute-mathbfv_b",
    "href": "la-46.html#interactive-example-compute-mathbfv_b",
    "title": "Linear Algebra",
    "section": "Interactive Example: Compute \\([\\mathbf{v}]_{B}\\)",
    "text": "Interactive Example: Compute \\([\\mathbf{v}]_{B}\\)\nLet’s use Python to perform the matrix multiplication.\n\n\n\n\n\n\n\nThis Python code snippet confirms the calculation. Defining the transition matrix and the coordinate vector, then using numpy’s matrix multiplication operator (@), we quickly get the coordinate vector in the old basis \\(B\\). This is a common operation in engineering to switch between different representations of the same physical quantity or state."
  },
  {
    "objectID": "la-46.html#invertibility-of-transition-matrices",
    "href": "la-46.html#invertibility-of-transition-matrices",
    "title": "Linear Algebra",
    "section": "Invertibility of Transition Matrices",
    "text": "Invertibility of Transition Matrices\nTransition matrices are always invertible, and their inverse has a special meaning.\nIf \\(B\\) and \\(B^{\\prime}\\) are bases for \\(V\\): \\[\n[\\mathbf{v}]_{B} = P_{B^{\\prime} \\rightarrow B}[\\mathbf{v}]_{B^{\\prime}} \\quad \\text{and} \\quad [\\mathbf{v}]_{B^{\\prime}} = P_{B \\rightarrow B^{\\prime}}[\\mathbf{v}]_{B} \\tag{11}\n\\]\nSubstituting the second into the first: \\[\n[\\mathbf{v}]_{B} = P_{B^{\\prime} \\rightarrow B}(P_{B \\rightarrow B^{\\prime}}[\\mathbf{v}]_{B}) = (P_{B^{\\prime} \\rightarrow B} P_{B \\rightarrow B^{\\prime}})[\\mathbf{v}]_{B}\n\\] This implies that \\((P_{B^{\\prime} \\rightarrow B} P_{B \\rightarrow B^{\\prime}})\\) must be the identity matrix \\(I\\). Thus, \\(P_{B^{\\prime} \\rightarrow B}\\) and \\(P_{B \\rightarrow B^{\\prime}}\\) are inverses of each other.\n\n\n\n\n\n\nTHEOREM 4.6.1\n\n\nIf \\(P\\) is the transition matrix from a basis \\(B^{\\prime}\\) to a basis \\(B\\) for a finite-dimensional vector space \\(V\\), then \\(P\\) is invertible and \\(P^{- 1}\\) is the transition matrix from \\(B\\) to \\(B^{\\prime}\\). \\[\nP_{B^{\\prime} \\rightarrow B}^{-1} = P_{B \\rightarrow B^{\\prime}}\n\\]\n\n\n\n\nAn important property of transition matrices is their invertibility. If you can go from basis \\(B'\\) to \\(B\\), you should be able to go back from \\(B\\) to \\(B'\\). This means the transition matrices are inverses of each other. This is formally stated in Theorem 4.6.1. We can see this intuitively because transforming coordinates from \\(B\\) to \\(B'\\) and then back to \\(B\\) should result in the original coordinates, implying their product is the identity matrix."
  },
  {
    "objectID": "la-46.html#interactive-example-verify-inverse-relationship",
    "href": "la-46.html#interactive-example-verify-inverse-relationship",
    "title": "Linear Algebra",
    "section": "Interactive Example: Verify Inverse Relationship",
    "text": "Interactive Example: Verify Inverse Relationship\nLet’s verify that \\(P_{B^{\\prime} \\rightarrow B}\\) and \\(P_{B \\rightarrow B^{\\prime}}\\) are inverses.\n\n\n\n\n\n\n\nUsing numpy, we can multiply the two transition matrices we found earlier. As expected, their product is the identity matrix. Furthermore, we can compute the inverse of \\(P_{B' \\to B}\\) directly using np.linalg.inv and confirm that it matches \\(P_{B \\to B'}\\). This numerical verification reinforces Theorem 4.6.1 and builds confidence in our calculations."
  },
  {
    "objectID": "la-46.html#efficient-method-for-computing-transition-matrices-for-rn",
    "href": "la-46.html#efficient-method-for-computing-transition-matrices-for-rn",
    "title": "Linear Algebra",
    "section": "Efficient Method for Computing Transition Matrices for \\(R^n\\)",
    "text": "Efficient Method for Computing Transition Matrices for \\(R^n\\)\nFor \\(R^n\\), there’s an efficient procedure using augmented matrices. This avoids solving multiple systems separately.\n\n\n\n\n\n\nA Procedure for Computing \\(P_{B\\rightarrow B^{\\prime}}\\) (Old to New)\n\n\nStep 1. Form the augmented matrix \\([B^{\\prime} \\mid B]\\). (Here \\(B'\\) is the new basis, \\(B\\) is the old basis. Columns of \\(B'\\) and \\(B\\) are the basis vectors).\nStep 2. Use elementary row operations to reduce the matrix in Step 1 to reduced row echelon form.\nStep 3. The resulting matrix will be \\([I \\mid P_{B \\rightarrow B^{\\prime}}]\\).\nStep 4. Extract the matrix \\(P_{B \\rightarrow B^{\\prime}}\\) from the right side of the matrix in Step 3.\n\n\n\nThis procedure is summarized by the diagram: row operations \\([ \\text{new basis} \\mid \\text{old basis} ] \\quad \\to \\quad [ I \\mid \\text{transition from old to new} ]\\)\n\nWhen working with \\(R^n\\), solving \\(n\\) separate linear systems to find the coordinates of each basis vector can be tedious. This efficient procedure streamlines the process by using an augmented matrix and row operations. The key is to set up the augmented matrix correctly: the “new basis” vectors on the left, and the “old basis” vectors on the right. After row reduction, the right side will directly give you the transition matrix from the old basis to the new basis. This method is incredibly useful in practical computations."
  },
  {
    "objectID": "la-46.html#example-3-example-1-revisited-efficient-method",
    "href": "la-46.html#example-3-example-1-revisited-efficient-method",
    "title": "Linear Algebra",
    "section": "EXAMPLE 3: Example 1 Revisited (Efficient Method)",
    "text": "EXAMPLE 3: Example 1 Revisited (Efficient Method)\nUsing the bases from Example 1: \\(B = \\{\\mathbf{u}_{1}=(1,0), \\mathbf{u}_{2}=(0,1)\\}\\) \\(B^{\\prime} = \\{\\mathbf{u}_{1}^{\\prime}=(1,1), \\mathbf{u}_{2}^{\\prime}=(2,1)\\}\\)\n(a) Find \\(P_{B^{\\prime} \\rightarrow B}\\) (from \\(B'\\) to \\(B\\)).\nHere, \\(B'\\) is the old basis; \\(B\\) is the new basis. So, we want to find \\(P_{B' \\to B}\\). According to the general formula \\(P_{B' \\to B} = [[\\mathbf{u}_1']_B \\mid [\\mathbf{u}_2']_B]\\). Since \\(B\\) is the standard basis, \\([\\mathbf{u}_{1}^{\\prime}]_{B} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) and \\([\\mathbf{u}_{2}^{\\prime}]_{B} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\\). Thus, \\(P_{B^{\\prime}\\rightarrow B} = \\begin{bmatrix} 1 & 2\\\\ 1 & 1 \\end{bmatrix}\\). (This was trivial as \\(B\\) is standard basis). The procedure (new basis | old basis) would be \\([B \\mid B'] = \\left[ \\begin{array}{l l l}{1} & 0 & {1} & {2}\\\\ {0} & {1} & {1} & {1} \\end{array} \\right]\\). Since the left side is already \\(I\\), \\(P_{B^{\\prime}\\rightarrow B}\\) is the right side.\n\nLet’s apply the efficient method. For part (a), finding \\(P_{B' \\to B}\\), we set up the augmented matrix with the new basis (\\(B\\)) on the left and the old basis (\\(B'\\)) on the right. Since \\(B\\) is the standard basis, the left block is already the identity matrix, and the right block immediately gives us the transition matrix. This matches our earlier result."
  },
  {
    "objectID": "la-46.html#example-3-example-1-revisited-efficient-method-continued",
    "href": "la-46.html#example-3-example-1-revisited-efficient-method-continued",
    "title": "Linear Algebra",
    "section": "EXAMPLE 3: Example 1 Revisited (Efficient Method Continued)",
    "text": "EXAMPLE 3: Example 1 Revisited (Efficient Method Continued)\n(b) Find \\(P_{B \\rightarrow B^{\\prime}}\\) (from \\(B\\) to \\(B'\\)).\nHere, \\(B\\) is the old basis; \\(B'\\) is the new basis. We want to find \\(P_{B \\rightarrow B^{\\prime}}\\). Form the augmented matrix \\([B^{\\prime} \\mid B]\\): \\[\n\\left[ \\begin{array}{ll|ll}1 & 2 & 1 & 0\\\\ 1 & 1 & 0 & 1 \\end{array} \\right]\n\\] Reduce to reduced row echelon form: \\[\n\\left[ \\begin{array}{ll|cc}1 & 0 & -1 & 2\\\\ 0 & 1 & 1 & -1 \\end{array} \\right]\n\\] So the transition matrix is \\(P_{B\\rightarrow B^{\\prime}} = \\begin{bmatrix} - 1 & 2\\\\ 1 & -1 \\end{bmatrix}\\), which matches our previous result.\n\nFor part (b), we are finding \\(P_{B \\to B'}\\). So, \\(B'\\) is the new basis, and \\(B\\) is the old basis. We construct the augmented matrix \\([B' \\mid B]\\). Then, we perform row operations to transform the left block (representing \\(B'\\)) into the identity matrix. The right block will then automatically become our desired transition matrix \\(P_{B \\to B'}\\). This is a much faster way than solving two separate systems of equations."
  },
  {
    "objectID": "la-46.html#interactive-example-efficient-method-for-p_b-rightarrow-b",
    "href": "la-46.html#interactive-example-efficient-method-for-p_b-rightarrow-b",
    "title": "Linear Algebra",
    "section": "Interactive Example: Efficient Method for \\(P_{B \\rightarrow B'}\\)",
    "text": "Interactive Example: Efficient Method for \\(P_{B \\rightarrow B'}\\)\nLet’s use Python to perform the row operations for \\(P_{B \\rightarrow B'}\\).\n\n\n\n\n\n\n\nWhile numpy doesn’t have a direct “row reduce augmented matrix” function like some symbolic tools, we can achieve the same result. The transition matrix \\(P_{B \\to B'}\\) is effectively the inverse of the matrix formed by \\(B'\\) vectors, multiplied by the matrix formed by \\(B\\) vectors. The code calculates this, confirming the result obtained by manual row operations. This highlights how computational tools simplify complex matrix algebra."
  },
  {
    "objectID": "la-46.html#transition-to-the-standard-basis-for-rn",
    "href": "la-46.html#transition-to-the-standard-basis-for-rn",
    "title": "Linear Algebra",
    "section": "Transition to the Standard Basis for \\(R^n\\)",
    "text": "Transition to the Standard Basis for \\(R^n\\)\nA special case arises when one of the bases is the standard basis.\n\n\n\n\n\n\nTHEOREM 4.6.2\n\n\nLet \\(B^{\\prime} = \\{\\mathbf{u}_{1},\\mathbf{u}_{2},\\ldots ,\\mathbf{u}_{n}\\}\\) be any basis for the vector space \\(R^{n}\\) and let \\(S = \\{\\mathbf{e}_{1},\\mathbf{e}_{2},\\ldots ,\\mathbf{e}_{n}\\}\\) be the standard basis for \\(R^{n}\\). If the vectors in these bases are written in column form, then: \\[\nP_{B^{\\prime}\\rightarrow S} = [\\mathbf{u}_{1}\\mid \\mathbf{u}_{2}\\mid \\dots \\mid \\mathbf{u}_{n}] \\tag{15}\n\\] That is, the transition matrix from \\(B'\\) to the standard basis \\(S\\) is simply the matrix whose columns are the basis vectors of \\(B'\\).\n\n\n\nThis means if \\(A = [\\mathbf{u}_{1}\\mid \\mathbf{u}_{2}\\mid \\dots \\mid \\mathbf{u}_{n}]\\) is any invertible \\(n\\times n\\) matrix, then \\(A\\) can be viewed as the transition matrix from the basis \\(\\{\\mathbf{u}_{1},\\mathbf{u}_{2},\\ldots ,\\mathbf{u}_{n}\\}\\) for \\(R^{n}\\) to the standard basis for \\(R^{n}\\).\n\nTheorem 4.6.2 is a very convenient shortcut. If you’re transitioning to the standard basis \\(S\\), the transition matrix \\(P_{B' \\to S}\\) is simply the matrix formed by placing the vectors of \\(B'\\) as its columns. This is because expressing a vector like \\(\\mathbf{u}_1'\\) in terms of the standard basis \\(\\mathbf{e}_1, \\mathbf{e}_2, \\ldots\\) just means its components are the coefficients themselves. This means any invertible matrix can be seen as a transition matrix from some basis to the standard basis."
  },
  {
    "objectID": "la-46.html#interactive-example-transition-to-standard-basis",
    "href": "la-46.html#interactive-example-transition-to-standard-basis",
    "title": "Linear Algebra",
    "section": "Interactive Example: Transition to Standard Basis",
    "text": "Interactive Example: Transition to Standard Basis\nLet’s use a non-standard basis for \\(R^3\\) and find its transition matrix to the standard basis.\nConsider the basis: \\(\\mathbf{u}_{1} = (1,2,1)\\), \\(\\mathbf{u}_{2} = (2,5,0)\\), \\(\\mathbf{u}_{3} = (3,3,8)\\)\nThe transition matrix \\(P_{B \\rightarrow S}\\) would simply be: \\[\nP_{B\\rightarrow S} = \\left[ \\begin{array}{lll}1 & 2 & 3\\\\ 2 & 5 & 3\\\\ 1 & 0 & 8 \\end{array} \\right]\n\\]\n\n\n\n\n\n\n\nHere, we define a basis \\(B\\) for \\(R^3\\). According to Theorem 4.6.2, the transition matrix from this basis \\(B\\) to the standard basis \\(S\\) is simply the matrix formed by taking \\(\\mathbf{u}_1, \\mathbf{u}_2, \\mathbf{u}_3\\) as its columns. The Python code demonstrates this construction and then uses this transition matrix to convert a vector from \\(B\\) coordinates to standard coordinates. This is a very common operation in engineering, for example, transforming sensor readings from a device’s local coordinate system to a global reference frame."
  },
  {
    "objectID": "la-46.html#summary",
    "href": "la-46.html#summary",
    "title": "Linear Algebra",
    "section": "Summary",
    "text": "Summary\n\nCoordinate Maps: Connect vectors in a general space \\(V\\) to coordinate vectors in \\(R^n\\).\nChange of Basis Problem: Relates coordinate vectors of a fixed vector in different bases (\\([\\mathbf{v}]_{B}\\) vs. \\([\\mathbf{v}]_{B'}\\)).\nTransition Matrix \\(P_{B' \\rightarrow B}\\): Converts coordinates from new basis \\(B'\\) to old basis \\(B\\). Its columns are the new basis vectors expressed in the old basis.\nInvertibility: \\(P_{B' \\rightarrow B}^{-1} = P_{B \\rightarrow B'}\\).\nEfficient Method for \\(R^n\\): Use augmented matrix \\([B' \\mid B] \\to [I \\mid P_{B \\rightarrow B'}]\\).\nStandard Basis Shortcut: \\(P_{B' \\rightarrow S}\\) is simply the matrix whose columns are the vectors of \\(B'\\).\n\n\nIn summary, we’ve learned how to navigate between different coordinate systems in vector spaces using change of basis. We defined coordinate maps, understood the change of basis problem, and derived the concept of transition matrices. These matrices allow us to convert coordinate vectors from one basis to another, and we saw that they are always invertible, with the inverse matrix performing the reverse conversion. We also covered an efficient method for \\(R^n\\) using augmented matrices and a useful shortcut for transitions involving the standard basis. These tools are indispensable for working with complex systems in various engineering disciplines."
  },
  {
    "objectID": "index.html#week-8",
    "href": "index.html#week-8",
    "title": "Linear Algebra",
    "section": "Week 8",
    "text": "Week 8\n\nLinear Algebra 4.3: Linear Independence\nLinear Algebra 4.4: Coordinates and Basis\nLinear Algebra 4.5: Dimension\nLinear Algebra 4.6: Change of Basis"
  }
]