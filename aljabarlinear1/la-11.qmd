---
title: "Linear Algebra"
subtitle: "1.1 Introduction to Systems of Linear Equations"
author: "Imron Rosyadi"
format:
  live-revealjs:
    logo: "qrjs_assets/unsoed_logo.png"
    footer: "[irosyadi-2025](https://imron-slide.vercel.app)"
    slide-number: true
    chalkboard: true
    scrollable: true
    controls: true
    progress: true
    preview-links: false
    transition: fade
    incremental: false
    smaller: false
    theme: [default, qrjs_assets/ir_style.scss]
filters:
  - pyodide
---

## Introduction to Systems of Linear Equations

Systems of linear equations are fundamental to various engineering disciplines. In this section, we'll introduce key terminology and explore methods for solving such systems.

::: {.notes}
Good morning, everyone! Today, we embark on a crucial journey into the world of Linear Algebra, starting with the very foundation: Systems of Linear Equations. These systems are not just abstract mathematical constructs; they are the bedrock for understanding many real-world phenomena and engineering problems, from analyzing electrical circuits to optimizing control systems.

Our main goals for this session are:
1. To define what constitutes a linear equation and a system of linear equations.
2. To understand the different types of solutions a linear system can have.
3. To introduce the concept of augmented matrices and elementary row operations as a systematic way to solve these systems.
4. To see how these concepts are visualized geometrically, especially in 2D and 3D.

Let's dive in!
:::

---

## What is a Linear Equation?

A linear equation is an algebraic equation of the form:

*   In two variables ($x, y$):
    $$
    ax + by = c \quad (a, b \text{ not both } 0)
    $$
*   In three variables ($x, y, z$):
    $$
    ax + by + cz = d \quad (a, b, c \text{ not all } 0)
    $$
*   In $n$ variables ($x_1, x_2, \ldots, x_n$):
    $$
    a_1x_1 + a_2x_2 + \cdots + a_nx_n = b \quad (a_i \text{ not all } 0)
    $$

A special case where $b=0$ is called a **homogeneous linear equation**:
$$
a_1x_1 + a_2x_2 + \cdots + a_nx_n = 0
$$

::: {.notes}
Let's start with the basic building block: the linear equation. You've encountered these since high school, especially in two dimensions, where they represent straight lines. Think about the equation $y = mx + b$; it can be rewritten as $mx - y = -b$, which fits our definition.

In three dimensions, a linear equation represents a plane. For example, in circuit analysis, Kirchhoff's voltage and current laws often lead to linear equations.

The key characteristic of a linear equation is that variables only appear to the first power, and there are no products or roots of variables. They also cannot be arguments of functions like sine, log, or exponential. This linearity is what makes these systems so powerful and solvable through systematic methods.
:::

---

## Linear vs. Non-Linear Equations

:::: {.columns}
::: {.column width="50%"}
**Linear Equations**

Characterized by:

*   Variables only to the first power.
*   No products or roots of variables.
*   No variables in trigonometric, logarithmic, or exponential functions.

Examples:
$$
\begin{array}{l} x + 3y = 7 \\ \frac{1}{2}x - y + 3z = -1 \end{array}
$$
$$
\begin{array}{l} x_1 - 2x_2 - 3x_3 + x_4 = 0 \\ x_1 + x_2 + \cdots + x_n = 1 \end{array}
$$
:::
::: {.column width="50%"}
**Non-Linear Equations**

Violate linearity conditions.

Examples:
$$
\begin{array}{l} x + 3y^2 = 4 \\ \sin x + y = 0 \end{array}
$$
$$
\begin{array}{l} 3x + 2y - xy = 5 \\ \sqrt{x_1} + 2x_2 + x_3 = 1 \end{array}
$$
:::
::::

::: {.notes}
It's crucial to distinguish between linear and non-linear equations, as the methods we'll learn are specifically for linear systems. Look at the examples on the left: all variables are simply $x$, $y$, $z$, or $x_i$ to the power of 1.

On the right, we see squared terms like $y^2$, products like $xy$, variables inside functions like $\sin x$, or under radicals like $\sqrt{x_1}$. These violate the linearity conditions. While non-linear systems exist and are vital in many applications, their solution methods are much more complex and outside the scope of this course.
:::

---

## Systems of Linear Equations

A **system of linear equations** (or **linear system**) is a finite set of linear equations. The variables are called **unknowns**.

General system of $m$ equations in $n$ unknowns:
$$
\begin{array}{c} a_{11}x_{1}+a_{12}x_{2}+\cdots+a_{1n}x_{n}=b_{1} \\ a_{21}x_{1}+a_{22}x_{2}+\cdots+a_{2n}x_{n}=b_{2} \\ \vdots\qquad\vdots\qquad\vdots\qquad\vdots\qquad\vdots \\ a_{m1}x_{1}+a_{m2}x_{2}+\cdots+a_{mn}x_{n}=b_{m} \end{array}
$$
Here, $a_{ij}$ (coefficients) indicate location: $a_{12}$ is in the first equation, multiplying $x_2$. $b_k$ are constants.

A **solution** is a sequence of $n$ numbers $(s_1, s_2, \ldots, s_n)$ that makes each equation a true statement when $x_1=s_1, \ldots, x_n=s_n$. This is also called an **ordered $n$-tuple**.

::: {.notes}
When we have multiple linear equations that must be satisfied simultaneously, we call that a system of linear equations. These are very common in engineering for modeling multiple interacting components or constraints.

For instance, in circuit analysis, if you apply Kirchhoff's Voltage Law to several loops in a circuit, you'll end up with a system of linear equations where the unknowns might be the currents flowing through different branches.

The double subscript $a_{ij}$ might look intimidating at first but it's very logical: the first index `i` tells you which equation the coefficient belongs to, and the second index `j` tells you which variable it's multiplying. So $a_{23}$ would be the coefficient of $x_3$ in the second equation.

A solution to such a system is simply a set of values for the variables that satisfy *all* equations simultaneously. We can write these solutions as an ordered n-tuple, which gives us a concise way to represent the point in n-dimensional space where all the equations intersect.
:::

---

## Geometric Interpretation (2D)

For a system of two linear equations in two unknowns, graphs are lines in the $xy$-plane.
Consider:
$$
\begin{array}{r}a_{1}x + b_{1}y = c_{1} \\ a_{2}x + b_{2}y = c_{2} \end{array}
$$

There are three possibilities for the intersection of two lines, hence three possibilities for the solution set:

1.  **No Solution:** Lines are parallel and distinct.
2.  **Exactly One Solution:** Lines intersect at a single point.
3.  **Infinitely Many Solutions:** Lines coincide.

---

## Geometric Interpretation (2D)

::: {layout-browse}
![Figure 1.1.1](https://cdn-mineru.openxlab.org.cn/result/2025-08-19/d28de157-cffb-4586-85b2-10e6c17e5d20/f66157e228b344fc6efca14be3fec45f1ec3a4ef621540dc7780d5b82a03b0ce.jpg){width="60%"}
:::

::: {.notes}
Let's first visualize this in two dimensions, which is intuitive for most of us. Each linear equation here represents a straight line. The solution to the system is the point or points where these lines intersect.

As you can see from the diagram, there are only three ways two lines can interact:
1. They are parallel and never meet. In this case, there is no solution that satisfies both equations simultaneously.
2. They cross at exactly one point. This point is the unique solution to the system.
3. They are the exact same line, meaning they overlap everywhere. In this case, every point on that line is a solution, leading to infinitely many solutions.

This visualization helps set the stage for a fundamental theorem in linear algebra: A linear system *cannot* have, for example, exactly two solutions. It's either none, one, or infinitely many.
:::


## Geometric Interpretation (3D)

Similarly, for a system of three linear equations in three unknowns, graphs are planes in the $xyz$-plane.
Consider:
$$
\begin{array}{r}a_{1}x + b_{1}y + c_{1}z = d_{1} \\ a_{2}x + b_{2}y + c_{2}z = d_{2} \\ a_{3}x + b_{3}y + c_{3}z = d_{3} \end{array}
$$
Solutions correspond to points where all three planes intersect. Again, there are only three possibilities: No solutions, one solution, or infinitely many solutions.

---

## Geometric Interpretation (3D)

::: {layout-browse}
![Figure 1.1.2](https://cdn-mineru.openxlab.org.cn/result/2025-08-19/d28de157-cffb-4586-85b2-10e6c17e5d20/ec323eb5474a62719ee4c180a4704450413a79435960bcc60d4a6f57c21ef19b.jpg){width="60%"}
:::

::: {.notes}
Extending this concept to three dimensions, each linear equation now represents a plane. The solution to a system of three linear equations in three unknowns is the point or set of points where all three planes intersect.

While visualizing this is a bit harder than lines, the fundamental principle remains the same. You can have:
*   No common intersection (e.g., three parallel planes, or two parallel and one intersecting, or planes intersecting pairwise but not all at one common line/point).
*   A single point of intersection (e.g., like the corner of a room where three walls meet).
*   An infinite number of solutions (e.g., all three planes coincide, or intersect along a common line).

The core takeaway is that the number of solutions for *any* linear system is restricted to these three categories.
:::

---

## The Fundamental Theorem of Linear Systems

Every system of linear equations has **zero, one, or infinitely many solutions**. There are no other possibilities.

*   **Consistent** system: Has at least one solution (one or infinitely many).
*   **Inconsistent** system: Has no solutions.

This principle holds true regardless of the number of equations or variables.

::: {.notes}
This theorem is incredibly powerful and simplifies our understanding of linear systems. Unlike non-linear systems, which can exhibit complex behaviors like having exactly two solutions, linear systems are strict. This property is a direct consequence of their linear nature and the methods we use to solve them.

For engineers, understanding this classification is crucial. If you're designing a control system and end up with a linear system, knowing that it will either have a unique solution, no solution (indicating a problem in your design or model), or infinite solutions (implying design flexibility or redundancy) helps in troubleshooting and interpretation.
:::

---

## Example 2: A Linear System with One Solution

Solve the linear system:
$$
\begin{array}{c}x - y = 1 \\ 2x + y = 6 \end{array}
$$

**Solution:**

-  Add $2 \times$ (first equation) to (second equation) to eliminate $x$:

Equation 1: $x - y = 1$  
Equation 2: $2x + y = 6$

- Add Eq1 and Eq2 directly to eliminate $y$:  
$(x - y) + (2x + y) = 1 + 6$  
$3x = 7 \implies x = \frac{7}{3}$  

- Substitute $x$ into Eq1:  
$\frac{7}{3} - y = 1$  
$y = \frac{7}{3} - 1 = \frac{7}{3} - \frac{3}{3} = \frac{4}{3}$  

Thus, the unique solution is $(x, y) = \left(\frac{7}{3}, \frac{4}{3}\right)$.

---

## Example 2: A Linear System with One Solution

```{pyodide}
#| max-lines: 10
import matplotlib.pyplot as plt
import numpy as np

fig, ax = plt.subplots(figsize=(7, 4))
x_vals = np.linspace(-1, 5, 400)

# Equation 1: y = x - 1
y1_vals = x_vals - 1

# Equation 2: y = 6 - 2x
y2_vals = 6 - 2 * x_vals

ax.plot(x_vals, y1_vals, label='$y = x - 1$', color='blue')
ax.plot(x_vals, y2_vals, label='$y = 6 - 2x$', color='red')

# Solution point
# sol_x = 7/3
# sol_y = 4/3
# ax.plot(sol_x, sol_y, 'go', markersize=8, label=f'Solution: ({sol_x:.2f}, {sol_y:.2f})')

ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_title('Example 2: One Solution (Geometric View)')
ax.set_xlim(0, 4)
ax.set_ylim(0, 4)
ax.axhline(0, color='gray', linewidth=0.5)
ax.axvline(0, color='gray', linewidth=0.5)
ax.grid(True, linestyle="--", alpha=0.7)
ax.legend()
plt.show()
```

::: {.notes}
Let's walk through our first example of solving a system. The original source text had a small typo in the elimination example, so I've corrected it here to show the simplest way to solve this system.

Here, we have a simple system of two equations. Notice that the 'y' terms have coefficients -1 and +1. This makes it very easy to eliminate 'y' by simply adding the two equations together.

When we add Equation 1 to Equation 2, the 'y' terms cancel out, leaving us with $3x = 7$, which quickly gives us $x = 7/3$.

Once we have $x$, we can substitute it back into either original equation to find $y$. Using the first equation, $7/3 - y = 1$, we get $y = 4/3$.

So, the unique solution is $(7/3, 4/3)$. Geometrically, this is the single point where these two lines intersect, as shown in the plot.
:::

---

## Example 3: A Linear System with No Solutions

Solve the linear system:
$$
\begin{array}{c}x + y = 4 \\ 3x + 3y = 6 \end{array}
$$

**Solution:**

-  Multiply the first equation by -3 and add it to the second equation:

Equation 1: $x + y = 4$  
$(-3)(x + y) + (3x + 3y) = (-3)(4) + 6$  
$-3x - 3y + 3x + 3y = -12 + 6$  
$0 = -6$

The resulting equation $0 = -6$ is a contradiction.
Thus, the system has **no solution**. The lines are parallel and distinct.

---

## Example 3: A Linear System with No Solutions

```{pyodide}
#| max-lines: 10
import matplotlib.pyplot as plt
import numpy as np

fig, ax = plt.subplots(figsize=(7, 4))
x_vals = np.linspace(-1, 5, 400)

# Equation 1: y = 4 - x
y1_vals = 4 - x_vals

# Equation 2: y = (6 - 3x) / 3 = 2 - x
y2_vals = 2 - x_vals

ax.plot(x_vals, y1_vals, label='$x + y = 4$', color='purple')
ax.plot(x_vals, y2_vals, label='$3x + 3y = 6$', color='orange', linestyle='--')

ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_title('Example 3: No Solution (Geometric View)')
ax.set_xlim(-1, 5)
ax.set_ylim(-1, 5)
ax.axhline(0, color='gray', linewidth=0.5)
ax.axvline(0, color='gray', linewidth=0.5)
ax.grid(True, linestyle="--", alpha=0.7)
ax.legend()
plt.show()
```

::: {.notes}
Next, let's explore a system with no solutions.
We have $x + y = 4$ and $3x + 3y = 6$.
If we try to eliminate one variable, say $x$, we can multiply the first equation by -3 and add it to the second.
This straightforward algebraic manipulation leads to the equation $0 = -6$.
This is a mathematical impossibility. A statement like "0 equals -6" is a contradiction.

What does this mean for our system? It means there are no values of $x$ and $y$ that can simultaneously satisfy both equations.
Geometrically, as the plot shows, these two lines are parallel and distinct. They have the same slope (which you can see by rewriting them as $y=-x+4$ and $y=-x+2$), but different y-intercepts, meaning they will never intersect.
This inconsistency is a clear indicator of "no solution."
:::

---

## Example 4: A Linear System with Infinitely Many Solutions

Solve the linear system:
$$
\begin{array}{r}4x - 2y = 1 \\ 16x - 8y = 4 \end{array}
$$

**Solution:**

-  Multiply the first equation by -4 and add it to the second equation:

Equation 1: $4x - 2y = 1$  
$(-4)(4x - 2y) + (16x - 8y) = (-4)(1) + 4$  
$-16x + 8y + 16x - 8y = -4 + 4$
$0 = 0$  

- The resulting equation $0 = 0$ is always true and imposes no additional restriction.
- The solution set is given by the single equation $4x - 2y = 1$.
- To describe the infinite solutions, we use **parametric equations**. Let $y = t$ (where $t$ is any real number). Then $4x - 2t = 1 \implies 4x = 1 + 2t \implies x = \frac{1}{4} + \frac{1}{2}t$.
- The solution is $(x, y) = \left(\frac{1}{4} + \frac{1}{2}t, t\right)$.

---

## Example 4: A Linear System with Infinitely Many Solutions

```{pyodide}
#| max-lines: 10
import matplotlib.pyplot as plt
import numpy as np

fig, ax = plt.subplots(figsize=(7, 4))
x_vals = np.linspace(-1, 2, 400)

# Equation 1: y = (4x - 1) / 2 = 2x - 0.5
y1_vals = 2 * x_vals - 0.5

# Equation 2: y = (16x - 4) / 8 = 2x - 0.5
# Both equations are the same line
ax.plot(x_vals, y1_vals, label='$4x - 2y = 1$', color='teal', linewidth=3)
ax.plot(x_vals, y1_vals, label='$16x - 8y = 4$ (coincident)', color='gray', linestyle=':', linewidth=5, alpha=0.7)

ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_title('Example 4: Infinitely Many Solutions (Geometric View)')
ax.set_xlim(-1, 2)
ax.set_ylim(-3, 4)
ax.axhline(0, color='gray', linewidth=0.5)
ax.axvline(0, color='gray', linewidth=0.5)
ax.grid(True, linestyle="--", alpha=0.7)
ax.legend()
plt.show()
```

::: {.notes}
Our final common case is a system with infinitely many solutions.
Consider $4x - 2y = 1$ and $16x - 8y = 4$.
If we multiply the first equation by -4 and add it to the second equation, we get $0 = 0$.

Unlike the previous case, $0 = 0$ is a true statement, but it doesn't give us any information about $x$ or $y$. This indicates that the two original equations are essentially the same equation; one is a multiple of the other.

Geometrically, as you can see, the lines are coincident—they overlap perfectly. Every point on that line is a solution.
To represent these infinite solutions, we use a technique called **parametrization**. We introduce a dummy variable, often called a parameter (like $t$ or $s$), and express our variables in terms of this parameter. Here, we let $y=t$. Then, from $4x-2y=1$, we solve for $x$ in terms of $t$, giving $x = 1/4 + 1/2t$.
This pair of equations allows us to generate specific solutions by picking any value for $t$. For example, if $t=0$, we get $(1/4, 0)$. If $t=1$, we get $(3/4, 1)$. Both of these points lie on the line and satisfy both original equations.
:::

---

## Example 5a: A Linear System with Infinitely Many Solutions (3D)

Solve the linear system:
$$
\begin{array}{r}x - y + 2z = 5 \\ 2x - 2y + 4z = 10 \\ 3x - 3y + 6z = 15 \end{array}
$$

**Solution:**
Observe that the second equation is $2 \times$ (first equation), and the third equation is $3 \times$ (first equation).
This means all three equations represent the same plane.
Thus, finding solutions to this system is equivalent to finding solutions to the single equation:
$$
x - y + 2z = 5
$$
To describe the infinite solutions, we use **two parameters** since we have one equation and three unknowns.
Let $y = r$ and $z = s$ (where $r, s$ are any real numbers).
Substitute these into the equation: $x - r + 2s = 5 \implies x = 5 + r - 2s$.

The solution is given by the parametric equations:
$$
x = 5 + r - 2s, \quad y = r, \quad z = s
$$
For example, taking $r=1, s=0$ yields the solution $(6, 1, 0)$.

::: {.notes}
Now, let's extend this idea of infinitely many solutions to three dimensions.
Here, we have a system of three equations in three unknowns.
If we look closely, the second equation ($2x - 2y + 4z = 10$) is simply two times the first equation ($x - y + 2z = 5$).
Similarly, the third equation ($3x - 3y + 6z = 15$) is three times the first equation.

This means all three equations describe the exact same plane in 3D space. Any point on this plane is a solution to the system.
To represent these infinite solutions, we need not one, but *two* parameters because we have a 3D space and our solution is a 2D plane within it. We can choose any two variables to be our parameters. Here, we've chosen $y=r$ and $z=s$.
Then, we solve for $x$ in terms of $r$ and $s$, getting $x = 5 + r - 2s$.
This gives us a set of parametric equations that describes every point on the plane. You can pick any real numbers for $r$ and $s$, and you'll get a valid solution $(x,y,z)$. For example, if $r=1$ and $s=0$, we get the solution $(6,1,0)$.
:::

---

## Example 5b: A Linear System with Infinitely Many Solutions (3D)

Consider the system of two linear equations

$x+y+z=1$  
$x−z=0$ 

Each equation individually defines a plane in space. The solutions of the system of both equations are the points that lie on both planes. We can see in the picture below that the planes intersect in a line. In particular, this system has infinitely many solutions.

<iframe src="https://personal.math.ubc.ca/~tbjw/ila/demos/parametric2.html?vers=5c2a53" width="100%" height="500" frameborder="0" allowfullscreen></iframe>

---

## Augmented Matrices

Solving linear systems by algebraic substitution can become cumbersome. We can simplify notation using **augmented matrices**.

A system of linear equations:
$$
\begin{array}{c} a_{11}x_{1}+a_{12}x_{2}+\cdots+a_{1n}x_{n}=b_{1} \\ a_{21}x_{1}+a_{22}x_{2}+\cdots+a_{2n}x_{n}=b_{2} \\ \vdots\qquad\vdots\qquad\vdots\qquad\vdots\qquad\vdots \\ a_{m1}x_{1}+a_{m2}x_{2}+\cdots+a_{mn}x_{n}=b_{m} \end{array}
$$
can be abbreviated by its **augmented matrix**:
$$
\left[ \begin{array}{c c c c | c}{a_{11}} & {a_{12}} & \dots & {a_{1n}} & {b_{1}}\\ {a_{21}} & {a_{22}} & \dots & {a_{2n}} & {b_{2}}\\ \vdots & \vdots & & \vdots & \vdots \\ {a_{m1}} & {a_{m2}} & \dots & {a_{mn}} & {b_{m}} \end{array} \right]
$$
The vertical line conceptually separates the coefficient matrix from the constant terms.

---

## Augmented Matrices

Example: For $x_{1} + x_{2} + 2x_{3} = 9$, $2x_{1} + 4x_{2} - 3x_{3} = 1$, $3x_{1} + 6x_{2} - 5x_{3} = 0$, the augmented matrix is:
$$
\left[ \begin{array}{r r r | r}{1} & 1 & 2 & 9\\ 2 & 4 & {-3} & 1\\ 3 & 6 & {-5} & 0 \end{array} \right]
$$

::: {.notes}
As systems grow larger, keeping track of variables and 'plus' signs becomes tedious. This is where the brilliant idea of **augmented matrices** comes in. A matrix is simply a rectangular array of numbers.

We can represent the entire system just by its coefficients and the constant terms on the right-hand side. The augmented matrix captures all the essential information needed to solve the system without the clutter of variables and operators. The vertical line helps visually distinguish the coefficients from the constants, but mathematically it's just a single matrix. This compact notation is incredibly useful for organization and computation.
:::

---

## Elementary Row Operations

The algebraic operations on a system that do not alter the solution set correspond to **Elementary Row Operations** on the augmented matrix:

1.  **Multiply a row through by a nonzero constant.**
    *   (Corresponds to multiplying an equation by a nonzero constant.)
2.  **Interchange two rows.**
    *   (Corresponds to interchanging two equations.)
3.  **Add a constant times one row to another.**
    *   (Corresponds to adding a constant times one equation to another.)

These operations allow us to systematically simplify the matrix (and thus the system) to a form from which the solution can be easily found.

::: {.notes}
Why are these matrix operations useful? Because they directly mirror the standard algebraic operations we perform on equations, and crucially, these operations *do not change the solution set* of the system. If $(x,y,z)$ is a solution to the original system, it will still be a solution after any of these operations are applied.

1.  Multiplying an entire row by a non-zero constant is like scaling an equation. If $2x+4y=6$ (scaled by 2) is true, then $x+2y=3$ (original) is also true, and vice-versa.
2.  Interchanging two rows simply means swapping the order of two equations, which doesn't affect the overall solution.
3.  Adding a multiple of one row to another is exactly what we did in the previous examples to eliminate variables. This is the most powerful operation for simplification.

By applying these elementary row operations, we can transform a complex system into a much simpler, equivalent system that's easy to solve, often in a triangle-like form. This systematic process is what we'll explore more deeply in the next section.
:::

---

## Example 6: Using Elementary Row Operations (Step 1)

Let's solve the system alongside its augmented matrix:
$$
\begin{array}{r}x + y + 2z = 9\\ 2x + 4y - 3z = 1\\ 3x + 6y - 5z = 0 \end{array} \qquad \left[ \begin{array}{r r r | r}{1} & 1 & 2 & 9\\ {2} & 4 & {-3} & 1\\ {3} & 6 & {-5} & 0 \end{array} \right]
$$

**Operation:** Add $- 2$ times the first equation to the second.  
**Matrix Operation:** Add $- 2$ times the first row to the second row (Notation: $R_2 \leftarrow R_2 - 2R_1$).

$$
\begin{array}{r}x + y + 2z = 9\\ \quad (2-2)x + (4-2)y + (-3-4)z = 1-18 \\ 3x + 6y - 5z = 0 \end{array} \rightarrow \begin{array}{r}x + y + 2z = 9\\ 2y - 7z = -17\\ 3x + 6y - 5z = 0 \end{array}
$$
$$
\left[ \begin{array}{r r r | r}{1} & 1 & 2 & 9\\ 0 & 2 & {-7} & {-17}\\ 3 & 6 & {-5} & 0 \end{array} \right]
$$

::: {.notes}
Now, let's see how elementary row operations work in practice, side-by-side with the algebraic operations. Our goal is to transform the system into a simpler form where `x`, `y`, and `z` values are easily found, usually in a triangular or diagonal matrix form.

Our first step is to eliminate $x$ from the second equation. We do this by multiplying the first equation by -2 and adding it to the second equation. This ensures that the coefficient of $x$ in the second equation becomes zero.

Correspondingly, in the augmented matrix, we perform the same operation on the rows: $-2$ times Row 1 added to Row 2. Notice how the first column in the second row becomes zero, effectively eliminating $x$ from that equation.
:::

---

## Example 6: Using Elementary Row Operations (Step 2)

**Operation:** Add $- 3$ times the first equation to the third.  
**Matrix Operation:** Add $- 3$ times the first row to the third row (Notation: $R_3 \leftarrow R_3 - 3R_1$).

Current system/matrix:
$$
\begin{array}{r}x + y + 2z = 9\\ 2y - 7z = -17\\ 3x + 6y - 5z = 0 \end{array} \qquad \left[ \begin{array}{r r r | r}{1} & 1 & 2 & 9\\ 0 & 2 & {-7} & {-17}\\ 3 & 6 & {-5} & 0 \end{array} \right]
$$

Result:
$$
\begin{array}{r}x + y + 2z = 9\\ 2y - 7z = -17\\ (3-3)x + (6-3)y + (-5-6)z = 0-27 \end{array} \rightarrow \begin{array}{r}x + y + 2z = 9\\ 2y - 7z = -17\\ 3y - 11z = -27 \end{array}
$$
$$
\left[{\begin{array}{r r r | r}{1}&{1}&{2}&{9}\\ {0}&{2}&{-7}&{-17}\\ {0}&{3}&{-11}&{-27}\end{array}}\right]
$$

::: {.notes}
Continuing our elimination process, our next target is to eliminate $x$ from the *third* equation.
We apply a similar logic: multiply the first equation by -3 and add it to the third equation. This will make the $x$ coefficient in the third equation zero.

In the matrix, this means adding $-3$ times Row 1 to Row 3. This operation creates a zero in the first column of the third row as well. We are slowly getting the matrix into an upper triangular form, where coefficients below the main diagonal are zero. This structure makes it very easy to solve later using back-substitution.
:::

---

## Example 6: Using Elementary Row Operations (Step 3)

**Operation:** Multiply the second equation by $\frac{1}{2}$.  
**Matrix Operation:** Multiply the second row by $\frac{1}{2}$ (Notation: $R_2 \leftarrow \frac{1}{2}R_2$).

Current system/matrix:
$$
\begin{array}{r}x + y + 2z = 9\\ 2y - 7z = -17\\ 3y - 11z = -27 \end{array} \qquad \left[{\begin{array}{r r r | r}{1}&{1}&{2}&{9}\\ {0}&{2}&{-7}&{-17}\\ {0}&{3}&{-11}&{-27}\end{array}}\right]
$$

Result:
$$
\begin{array}{r}x + y + 2z = 9\\ \frac{1}{2}(2y - 7z) = \frac{1}{2}(-17)\\ 3y - 11z = -27 \end{array} \rightarrow \begin{array}{r}x + y + 2z = 9\\ y - \frac{7}{2} z = -\frac{17}{2}\\ 3y - 11z = -27 \end{array}
$$
$$
\left[{\begin{array}{r r r | r}{1}&{1}&{2}&{9}\\ {0}&{1}&{-{\frac{7}{2}}}&{-{\frac{17}{2}}}\\ {0}&{3}&{-11}&{-27}\end{array}}\right]
$$

::: {.notes}
With $x$ coefficients now zero in the second and third rows (equations), our next strategic move is to simplify the second equation. We want the coefficient of $y$ in the second equation to be 1. This is often called creating a "leading 1".

We achieve this by multiplying the entire second equation by $1/2$. In the matrix, this means multiplying Row 2 by $1/2$. This makes the leading entry in the second row equal to 1, which is a standard step in Gaussian elimination.
:::

---

## Example 6: Using Elementary Row Operations (Step 4)

**Operation:** Add $- 3$ times the second equation to the third.  
**Matrix Operation:** Add $- 3$ times the second row to the third row (Notation: $R_3 \leftarrow R_3 - 3R_2$).

Current system/matrix:
$$
\begin{array}{r}x + y + 2z = 9\\ y - \frac{7}{2} z = -\frac{17}{2}\\ 3y - 11z = -27 \end{array} \qquad \left[{\begin{array}{r r r | r}{1}&{1}&{2}&{9}\\ {0}&{1}&{-{\frac{7}{2}}}&{-{\frac{17}{2}}}\\ {0}&{3}&{-11}&{-27}\end{array}}\right]
$$

Result:
$$
\begin{array}{r}
x+y+2z=9\\ 
y-\frac{7}{2}z=-\frac{17}{2}\\ 
(3-3)y + (-11 - 3(-\frac{7}{2}))z = -27 - 3(-\frac{17}{2}) 
\end{array}
\rightarrow 
\begin{array}{r}
x+y+2z={9}\\
{y-\frac{7}{2}z=-\frac{17}{2}}\\
-\frac{1}{2}z=-\frac{3}{2}
\end{array}
$$
$$
\left[{\begin{array}{r r r | r}{1}&{1}&{2}&{9}\\ {0}&{1}&{-{\frac{7}{2}}}&{-{\frac{17}{2}}}\\ {0}&{0}&{-{\frac{1}{2}}}&{-{\frac{3}{2}}}\end{array}}\right]
$$

::: {.notes}
Now that the leading '1' is in place for $y$ in the second equation, we use it to eliminate $y$ from the *third* equation.
We perform the operation of adding $-3$ times the second equation to the third equation. This will make the $y$ coefficient in the third equation zero.

On the matrix side, this is applying $-3$ times Row 2 to Row 3. This completes the "forward elimination" phase of Gaussian elimination, yielding an upper triangular matrix. From this form, we can easily solve for $z$, then back-substitute to find $y$, then $x$. This process is often called **back-substitution**.
:::

---

## Example 6: Using Elementary Row Operations (Step 5)

**Operation:** Multiply the third equation by $- 2$.  
**Matrix Operation:** Multiply the third row by $- 2$ (Notation: $R_3 \leftarrow -2R_3$).

Current system/matrix:
$$
\begin{array}{r}x+y+2z=\begin{array}{r}{9}\\ {y-\frac{7}{2}z=-\frac{17}{2}}\end{array}\\ {-\frac{1}{2}z=-\frac{3}{2}}\end{array} \qquad \left[{\begin{array}{r r r | r}{1}&{1}&{2}&{9}\\ {0}&{1}&{-{\frac{7}{2}}}&{-{\frac{17}{2}}}\\ {0}&{0}&{-{\frac{1}{2}}}&{-{\frac{3}{2}}}\end{array}}\right]
$$

Result:
$$
\begin{array}{r}
x+y+2z=9\\ 
{y-\frac{7}{2}z=-\frac{17}{2}}\\ 
{(-2)(-\frac{1}{2}z)=(-2)}{(-\frac{3}{2})}
\end{array} 
\rightarrow 
\begin{array}{r}
x+y+2z=9\\ 
{y-\frac{7}{2}z=-\frac{17}{2}}\\ 
{z=3}
\end{array}
$$
$$
\left[{\begin{array}{r r r | r}{1}&{1}&{2}&{9}\\ {0}&{1}&{-{\frac{7}{2}}}&{-{\frac{17}{2}}}\\ {0}&{0}&{1}&{3}\end{array}}\right]
$$

::: {.notes}
Now we have our system in an "echelon form." We can see that $-1/2 z = -3/2$. To get a clean value for $z$, we multiply the third equation by -2. This results in $z=3$.
In the matrix, multiplying Row 3 by -2 gives us a leading '1' in the third row, which is desirable standard form for solving this system.
Now that we have $z=3$, we can use back-substitution.
:::

---

## Example 6: Using Elementary Row Operations (Step 6)

**Operation (Part 1):** Add $- 1$ times the second equation to the first.  
**Matrix Operation (Part 1):** ($R_1 \leftarrow R_1 - R_2$) to eliminate $y$ from the first equation.

Current system/matrix:

$$
\begin{array}{r}x + y + 2z = 9\\ y - \frac{7}{2} z = -\frac{17}{2}\\ z = 3 \end{array} \qquad \left[{\begin{array}{r r r | r}{1}&{1}&{2}&{9}\\ {0}&{1}&{-{\frac{7}{2}}}&{-{\frac{17}{2}}}\\ {0}&{0}&{1}&{3}\end{array}}\right]
$$

Resulting simplified terms in $R_1$: (This intermediate step is a conceptual one leading to `x = ... + z` related terms directly)

$$
\begin{array}{r}
{x + (1-1)y + (2-(-\frac{7}{2}))z}=9-(-\frac{17}{2})\\ 
{y-\frac{7}{2}z}=-\frac{17}{2}\\ 
{z}=3
\end{array} 
\rightarrow 
\begin{array}{r}
{x+\frac{11}{2}z}=\frac{35}{2}\\ 
{y-\frac{7}{2}z}=-\frac{17}{2}\\ 
{z}={3}
\end{array}
$$

$$
\left[{\begin{array}{r r r | r}{1}&{0}&{{\frac{11}{2}}}&{{\frac{35}{2}}}\\ {0}&{1}&{-{\frac{7}{2}}}&{-{\frac{17}{2}}}\\ {0}&{0}&{1}&{3}\end{array}}\right]
$$

::: {.notes}
This is the start of the "backward elimination" or reduction phase, aiming to put the matrix into "reduced row echelon form" (identity matrix on the left).
First, we want to eliminate the $y$ term from the first equation. We do this by adding $-1$ times the second equation to the first equation.
In matrix form ($R_1 \leftarrow R_1 - R_2$), this operation clears the $y$ coefficient in the first row. We are now closer to having a diagonal matrix on the left side, which directly gives us the values of $x, y, z$.
:::

---

## Example 6: Using Elementary Row Operations (Step 7)

**Operation (Part 2):** Substitute $z=3$ into the first two equations (or eliminate $z$ directly).  
Add $-\frac{11}{2}$ times the third equation to the first. ($R_1 \leftarrow R_1 - \frac{11}{2}R_3$)  
Add $\frac{7}{2}$ times the third equation to the second. ($R_2 \leftarrow R_2 + \frac{7}{2}R_3$)  

Current system/matrix:
$$
\begin{array}{r}
{x+\frac{11}{2}z=\frac{35}{2}}\\ 
{y-\frac{7}{2}z=-\frac{17}{2}}\\ 
{z=3}
\end{array} 
\qquad 
\left[{\begin{array}{r r r | r}{1}&{0}&{{\frac{11}{2}}}&{{\frac{35}{2}}}\\ {0}&{1}&{-{\frac{7}{2}}}&{-{\frac{17}{2}}}\\ {0}&{0}&{1}&{3}\end{array}}\right]
$$

Final Result:
$$
\begin{array}{r}
{x=1}\\ 
{y=2}\\ 
{z=3}
\end{array}
$$

$$
\left[{\begin{array}{r r r | r}{1}&{0}&{0}&{1}\\ {0}&{1}&{0}&{2}\\ {0}&{0}&{1}&{3}\end{array}}\right]
$$

The solution is $(x, y, z) = (1, 2, 3)$.

::: {.notes}
This is the final step in the systematic solution process, called **Gauss-Jordan elimination**. Already we know $z=3$. We now use this value to simplify the first two equations further.

We want to eliminate the $z$ terms from the first and second equations directly, making their coefficients zero.
For the first equation, we add $-\frac{11}{2}$ times the third equation to it.
For the second equation, we add $\frac{7}{2}$ times the third equation to it.

The amazing result is that we arrive at a matrix where the left side is the identity matrix. This directly tells us the solution: $x=1, y=2, z=3$. This methodical approach, using elementary row operations on the augmented matrix, is extremely efficient for solving large systems of linear equations and is the basis for many computational algorithms in engineering, such as those used in finite element analysis or optimization.
:::

---

## Interactive: Reduced Row Echelon Form Calculator

<iframe src="https://linear.zsrobinson.com/row-reduction" width="100%" height="500" frameborder="0" allowfullscreen></iframe>

---

## Conclusion

*   Linear equations are fundamental building blocks.
*   Systems of linear equations can have zero, one, or infinitely many solutions.
*   Augmented matrices provide a compact way to represent linear systems.
*   Elementary row operations are systematic tools to solve linear systems efficiently, preserving the solution set.

These concepts are crucial for solving problems in **control systems, signal processing, circuit analysis, optimization**, and many other areas in Electrical and Computer Engineering.

::: {.notes}
To summarize today's discussion:
We started by defining linear equations and understanding their distinct properties compared to non-linear ones.
We then introduced systems of linear equations and classified their solutions into three fundamental categories: no solution, exactly one solution, or infinitely many solutions.
Finally, we learned about augmented matrices as a powerful notation and elementary row operations as systematic tools to transform linear systems into simpler forms, making them solvable.

These foundational concepts are not just theoretical; they are the bedrock of many advanced topics you'll encounter in your ECE curriculum. From analyzing complex electrical circuits using Kirchhoff's laws, to designing controllers for robotic systems, to optimizing resource allocation in networks, linear algebra provides the essential mathematical framework.

In the next section, we'll formalize the process of using elementary row operations to solve systems, moving towards a robust algorithm called Gaussian elimination and Gauss-Jordan elimination.
Thank you!
:::
