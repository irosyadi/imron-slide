---
title: "Linear Algebra"
subtitle: "Markov Chains: Bridging Theory & ECE Applications"
author: "Imron Rosyadi"
format:
  live-revealjs:
    logo: "qrjs_assets/unsoed_logo.png"
    footer: "[irosyadi-2025](https://imron-slide.vercel.app)"
    slide-number: true
    chalkboard: true
    scrollable: true
    controls: true
    progress: true
    preview-links: false
    transition: fade
    incremental: false
    smaller: false
    theme: [default, qrjs_assets/ir_style.scss]
    mermaid:
        theme: neutral
pyodide:
  packages:
    - numpy
    - plotly
    - nbformat
---

# Linear Algebra
Markov Chains: Bridging Theory & ECE Applications

::: {.notes}
Welcome to this lecture on Markov Chains, a powerful concept in linear algebra with wide-ranging applications, especially in Electrical and Computer Engineering.
Today, we'll explore the theoretical foundations of Markov Chains and then dive into practical, hands-on demonstrations using Python.
Our goal is to understand how these mathematical models can describe and predict sequences of events, which is crucial for many ECE problems.
:::

---

## What are Markov Chains?

A Markov Chain is a mathematical system describing a sequence of random events.
It transitions from one state to another according to probabilistic rules.

### The Markov Property
The probability of transitioning to any particular state depends *solely* on the current state and time elapsed.
It does *not* depend on the sequence of states that preceded it.
This unique characteristic makes Markov processes **memoryless**.

::: {.notes}
At its core, a Markov Chain is about predicting the future based only on the present.
Think of it like a game where your next move only depends on your current position, not how you got there.
This 'memoryless' property is what simplifies complex systems into manageable probabilistic models.
We'll see how this concept is formalized and applied throughout this lecture.
:::

---

## Why Study Markov Chains in ECE?
Markov Chains have prolific usage across various fields, including many relevant to ECE:

-   **Control Systems:** Modeling cruise control in vehicles.
-   **Networking:** Analyzing queues or lines of customers/data packets.
-   **Financial Engineering:** Exchange rates of currencies.
-   **Search Algorithms:** PageRank algorithm (Google).
-   **Natural Language Processing:** Text generation (e.g., Whatsapp text simulator).

::: {.notes}
The versatility of Markov Chains makes them indispensable in ECE.
For example, in communication theory, they can model how a signal transitions between different states of distortion or error.
In network analysis, they help us understand traffic flow and congestion in queues.
And for those interested in AI and data science, PageRank is a foundational algorithm, and text generation demonstrates their power in modeling sequential data.
:::

---

## Discrete-Time Markov Chains (DTMC)

A Markov chain is a random process with the Markov property.
A random process (or stochastic process) is a collection of random variables.

### Key Characteristics
-   **Discrete State Space:** A countable set of possible values for the random variables.
-   **Discrete Index Set:** Often representing time steps (e.g., $t=0, 1, 2, \dots$).

The term "Markov chain" is typically reserved for a process with a discrete set of times, known as a **Discrete Time Markov Chain (DTMC)**.

::: {.notes}
When we talk about DTMCs, we're looking at systems that change states at specific, distinct moments in time.
This is different from continuous-time Markov chains, where transitions can happen at any instant.
The discrete nature of DTMCs makes them particularly amenable to computational analysis and modeling, which is why they are so prevalent in engineering.
:::

---

## The DTMC Markov Property in Detail

A DTMC involves a system in a certain state at each step, with the state changing randomly between steps.
The steps are often thought of as moments in time.

A DTMC is a sequence of random variables $X_1, X_2, X_3, \dots$ with the Markov property:
The probability of moving to the next state depends *only* on the present state and not on the previous states.

### Mathematical Formulation
$$
\text{Pr}( X_{n+1} = x \mid X_1 = x_1, X_2 = x_2, \dots, X_n = x_n) = \text{Pr}( X_{n+1} = x \mid X_n = x_n)
$$
This means the knowledge of the previous state ($X_n$) is all that is necessary to determine the probability distribution of the current state ($X_{n+1}$).
This is the rule of **conditional independence**.

### State Space $S$
The possible values of $X_i$ form a countable set $S$ called the **state space** of the chain.
$S$ can be anything: letters, numbers, sensor readings, or system modes.

::: {.notes}
This mathematical formula is the heart of the Markov property. It elegantly captures the 'memoryless' nature.
In an ECE context, imagine a digital communication channel. The probability of the next received bit being an error might only depend on the current channel condition, not on the entire history of past errors.
The state space defines all possible conditions or configurations our system can be in.
:::

---

## Modeling a Markov Chain: Key Components

A Markov chain is represented using a probabilistic automaton.
The changes of state of the system are called **transitions**.

### Transition Probabilities
The probabilities associated with various state changes.
A probabilistic automaton includes the probability of a given transition into the transition function.

### Transition Matrix
Denoted as $P$.
Every state in the state space is included once as a row and again as a column.
Each cell $P_{ij}$ tells you the probability of transitioning from state $i$ to state $j$.
If there are $N$ possible states, the matrix will be an $N \times N$ matrix.

---

## Modeling a Markov Chain: Key Components

### Stochastic Matrix Property
The transition matrix must be a **stochastic matrix**.
This means its entries in each row must add up to exactly 1.
$$
\sum_{j=1}^{N} P_{ij} = 1 \quad \text{for all } i
$$
This is because each row represents a complete probability distribution for transitions *from* that state.

### Initial Distribution $\pi$
A row vector $\pi = [\pi_1, \pi_2, \dots, \pi_N]$ where $\pi_i$ is the probability that the Markov chain starts in state $i$.
$\sum_{i=1}^{N} \pi_i = 1$.


::: {.notes}
Understanding these components is crucial for building and analyzing Markov models.
The transition matrix is a linear algebra construct that encapsulates all the probabilistic rules of our system.
For example, in a control system, if your system can be in "steady state", "overdamped", or "underdamped" states, the transition matrix would tell you the likelihood of moving between these states.
The initial distribution defines where our system starts, which is often known or can be estimated.
:::

---

## Key Properties of Markov Chains

Understanding these properties helps characterize the behavior of Markov chains.

**Reducibility**
A Markov chain is **irreducible** if it is possible to get to any state from any other state (maybe through several steps).
There exists a chain of steps between any two states with positive probability.

**Periodicity**
A state $i$ is **periodic** if the chain can return to the state only at multiples of some integer $k > 1$.
It's **aperiodic** if $k=1$.

**Transience and Recurrence**
A state $i$ is **transient** if, starting in $i$, there's a non-zero probability of never returning to $i$. The system might never return to this state.
A state $i$ is **recurrent** (or persistent) if it is not transient. The system is certain to return eventually

**Ergodicity**
A state $i$ is **ergodic** if it is aperiodic and positive recurrent.
If all states in an irreducible Markov chain are ergodic, the chain is **ergodic**.
Ergodic chains have a unique stationary distribution.

**Absorbing State**
A state $i$ is **absorbing** if it is impossible to leave this state.
Therefore, $P_{ii} = 1$ and $P_{ij} = 0$ for $i \ne j$. Once entered, the process cannot leave i.e. no outgoing transitions.
If every state can reach an absorbing state, the Markov chain is an **absorbing Markov chain**. 

::: {.notes}
These properties are vital for analyzing the long-term behavior of dynamic systems.
For example, in a control system, we might want to ensure that certain failure states are absorbing, meaning once the system enters them, it stays there.
Or we might want an ergodic system, which implies that over long periods, the system will spend a predictable proportion of its time in each state, regardless of the starting point. This is crucial for stability analysis.
:::

---

## Markov Assumption & Order

The Markov chain assumes that the occurrence of each event/observation is statistically dependent *only* on the previous one(s).

### First-Order Markov Chain (Bigram Model)
Predicting the probability of a future state depends only on the current observation.
For states $Q = \{q_1, \dots, q_n\}$,
$$
\text{Pr}(Q_n \mid Q_1, \dots, Q_{n-1}) = \text{Pr}(Q_n \mid Q_{n-1})
$$

### $m^{\text{th}}$-Order Markov Chain ($m$-gram Model)
Predicting the probability of a future observation depends only on the previous $m$ observations.

---

## Markov Assumption & Order

### Joint Probability Distribution
For a set of $n$ random variables $Q = \{q_1, \dots, q_n\}$, their joint probability is:
$$
\text{Pr}(Q_1, \dots, Q_n) = \text{Pr}(Q_1) \cdot \text{Pr}(Q_2 \mid Q_1) \cdot \text{Pr}(Q_3 \mid Q_1, Q_2) \cdots \text{Pr}(Q_n \mid Q_1, \dots, Q_{n-1})
$$

If the observations follow an $m^{\text{th}}$-order Markov chain, this simplifies to:
$$
\text{Pr}(Q_1, \dots, Q_n) = \text{Pr}(Q_1) \cdot \prod_{i=2}^{m} \text{Pr}(Q_i \mid Q_1, \dots, Q_{i-1}) \cdot \prod_{i=m+1}^{n} \text{Pr}(Q_i \mid Q_{i-m}, \dots, Q_{i-1})
$$

::: {.notes}
The order of a Markov chain dictates how much "memory" the system has.
A first-order chain is the simplest, only considering the immediate past.
Higher-order chains capture more complex dependencies, but also require more data to estimate transition probabilities.
This concept is directly applicable in signal processing for modeling sequences of data, or in machine learning for sequence prediction tasks like speech recognition or natural language processing.
:::

---

## Assumptions for 1st order and 2nd order Markov chains

![](https://i0.wp.com/www.gaussianwaves.com/gaussianwaves/wp-content/uploads/2020/03/Markov-chain-assumptions.png?w=406&ssl=1)

Source: [GaussianWaves](https://www.gaussianwaves.com/2020/03/markov-chains-simplified/)

---

## Example 1: EE Student's Sad Day Forecast
Let's consider a simple example to understand these concepts.
EE Student's sad day activities: Sleep, Run, Icecream.

### State Diagram
Visualizing the transitions and their probabilities:

```{mermaid}
graph TD
    A[Sleep] -->|0.6| B(Run)
    A -->|0.2| A
    A -->|0.2| C(Icecream)

    B[Run] -->|0.6| B
    B -->|0.1| A
    B -->|0.3| C

    C[Icecream] -->|0.7| B
    C -->|0.2| A
    C -->|0.1| C
```

**Probabilities:**

-   **From Sleep:** 60% Run, 20% Sleep, 20% Icecream.
-   **From Run:** 60% Run, 10% Sleep, 30% Icecream.
-   **From Icecream:** 70% Run, 20% Sleep, 10% Icecream.

::: {.notes}
This state diagram is a graphical representation of our Markov chain.
Each circle represents a state, and the arrows represent possible transitions between states.
The numbers on the arrows are the transition probabilities.
Notice how for each state, the probabilities on the outgoing arrows sum up to 1, reflecting the stochastic matrix property.
This diagram provides an intuitive way to grasp the system's dynamics.
:::

---

## Example 1: EE Student's Sad Day Forecast

Source: [DataCamp](https://www.datacamp.com/tutorial/markov-chains-python-tutorial)

![](https://media.datacamp.com/legacy/image/upload/f_auto,q_auto:best/v1523011817/state_diagram_pfkfld.png)

![](https://media.datacamp.com/legacy/image/upload/f_auto,q_auto:best/v1523011817/transition_matrix_gj27nq.png)

---

## Example 1: EE Student's Sad Day Transition Matrix

The Markov Chain has 3 possible states: Sleep (S), Run (R), Icecream (I).
So, the transition matrix will be a $3 \times 3$ matrix.

$$
P = \begin{pmatrix}
P_{SS} & P_{SR} & P_{SI} \\
P_{RS} & P_{RR} & P_{RI} \\
P_{IS} & P_{IR} & P_{II}
\end{pmatrix}
= \begin{pmatrix}
0.2 & 0.6 & 0.2 \\
0.1 & 0.6 & 0.3 \\
0.2 & 0.7 & 0.1
\end{pmatrix}
$$

**Rows sum to 1:**

-   Row 1 (Sleep): $0.2 + 0.6 + 0.2 = 1.0$
-   Row 2 (Run): $0.1 + 0.6 + 0.3 = 1.0$
-   Row 3 (Icecream): $0.2 + 0.7 + 0.1 = 1.0$

::: {.notes}
This matrix is the mathematical embodiment of our state diagram.
Each row corresponds to the "from" state, and each column corresponds to the "to" state.
For instance, the entry $P_{SR} = 0.6$ means there's a 60% chance of transitioning from Sleep to Run.
This matrix is fundamental for performing calculations and simulations with the Markov chain.
:::

---

## Practical Application: 2-Day Forecast

**Question:** Starting from the state: Sleep, what is the probability that EE Student will be running (state: Run) at the end of a sad 2-day duration?

Let's denote states as S (Sleep), R (Run), I (Icecream).
We start in S. We want to end in R after 2 days.

Possible paths from S to R in 2 days:

1.  S $\to$ S $\to$ R: Probability = $P_{SS} \times P_{SR} = 0.2 \times 0.6 = 0.12$
2.  S $\to$ R $\to$ R: Probability = $P_{SR} \times P_{RR} = 0.6 \times 0.6 = 0.36$
3.  S $\to$ I $\to$ R: Probability = $P_{SI} \times P_{IR} = 0.2 \times 0.7 = 0.14$

Total Probability = Sum of probabilities of all paths:
$0.12 + 0.36 + 0.14 = 0.62$

So, there is a **62% chance** that EE Student will move to state: Run after two days, if she started in state: Sleep.

::: {.notes}
This calculation demonstrates how we can use the transition probabilities to predict future states.
This is a direct application of matrix multiplication in linear algebra. If we had the initial distribution as a row vector and multiplied it by the transition matrix, and then by the transition matrix again, the resulting vector would give us the probabilities of being in each state after 2 steps.
This concept of multi-step transitions is critical in analyzing long-term behavior of systems.
:::

---

## Python Implementation: EE Student's Forecast Setup

Let's implement EE Student's sad day forecast using Python.
We'll use `numpy` for numerical operations and `random` for choices.

### Define States and Transition Matrix

```{pyodide}
import numpy as np
import random as rm

# The statespace
states = ["Sleep","Run","Icecream"] # Corrected order based on matrix

# Possible sequences of events (not directly used in the simple forecast, but good for understanding)
transitionName = [["SS","SR","SI"],["RS","RR","RI"],["IS","IR","II"]]

# Probabilities matrix (transition matrix)
# Rows: From Sleep, From Run, From Icecream
# Cols: To Sleep, To Run, To Icecream
transitionMatrix = [[0.2,0.6,0.2], # From Sleep: 20%S, 60%R, 20%I
                    [0.1,0.6,0.3], # From Run: 10%S, 60%R, 30%I
                    [0.2,0.7,0.1]] # From Icecream: 20%S, 70%R, 10%I

# Verify that rows sum to 1
if not (np.isclose(np.sum(transitionMatrix[0]), 1) and \
        np.isclose(np.sum(transitionMatrix[1]), 1) and \
        np.isclose(np.sum(transitionMatrix[2]), 1)):
    print("Somewhere, something went wrong. Transition matrix, perhaps?")
else:
    print("Transition matrix is properly defined.")
```

::: {.notes}
Here, we set up the basic parameters for our simulation.
The `states` list defines the labels for our system's conditions.
The `transitionMatrix` is a nested list that directly maps to the mathematical transition matrix we discussed.
The `np.isclose` check is a good practice to ensure our probabilities are correctly normalized, which is essential for a valid Markov chain model.
:::

---

## Python Implementation: Single Forecast

This function simulates a single path through the Markov chain for a given number of days.

```{pyodide}
# A function that implements the Markov model to forecast the state/mood.
def activity_forecast(days):
    # Choose the starting state (Fixed for this example as "Sleep")
    activityToday = "Sleep"
    print("Start state: " + activityToday)
    # Shall store the sequence of states taken. So, this only has the starting state for now.
    activityList = [activityToday]
  
    # Store probability of this specific path (not needed for simple forecast, but good for understanding)
    prob_path = 1.0 
  
    for _ in range(days): # Loop for 'days' number of transitions
        current_state_idx = states.index(activityToday)
      
        # Use np.random.choice to select the next state based on probabilities
        # The 'p' argument specifies the probability distribution for the choices
        next_state = np.random.choice(states, p=transitionMatrix[current_state_idx])
      
        # Update probability of the path (optional, for tracing specific path probability)
        next_state_idx = states.index(next_state)
        prob_path *= transitionMatrix[current_state_idx][next_state_idx]
      
        activityToday = next_state
        activityList.append(activityToday)
      
    print("Possible states sequence: " + str(activityList))
    print("End state after "+ str(days) + " days: " + activityToday)
    print("Probability of this specific sequence: " + str(prob_path))

# Function that forecasts the possible state for the next 2 days
activity_forecast(2)
```

::: {.notes}
In this code, `np.random.choice` is the key. It allows us to simulate the probabilistic transitions.
We provide it with the possible states and the probability distribution for the current state (which is a row from our `transitionMatrix`).
Each run of this function will give a *single* possible sequence of events.
The `prob_path` calculation shows the probability of *that specific sequence* occurring, not the overall probability of reaching a state.
:::

---

## Python Implementation: Simulating Multiple Forecasts

To approximate the overall probability (like our 62% calculation), we need to run the simulation many times due to the **Law of Large Numbers**.

```{pyodide}
# A modified function that returns the activity list for simulation
def activity_forecast_sim(days, start_state="Sleep"):
    activityToday = start_state
    activityList = [activityToday]
  
    for _ in range(days):
        current_state_idx = states.index(activityToday)
        next_state = np.random.choice(states, p=transitionMatrix[current_state_idx])
        activityToday = next_state
        activityList.append(activityToday)
      
    return activityList

# To save every activityList
list_activity = []
num_simulations = 10000

# Run simulations
for _ in range(num_simulations):
    list_activity.append(activity_forecast_sim(2, start_state="Sleep"))

# Count activities ending in state:'Run'
count_run_end = 0
for smaller_list in list_activity:
    if smaller_list[-1] == "Run": # Check the last state in the sequence
        count_run_end += 1

# Calculate the probability
percentage_run = (count_run_end / num_simulations) * 100
print(f"The probability of starting at state:'Sleep' and ending at state:'Run' after 2 days (simulated): {percentage_run:.2f}%")

# Compare with theoretical calculation
theoretical_prob = (0.2 * 0.6) + (0.6 * 0.6) + (0.2 * 0.7)
print(f"Theoretical probability: {theoretical_prob * 100:.2f}%")
```

::: {.notes}
Here, we leverage the power of simulation.
By running the `activity_forecast_sim` function many times (e.g., 10,000 iterations), we collect a large sample of possible 2-day sequences.
Then, we simply count how many of these sequences end in the 'Run' state.
As the number of simulations increases, this empirical probability will converge towards the theoretical probability of 62%, illustrating the Law of Large Numbers.
This approach is very useful when analytical solutions are complex or impossible.
:::

---

## Example 2: Driver Behavior Modeling

Source: [gaussianwaves.com](https://www.gaussianwaves.com/2020/03/markov-chains-simplified/)

We can model a car's behavior using Markov chains, crucial for autonomous systems and control.

### States
-   `accelerate` (State 1)
-   `constant speed` (State 2)
-   `idling` (State 3)
-   `brake` (State 4)

---

## Example 2: Driver Behavior Modeling

### State Diagram
Illustrating transitions between driver behaviors:

![](https://i0.wp.com/www.gaussianwaves.com/gaussianwaves/wp-content/uploads/2020/03/Markov-chain-example-using-driver-behavior-transition-probabilities-and-starting-probabilities.png?w=600&ssl=1)

```{mermaid}
graph TD
    A[accelerate] -->|0.3| A
    A -->|0.2| B(constant speed)
    A -->|0.5| D(brake)

    B[constant speed] -->|0.1| A
    B -->|0.4| B
    B -->|0.5| D

    C[idling] -->|0.8| A
    C -->|0.2| C

    D[brake] -->|0.4| A
    D -->|0.05| B
    D -->|0.5| C
    D -->|0.05| D
```

::: {.notes}
This model can be used in the design of adaptive cruise control systems or in researching driver assistance systems.
For instance, understanding the probability of a driver transitioning from 'constant speed' to 'brake' is vital for collision avoidance systems.
The Markov property here means that the driver's next action depends only on their current action, simplifying the prediction problem.
:::

---

## Driver Behavior Model: Transition & Initial Probabilities

### Transition Probability Matrix $P$
Each entry $P_{ij}$ represents the probability of transitioning from state $i$ to state $j$.

$$
P = \begin{pmatrix}
0.3 & 0.2 & 0.0 & 0.5 \\
0.1 & 0.4 & 0.0 & 0.5 \\
0.8 & 0.0 & 0.2 & 0.0 \\
0.4 & 0.05 & 0.5 & 0.05
\end{pmatrix}
$$

**Property:** $\sum_{j=1}^{N} P_{ij} = 1$ for each row $i$.

### Initial Probability Distribution $\pi$
The probability that the Markov chain starts in a given state.
$\pi = [\pi_{\text{accel}}, \pi_{\text{const}}, \pi_{\text{idle}}, \pi_{\text{brake}}]$

For example, if the car always starts from a stopped position:
$\pi = [0, 0, 0, 1]$ (i.e., 100% probability of starting in 'brake' state).

::: {.notes}
This matrix is a core component for simulating and analyzing the driver's behavior.
For instance, if we're designing an autonomous vehicle, this matrix could be learned from real driving data.
The initial probability distribution is equally important. If we know the car always starts from 'brake', it simplifies our initial conditions for simulation or analytical prediction.
These matrices define the complete dynamics of the system.
:::

---

## Python for Driver Behavior Model

Implementing the driver behavior model in Python.

```{pyodide}
import random

# Define a transition matrix for the Markov chain (dictionary format)
transition_matrix_driver = {
    'accelerate': {'accelerate': 0.3, 'constant speed': 0.2, 'idling': 0 , 'brake': 0.5 },
    'constant speed': {'accelerate': 0.1, 'constant speed': 0.4, 'idling': 0 , 'brake': 0.5 },
    'idling': {'accelerate': 0.8, 'constant speed': 0, 'idling': 0.2 , 'brake': 0 },
    'brake': {'accelerate': 0.4, 'constant speed': 0.05, 'idling': 0.5 , 'brake': 0.05 },
}

# Define starting probabilities for each state
starting_probabilities_driver = {'accelerate': 0, 'constant speed': 0, 'idling': 0, 'brake': 1}

# Choose the starting state randomly based on the starting probabilities
current_state_driver = random.choices(
    population=list(starting_probabilities_driver.keys()),
    weights=list(starting_probabilities_driver.values())
)[0]

print("Starting state:", current_state_driver)

# Generate a sequence of states using the transition matrix
num_iterations_driver = 10
print("\nGenerated sequence:")
for i in range(num_iterations_driver):
    print(current_state_driver)
    next_state_driver = random.choices(
        population=list(transition_matrix_driver[current_state_driver].keys()),
        weights=list(transition_matrix_driver[current_state_driver].values())
    )[0]
    current_state_driver = next_state_driver
```

::: {.notes}
This code block demonstrates an alternative way to represent the transition probabilities using dictionaries, which can be more readable for symbolic states.
The `random.choices` function is again used to simulate the transitions based on the defined probabilities.
Running this simulation multiple times would give you different sequences of driver behavior, which could be used to test or train autonomous driving algorithms.
It shows the flexibility of Python in modeling these probabilistic systems.
:::

---

## Example 3: Text Generation with Markov Chains

Markov chains are surprisingly effective for generating "readable" but nonsensical text, useful for predictive text and basic NLP tasks.

### Algorithm
1.  **Input Text:** Start with a body of text.
2.  **Build Model:** For every sequence of words (e.g., bigrams - pairs of words), record a list of all possible words that come after that sequence.
    -   Example: If "shalt not" is followed by "kill", "commit", "steal", etc., record these as possibilities for "shalt not".
3.  **Generate Text:**
    -   Start with an initial word sequence from the input.
    -   Randomly pick one of the possible next words based on the model.
    -   Shift the window (e.g., if using bigrams, the second word of the current pair becomes the first word of the next pair, and the newly generated word becomes the second).
    -   Repeat until desired length.

::: {.notes}
This application might seem abstract, but it's a foundational concept in sequence modeling.
For ECE students, think about how this relates to speech recognition, where a sequence of phonemes or words is predicted.
Or in communication systems, modeling sequences of symbols or codes.
The "memory" of the Markov chain (its order) directly impacts the coherence of the generated text.
:::

---

## Text Generation: Example & Data Structure

Consider the phrase: "Thou shalt not kill. Thou shalt not commit adultery."

### Bigram Model Data Structure (Simplified)
`{Current Word: [List of Next Possible Words]}`

```
"shalt": ["not", "not", ...]
"not": ["kill.", "commit", "steal.", ...]
"kill.": ["Thou"]
"Thou": ["shalt", "shalt", ...]
...
```

**Punctuation and Capitalization:** Keeping punctuation and capitalization with words simplifies splitting and allows generated text to retain these features naturally.

**Small Input Limitation:** With small input texts, the generated output often closely resembles or is a verbatim copy of the input. Larger texts yield more interesting and varied results.

::: {.notes}
This simplified data structure highlights how the model is built.
The `defaultdict(list)` in Python is perfect for this, automatically creating a list for each new key and appending subsequent words.
The frequency of words in the `[List of Next Possible Words]` implicitly provides probabilities for `random.choice`.
This approach cleverly handles sentence structure and even basic grammar by learning from the input text itself.
:::

---

## Text Generation: Python Implementation

```{pyodide}
import random
from collections import defaultdict

def build_bigram_model(text):
    words = text.split()
    bigram_model = defaultdict(list)
  
    # Iterate through words to build the bigram model
    for i in range(len(words) - 1):
        curr_word = words[i]
        next_word = words[i + 1]
        bigram_model[curr_word].append(next_word)
  
    return bigram_model

def generate_text(bigram_model, start_word, length=10):
    word = start_word
    result = [word]
  
    for _ in range(length - 1):
        # Get possible next words from the model
        next_words = bigram_model.get(word)
        if not next_words:
            # Stop if no continuation found for the current word
            break
        # Randomly choose the next word
        word = random.choice(next_words)
        result.append(word)
  
    return " ".join(result)

# Example Usage: Simple ECE-related sentence
text_ece = "Linear algebra is fundamental for electrical engineering and computer engineering because linear algebra is used in signal processing and control systems."
bigram_model_ece = build_bigram_model(text_ece)

print("Bigram Model (ECE Example):")
# Print a subset of the model for brevity and clarity
for k, v in list(bigram_model_ece.items())[:5]: # Show first 5 entries
    print(f"  '{k}': {v}")

print("\nGenerated Text (ECE Example):")
generated_ece = generate_text(bigram_model_ece, "Linear", length=15)
print(generated_ece)
```

::: {.notes}
This Python code provides a concrete implementation of the text generation Markov chain.
The `build_bigram_model` function populates our `defaultdict` by iterating through the input text.
The `generate_text` function then uses this model to produce new sequences, making random choices at each step.
Notice how `random.choice` is implicitly using the frequencies of words in the list to determine probabilities.
This simple yet powerful technique illustrates how probabilistic models can generate complex outputs.
:::
