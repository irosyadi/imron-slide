<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/quarto-contrib/live-runtime/live-runtime.js" type="module"></script>
<link href="../site_libs/quarto-contrib/live-runtime/live-runtime.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.25">

  <meta name="author" content="Imron Rosyadi">
  <title>Machine Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-b0356e9119c1bdfd0db189d130feb51c.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script type="module" src="../site_libs/quarto-ojs/quarto-ojs-runtime.js"></script>
  <link href="../site_libs/quarto-ojs/quarto-ojs.css" rel="stylesheet">
  <meta name="mermaid-theme" content="neutral">
  <script src="../site_libs/quarto-diagram/mermaid.min.js"></script>
  <script src="../site_libs/quarto-diagram/mermaid-init.js"></script>
  <link href="../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Machine Learning</h1>
  <p class="subtitle">1.7 Neural Networks Part 3: Learning and Evaluation</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Imron Rosyadi 
</div>
</div>
</div>

</section>
<section>
<section id="neural-networks-learning-optimization-dynamics-of-training-and-fine-tuning" class="title-slide slide level1 center">
<h1>Neural Networks: Learning &amp; Optimization: Dynamics of Training and Fine-tuning</h1>

</section>
<section id="sources" class="slide level2">
<h2>Sources</h2>
<p><a href="https://cs231n.stanford.edu/">Stanford University CS231n: Deep Learning for Computer Vision</a></p>
<p><a href="https://cs231n.github.io/">CS231n Deep Learning for Computer Vision</a></p>
</section>
<section id="table-of-contents" class="slide level2 scrollable">
<h2>Table of Contents</h2>
<ol type="1">
<li><a href="#/quick">Quick Intro: Linear vs NN</a></li>
<li><a href="#/intro">Modeling One Neuron</a>
<ul>
<li><a href="#/bio">Biological Motivation</a></li>
<li><a href="#/classifier">Single Neuron as Linear Classifier</a></li>
<li><a href="#/actfun">Activation Functions</a></li>
</ul></li>
<li><a href="#/nn">Neural Network Architectures</a>
<ul>
<li><a href="#/layers">Layer-wise Organization</a></li>
<li><a href="#/feedforward">Feed-Forward Computation</a></li>
<li><a href="#/power">Representational Power</a></li>
<li><a href="#/arch">Setting Layers &amp; Sizes</a></li>
</ul></li>
<li><a href="#/summary-p1">Summary P1</a></li>
</ol>
</section>
<section id="table-of-contents-part-2" class="slide level2 scrollable">
<h2>Table of Contents (Part 2)</h2>
<ol start="5" type="1">
<li><a href="#/intro-p2">Setting up the Data and Model</a>
<ul>
<li><a href="#/datapre">Data Preprocessing</a></li>
<li><a href="#/init">Weight Initialization</a></li>
<li><a href="#/batchnorm">Batch Normalization</a></li>
<li><a href="#/reg">Regularization</a></li>
</ul></li>
<li><a href="#/losses">Loss Functions</a></li>
<li><a href="#/summary-p2">Summary P2</a></li>
</ol>
</section>
<section id="table-of-contents-continued" class="slide level2 scrollable">
<h2>Table of Contents (Continued)</h2>
<ol start="8" type="1">
<li><a href="#/learning">Learning Process</a>
<ul>
<li><a href="#/gradcheck">Gradient Checks</a></li>
<li><a href="#/sanitycheck">Sanity Checks</a></li>
<li><a href="#/baby">Babysitting the Learning Process</a></li>
</ul></li>
<li><a href="#/update">Parameter Updates</a>
<ul>
<li><a href="#/sgd">First-Order Methods</a></li>
<li><a href="#/anneal">Learning Rate Annealing</a></li>
<li><a href="#/ada">Adaptive Learning Rates</a></li>
</ul></li>
<li><a href="#/hyper">Hyperparameter Optimization</a></li>
<li><a href="#/ensemble">Evaluation: Model Ensembles</a></li>
<li><a href="#/summary-p3">Summary P3</a></li>
<li><a href="#/add">Additional References</a></li>
</ol>
</section>
<section id="the-learning-process" class="slide level2">
<h2><a name="learning"></a>8. The Learning Process</h2>
<p>We’ve covered the static aspects: network architecture, data setup, and loss functions. Now, we dive into the <strong>dynamics</strong>: the process of learning the parameters and fine-tuning hyperparameters.</p>
<p>This section covers:</p>
<ul>
<li><strong>Gradient Checks</strong>: Verifying the correctness of backpropagation.</li>
<li><strong>Sanity Checks</strong>: Quick tests before expensive training.</li>
<li><strong>Babysitting</strong>: Monitoring training progress for insights.</li>
</ul>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>The learning process is an iterative optimization endeavor, where parameters are adjusted to minimize the loss function.</p>
</div>
</div>
</div>
</section>
<section id="gradient-checks-verifying-backpropagation" class="slide level2">
<h2><a name="gradcheck"></a>8.1 Gradient Checks: Verifying Backpropagation</h2>
<p>Ensuring your analytically derived gradients match numerical approximations.</p>
<h3 id="centered-difference-formula">Centered Difference Formula</h3>
<ul>
<li><strong>Bad (Do not use):</strong> \( = O(h) \)</li>
<li><strong>Good (Use instead):</strong> \( = O(h^2) \)
<ul>
<li>More accurate, worth the 2x computational cost.</li>
</ul></li>
</ul>
<h3 id="relative-error-for-comparison">Relative Error for Comparison</h3>
<ul>
<li>Compare numerical gradient (\(f’_n\)) and analytic gradient (\(f’_a\)) using: <span class="math display">\[ \frac{\mid f'_a - f'_n \mid}{\max(\mid f'_a \mid, \mid f'_n \mid)} \]</span></li>
<li><strong>Interpretation:</strong>
<ul>
<li><code>&gt; 1e-2</code>: Probably wrong.</li>
<li><code>1e-2</code> to <code>1e-4</code>: Uncomfortable, often indicates a mistake.</li>
<li><code>1e-4</code> to <code>1e-7</code>: Acceptable, especially with “kinks” or deep networks.</li>
<li><code>&lt; 1e-7</code>: Excellent!</li>
</ul></li>
</ul>
<aside class="notes">
<p>Gradient checking is a critical debugging step for neural networks. It verifies that your manually implemented backpropagation algorithm (analytic gradient calculation) is indeed correct by comparing its output to a numerical approximation of the gradient.</p>
<p>The centered difference formula is preferred for its significantly higher accuracy. While it requires two evaluations of the loss function instead of one per parameter, its Taylor series expansion reveals an error on the order of <code>h^2</code>, which is much better than the <code>O(h)</code> error of the simpler forward difference formula.</p>
<p>For comparing the gradients, direct absolute difference is unreliable. The <em>relative error</em> metric provides a robust way to determine similarity, as it accounts for the magnitude of the gradients themselves. I’ve provided practical thresholds for interpreting the relative error. Remember that for deeper networks, errors can accumulate, so a slightly higher threshold might be acceptable for earlier layers.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="gradient-check-practical-considerations" class="slide level2">
<h2>Gradient Check: Practical Considerations</h2>
<ul>
<li><strong>Use double precision:</strong> Often reduces relative error significantly (e.g., from 1e-2 to 1e-8).</li>
<li><strong>Active range of floating point:</strong> Ensure gradients are not extremely small (e.g., ~1e-10). Temporarily scale loss if needed.</li>
<li><strong>Kinks in the objective:</strong> Non-differentiable points (e.g., ReLU, SVM loss) can cause discrepancies.
<ul>
<li>If \((f(x+h)\) and \(f(x-h))\) cross a kink (e.g., a ReLU switches from 0 to positive), the numerical gradient will be inaccurate.</li>
</ul></li>
<li><strong>Use few data points:</strong> Reduces the chance of hitting kinks in loss functions with many elements (e.g., SVM). Faster checks too.
<ul>
<li>Check with ~2-3 datapoints for robust results.</li>
</ul></li>
<li><strong>Careful with step size <code>h</code>:</strong> Not always smaller is better. Too small can lead to numerical precision issues. Try <code>1e-4</code> or <code>1e-6</code>.</li>
<li><strong>“Characteristic” mode:</strong> Perform gradcheck after a short “burn-in” training period. Random initialization might hit pathological edge cases.</li>
<li><strong>Regularization:</strong> Turn off regularization when checking data loss, then check regularization loss independently.
<ul>
<li>Reg loss has simpler gradients; an issue in data loss might be masked.</li>
</ul></li>
<li><strong>Turn off non-determinism:</strong> Disable dropout, random data augmentations during gradcheck.</li>
<li><strong>Check few dimensions:</strong> For large parameter vectors, sample dimensions but ensure even coverage across all parameter types (weights, biases for each layer).</li>
</ul>
<aside class="notes">
<p>Gradient checking is a powerful tool, but it comes with several caveats. These practical considerations are crucial for successfully debugging your backpropagation implementation:</p>
<ul>
<li><strong>Double Precision:</strong> Floating point arithmetic subtly affects numerical stability. Using <code>float64</code> (double precision) provides higher accuracy and can noticeably improve your relative error values.</li>
<li><strong>Floating-Point Range:</strong> Gradients that are extremely small (e.g., 1e-10) can also suffer from precision issues. Temporarily scaling your loss to bring gradients into a more “dense” floating-point range (around 1.0) can help.</li>
<li><strong>Kinks:</strong> Functions like ReLU are not differentiable everywhere. If your <code>x+h</code> and <code>x-h</code> evaluations straddle one of these “kinks,” your numerical approximation will be incorrect. Checking with a small number of data points typically mitigates this by reducing the number of kinks.</li>
<li><strong>Step Size <code>h</code>:</strong> There’s an optimal <code>h</code>. Too small, and you hit floating-point precision limits. Too large, and your approximation is poor. Experiment.</li>
<li><strong>“Characteristic” Mode:</strong> Don’t gradcheck at iteration 0 with random weights. Train for a few iterations until the loss starts decreasing, then check. This avoids edge cases of initial randomness that might mask errors.</li>
<li><strong>Regularization:</strong> Check your data loss gradient <em>first</em> with regularization strength set to zero. Then, separately, check the regularization gradient. The simple nature of regularization gradients can mask complex data loss bugs.</li>
<li><strong>Non-Determinism:</strong> Features like Dropout or random data augmentation introduce randomness. Disable them during gradient checks, or ensure you fix the random seed for <em>both</em> <code>f(x+h)</code> and <code>f(x-h)</code> evaluations, as well as for the analytic gradient.</li>
<li><strong>Dimensionality:</strong> For networks with millions of parameters, you can’t check every single one. Sample a few, but ensure you test parameters from <em>all</em> parts of your network (e.g., biases, different layers’ weights).</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="interactive-gradient-check-example" class="slide level2 scrollable">
<h2>Interactive Gradient Check Example</h2>
<p>Compare analytic vs.&nbsp;numerical gradient for \(f(x) = x^2\) or \(f(x) = (0, x)\). Adjust <code>x_value</code> and <code>h_step_size</code> to see their impact on accuracy.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code hidden" id="cb1" data-startfrom="165" data-source-offset="-0"><pre class="sourceCode numberSource js number-lines code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 164;"><span id="cb1-165"><a></a>viewof x_value <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="op">-</span><span class="dv">5</span><span class="op">,</span> <span class="dv">5</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="fl">1.0</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Input x Value"</span>})<span class="op">;</span></span>
<span id="cb1-166"><a></a>viewof h_step_size <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="fl">1e-7</span><span class="op">,</span> <span class="fl">1e-2</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="fl">1e-5</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">1e-7</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"h (Step Size)"</span>})<span class="op">;</span></span>
<span id="cb1-167"><a></a>viewof function_choice <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">select</span>([<span class="st">"x_squared"</span><span class="op">,</span> <span class="st">"relu"</span>]<span class="op">,</span> {<span class="dt">label</span><span class="op">:</span> <span class="st">"Function to check"</span>})<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-2" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-3" data-nodetype="declaration">

</div>
</div>
</div>
</div>
<div>
<div id="pyodide-1" class="exercise-cell">

</div>
<script type="pyodide-1-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWUsImlucHV0IjpbInhfdmFsdWUiLCJoX3N0ZXBfc2l6ZSIsImZ1bmN0aW9uX2Nob2ljZSJdLCJtYXgtbGluZXMiOjEwfSwiY29kZSI6ImltcG9ydCBudW1weSBhcyBucFxuXG54ID0geF92YWx1ZVxuaCA9IGhfc3RlcF9zaXplXG5cbmRlZiBmX3hfc3F1YXJlZCh2YWwpOlxuICAgIHJldHVybiB2YWwqKjJcblxuZGVmIGRmX3hfc3F1YXJlZCh2YWwpOlxuICAgIHJldHVybiAyICogdmFsXG5cbmRlZiBmX3JlbHUodmFsKTpcbiAgICByZXR1cm4gbWF4KDAsIHZhbClcblxuZGVmIGRmX3JlbHUodmFsKTpcbiAgICByZXR1cm4gMSBpZiB2YWwgPj0gMCBlbHNlIDBcblxuY3VycmVudF9mID0gTm9uZVxuY3VycmVudF9kZiA9IE5vbmVcbmlmIGZ1bmN0aW9uX2Nob2ljZSA9PSBcInhfc3F1YXJlZFwiOlxuICAgIGN1cnJlbnRfZiA9IGZfeF9zcXVhcmVkXG4gICAgY3VycmVudF9kZiA9IGRmX3hfc3F1YXJlZFxuZWxpZiBmdW5jdGlvbl9jaG9pY2UgPT0gXCJyZWx1XCI6XG4gICAgY3VycmVudF9mID0gZl9yZWx1XG4gICAgY3VycmVudF9kZiA9IGRmX3JlbHVcblxuIyBBbmFseXRpYyBncmFkaWVudFxuZl9hID0gY3VycmVudF9kZih4KVxuXG4jIE51bWVyaWNhbCBncmFkaWVudCAoY2VudGVyZWQgZm9ybXVsYSlcbmZfbiA9IChjdXJyZW50X2YoeCArIGgpIC0gY3VycmVudF9mKHggLSBoKSkgLyAoMiAqIGgpXG5cbiMgQ2FsY3VsYXRlIHJlbGF0aXZlIGVycm9yXG5pZiBhYnMoZl9hKSA8IDFlLTEwIGFuZCBhYnMoZl9uKSA8IDFlLTEwOiAjIEJvdGggYXJlIHZlcnkgY2xvc2UgdG8gemVyb1xuICAgIHJlbGF0aXZlX2Vycm9yID0gMC4wICMgUGFzc1xuZWxpZiBtYXgoYWJzKGZfYSksIGFicyhmX24pKSA+IDA6XG4gICAgcmVsYXRpdmVfZXJyb3IgPSBhYnMoZl9hIC0gZl9uKSAvIG1heChhYnMoZl9hKSwgYWJzKGZfbikpXG5lbHNlOiAjIFNob3VsZCBub3QgaGFwcGVuIGlmIHByZXZpb3VzIG1heCBjaGVjayA+IDBcbiAgICByZWxhdGl2ZV9lcnJvciA9IGZsb2F0KCdpbmYnKVxuXG5wcmludChmXCJGdW5jdGlvbjogZih4KSA9IHtmdW5jdGlvbl9jaG9pY2V9XCIpXG5wcmludChmXCJJbnB1dCB4OiB7eDouMmV9XCIpXG5wcmludChmXCJTdGVwIHNpemUgaDoge2g6LjJlfVwiKVxucHJpbnQoZlwiQW5hbHl0aWMgR3JhZGllbnQgKGZfYSk6IHtmX2E6LjZlfVwiKVxucHJpbnQoZlwiTnVtZXJpY2FsIEdyYWRpZW50IChmX24pOiB7Zl9uOi42ZX1cIilcbnByaW50KGZcIlJlbGF0aXZlIEVycm9yOiB7cmVsYXRpdmVfZXJyb3I6LjZlfVwiKVxuXG5pZiByZWxhdGl2ZV9lcnJvciA8IDFlLTc6XG4gICAgc3RhdHVzID0gXCJFeGNlbGxlbnQhXCJcbmVsaWYgcmVsYXRpdmVfZXJyb3IgPCAxZS00OlxuICAgIHN0YXR1cyA9IFwiQWNjZXB0YWJsZSAod2l0aCBraW5rcy9kZWVwIG5ldHdvcmtzLCBvciBmb3IgeF9zcXVhcmVkIGlmIHN0cmljdCBlcnJvciBpcyBuZWVkZWQpXCJcbmVsaWYgcmVsYXRpdmVfZXJyb3IgPCAxZS0yOlxuICAgIHN0YXR1cyA9IFwiVW5jb21mb3J0YWJsZSAoY2hlY2sgaW1wbGVtZW50YXRpb24pXCJcbmVsc2U6XG4gICAgc3RhdHVzID0gXCJQcm9iYWJseSB3cm9uZyAoY2hlY2sgaW1wbGVtZW50YXRpb24pXCJcblxucHJpbnQoZlwiU3RhdHVzOiB7c3RhdHVzfVwiKSJ9
</script>
</div>
<aside class="notes">
<p>This interactive block demonstrates the gradient checking process for two simple functions: \(f(x) = x^2\) (always differentiable) and \(f(x) = (0, x)\) (ReLU, with a kink at 0).</p>
<ul>
<li><strong><code>Input x Value</code></strong>: The point at which to compute the gradient.</li>
<li><strong><code>h (Step Size)</code></strong>: The small perturbation used for the numerical approximation.</li>
<li><strong><code>Function to check</code></strong>: Select between <code>x_squared</code> and <code>relu</code>.</li>
</ul>
<p><strong>Experiment and Observe:</strong></p>
<ol type="1">
<li><strong>For <code>x_squared</code></strong>: Change <code>x_value</code> and <code>h_step_size</code>. You should consistently get very small relative errors, demonstrating accurate gradient calculation for a smooth function.</li>
<li><strong>For <code>relu</code></strong>:
<ul>
<li>Try <code>x_value = 1.0</code> (positive region). The relative error should be small, as the ReLU is differentiable here.</li>
<li>Try <code>x_value = -1.0</code> (negative region). The relative error should also be small.</li>
<li>Now, try <code>x_value = 0.0</code> (the kink). You’ll likely see the relative error increase significantly. This is because <code>f(x+h)</code> (e.g., <code>f(0+h) = h</code>) and <code>f(x-h)</code> (e.g., <code>f(0-h) = 0</code> if <code>h</code> positive) cause a discontinuity in the theoretical derivative, and the numerical approximation struggles. This illustrates the “kinks” problem in gradient checking.</li>
<li>Also, try playing with <code>h_step_size</code>: Notice how extremely small <code>h</code> can sometimes lead to <em>higher</em> relative errors due to floating-point precision limits.</li>
</ul></li>
</ol>
<p>This direct interaction helps to build intuition about the details and challenges of gradient checking in practice.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="sanity-checks-before-expensive-optimization" class="slide level2 scrollable">
<h2><a name="sanitycheck"></a>8.2 Sanity Checks: Before Expensive Optimization</h2>
<p>Run these quick checks before full training to save time and identify fundamental issues.</p>
<ol type="1">
<li><strong>Check for correct loss at chance performance.</strong>
<ul>
<li>Set regularization to zero.</li>
<li>With small parameters, verify initial loss matches theoretical chance performance.</li>
<li><strong>Example (CIFAR-10):</strong>
<ul>
<li>Softmax (10 classes): Expected initial loss \( - (0.1) \).</li>
<li>SVM (10 classes): Expected initial loss \( 9 \) (margin = 1 for 9 wrong classes).</li>
</ul></li>
<li>If loss is far off, check data loading, labels, and initialization.</li>
</ul></li>
<li><strong>Increasing regularization strength should increase the loss.</strong>
<ul>
<li>A simple test: introduce a large L2 regularization, and the total loss should go up. If not, regularization is incorrectly implemented or not applied.</li>
</ul></li>
<li><strong>Overfit a tiny subset of data.</strong>
<ul>
<li>Select a very small portion of your training data (e.g., 5-20 examples).</li>
<li>Set regularization to zero.</li>
<li>Train the network until it achieves <strong>zero cost</strong> (or near zero, e.g., 100% training accuracy).</li>
<li>If you can’t overfit a small dataset, your network (or code) has a fundamental bug. <strong>Do not proceed to full dataset training.</strong></li>
</ul></li>
</ol>
<div class="callout callout-caution callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Caution</strong></p>
</div>
<div class="callout-content">
<p>Even if you overfit a small dataset, ensure your features are not random due to a bug. Random features will overfit but generalize poorly on the full dataset.</p>
</div>
</div>
</div>
<aside class="notes">
<p>These sanity checks are a lifeline. They are quick, cheap tests that can catch major errors in your model setup before you spend hours or days running expensive training jobs.</p>
<ol type="1">
<li><strong>Chance Loss:</strong> This is a surprisingly effective check. If your loss doesn’t start at the expected random baseline, something is fundamentally wrong with how you’re computing loss, loading data, or initializing. For a 10-class problem, a random guess for the correct class is 10%, hence <code>-log(0.1)</code>. For SVM, with tiny scores, all incorrect classes will violate the margin, leading to <code>(num_classes - 1) * margin</code>.</li>
<li><strong>Regularization Impact:</strong> This confirms that your regularization term is actually wired into your loss function and that its gradient is being correctly computed (though not as exhaustively as gradient check).</li>
<li><strong>Overfitting a Small Subset:</strong> This is the most crucial test. A correctly implemented neural network, even a complex one, should be able to perfectly memorize a tiny number of examples if given enough capacity. If it cannot, there is almost certainly a bug in your forward pass, backward pass, or optimizer. Only after passing this, should you expand to the full dataset and consider regularization.</li>
</ol>
<p>The caution about random features is important: if your data pipeline is subtly broken and feeding random noise as input features, your model might still overfit a small set of <em>that</em> random noise, but it will never learn useful patterns from your real data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="babysitting-the-learning-process" class="slide level2">
<h2><a name="baby"></a>8.3 Babysitting the Learning Process</h2>
<p>Monitoring key metrics during training provides invaluable insights into hyperparameter tuning. All plots below typically have <strong>epochs</strong> on the x-axis.</p>
<h3 id="loss-function-plot">Loss Function Plot</h3>
<ul>
<li>Track loss on individual batches.</li>
<li><strong>Wiggle</strong>: Related to batch size (smaller batches = more wiggle).</li>
<li><strong>Shape:</strong>
<ul>
<li><strong>Low LR:</strong> Linear, slow decay.</li>
<li><strong>High LR:</strong> Exponential decay, then plateaus at higher loss (bouncing chaotically).</li>
</ul></li>
<li>Consider plotting in <strong>log domain</strong> to better visualize progress and compare models.</li>
</ul>
<div class="fig figcenter fighighlight">
<img src="https://cs231n.github.io/assets/nn3/learningrates.jpeg" width="49%"> <img src="https://cs231n.github.io/assets/nn3/loss.jpeg" width="49%">
<div class="figcaption">
<pre><code>&lt;b&gt;Left:&lt;/b&gt; Effect of different learning rates on loss. High LR plateaus faster at worse loss values due to chaotic bouncing.
&lt;b&gt;Right:&lt;/b&gt; Typical loss function over time. Some noise suggests a small batch size.</code></pre>
</div>
</div>
<aside class="notes">
<p>“Babysitting” the learning process refers to the continuous monitoring of various metrics during training. These plots are your primary diagnostic tools for understanding what your network is doing and how well it’s learning.</p>
<p>The loss function plot is typically the first thing you’ll look at.</p>
<ul>
<li><strong>Wiggle:</strong> The jaggedness or “wiggle” in the loss curve is directly related to the batch size. Smaller batches lead to noisier gradients and thus more variation in the loss per iteration. A perfectly smooth loss might indicate a very large batch size or a constant loss.</li>
<li><strong>Shape and Learning Rate:</strong> This is critical.
<ul>
<li>If your learning rate is too low, the loss will decrease very slowly, often in a nearly linear fashion.</li>
<li>If your learning rate is too high, the loss might initially drop quickly but then start oscillating wildly or even increase. A very high learning rate can lead to the loss becoming <code>NaN</code> (Not a Number) because updates are too large and explode parameters. The provided cartoon (left image) clearly illustrates these behaviors.</li>
</ul></li>
<li><strong>Logarithmic Scale:</strong> Plotting the loss on a logarithmic y-axis can reveal details of sustained improvement that are hard to see on a linear scale, especially when the loss has decreased by several orders of magnitude. It also makes comparing the decay rates of different models easier.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="interactive-learning-rate-loss-simulation" class="slide level2">
<h2>Interactive Learning Rate Loss Simulation</h2>
<p>Observe how different learning rates affect the simulated loss curve. Adjust the <code>learning_rate</code> to see its impact.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code hidden" id="cb3" data-startfrom="336" data-source-offset="-0"><pre class="sourceCode numberSource js number-lines code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 335;"><span id="cb3-336"><a></a>viewof learning_rate <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="fl">0.0001</span><span class="op">,</span> <span class="fl">0.1</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="fl">0.01</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.0001</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Learning Rate"</span>})<span class="op">;</span></span>
<span id="cb3-337"><a></a>viewof noise_level <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="fl">0.0</span><span class="op">,</span> <span class="fl">0.5</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="fl">0.1</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.01</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Noise Level (Batch Size proxy)"</span>})<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-2-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-2-2" data-nodetype="declaration">

</div>
</div>
</div>
</div>
<div>
<div id="pyodide-2" class="exercise-cell">

</div>
<script type="pyodide-2-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWUsImlucHV0IjpbImxlYXJuaW5nX3JhdGUiLCJub2lzZV9sZXZlbCJdLCJtYXgtbGluZXMiOjEwfSwiY29kZSI6ImltcG9ydCBudW1weSBhcyBucFxuaW1wb3J0IHBsb3RseS5ncmFwaF9vYmplY3RzIGFzIGdvXG5cbmxyID0gbGVhcm5pbmdfcmF0ZVxubm9pc2UgPSBub2lzZV9sZXZlbFxubnVtX2Vwb2NocyA9IDUwXG5pbml0aWFsX2xvc3MgPSAyLjVcblxubG9zc2VzID0gW2luaXRpYWxfbG9zc11cbmN1cnJlbnRfbG9zcyA9IGluaXRpYWxfbG9zc1xucGFyYW1zID0gMC41ICMgQSBkdW1teSBwYXJhbWV0ZXIgdG8gb3B0aW1pemVcblxuIyBTaW11bGF0ZSBhIHNpbXBsZSBxdWFkcmF0aWMgbG9zcyBmdW5jdGlvbjogKHBhcmFtIC0gdGFyZ2V0KSoqMlxuIyBUYXJnZXQgPSAwIGZvciBzaW1wbGljaXR5XG50YXJnZXQgPSAwXG5cbmZvciBlcG9jaCBpbiByYW5nZShudW1fZXBvY2hzKTpcbiAgICAjIFNpbXVsYXRlIGdyYWRpZW50IChlLmcuLCAyICogKHBhcmFtIC0gdGFyZ2V0KSlcbiAgICBncmFkID0gMiAqIChwYXJhbXMgLSB0YXJnZXQpXG4gIFxuICAgICMgQWRkIHNvbWUgXCJub2lzZVwiIHJlZmxlY3RpbmcgYmF0Y2ggc3RhdGlzdGljc1xuICAgIGdyYWRfbm9pc3kgPSBncmFkICsgbnAucmFuZG9tLm5vcm1hbCgwLCBub2lzZSlcbiAgXG4gICAgIyBVcGRhdGUgZHVtbXkgcGFyYW1ldGVyXG4gICAgcGFyYW1zIC09IGxyICogZ3JhZF9ub2lzeVxuICBcbiAgICAjIFNpbXVsYXRlIGN1cnJlbnQgbG9zc1xuICAgIHNpbXVsYXRlZF9sb3NzID0gKHBhcmFtcyAtIHRhcmdldCkqKjJcbiAgXG4gICAgIyBFbnN1cmUgbG9zcyBkb2Vzbid0IGdvIG5lZ2F0aXZlIG9yIGV4cGxvZGUgdG9vIG11Y2hcbiAgICBpZiBzaW11bGF0ZWRfbG9zcyA8IDA6IHNpbXVsYXRlZF9sb3NzID0gMC4wMVxuICAgIGlmIHNpbXVsYXRlZF9sb3NzID4gMTA6IHNpbXVsYXRlZF9sb3NzID0gMTAuMFxuICBcbiAgICBjdXJyZW50X2xvc3MgPSBzaW11bGF0ZWRfbG9zc1xuICAgIGxvc3Nlcy5hcHBlbmQoY3VycmVudF9sb3NzKVxuXG5lcG9jaHMgPSBucC5hcmFuZ2UobnVtX2Vwb2NocyArIDEpXG5cbmZpZyA9IGdvLkZpZ3VyZSgpXG5maWcuYWRkX3RyYWNlKGdvLlNjYXR0ZXIoeD1lcG9jaHMsIHk9bG9zc2VzLCBtb2RlPSdsaW5lcyttYXJrZXJzJywgbmFtZT0nU2ltdWxhdGVkIExvc3MnKSlcbmZpZy51cGRhdGVfbGF5b3V0KFxuICAgIHRpdGxlPWZcIlNpbXVsYXRlZCBMb3NzIEN1cnZlIChMUjoge2xyOi40Zn0sIE5vaXNlOiB7bm9pc2U6LjJmfSlcIixcbiAgICB4YXhpc190aXRsZT1cIkVwb2NoXCIsXG4gICAgeWF4aXNfdGl0bGU9XCJMb3NzXCIsXG4gICAgeWF4aXNfcmFuZ2U9WzAsIGluaXRpYWxfbG9zcyArIDAuNV0sXG4gICAgd2lkdGg9OTAwLCBoZWlnaHQ9NTAwLFxuICAgIG1hcmdpbj1kaWN0KGw9NTAsIHI9NTAsIGI9NTAsIHQ9NTApXG4pXG5maWcifQ==
</script>
</div>
<aside class="notes">
<p>This interactive plot simulates the training loss over epochs under different learning rates and noise levels (as a proxy for batch size variability).</p>
<ul>
<li><strong><code>Learning Rate</code></strong>: Adjust this slider to see how aggressively the loss decreases.
<ul>
<li><strong>Too Low</strong>: The loss will descend slowly and linearly.</li>
<li><strong>Optimal</strong>: The loss decreases relatively fast and smoothly towards a minimum.</li>
<li><strong>Too High</strong>: The loss might oscillate wildly, decrease erratically, or even increase, indicating that the optimization steps are too large and skipping over minima.</li>
</ul></li>
<li><strong><code>Noise Level (Batch Size proxy)</code></strong>: This simulates the noisiness of gradients from mini-batches.
<ul>
<li><strong>Low Noise</strong>: Smoother loss curve, typical of larger batch sizes or smoother gradients.</li>
<li><strong>High Noise</strong>: More “wiggle” in the loss, characteristic of smaller batch sizes where individual batch gradients vary more.</li>
</ul></li>
</ul>
<p><strong>Experimentation</strong>: * Start with a <code>learning_rate</code> around <code>0.01</code> and <code>noise_level</code> at <code>0.1</code>. * Increase <code>learning_rate</code> significantly (e.g., <code>0.05</code> or higher). Observe the erratic behavior. * Reduce <code>learning_rate</code> significantly (e.g., <code>0.001</code>). Notice the slow, linear progress. * Increase <code>noise_level</code> while keeping <code>learning_rate</code> moderate. See the increased “wiggle” in the curve.</p>
<p>This simulation gives a simplified, yet illustrative, understanding of how these hyperparameter choices manifest in the loss curve, guiding your practical tuning decisions.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="babysitting-trainval-accuracy" class="slide level2">
<h2><a name="accuracy"></a>8.3 Babysitting: Train/Val Accuracy</h2>
<p>The most direct indicator of model performance and overfitting.</p>
<div class="fig figleft fighighlight">
<img src="https://cs231n.github.io/assets/nn3/accuracies.jpeg" width="70%">
<div class="figcaption">
<pre><code>The gap between training and validation accuracy indicates overfitting.
&lt;b&gt;Blue (strong overfitting):&lt;/b&gt; Validation accuracy is significantly lower than training accuracy, and may even decrease after a certain point.
&lt;b&gt;Green (good fit):&lt;/b&gt; Training and validation accuracy are close, with continuous improvement.</code></pre>
</div>
</div>
<ul>
<li><strong>Gap between Training and Validation Accuracy:</strong>
<ul>
<li><strong>Large Gap (Blue):</strong> Strong overfitting. Your model is memorizing the training data but not generalizing well.
<ul>
<li><strong>Action:</strong> Increase regularization (L2, dropout), or collect more data.</li>
</ul></li>
<li><strong>Small Gap (Green):</strong> Good fit. Model generalizes well.</li>
<li><strong>Both Low:</strong> Model is underfitting (too simple, or not trained enough).</li>
</ul></li>
</ul>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>Validation accuracy is your true measure of how well your model will perform on unseen data.</p>
</div>
</div>
</div>
<aside class="notes">
<p>The comparison between training and validation accuracy is arguably the most important set of plots for diagnosing model behavior. It tells you directly about generalization.</p>
<ul>
<li><strong>Overfitting (Blue Curve):</strong> If your training accuracy is high but validation accuracy is much lower, especially if validation accuracy starts <em>decreasing</em> while training accuracy continues to rise, your model is heavily overfitting. It’s essentially memorizing the training set, including its noise, and failing to learn generalizable patterns. The indicated actions are: increase regularization to reduce the model’s capacity or its reliance on specific training examples, or get more diverse training data if possible.</li>
<li><strong>Good Fit (Green Curve):</strong> This is the ideal scenario. Both training and validation accuracies are high and continue to improve in tandem, with a small, consistent gap. This indicates the model is learning well and generalizing effectively.</li>
<li><strong>Underfitting:</strong> If both training and validation accuracies are low, your model is likely underfitting. This mean’s its capacity might be too low (not enough layers/neurons), it hasn’t been trained long enough, or the learning rate is too low.</li>
</ul>
<p>Always prioritize validation accuracy. It’s what truly matters for real-world application of your model.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="babysitting-weightsupdates-ratio-debugging" class="slide level2">
<h2>8.3 Babysitting: Weights:Updates Ratio (DEBUGGING)</h2>
<p>A diagnostic for the health of your updates, especially for debugging.</p>
<ul>
<li>A good heuristic: The ratio of the magnitude of weight updates to the magnitude of the weights should be around <code>1e-3</code>.
<ul>
<li>For a layer <code>W</code>, calculate <code>std(dW) / std(W)</code>.</li>
<li>If <code>std(dW) / std(W)</code> is too low (e.g., <code>1e-6</code>), updates are too small; the network learns slowly. Adjust learning rate up.</li>
<li>If <code>std(dW) / std(W)</code> is too high (e.g., <code>1e-1</code>), updates are too large; parameters are jumping chaotically. Adjust learning rate down.</li>
</ul></li>
</ul>
<h3 id="activationgradient-distributions-per-layer-debugging">Activation/Gradient Distributions per Layer (DEBUGGING)</h3>
<ul>
<li>For each layer, plot histograms of activations, weights, and gradients.</li>
<li><strong>Goal:</strong> Maintain healthy distributions (e.g., not all zero, not all saturated).</li>
<li><strong>Problem:</strong> If activations/gradients consistently go to zero or become saturated (e.g., \(\) for Tanh), it indicates vanishing/exploding gradient problems.
<ul>
<li>This is where <strong>Batch Normalization</strong> shines, by actively normalizing these distributions.</li>
</ul></li>
</ul>
<aside class="notes">
<p>These are more advanced debugging metrics, especially useful when your network isn’t learning well and the loss/accuracy plots aren’t providing enough insight.</p>
<ul>
<li><p><strong>Weights:Updates Ratio:</strong> This ratio gives you a direct sense of how much your weights are changing relative to their current values. If the changes are too small (e.g., 1e-6), your learning rate is likely too low, or you’re encountering vanishing gradients. If they’re too large (e.g., 1e-1), your learning rate is too high, and your optimizer is likely overshooting or oscillating. The <code>1e-3</code> rule of thumb is a good starting point, indicating a healthy rate of change.</p></li>
<li><p><strong>Activation/Gradient Distributions:</strong> Visualizing these distributions (e.g., using histograms) for each layer can reveal problems like vanishing or exploding activations/gradients.</p>
<ul>
<li>If activations are all clustered around zero, or heavily saturated at the extremes of an activation function (like 0 or 1 for sigmoid, or -1 and 1 for tanh), information flow is hindered.</li>
<li>Similarly, if gradients are all zero, upstream layers aren’t getting learning signals. If they’re exploding, updates become unstable. This is precisely the problem that <strong>Batch Normalization</strong> was designed to address, by maintaining well-behaved (unit Gaussian-like) activation distributions throughout the network, leading to much more stable gradients.</li>
</ul></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="babysitting-visualization" class="slide level2">
<h2>8.3 Babysitting: Visualization</h2>
<p>Visualizing network internal states or learned features.</p>
<ul>
<li><strong>First Layer Weights:</strong>
<ul>
<li>For CNNs, visualize the weights of the first convolutional layer directly.</li>
<li>Each filter (weight vector) often resembles learned features like edges, blobs, or color gradients.</li>
</ul></li>
<li><strong>Activations:</strong>
<ul>
<li>For CNNs, plot activations of different filters for a given input image.</li>
<li>Helps understand what features different parts of the network activate on.</li>
</ul></li>
<li><strong>Dimensionality Reduction:</strong>
<ul>
<li>Use PCA or t-SNE (t-Distributed Stochastic Neighbor Embedding) to reduce high-dimensional feature vectors (from deeper layers) to 2D/3D.</li>
<li>Plot these to see if classes cluster well in the feature space.</li>
</ul></li>
</ul>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p>Visualization is more art than science; use it to build intuition rather than for concrete debugging.</p>
</div>
</div>
</div>
<aside class="notes">
<p>Visualization is an art form in deep learning, offering qualitative insights into what your network has learned. It’s less about precise debugging and more about building intuition.</p>
<ul>
<li><strong>First-layer weights:</strong> Especially in convolutional neural networks, the filters in the first layer often learn interpretable patterns such as edges, textures, or color blobs. Visualizing them can confirm that your network is picking up meaningful low-level features.</li>
<li><strong>Activations:</strong> Looking at the output of different filters throughout the network for a specific input image can show you which parts of the image activate particular neurons. This helps understand the feature hierarchy.</li>
<li><strong>Dimensionality Reduction:</strong> Tools like PCA or t-SNE can project the high-dimensional feature vectors from deeper layers into a 2D or 3D space. Plotting these reduced features can reveal if data points belonging to the same class are clustering together, which indicates good feature learning.</li>
</ul>
<p>While not direct debugging tools, these visualizations are invaluable for developing an intuitive understanding of your network’s internal representations and capabilities.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="parameter-updates-optimization-algorithms" class="slide level2">
<h2><a name="update"></a>9. Parameter Updates (Optimization Algorithms)</h2>
<p>Once gradients are computed, how do we update the weights?</p>
<h3 id="first-order-methods"><a name="sgd"></a>9.1 First-Order Methods</h3>
<ul>
<li><strong>Stochastic Gradient Descent (SGD):</strong>
<ul>
<li>Update rule: \( W W - _W L \)</li>
<li>\(\) is the learning rate.</li>
<li>Takes small steps in the direction opposite to the gradient.</li>
<li>Suffers from oscillations and slow convergence in ravines.</li>
</ul></li>
<li><strong>Momentum:</strong>
<ul>
<li>Introduces a “velocity” term to accelerate SGD in the right direction and damp oscillations.</li>
<li>Update rule:
<ul>
<li>\( v v - _W L \)</li>
<li>\( W W + v \)</li>
</ul></li>
<li>\(\) is the momentum coefficient (e.g., 0.9, 0.99).</li>
</ul></li>
<li><strong>Nesterov Momentum (NAG - Nesterov Accelerated Gradient):</strong>
<ul>
<li>“Look ahead” before making update.</li>
<li>Computes gradient at a point slightly ahead in the direction of momentum.</li>
<li>Update rule:
<ul>
<li>\( v v - _W L(W + v) \)</li>
<li>\( W W + v \)</li>
</ul></li>
<li>Often yields slightly better convergence than regular momentum.</li>
</ul></li>
</ul>
<aside class="notes">
<p>Optimization algorithms dictate how our network’s parameters are adjusted based on the gradients. First-order methods leverage only the first derivative (gradient) of the loss function.</p>
<ul>
<li><strong>SGD:</strong> This is the baseline. It takes steps directly opposite to the calculated gradient. While effective, it can be slow, especially in “ravines” of the loss landscape, where it oscillates back and forth across the narrow dimension.</li>
<li><strong>Momentum:</strong> Inspired by physics, momentum introduces a “velocity” that accumulates gradients. Instead of only reacting to the current gradient, it also considers past gradients. This helps to overcome small local optima, smooth out oscillations, and accelerate convergence, especially through flatter regions or ravines. The momentum coefficient <code>μ</code> controls how much of the previous velocity is retained.</li>
<li><strong>Nesterov Momentum:</strong> A refinement of momentum, Nesterov’s method is a “look-ahead” optimizer. It first makes a jump in the direction of the accumulated momentum, then calculates the gradient at this “looked-ahead” position, and finally applies the update. This allows it to correct its path more quickly and prevent overshooting, often resulting in slightly faster or more stable convergence.</li>
</ul>
<p>In practice, momentum-based optimizers (SGD with momentum or Nesterov) are almost always preferred over plain SGD for deep learning.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="optimization-paths-visualized" class="slide level2">
<h2>Optimization Paths Visualized</h2>
<div class="cell" data-reveal="true" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
    subgraph Initial State
        P0((Start Point))
    end

    subgraph SGD Path
        P0 --&gt; SGD1(SGD Step)
        SGD1 --&gt; SGD2(SGD Step)
        SGD2 --&gt; SGD_Oscillate[SGD learns slowly, oscillates]
        style SGD_Oscillate fill:#fcc,stroke:#333,stroke-width:2px;
    end

    subgraph Momentum Path
        P0 --&gt; M1(Momentum Step: Look at current gradient + accumulate velocity)
        M1 --&gt; M2(Momentum Step)
        M2 --&gt; M_Accelerate[Momentum accelerates through ravines]
        style M_Accelerate fill:#cfc,stroke:#333,stroke-width:2px;
    end

    subgraph Nesterov Momentum Path
        P0 --&gt; N1(Nesterov: Look ahead based on accumulated velocity)
        N1 --&gt; N2(Nesterov: Compute gradient at 'look-ahead' point)
        N2 --&gt; N3(Nesterov: Update based on 'look-ahead' gradient)
        N3 --&gt; N_Converge[Nesterov converges faster, more direct]
        style N_Converge fill:#cce,stroke:#333,stroke-width:2px;
    end
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<aside class="notes">
<p>This Mermaid diagram provides a conceptual visualization of how plain SGD, Momentum, and Nesterov Momentum navigate the optimization landscape from a starting point towards a minimum.</p>
<ul>
<li><strong>SGD</strong>: Typically takes direct steps based on the local gradient. In narrow valleys or “ravines,” it tends to oscillate back and forth, making slow progress toward the minimum.</li>
<li><strong>Momentum</strong>: Introduces a “memory” of past gradients. It accumulates velocity, allowing it to accelerate more smoothly through flat regions and maintain direction through oscillations, thus dampening them and speeding up convergence.</li>
<li><strong>Nesterov Momentum</strong>: Takes it a step further by being “aware” of where it’s going. It calculates the gradient not at the current position, but at the position it would be in after applying the current momentum. This “look-ahead” capability allows it to anticipate the curvature of the loss surface and make more informed, direct updates, often leading to faster convergence than classical momentum.</li>
</ul>
<p>While simplified, this diagram helps reinforce the intuitive understanding of these optimizers’ behaviors.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="annealing-the-learning-rate" class="slide level2 scrollable">
<h2><a name="anneal"></a>9.2 Annealing the Learning Rate</h2>
<p>Gradually decreasing the learning rate over time.</p>
<ul>
<li>High learning rates initially help rapid progress, but can prevent fine-tuning near the minimum.</li>
<li>Lower learning rates allow for finer adjustments and better convergence.</li>
</ul>
<h3 id="common-strategies">Common Strategies:</h3>
<ol type="1">
<li><strong>Step Decay:</strong>
<ul>
<li>Reduce learning rate by a fixed factor (e.g., 0.1) at specific epochs or after a fixed number of iterations.</li>
<li><strong>Pros:</strong> Simple and widely used.</li>
<li><strong>Cons:</strong> Requires careful tuning of decay rate and decay schedule.</li>
</ul></li>
<li><strong>Exponential Decay:</strong>
<ul>
<li>Reduce learning rate exponentially: \( = _0 e^{-kt} \)</li>
<li>\(_0\): initial learning rate, \(k\): decay rate.</li>
<li><strong>Pros:</strong> Smooth decay.</li>
<li><strong>Cons:</strong> Can decay too quickly or too slowly for certain phases.</li>
</ul></li>
<li><strong>Cosine Decay:</strong>
<ul>
<li>Learning rate follows a cosine curve, smoothly decreasing from a maximum to a minimum.</li>
<li><strong>Pros:</strong> Modern, effective strategy, often seen in research. Empirically good for deep networks.</li>
</ul></li>
</ol>
<aside class="notes">
<p>Learning rate annealing, or scheduling, is a crucial technique. Initial large learning rates allow for rapid exploration of the loss landscape, escaping saddle points and quickly getting into a general region of a minimum. However, those large steps can prevent the optimizer from precisely converging to the exact minimum. As training progresses, we want to take smaller and smaller steps to “fine-tune” the parameters and settle into a good minimum.</p>
<p>We discussed three common strategies:</p>
<ol type="1">
<li><strong>Step Decay:</strong> The simplest. You train for some epochs, then drop the learning rate, then train more, drop again, etc. It’s like descending stairs. You need to pick when to drop and by how much.</li>
<li><strong>Exponential Decay:</strong> A continuous, smooth decrease. The learning rate shrinks by a constant factor every iteration.</li>
<li><strong>Cosine Decay:</strong> A more recent and often very effective schedule. The learning rate starts high, decreases following a cosine curve, and then increases slightly or stays low before the next cycle. It’s known for its good performance, often preventing the learning rate from getting stuck too low too early.</li>
</ol>
<p>The choice of learning rate schedule is a hyperparameter that requires careful tuning, often with randomized search.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="interactive-learning-rate-schedule-visualization" class="slide level2">
<h2>Interactive Learning Rate Schedule Visualization</h2>
<p>Visualize how different learning rate annealing strategies change over time.</p>
<p>Adjust parameters for each schedule to see their impact.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code hidden" id="cb5" data-startfrom="639" data-source-offset="-0"><pre class="sourceCode numberSource js number-lines code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 638;"><span id="cb5-639"><a></a>viewof initial_lr <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="fl">0.001</span><span class="op">,</span> <span class="fl">1.0</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="fl">0.1</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.001</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Initial Learning Rate"</span>})<span class="op">;</span></span>
<span id="cb5-640"><a></a>viewof num_epochs_anneal <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">50</span><span class="op">,</span> <span class="dv">200</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="dv">100</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="dv">10</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Total Epochs"</span>})<span class="op">;</span></span>
<span id="cb5-641"><a></a></span>
<span id="cb5-642"><a></a>viewof decay_factor_step <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="fl">0.1</span><span class="op">,</span> <span class="fl">0.9</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="fl">0.5</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Step Decay Factor"</span>})<span class="op">;</span></span>
<span id="cb5-643"><a></a>viewof decay_epochs_step <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">5</span><span class="op">,</span> <span class="dv">50</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="dv">20</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="dv">5</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Step Decay Every N Epochs"</span>})<span class="op">;</span></span>
<span id="cb5-644"><a></a></span>
<span id="cb5-645"><a></a>viewof decay_rate_exp <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="fl">0.01</span><span class="op">,</span> <span class="fl">0.1</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="fl">0.05</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.01</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Exponential Decay Rate (k)"</span>})<span class="op">;</span></span>
<span id="cb5-646"><a></a></span>
<span id="cb5-647"><a></a>viewof min_lr_cosine <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="fl">0.0</span><span class="op">,</span> <span class="fl">0.01</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="fl">0.001</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.001</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Cosine Min LR"</span>})<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-3-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-3-2" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-3-3" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-3-4" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-3-5" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-3-6" data-nodetype="declaration">

</div>
</div>
</div>
</div>
<div>
<div id="pyodide-3" class="exercise-cell">

</div>
<script type="pyodide-3-contents">
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
</script>
</div>
<aside class="notes">
<p>This interactive plot helps you visualize how the learning rate changes over epochs for three common annealing strategies: Step Decay, Exponential Decay, and Cosine Decay.</p>
<ul>
<li><strong><code>Initial Learning Rate</code></strong>: The starting learning rate for all schedules.</li>
<li><strong><code>Total Epochs</code></strong>: The total duration of the training.</li>
</ul>
<p><strong>For Step Decay:</strong></p>
<ul>
<li><code>Decay Factor</code>: The multiplier for the learning rate (e.g., 0.5 halves it).</li>
<li><code>Decay Every N Epochs</code>: How often the learning rate is multiplied by the decay factor.
<ul>
<li>Observe the “stair-step” pattern. Higher decay factors and shorter intervals lead to a faster drop.</li>
</ul></li>
</ul>
<p><strong>For Exponential Decay:</strong></p>
<ul>
<li><code>Decay Rate (k)</code>: Controls how quickly the learning rate drops off.
<ul>
<li>Notice the smooth, continuous decline. A higher <code>k</code> value makes it drop faster.</li>
</ul></li>
</ul>
<p><strong>For Cosine Decay:</strong></p>
<ul>
<li><code>Min LR</code>: The minimum learning rate reached during the cycle.
<ul>
<li>Observe the sinusoidal curve, smoothly moving from initial LR down to <code>Min LR</code> and back (or just down depending on implementation). This smooth, non-linear progression is often very effective.</li>
</ul></li>
</ul>
<p><strong>Experimentation:</strong></p>
<ul>
<li>Adjust the initial LR, then try different combinations of decay parameters for each schedule.</li>
<li>Pay attention to how quickly each method drops the learning rate to very low values. A premature drop can lead to the model getting stuck.</li>
</ul>
<p>Understanding these curves helps you choose and tune an appropriate learning rate schedule for your specific problem.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="adaptive-learning-rate-methods" class="slide level2">
<h2>9.3 Adaptive Learning Rate Methods</h2>
<p>Go beyond global learning rates with per-parameter adaptive adjustments.</p>
<ul>
<li><strong>Motivation:</strong> Different parameters (and even dimensions of a single parameter) might benefit from different learning rates.
<ul>
<li>Parameters related to rare features benefit from larger updates.</li>
<li>Parameters related to common features benefit from smaller updates.</li>
</ul></li>
</ul>
<h3 id="adagrad-adaptive-gradient-algorithm">Adagrad (Adaptive Gradient Algorithm)</h3>
<ul>
<li>Accumulates squared gradients for each parameter.</li>
<li>Adjusts learning rate inversely proportional to the square root of the sum of squared historical gradients.</li>
<li><strong>Pros:</strong> Automatically adapts learning rates to feature frequency.</li>
<li><strong>Cons:</strong> Accumulated squared gradients always increase, can lead to learning rate decaying too aggressively and prematurely stopping learning.</li>
</ul>
<h3 id="rmsprop-root-mean-square-propagation">RMSProp (Root Mean Square Propagation)</h3>
<ul>
<li>Addresses Adagrad’s aggressive decay.</li>
<li>Uses a <em>moving average</em> of squared gradients instead of a simple sum.</li>
<li><strong>Pros:</strong> More robust, better long-term performance than Adagrad.</li>
</ul>
<h3 id="adam-adaptive-moment-estimation">Adam (Adaptive Moment Estimation)</h3>
<ul>
<li>Combines ideas from RMSProp and Momentum.</li>
<li>Keeps an exponentially decaying average of past gradients (<strong>first moment</strong>) and past <em>squared</em> gradients (<strong>second moment</strong>).</li>
<li><strong>Pros:</strong> Empirically very effective, widely used default optimizer.</li>
<li><strong>Cons:</strong> Can sometimes generalize worse than SGD with momentum on certain tasks.</li>
</ul>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>Adam is often the recommended default optimizer due to its strong empirical performance and ease of use (less hyperparameter tuning than SGD+Momentum).</p>
</div>
</div>
</div>
<aside class="notes">
<p>While SGD with momentum and learning rate schedules are powerful, they apply a single learning rate globally to all parameters. Adaptive learning rate methods take this a step further by tailoring the learning rate for each individual parameter based on its past gradients.</p>
<ul>
<li><p><strong>Adagrad:</strong> This was one of the first popular adaptive methods. It makes updates smaller for parameters with frequent, large gradients (common features) and larger for parameters with rare, small gradients (rare features). Its main drawback is that the sum of squared gradients continuously grows, causing the learning rate to eventually become tiny and stop learning altogether.</p></li>
<li><p><strong>RMSProp:</strong> Developed to mitigate Adagrad’s aggressive decay. Instead of a simple sum, it uses an exponentially decaying moving average of squared gradients. This prevents the learning rate from monotonically decreasing too quickly, allowing the algorithm to continue learning.</p></li>
<li><p><strong>Adam:</strong> Currently the most popular adaptive optimizer. It combines the benefits of RMSProp (adaptive learning rates based on second moments) with the concept of momentum (using an exponentially decaying average of past gradients, the “first moment”). It also includes bias correction terms to account for the initialization of these moving averages. Adam generally performs very well across a wide range of tasks and is often the first optimizer to try.</p></li>
</ul>
<p>While adaptive optimizers like Adam are great for quick prototyping and often achieve good performance, sometimes a well-tuned SGD with momentum and an aggressive learning rate schedule can achieve even better generalization on specific, critical tasks. It’s a trade-off between ease of use and ultimate performance ceiling.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="hyperparameter-optimization" class="slide level2">
<h2><a name="hyper"></a>10. Hyperparameter Optimization</h2>
<p>Systematically finding the best set of hyperparameters for your model.</p>
<h3 id="cross-validation">1. Cross-Validation</h3>
<ul>
<li>Divide training data into training and validation sets.</li>
<li>Hold-out validation is common: 70-90% for training, rest for validation.</li>
<li>Use validation set to tune hyperparameters (never the test set!).</li>
</ul>
<h3 id="hyperparameter-search-strategies">2. Hyperparameter Search Strategies</h3>
<ul>
<li><strong>Grid Search:</strong> Try all combinations of hyperparameters from a predefined grid.
<ul>
<li><strong>Pros:</strong> Exhaustive (if grid is dense enough).</li>
<li><strong>Cons:</strong> Computationally expensive; easily misses optimal points if grid is coarse.</li>
</ul></li>
<li><strong>Random Search:</strong> Sample hyperparameter values randomly from predefined ranges.
<ul>
<li><strong>Pros:</strong> More efficient than grid search for the same computational budget, often finds better models. Better at exploring disparate impactful hyperparameters.</li>
<li><strong>Cons:</strong> Still heuristic; no guarantee of finding the global optimum.</li>
</ul></li>
<li><strong>Coarse-to-Fine Search:</strong>
<ul>
<li>Perform a random search over a wide range.</li>
<li>Identify promising regions.</li>
<li>Then, perform another (random or grid) search within those narrower, promising regions.</li>
</ul></li>
<li><strong>Bayesian Optimization:</strong>
<ul>
<li>Uses a probabilistic model to predict which hyperparameters to try next, aiming to minimize the number of expensive evaluations.</li>
<li><strong>Pros:</strong> Can be much more efficient for complex models and large search spaces.</li>
</ul></li>
</ul>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Always tune hyperparameters on the validation set, and evaluate final performance <em>only once</em> on the test set.</p>
</div>
</div>
</div>
<aside class="notes">
<p>Hyperparameter optimization is the process of selecting the best set of values for the parameters that control the learning process itself (e.g., learning rate, regularization strength, network architecture, etc.). It’s essentially “learning to learn.”</p>
<ul>
<li><p><strong>Cross-Validation:</strong> The foundation of reliable hyperparameter tuning. You split your data and use the validation set as a proxy for unseen data to evaluate different hyperparameter settings. Critically, the test set is reserved only for the <em>final</em> evaluation of the chosen best model to get an unbiased estimate of its true performance.</p></li>
<li><p><strong>Search Strategies:</strong></p>
<ul>
<li><strong>Grid Search:</strong> Conceptually simple, but inefficient. If one hyperparameter has little impact and another has a huge impact, grid search wastes effort trying many combinations along the ineffective dimension.</li>
<li><strong>Random Search:</strong> This was shown to be more effective than grid search for the same computational budget. By randomly sampling, it can more efficiently explore the hyperparameter space, especially when some hyperparameters are much more important than others and interactions are complex.</li>
<li><strong>Coarse-to-Fine:</strong> A pragmatic approach combining exploration and exploitation. Find rough promising areas, then refine the search.</li>
<li><strong>Bayesian Optimization:</strong> A more sophisticated approach that builds a statistical model (a surrogate model) of the objective function (e.g., validation accuracy) with respect to the hyperparameters. It then uses this model to intelligently choose the next set of hyperparameters to evaluate, balancing exploration (trying new areas) and exploitation (refining known good areas). This is often implemented with libraries like Optuna, Hyperopt, or scikit-optimize.</li>
</ul></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="evaluation-model-ensembles" class="slide level2 scrollable">
<h2><a name="eval"></a>11. Evaluation: Model Ensembles</h2>
<p>Combine multiple models to achieve superior performance.</p>
<ul>
<li><strong>Intuition:</strong> Different models make different errors. Combining their predictions can cancel out individual model weaknesses and lead to a more robust, accurate overall prediction.</li>
</ul>
<h3 id="common-ensemble-strategies">Common Ensemble Strategies:</h3>
<ol type="1">
<li><strong>Same Model, Different Initializations:</strong>
<ul>
<li>Train multiple models with identical architecture and hyperparameters, but with different random initializations.</li>
<li>Average their predictions.</li>
<li><strong>Pros:</strong> Simple, effective, especially for non-convex loss landscapes.</li>
</ul></li>
<li><strong>Top Models from Cross-Validation:</strong>
<ul>
<li>Select the top few best-performing models from your hyperparameter search.</li>
<li>Combine their predictions.</li>
<li><strong>Pros:</strong> Leverages models that already performed well.</li>
</ul></li>
<li><strong>Different Checkpoints of a Single Model:</strong>
<ul>
<li>Save model weights at different epochs during a long training run.</li>
<li>Treat each saved model as a separate ensemble member.</li>
<li><strong>Pros:</strong> Very cheap, as only one training run is needed. Effective because a model at epoch 50 might be different from epoch 100.</li>
</ul></li>
<li><strong>Averaging Parameters:</strong>
<ul>
<li>Instead of averaging predictions, average the <em>weights</em> of multiple models.</li>
<li>Can sometimes work well, but requires careful consideration as parameter spaces are non-linear.</li>
</ul></li>
</ol>
<p><strong>How to Combine Predictions (Classification):</strong></p>
<ul>
<li><strong>Average Softmax Probabilities:</strong> Simplest and most common.</li>
<li><strong>Vote:</strong> Each model “votes” for its predicted class.</li>
</ul>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p>Ensembles consistently deliver 1-2% (or more) performance boosts, often winning competitions. Always consider using an ensemble for production systems if latency/compute budget allows.</p>
</div>
</div>
</div>
<aside class="notes">
<p>Model ensembles are a powerful technique to squeeze out an extra performance boost from your models. The core idea is that if you have several models that are individually good but make different types of errors, then combining their predictions can lead to a more accurate and robust final decision.</p>
<p>We discussed several strategies for creating ensembles:</p>
<ul>
<li><strong>Different Initializations:</strong> This is surprisingly effective. Because training neural networks involves optimizing a non-convex loss function, different random initializations will lead to different local minima, resulting in models with slightly different strengths and weaknesses. Averaging their predictions helps smooth out these individual biases.</li>
<li><strong>Top Models from Cross-Validation:</strong> If you have run extensive hyperparameter tuning, you likely have several good models. Instead of picking just one, ensemble the best ones.</li>
<li><strong>Different Checkpoints:</strong> This is a very cost-effective strategy. A model’s weights at epoch 50 might perform differently than at epoch 100 or epoch 150. Saving and ensembling these different snapshots can be thought of as getting several models for the cost of one training run.</li>
<li><strong>Averaging Parameters:</strong> While sometimes effective, averaging the <em>parameters</em> themselves is trickier because the parameter space is highly non-linear. This works better if the models being averaged are relatively close in parameter space and have similar functionalities.</li>
</ul>
<p>For classification, the most common way to combine predictions is to average the Softmax probabilities (which can be interpreted as confidence scores). Alternatively, models can “vote” for a class. Ensembles are a staple in achieving top performance in many machine learning competitions and are often employed in critical production systems where reliability and peak accuracy are paramount.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="summary-part-3" class="slide level2">
<h2><a name="summary-p3"></a>12. Summary (Part 3)</h2>
<ul>
<li><strong>Gradient Checks:</strong> Crucial for verifying backpropagation correctness. Use the centered formula and relative error. Be mindful of practical issues like kinks, precision, and non-determinism.</li>
<li><strong>Sanity Checks:</strong> Essential quick tests before extensive training: check initial loss, regularization impact, and ability to overfit a small dataset.</li>
<li><strong>Babysitting Training:</strong> Monitoring loss and accuracy curves (training vs.&nbsp;validation) is key to diagnosing learning problems (underfitting, overfitting, bad learning rates). Debugging metrics like Weights:Updates ratio and activation/gradient distributions can provide deeper insights.</li>
<li><strong>Parameter Updates (Optimizers):</strong>
<ul>
<li><strong>SGD with Momentum / Nesterov Momentum:</strong> Preferred over plain SGD; accelerate convergence.</li>
<li><strong>Learning Rate Annealing:</strong> Gradually decrease learning rate (step, exponential, cosine decay) for better convergence and fine-tuning.</li>
<li><strong>Adaptive Methods (Adagrad, RMSProp, Adam):</strong> Adjust learning rates per-parameter; Adam is a highly recommended default.</li>
</ul></li>
<li><strong>Hyperparameter Optimization:</strong> Use cross-validation with random search (often coarse-to-fine) to find optimal hyperparameters.</li>
<li><strong>Model Ensembles:</strong> Combining predictions from multiple models (different initializations, checkpoints) consistently improves performance and robustness.</li>
</ul>
</section>
<section id="additional-references" class="slide level2">
<h2><a name="add"></a>13. Additional References</h2>
<ul>
<li><a href="http://www.deeplearning.net/tutorial/mlp.html">deeplearning.net tutorial</a> with Theano</li>
<li><a href="http://cs.stanford.edu/people/karpathy/convnetjs/">ConvNetJS</a> demos for intuitions</li>
<li><a href="http://neuralnetworksanddeeplearning.com/chap1.html">Michael Nielsen’s</a> tutorials</li>
<li><a href="http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html">What Every Computer Scientist Should Know About Floating-Point Arithmetic</a></li>
<li><a href="https://cs231n.github.io/neural-networks-3/#update">Optimization Algorithms</a> (CS231n lecture notes)</li>
<li><a href="https://ruder.io/optimizing-gradient-descent/">An overview of gradient descent optimization algorithms</a> by Sebastian Ruder</li>
<li><a href="https://towardsdatascience.com/a-conceptual-introduction-to-bayesian-optimization-for-beginners-part-i-e131d77a79e4">Bayesian Optimization for Hyperparameter Tuning</a></li>
<li>Recommended papers on various topics from the lecture notes (e.g., Glorot, He, Srivastava et al., Ioffe and Szegedy).</li>
</ul>
</section>
<section class="slide level2">



<script type="pyodide-data">
eyJvcHRpb25zIjp7ImVudiI6eyJQTE9UTFlfUkVOREVSRVIiOiJwbG90bHlfbWltZXR5cGUifSwiaW5kZXhVUkwiOiJodHRwczovL2Nkbi5qc2RlbGl2ci5uZXQvcHlvZGlkZS92MC4yNy4wL2Z1bGwvIn0sInBhY2thZ2VzIjp7InBrZ3MiOlsicHlvZGlkZV9odHRwIiwibWljcm9waXAiLCJpcHl0aG9uIiwibnVtcHkiLCJwbG90bHkiLCJuYmZvcm1hdCJdfX0=
</script>
<script type="ojs-module-contents">
{"contents":[{"cellName":"pyodide-3","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_3 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-3-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-3-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_3 = pyodideOjs.process(_pyodide_editor_3, {initial_lr, num_epochs_anneal, decay_factor_step, decay_epochs_step, decay_rate_exp, min_lr_cosine});\n"},{"cellName":"pyodide-2","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_2 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-2-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-2-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_2 = pyodideOjs.process(_pyodide_editor_2, {learning_rate, noise_level});\n"},{"cellName":"pyodide-1","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_1 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-1-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-1-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_1 = pyodideOjs.process(_pyodide_editor_1, {x_value, h_step_size, function_choice});\n"},{"cellName":"pyodide-prelude","inline":false,"methodName":"interpretQuiet","source":"pyodideOjs = {\n  const {\n    PyodideEvaluator,\n    PyodideEnvironmentManager,\n    setupPython,\n    startPyodideWorker,\n    b64Decode,\n    collapsePath,\n  } = window._exercise_ojs_runtime;\n\n  const statusContainer = document.getElementById(\"exercise-loading-status\");\n  const indicatorContainer = document.getElementById(\"exercise-loading-indicator\");\n  indicatorContainer.classList.remove(\"d-none\");\n\n  let statusText = document.createElement(\"div\")\n  statusText.classList = \"exercise-loading-details\";\n  statusText = statusContainer.appendChild(statusText);\n  statusText.textContent = `Initialise`;\n\n  // Hoist indicator out from final slide when running under reveal\n  const revealStatus = document.querySelector(\".reveal .exercise-loading-indicator\");\n  if (revealStatus) {\n    revealStatus.remove();\n    document.querySelector(\".reveal > .slides\").appendChild(revealStatus);\n  }\n\n  // Make any reveal slides with live cells scrollable\n  document.querySelectorAll(\".reveal .exercise-cell\").forEach((el) => {\n    el.closest('section.slide').classList.add(\"scrollable\");\n  })\n\n  // Pyodide supplemental data and options\n  const dataContent = document.querySelector(`script[type=\\\"pyodide-data\\\"]`).textContent;\n  const data = JSON.parse(b64Decode(dataContent));\n\n  // Grab list of resources to be downloaded\n  const filesContent = document.querySelector(`script[type=\\\"vfs-file\\\"]`).textContent;\n  const files = JSON.parse(b64Decode(filesContent));\n\n  let pyodidePromise = (async () => {\n    statusText.textContent = `Downloading Pyodide`;\n    const pyodide = await startPyodideWorker(data.options);\n\n    statusText.textContent = `Downloading package: micropip`;\n    await pyodide.loadPackage(\"micropip\");\n    const micropip = await pyodide.pyimport(\"micropip\");\n    await data.packages.pkgs.map((pkg) => () => {\n      statusText.textContent = `Downloading package: ${pkg}`;\n      return micropip.install(pkg);\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n    await micropip.destroy();\n\n    // Download and install resources\n    await files.map((file) => async () => {\n      const name = file.substring(file.lastIndexOf('/') + 1);\n      statusText.textContent = `Downloading resource: ${name}`;\n      const response = await fetch(file);\n      if (!response.ok) {\n        throw new Error(`Can't download \\`${file}\\`. Error ${response.status}: \"${response.statusText}\".`);\n      }\n      const data = await response.arrayBuffer();\n\n      // Store URLs in the cwd without any subdirectory structure\n      if (file.includes(\"://\")) {\n        file = name;\n      }\n\n      // Collapse higher directory structure\n      file = collapsePath(file);\n\n      // Create directory tree, ignoring \"directory exists\" VFS errors\n      const parts = file.split('/').slice(0, -1);\n      let path = '';\n      while (parts.length > 0) {\n        path += parts.shift() + '/';\n        try {\n          await pyodide.FS.mkdir(path);\n        } catch (e) {\n          if (e.name !== \"ErrnoError\") throw e;\n          if (e.errno !== 20) {\n            const errorTextPtr = await pyodide._module._strerror(e.errno);\n            const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n            throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n          }\n        }\n      }\n\n      // Write this file to the VFS\n      try {\n        return await pyodide.FS.writeFile(file, new Uint8Array(data));\n      } catch (e) {\n        if (e.name !== \"ErrnoError\") throw e;\n        const errorTextPtr = await pyodide._module._strerror(e.errno);\n        const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n        throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n      }\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n\n    statusText.textContent = `Pyodide environment setup`;\n    await setupPython(pyodide);\n\n    statusText.remove();\n    if (statusContainer.children.length == 0) {\n      statusContainer.parentNode.remove();\n    }\n    return pyodide;\n  })().catch((err) => {\n    statusText.style.color = \"var(--exercise-editor-hl-er, #AD0000)\";\n    statusText.textContent = err.message;\n    //indicatorContainer.querySelector(\".spinner-grow\").classList.add(\"d-none\");\n    throw err;\n  });\n\n  // Keep track of initial OJS block render\n  const renderedOjs = {};\n\n  const process = async (context, inputs) => {\n    const pyodide = await pyodidePromise;\n    const evaluator = new PyodideEvaluator(pyodide, context);\n    await evaluator.process(inputs);\n    return evaluator.container;\n  }\n\n  return {\n    pyodidePromise,\n    renderedOjs,\n    process,\n  };\n}\n"}]}
</script>
<div id="exercise-loading-indicator" class="exercise-loading-indicator d-none d-flex align-items-center gap-2">
<div id="exercise-loading-status" class="d-flex gap-2">

</div>
<div class="spinner-grow spinner-grow-sm">

</div>
</div>
<script type="vfs-file">
W10=
</script>
</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="../qrjs_pics/unsoed_logo.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://imron-slide.vercel.app">irosyadi-2025</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script type="ojs-module-contents">
    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
    </script>
    <script type="module">
    if (window.location.protocol === "file:") { alert("The OJS runtime does not work with file:// URLs. Please use a web server to view this document."); }
    window._ojs.paths.runtimeToDoc = "../../cs231n";
    window._ojs.paths.runtimeToRoot = "../..";
    window._ojs.paths.docToRoot = "..";
    window._ojs.selfContained = false;
    window._ojs.runtime.interpretFromScriptTags();
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>