<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/quarto-contrib/live-runtime/live-runtime.js" type="module"></script>
<link href="../site_libs/quarto-contrib/live-runtime/live-runtime.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.25">

  <meta name="author" content="Imron Rosyadi">
  <title>Machine Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-b0356e9119c1bdfd0db189d130feb51c.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script type="module" src="../site_libs/quarto-ojs/quarto-ojs-runtime.js"></script>
  <link href="../site_libs/quarto-ojs/quarto-ojs.css" rel="stylesheet">
  <meta name="mermaid-theme" content="neutral">
  <script src="../site_libs/quarto-diagram/mermaid.min.js"></script>
  <script src="../site_libs/quarto-diagram/mermaid-init.js"></script>
  <link href="../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Machine Learning</h1>
  <p class="subtitle">1 Neural Networks</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Imron Rosyadi 
</div>
</div>
</div>

</section>
<section>
<section id="understanding-neural-networks-from-neurons-to-layers" class="title-slide slide level1 center">
<h1>Understanding Neural Networks: From Neurons to Layers</h1>

</section>
<section id="table-of-contents" class="slide level2 scrollable">
<h2>Table of Contents</h2>
<ol type="1">
<li><a href="#/quick">Quick Intro: Linear vs NN</a></li>
<li><a href="#/intro">Modeling One Neuron</a>
<ul>
<li><a href="#/bio">Biological Motivation</a></li>
<li><a href="#/classifier">Single Neuron as Linear Classifier</a></li>
<li><a href="#/actfun">Activation Functions</a></li>
</ul></li>
<li><a href="#/nn">Neural Network Architectures</a>
<ul>
<li><a href="#/layers">Layer-wise Organization</a></li>
<li><a href="#/feedforward">Feed-Forward Computation</a></li>
<li><a href="#/power">Representational Power</a></li>
<li><a href="#/arch">Setting Layers &amp; Sizes</a></li>
</ul></li>
<li><a href="#/summary">Summary</a></li>
<li><a href="#/add">Additional References</a></li>
</ol>
</section>
<section id="quick-intro-beyond-linear-models" class="slide level2">
<h2><a name="quick"></a>1. Quick Intro: Beyond Linear Models</h2>
<div class="columns">
<div class="column" style="width:48%;">
<p><strong>Linear Classification:</strong></p>
<ul>
<li>Scores obtained directly from weighted input sum.</li>
<li>Example: \( s = W x \)
<ul>
<li>\(x\): input vector (e.g., image pixels).</li>
<li>\(W\): weight matrix.</li>
<li>\(s\): class scores.</li>
</ul></li>
</ul>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p><strong>Issue:</strong> Limited to linear decision boundaries.</p>
</div>
</div>
</div>
</div><div class="column" style="width:48%;">
<p><strong>Neural Network Approach (2-layer):</strong></p>
<ul>
<li>Introduces a <strong>non-linear transformation</strong>.</li>
<li>Example: \( s = W_2 (0, W_1 x) \)
<ul>
<li>\(W_1\): First layer weights.</li>
<li>\((0, )\): Element-wise non-linearity (ReLU).</li>
<li>\(W_2\): Second layer weights.</li>
</ul></li>
</ul>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>The non-linearity is crucial for modeling complex relationships and introducing the “wiggle”.</p>
</div>
</div>
</div>
</div></div>
<aside class="notes">
<p>In traditional linear classification, like SVMs or Softmax, we directly compute scores as a linear combination of inputs and weights. For example, for an image classification task, say CIFAR-10, an input image <code>x</code> (3072 pixels) is multiplied by a weight matrix <code>W</code> (10x3072) to produce 10 class scores. This approach is simple and interpretable but inherently limited to “linear” decision boundaries.</p>
<p>Neural networks overcome this by introducing non-linear activation functions between layers. A simple two-layer network, as shown in the example <code>s = W2 max(0, W1 x)</code>, first transforms the input <code>x</code> using <code>W1</code>, then applies a non-linear function (here, <code>max(0,.)</code> which is ReLU), and finally transforms it again with <code>W2</code> to get the scores. This non-linearity is the key component; without it, multiple linear layers would collapse into a single linear operation, offering no additional power beyond a simple linear classifier. This “wiggle” allows neural networks to learn intricate, non-linear patterns in data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="quick-intro-visualizing-the-flow" class="slide level2">
<h2><a name="flowchart-intro"></a>Quick Intro: Visualizing the Flow</h2>
<div class="cell" data-reveal="true" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
    Input["Input Image (x)"] --&gt; W1_Layer("Layer 1 - W1")
    W1_Layer --&gt; Nonlinearity["Non-Linear Activation (max(0, .))"]
    Nonlinearity --&gt; W2_Layer("Layer 2 - W2")
    W2_Layer --&gt; Scores["Output Scores (s)"]

    style Input fill:#f9f,stroke:#333,stroke-width:2px;
    style Scores fill:#bbf,stroke:#333,stroke-width:2px;
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<ul>
<li><strong>Deeper Networks</strong>:
<ul>
<li>A three-layer network: \( s = W_3 (0, W_2 (0, W_1 x)) \)</li>
<li>Adds more non-linear transformations.</li>
<li>Parameters \(W_i\) are learned via <strong>stochastic gradient descent</strong> and <strong>backpropagation</strong>.</li>
</ul></li>
</ul>
<aside class="notes">
<p>This flowchart illustrates the forward pass of the two-layer neural network we just discussed. The input image <code>x</code> first goes through a linear transformation by <code>W1</code>. The result then undergoes a non-linear activation, which introduces the capacity to learn complex, non-linear relationships. Finally, another linear transformation by <code>W2</code> produces the output class scores <code>s</code>.</p>
<p>For deeper networks, this process simply stacks more such layers, each with its own weights and non-linear activation. The key takeaway is that each layer adds a level of abstraction and complexity, allowing the network to learn rich representations of the input data. The learning process involves adjusting all these weight matrices, <code>W1</code>, <code>W2</code>, <code>W3</code>, based on the error in the output, using algorithms like backpropagation.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="modeling-one-neuron-biological-motivation" class="slide level2">
<h2><a name="bio"></a>2. Modeling One Neuron: Biological Motivation</h2>
<div class="columns">
<div class="column" style="width:48%;">
<p><strong>Biological Neuron (Left):</strong></p>
<ul>
<li><strong>Dendrites</strong>: Receive input signals.</li>
<li><strong>Cell Body</strong>: Integrates signals.</li>
<li><strong>Axon</strong>: Transmits output signals.</li>
<li><strong>Synapses</strong>: Connections to other neurons, with variable strengths.</li>
</ul>
<div class="fig figcenter fighighlight">
<img src="https://cs231n.github.io/assets/nn1/neuron.png" width="90%">
<div class="figcaption">
A cartoon drawing of a biological neuron.
</div>
</div>
</div><div class="column" style="width:48%;">
<p><strong>Computational Model (Right):</strong></p>
<ul>
<li>Inputs (\(x_i\)) correspond to signals from other neurons.</li>
<li>Weights (\(w_i\)) represent synaptic strengths (learnable).</li>
<li>Summation: \(_i w_i x_i + b\) (cell body processing).</li>
<li><strong>Activation function</strong> (\(f\)): Simulates firing rate.</li>
</ul>
<div class="fig figcenter fighighlight">
<img src="https://cs231n.github.io/assets/nn1/neuron_model.jpeg" width="90%" style="border-left: 1px solid black;">
<div class="figcaption">
Mathematical model of a neuron.
</div>
</div>
</div></div>
<aside class="notes">
<p>The concept of a neural network is loosely inspired by biological neurons. On the left, you see a simplified diagram of a biological neuron. It receives signals through dendrites, processes them in the cell body, and sends out signals through its axon, which connects to other neurons via synapses.</p>
<p>On the right, we have the mathematical model. Each input <code>x_i</code> is multiplied by a corresponding weight <code>w_i</code>, mimicking synaptic strength. These weighted inputs are summed, along with a bias term <code>b</code>, representing the cell body’s integration. Finally, an activation function <code>f</code> (like sigmoid here) is applied to this sum. This function models the neuron’s “firing rate,” squashing the output to a specific range (e.g., 0 to 1). The weights <code>w_i</code> and bias <code>b</code> are the learnable parameters in this computational model.</p>
<p>It’s important to remember that this is a highly simplified model. Biological neurons are far more complex, with dynamic, non-linear dendritic computations and precise spike timing. We abstract away much of this complexity for computational tractability and effective machine learning.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="modeling-one-neuron-interactive-forward-pass" class="slide level2">
<h2>Modeling One Neuron: Interactive Forward Pass</h2>
<p>Let’s simulate a single neuron’s forward computation. You can modify <code>inputs</code>, <code>weights</code>, and <code>bias</code> to observe the <code>firing_rate</code>.</p>
<div>
<div id="pyodide-1" class="exercise-cell">

</div>
<script type="pyodide-1-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWUsIm1heC1saW5lcyI6MTB9LCJjb2RlIjoiaW1wb3J0IG51bXB5IGFzIG5wXG5pbXBvcnQgbWF0aFxuXG5kZWYgc2lnbW9pZCh4KTpcbiAgcmV0dXJuIDEuMCAvICgxLjAgKyBtYXRoLmV4cCgteCkpXG5cbiMgLS0tIEZlZWwgZnJlZSB0byBjaGFuZ2UgdGhlc2UgdmFsdWVzIC0tLVxuaW5wdXRzID0gbnAuYXJyYXkoWzAuNSwgLTAuMiwgMS4wXSlcbndlaWdodHMgPSBucC5hcnJheShbMC44LCAwLjEsIC0wLjNdKVxuYmlhcyA9IDAuNVxuIyAtLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS1cblxuY2VsbF9ib2R5X3N1bSA9IG5wLnN1bShpbnB1dHMgKiB3ZWlnaHRzKSArIGJpYXNcbmZpcmluZ19yYXRlID0gc2lnbW9pZChjZWxsX2JvZHlfc3VtKVxuXG5wcmludChmXCJJbnB1dHM6IHtpbnB1dHN9XCIpXG5wcmludChmXCJXZWlnaHRzOiB7d2VpZ2h0c31cIilcbnByaW50KGZcIkJpYXM6IHtiaWFzfVwiKVxucHJpbnQoZlwiQ2VsbCBCb2R5IFN1bToge2NlbGxfYm9keV9zdW06LjRmfVwiKVxucHJpbnQoZlwiRmlyaW5nIFJhdGUgKFNpZ21vaWQpOiB7ZmlyaW5nX3JhdGU6LjRmfVwiKSJ9
</script>
</div>
<aside class="notes">
<p>This interactive code block demonstrates the <code>forward</code> pass of our single neuron model. The <code>sigmoid</code> function is defined as our activation function. You are encouraged to change the values of <code>inputs</code>, <code>weights</code>, and <code>bias</code> within the designated section.</p>
<p>Observe how changes to these parameters affect the <code>cell_body_sum</code> and subsequently the final <code>firing_rate</code>. For instance, try making the weights larger, smaller, or even negative, and see how the neuron’s output responds to different input patterns. This helps build an intuition for how the neuron “reacts” to its inputs based on its learned parameters.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="single-neuron-as-a-linear-classifier" class="slide level2">
<h2><a name="classifier"></a>Single Neuron as a Linear Classifier</h2>
<p>A single neuron’s output \((_i w_i x_i + b)\) can be interpreted as a probability.</p>
<ul>
<li><strong>Binary Softmax Classifier (Logistic Regression)</strong>
<ul>
<li>\(P(y=1 x; w) = (_i w_i x_i + b)\)</li>
<li>\(P(y=0 x; w) = 1 - P(y=1 x; w)\)</li>
<li>Optimized with <strong>cross-entropy loss</strong>.</li>
</ul></li>
<li><strong>Binary SVM Classifier</strong>
<ul>
<li>The neuron’s output can be combined with a <strong>max-margin hinge loss</strong>.</li>
<li>The neuron effectively “fires” if the input falls into one class, and not for the other.</li>
</ul></li>
</ul>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p><strong>Regularization:</strong> In this context, regularization loss (e.g., L2) can be seen as “gradual forgetting” of synaptic weights.</p>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>A single neuron can be used to implement a binary classifier (e.g., binary Softmax or binary SVM classifiers).</p>
</div>
</div>
</div>
<aside class="notes">
<p>This slide highlights the classification capabilities of a single neuron. When combined with a sigmoid activation function, the neuron’s output, ranging from 0 to 1, can be directly interpreted as the probability of an input belonging to one of two classes. This forms the basis of a Binary Softmax classifier, often referred to as logistic regression. The model is trained by minimizing the cross-entropy loss, which measures the difference between predicted and true probabilities.</p>
<p>Alternatively, by attaching a max-margin hinge loss to the neuron’s output, it can be trained to function as a binary Support Vector Machine. In this case, the neuron aims to correctly classify inputs with a maximum margin between the classes.</p>
<p>The beauty of this is that the regularization terms used in these classifiers, like L2 regularization, can be seen from a biological perspective as a mechanism for “gradual forgetting,” naturally driving less important synaptic weights towards zero. This ensures that the model doesn’t become overly reliant on any single input feature, contributing to better generalization.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="commonly-used-activation-functions" class="slide level2">
<h2><a name="actfun"></a>3. Commonly Used Activation Functions</h2>
<p>An activation function (or non-linearity) takes a single number and performs a fixed mathematical operation.</p>
<div class="columns">
<div class="column" style="width:48%;">
<p><strong>Sigmoid Function</strong> \((x) = 1 / (1 + e^{-x})\)</p>
<ul>
<li>Squashes real numbers to range <strong>[0, 1]</strong>.</li>
<li>Historically popular for “firing rate” interpretation.</li>
</ul>
<div class="fig figcenter fighighlight">
<img src="https://cs231n.github.io/assets/nn1/sigmoid.jpeg" width="90%">
<div class="figcaption">
Sigmoid non-linearity.
</div>
</div>
<div class="callout callout-warning callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Warning</strong></p>
</div>
<div class="callout-content">
<p><strong>Drawbacks:</strong></p>
<ul>
<li><strong>Saturates and kills gradients:</strong> At tails (0 or 1), gradient is near zero, hindering learning.</li>
<li><strong>Non-zero-centered output:</strong> Can lead to zig-zagging gradient updates.</li>
</ul>
</div>
</div>
</div>
</div><div class="column" style="width:48%;">
<p><strong>Tanh Function</strong> \((x) = 2 (2x) -1\) * Squashes real numbers to range <strong>[-1, 1]</strong>. * <strong>Zero-centered output</strong> - an improvement over sigmoid.</p>
<div class="fig figcenter fighighlight">
<img src="https://cs231n.github.io/assets/nn1/tanh.jpeg" width="90%" style="border-left: 1px solid black;">
<div class="figcaption">
Tanh non-linearity.
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p><strong>Preferred over Sigmoid:</strong> Due to its zero-centered output, it generally performs better than sigmoid. Still suffers from saturation.</p>
</div>
</div>
</div>
</div></div>
<aside class="notes">
<p>Activation functions are crucial for introducing non-linearity into neural networks, allowing complex patterns to be learned. Without them, stacking multiple layers would simply result in another linear function, negating the benefits of depth.</p>
<p>The Sigmoid function, while historically significant, has severe limitations. Its “S” shape means that for very large positive or negative inputs, the output saturates quickly, causing its derivative (gradient) to become extremely small. During backpropagation, this small gradient effectively “kills” the learning signal for upstream layers, a problem known as the vanishing gradient problem. Additionally, its output is always positive, which can lead to inefficient “zig-zagging” in gradient descent as weight updates might always be in the same direction.</p>
<p>The Tanh function offers an improvement by being zero-centered, addressing one of sigmoid’s issues. However, it still suffers from the saturation problem, meaning large inputs will still lead to small gradients and hinder learning in those regions. Despite this, Tanh is generally preferred over Sigmoid when choosing classic activation functions.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="activation-functions-relu-and-variants" class="slide level2">
<h2>Activation Functions: ReLU and Variants</h2>
<div class="columns">
<div class="column" style="width:48%;">
<p><strong>Rectified Linear Unit (ReLU)</strong> \(f(x) = (0, x)\)</p>
<ul>
<li>Output is <strong>0 for negative input</strong>, \(x\) <strong>for positive input</strong>.</li>
<li><strong>Pros:</strong>
<ul>
<li>Accelerates convergence significantly.</li>
<li>Computationally efficient (simple thresholding).</li>
<li>Does not saturate in the positive region.</li>
</ul></li>
</ul>
<div class="fig figcenter fighighlight">
<img src="https://cs231n.github.io/assets/nn1/relu.jpeg" width="90%">
<div class="figcaption">
ReLU activation function.
</div>
</div>
</div><div class="column" style="width:48%;">
<p><strong>ReLU Cons &amp; Variants:</strong></p>
<ul>
<li><strong>“Dying ReLU” problem:</strong> Neurons can become inactive (output 0) for all future inputs if gradients are too large.</li>
<li><strong>Leaky ReLU:</strong> \(f(x) = (x &lt; 0) (x) + (x ) (x)\)
<ul>
<li>Introduces a small positive slope \(\) for negative inputs (e.g., 0.01).</li>
<li>Aims to prevent dying ReLUs.</li>
</ul></li>
<li><strong>Maxout:</strong> \((w_1^Tx+b_1, w_2^Tx + b_2)\)
<ul>
<li>Generalizes ReLU and its leaky version.</li>
<li>No saturation, no dying problem.</li>
<li><strong>Drawback:</strong> Doubles the number of parameters per neuron.</li>
</ul></li>
</ul>
<div class="callout callout-warning callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Warning</strong></p>
</div>
<div class="callout-content">
<p>For ReLU, monitor “dead” units. High learning rates can exacerbate the dying ReLU problem.</p>
</div>
</div>
</div>
</div></div>
<aside class="notes">
<p>ReLU has become the most popular activation function in deep learning. Its simple form, <code>max(0, x)</code>, offers significant advantages: it’s computationally very cheap and, crucially, it doesn’t suffer from saturation in the positive region, which greatly accelerates training compared to sigmoid or tanh.</p>
<p>However, ReLU has its own Achilles’ heel: the “dying ReLU” problem. If a neuron’s weights are updated in such a way that its output is always negative for all training examples, the ReLU function will consistently output zero. This means its gradient will also be zero, and the neuron will stop learning completely—it essentially “dies.”</p>
<p>To address this, variants like Leaky ReLU introduce a small, non-zero slope for negative inputs, ensuring that there’s always a gradient flowing through the neuron, even if it’s small. Maxout further generalizes this, offering even better properties but at the cost of increasing the number of parameters, which can make models more prone to overfitting if not properly regularized.</p>
<p>The TLDR for activation functions: <strong>Start with ReLU.</strong> Be mindful of its potential pitfalls and consider Leaky ReLU or Maxout if you encounter issues like dying neurons or want to push performance further. <strong>Avoid sigmoid in hidden layers.</strong> Tanh is an option, but generally, ReLU-based activations are preferred.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="interactive-activation-functions" class="slide level2">
<h2>Interactive Activation Functions</h2>
<p>Explore how Sigmoid, Tanh, and ReLU functions respond to different input values. Adjust the <code>Input X Value</code> slider to see the corresponding output for each function.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code hidden" id="cb1" data-startfrom="324" data-source-offset="0"><pre class="sourceCode numberSource js number-lines code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 323;"><span id="cb1-324"><a></a>viewof x_input <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="op">-</span><span class="dv">5</span><span class="op">,</span> <span class="dv">5</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="dv">0</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Input X Value"</span>})<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div id="ojs-cell-1" data-nodetype="declaration">

</div>
</div>
</div>
<div>
<div id="pyodide-2" class="exercise-cell">

</div>
<script type="pyodide-2-contents">
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
</script>
</div>
<aside class="notes">
<p>This interactive plot allows you to visually compare the behavior of the three main activation functions: Sigmoid, Tanh, and ReLU. Observe how the output <code>f(x)</code> changes as you move the <code>Input X Value</code> slider.</p>
<ul>
<li><strong>Sigmoid (Blue):</strong> Notice how it flattens out (saturates) at extreme positive and negative values, leading to very small slopes. Its output is always between 0 and 1.</li>
<li><strong>Tanh (Red):</strong> Similar to Sigmoid, it also saturates, but its output is centered around 0, ranging from -1 to 1.</li>
<li><strong>ReLU (Green):</strong> For positive inputs, it behaves linearly, maintaining a constant slope (gradient of 1). For negative inputs, it’s strictly zero. This linear behavior in the positive region is why it helps accelerate training.</li>
</ul>
<p>Pay close attention to the gradients (slopes) at different points. The flat regions for Sigmoid and Tanh demonstrate the “vanishing gradient” problem, which ReLU largely avoids in its positive region.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="neural-network-architectures" class="slide level2">
<h2><a name="nn"></a>4. Neural Network Architectures</h2>
<h3 id="layer-wise-organization"><a name="layers"></a>Layer-wise Organization</h3>
<p>Neural Networks are collections of neurons connected in an <strong>acyclic graph</strong>. Most common organizations are into distinct <strong>layers</strong>.</p>
<div class="columns">
<div class="column" style="width:48%;">
<p><strong>Fully-Connected Layer:</strong></p>
<ul>
<li>Neurons between adjacent layers are <strong>fully pairwise connected</strong>.</li>
<li>Neurons <em>within</em> a single layer share <strong>no connections</strong>.</li>
</ul>
<div class="fig figcenter fighighlight">
<img src="https://cs231n.github.io/assets/nn1/neural_net.jpeg" width="90%">
<div class="figcaption">
Two-layer Neural Network topology.
</div>
</div>
</div><div class="column" style="width:48%;">
<p><strong>Example - 3-Layer Network:</strong></p>
<ul>
<li>Three inputs.</li>
<li>Two hidden layers, each with 4 neurons.</li>
<li>One output layer.</li>
</ul>
<div class="fig figcenter fighighlight">
<img src="https://cs231n.github.io/assets/nn1/neural_net2.jpeg" width="90%" style="border-left: 1px solid black;">
<div class="figcaption">
Three-layer Neural Network topology.
</div>
</div>
</div></div>
<aside class="notes">
<p>Neural networks are structured as layers of interconnected neurons. The key rule is that connections are acyclic, meaning there are no feedback loops that would cause infinite computation in a feedforward network.</p>
<p>The most common layer type in a basic neural network is the “fully-connected” layer. In such a layer, every neuron in one layer sends its output to every neuron in the <em>next</em> layer. However, crucially, neurons <em>within the same layer</em> do not connect to each other. This clear, layered structure makes computation highly efficient, primarily through matrix operations.</p>
<p>The images show classic examples: a 2-layer network with one hidden layer and an output layer, and a 3-layer network with two hidden layers. Notice how each layer builds upon the outputs of the previous one, creating a hierarchical representation of the data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="neural-network-architecture-a-deeper-look" class="slide level2">
<h2>Neural Network Architecture: A Deeper Look</h2>
<p>Let’s visualize the connections in a 3-layer neural network with 3 inputs, two hidden layers of 4 neurons, and 1 output.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<svg width="960" height="480" viewbox="0.00 0.00 447.00 319.00" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style="; max-width: none; max-height: none; display: block; margin: auto auto auto auto">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 315)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-315 443,-315 443,4 -4,4"></polygon>
<g id="clust1" class="cluster">
<title>cluster_0</title>
<polygon fill="lightyellow" stroke="gray" points="1,-116 1,-194 82,-194 82,-116 1,-116"></polygon>
<text text-anchor="middle" x="41.5" y="-177.4" font-family="Times,serif" font-size="14.00">Input Layer</text>
</g>
<g id="clust2" class="cluster">
<title>cluster_1</title>
<polygon fill="none" stroke="gray" points="102,-8 102,-303 205,-303 205,-8 102,-8"></polygon>
<text text-anchor="middle" x="153.5" y="-286.4" font-family="Times,serif" font-size="14.00">Hidden Layer 1</text>
</g>
<g id="clust3" class="cluster">
<title>cluster_2</title>
<polygon fill="none" stroke="gray" points="225,-8 225,-303 328,-303 328,-8 225,-8"></polygon>
<text text-anchor="middle" x="276.5" y="-286.4" font-family="Times,serif" font-size="14.00">Hidden Layer 2</text>
</g>
<g id="clust4" class="cluster">
<title>cluster_3</title>
<polygon fill="none" stroke="gray" points="348,-110 348,-201 439,-201 439,-110 348,-110"></polygon>
<text text-anchor="middle" x="393.5" y="-184.4" font-family="Times,serif" font-size="14.00">Output Layer</text>
</g>
<!-- input_node -->
<g id="node1" class="node">
<title>input_node</title>
<polygon fill="palegreen" stroke="black" points="16,-125 16,-161 66,-161 66,-125 16,-125"></polygon>
<polygon fill="palegreen" stroke="transparent" points="25.5,-104 25.5,-182 56.5,-182 56.5,-104 25.5,-104"></polygon>
<polygon fill="none" stroke="black" points="26,-156 26,-182 57,-182 57,-156 26,-156"></polygon>
<text text-anchor="start" x="31" y="-164.4" font-family="Times,serif" font-size="14.00"> x0 </text>
<polygon fill="none" stroke="black" points="26,-130 26,-156 57,-156 57,-130 26,-130"></polygon>
<text text-anchor="start" x="31" y="-138.4" font-family="Times,serif" font-size="14.00"> x1 </text>
<polygon fill="none" stroke="black" points="26,-104 26,-130 57,-130 57,-104 26,-104"></polygon>
<text text-anchor="start" x="31" y="-112.4" font-family="Times,serif" font-size="14.00"> x2 </text>
</g>
<!-- H1_0 -->
<g id="node2" class="node">
<title>H1_0</title>
<ellipse fill="lightblue" stroke="black" cx="153" cy="-245" rx="25" ry="25"></ellipse>
<text text-anchor="middle" x="153" y="-240.8" font-family="Times,serif" font-size="14.00">H1_0</text>
</g>
<!-- input_node&#45;&gt;H1_0 -->
<g id="edge1" class="edge">
<title>input_node:x0-&gt;H1_0</title>
<path fill="none" stroke="black" d="M66.45,-143.45C85.32,-162.54 82.53,-174.05 102,-193 110.73,-201.5 121.81,-209.27 131.33,-216.92"></path>
<polygon fill="black" stroke="black" points="129.3,-219.79 139.18,-223.61 133.84,-214.46 129.3,-219.79"></polygon>
</g>
<!-- input_node&#45;&gt;H1_0 -->
<g id="edge5" class="edge">
<title>input_node:x1-&gt;H1_0</title>
<path fill="none" stroke="black" d="M66.45,-161.45C85.32,-180.54 82.53,-192.05 102,-211 108.03,-216.87 115.18,-222.4 122.17,-227.22"></path>
<polygon fill="black" stroke="black" points="120.55,-230.35 130.83,-232.93 124.41,-224.5 120.55,-230.35"></polygon>
</g>
<!-- input_node&#45;&gt;H1_0 -->
<g id="edge9" class="edge">
<title>input_node:x2-&gt;H1_0</title>
<path fill="none" stroke="black" d="M47.39,-161.16C52.72,-168.12 60.33,-173.33 66,-179 85.36,-198.36 82.38,-209.9 102,-229 107.21,-234.07 113.25,-238.88 119.3,-242.85"></path>
<polygon fill="black" stroke="black" points="117.67,-245.95 128.07,-247.99 121.22,-239.91 117.67,-245.95"></polygon>
</g>
<!-- H1_1 -->
<g id="node3" class="node">
<title>H1_1</title>
<ellipse fill="lightblue" stroke="black" cx="153" cy="-177" rx="25" ry="25"></ellipse>
<text text-anchor="middle" x="153" y="-172.8" font-family="Times,serif" font-size="14.00">H1_1</text>
</g>
<!-- input_node&#45;&gt;H1_1 -->
<g id="edge2" class="edge">
<title>input_node:x0-&gt;H1_1</title>
<path fill="none" stroke="black" d="M66.19,-139.19C83.46,-141.97 106.38,-148.87 124.18,-156.6"></path>
<polygon fill="black" stroke="black" points="122.94,-159.88 133.48,-160.96 125.91,-153.54 122.94,-159.88"></polygon>
</g>
<!-- input_node&#45;&gt;H1_1 -->
<g id="edge6" class="edge">
<title>input_node:x1-&gt;H1_1</title>
<path fill="none" stroke="black" d="M66.19,-150.72C81.76,-155.49 101.93,-161.66 118.8,-166.83"></path>
<polygon fill="black" stroke="black" points="118.22,-170.31 128.8,-169.9 120.27,-163.62 118.22,-170.31"></polygon>
</g>
<!-- input_node&#45;&gt;H1_1 -->
<g id="edge10" class="edge">
<title>input_node:x2-&gt;H1_1</title>
<path fill="none" stroke="black" d="M63.73,-161.1C79.42,-168.49 100.57,-175.43 118.22,-179.19"></path>
<polygon fill="black" stroke="black" points="117.68,-182.65 128.15,-181.01 118.95,-175.76 117.68,-182.65"></polygon>
</g>
<!-- H1_2 -->
<g id="node4" class="node">
<title>H1_2</title>
<ellipse fill="lightblue" stroke="black" cx="153" cy="-109" rx="25" ry="25"></ellipse>
<text text-anchor="middle" x="153" y="-104.8" font-family="Times,serif" font-size="14.00">H1_2</text>
</g>
<!-- input_node&#45;&gt;H1_2 -->
<g id="edge3" class="edge">
<title>input_node:x0-&gt;H1_2</title>
<path fill="none" stroke="black" d="M63.73,-124.9C79.42,-117.51 100.57,-110.57 118.22,-106.81"></path>
<polygon fill="black" stroke="black" points="118.95,-110.24 128.15,-104.99 117.68,-103.35 118.95,-110.24"></polygon>
</g>
<!-- input_node&#45;&gt;H1_2 -->
<g id="edge7" class="edge">
<title>input_node:x1-&gt;H1_2</title>
<path fill="none" stroke="black" d="M66.19,-135.28C81.76,-130.51 101.93,-124.34 118.8,-119.17"></path>
<polygon fill="black" stroke="black" points="120.27,-122.38 128.8,-116.1 118.22,-115.69 120.27,-122.38"></polygon>
</g>
<!-- input_node&#45;&gt;H1_2 -->
<g id="edge11" class="edge">
<title>input_node:x2-&gt;H1_2</title>
<path fill="none" stroke="black" d="M66.19,-146.81C83.46,-144.03 106.38,-137.13 124.18,-129.4"></path>
<polygon fill="black" stroke="black" points="125.91,-132.46 133.48,-125.04 122.94,-126.12 125.91,-132.46"></polygon>
</g>
<!-- H1_3 -->
<g id="node5" class="node">
<title>H1_3</title>
<ellipse fill="lightblue" stroke="black" cx="153" cy="-41" rx="25" ry="25"></ellipse>
<text text-anchor="middle" x="153" y="-36.8" font-family="Times,serif" font-size="14.00">H1_3</text>
</g>
<!-- input_node&#45;&gt;H1_3 -->
<g id="edge4" class="edge">
<title>input_node:x0-&gt;H1_3</title>
<path fill="none" stroke="black" d="M47.39,-124.84C52.72,-117.88 60.33,-112.67 66,-107 85.36,-87.64 82.38,-76.1 102,-57 107.21,-51.93 113.25,-47.12 119.3,-43.15"></path>
<polygon fill="black" stroke="black" points="121.22,-46.09 128.07,-38.01 117.67,-40.05 121.22,-46.09"></polygon>
</g>
<!-- input_node&#45;&gt;H1_3 -->
<g id="edge8" class="edge">
<title>input_node:x1-&gt;H1_3</title>
<path fill="none" stroke="black" d="M66.45,-124.55C85.32,-105.46 82.53,-93.95 102,-75 108.03,-69.13 115.18,-63.6 122.17,-58.78"></path>
<polygon fill="black" stroke="black" points="124.41,-61.5 130.83,-53.07 120.55,-55.65 124.41,-61.5"></polygon>
</g>
<!-- input_node&#45;&gt;H1_3 -->
<g id="edge12" class="edge">
<title>input_node:x2-&gt;H1_3</title>
<path fill="none" stroke="black" d="M66.45,-142.55C85.32,-123.46 82.53,-111.95 102,-93 110.73,-84.5 121.81,-76.73 131.33,-69.08"></path>
<polygon fill="black" stroke="black" points="133.84,-71.54 139.18,-62.39 129.3,-66.21 133.84,-71.54"></polygon>
</g>
<!-- H2_0 -->
<g id="node6" class="node">
<title>H2_0</title>
<ellipse fill="lightblue" stroke="black" cx="276" cy="-245" rx="25" ry="25"></ellipse>
<text text-anchor="middle" x="276" y="-240.8" font-family="Times,serif" font-size="14.00">H2_0</text>
</g>
<!-- H1_0&#45;&gt;H2_0 -->
<g id="edge13" class="edge">
<title>H1_0-&gt;H2_0</title>
<path fill="none" stroke="black" d="M178.2,-245C196.08,-245 220.69,-245 240.62,-245"></path>
<polygon fill="black" stroke="black" points="240.85,-248.5 250.85,-245 240.85,-241.5 240.85,-248.5"></polygon>
</g>
<!-- H2_1 -->
<g id="node7" class="node">
<title>H2_1</title>
<ellipse fill="lightblue" stroke="black" cx="276" cy="-177" rx="25" ry="25"></ellipse>
<text text-anchor="middle" x="276" y="-172.8" font-family="Times,serif" font-size="14.00">H2_1</text>
</g>
<!-- H1_0&#45;&gt;H2_1 -->
<g id="edge14" class="edge">
<title>H1_0-&gt;H2_1</title>
<path fill="none" stroke="black" d="M175.08,-233.16C194.39,-222.3 223.2,-206.11 244.93,-193.9"></path>
<polygon fill="black" stroke="black" points="246.76,-196.89 253.76,-188.94 243.33,-190.79 246.76,-196.89"></polygon>
</g>
<!-- H2_2 -->
<g id="node8" class="node">
<title>H2_2</title>
<ellipse fill="lightblue" stroke="black" cx="276" cy="-109" rx="25" ry="25"></ellipse>
<text text-anchor="middle" x="276" y="-104.8" font-family="Times,serif" font-size="14.00">H2_2</text>
</g>
<!-- H1_0&#45;&gt;H2_2 -->
<g id="edge15" class="edge">
<title>H1_0-&gt;H2_2</title>
<path fill="none" stroke="black" d="M175.99,-234.83C186.14,-229.13 197.56,-221.11 205,-211 223.67,-185.63 206.44,-168.46 225,-143 230.2,-135.87 237.38,-129.75 244.68,-124.75"></path>
<polygon fill="black" stroke="black" points="246.58,-127.69 253.18,-119.41 242.85,-121.76 246.58,-127.69"></polygon>
</g>
<!-- H2_3 -->
<g id="node9" class="node">
<title>H2_3</title>
<ellipse fill="lightblue" stroke="black" cx="276" cy="-41" rx="25" ry="25"></ellipse>
<text text-anchor="middle" x="276" y="-36.8" font-family="Times,serif" font-size="14.00">H2_3</text>
</g>
<!-- H1_0&#45;&gt;H2_3 -->
<g id="edge16" class="edge">
<title>H1_0-&gt;H2_3</title>
<path fill="none" stroke="black" d="M176.37,-235.4C186.73,-229.8 198.23,-221.7 205,-211 237.68,-159.38 192.56,-126.77 225,-75 229.73,-67.46 236.81,-61.18 244.17,-56.16"></path>
<polygon fill="black" stroke="black" points="246.13,-59.06 252.81,-50.84 242.46,-53.1 246.13,-59.06"></polygon>
</g>
<!-- H1_1&#45;&gt;H2_0 -->
<g id="edge17" class="edge">
<title>H1_1-&gt;H2_0</title>
<path fill="none" stroke="black" d="M175.08,-188.84C194.39,-199.7 223.2,-215.89 244.93,-228.1"></path>
<polygon fill="black" stroke="black" points="243.33,-231.21 253.76,-233.06 246.76,-225.11 243.33,-231.21"></polygon>
</g>
<!-- H1_1&#45;&gt;H2_1 -->
<g id="edge18" class="edge">
<title>H1_1-&gt;H2_1</title>
<path fill="none" stroke="black" d="M178.2,-177C196.08,-177 220.69,-177 240.62,-177"></path>
<polygon fill="black" stroke="black" points="240.85,-180.5 250.85,-177 240.85,-173.5 240.85,-180.5"></polygon>
</g>
<!-- H1_1&#45;&gt;H2_2 -->
<g id="edge19" class="edge">
<title>H1_1-&gt;H2_2</title>
<path fill="none" stroke="black" d="M175.08,-165.16C194.39,-154.3 223.2,-138.11 244.93,-125.9"></path>
<polygon fill="black" stroke="black" points="246.76,-128.89 253.76,-120.94 243.33,-122.79 246.76,-128.89"></polygon>
</g>
<!-- H1_1&#45;&gt;H2_3 -->
<g id="edge20" class="edge">
<title>H1_1-&gt;H2_3</title>
<path fill="none" stroke="black" d="M175.99,-166.83C186.14,-161.13 197.56,-153.11 205,-143 223.67,-117.63 206.44,-100.46 225,-75 230.2,-67.87 237.38,-61.75 244.68,-56.75"></path>
<polygon fill="black" stroke="black" points="246.58,-59.69 253.18,-51.41 242.85,-53.76 246.58,-59.69"></polygon>
</g>
<!-- H1_2&#45;&gt;H2_0 -->
<g id="edge21" class="edge">
<title>H1_2-&gt;H2_0</title>
<path fill="none" stroke="black" d="M175.99,-119.17C186.14,-124.87 197.56,-132.89 205,-143 223.67,-168.37 206.44,-185.54 225,-211 230.2,-218.13 237.38,-224.25 244.68,-229.25"></path>
<polygon fill="black" stroke="black" points="242.85,-232.24 253.18,-234.59 246.58,-226.31 242.85,-232.24"></polygon>
</g>
<!-- H1_2&#45;&gt;H2_1 -->
<g id="edge22" class="edge">
<title>H1_2-&gt;H2_1</title>
<path fill="none" stroke="black" d="M175.08,-120.84C194.39,-131.7 223.2,-147.89 244.93,-160.1"></path>
<polygon fill="black" stroke="black" points="243.33,-163.21 253.76,-165.06 246.76,-157.11 243.33,-163.21"></polygon>
</g>
<!-- H1_2&#45;&gt;H2_2 -->
<g id="edge23" class="edge">
<title>H1_2-&gt;H2_2</title>
<path fill="none" stroke="black" d="M178.2,-109C196.08,-109 220.69,-109 240.62,-109"></path>
<polygon fill="black" stroke="black" points="240.85,-112.5 250.85,-109 240.85,-105.5 240.85,-112.5"></polygon>
</g>
<!-- H1_2&#45;&gt;H2_3 -->
<g id="edge24" class="edge">
<title>H1_2-&gt;H2_3</title>
<path fill="none" stroke="black" d="M175.08,-97.16C194.39,-86.3 223.2,-70.11 244.93,-57.9"></path>
<polygon fill="black" stroke="black" points="246.76,-60.89 253.76,-52.94 243.33,-54.79 246.76,-60.89"></polygon>
</g>
<!-- H1_3&#45;&gt;H2_0 -->
<g id="edge25" class="edge">
<title>H1_3-&gt;H2_0</title>
<path fill="none" stroke="black" d="M176.37,-50.6C186.73,-56.2 198.23,-64.3 205,-75 237.68,-126.62 192.56,-159.23 225,-211 229.73,-218.54 236.81,-224.82 244.17,-229.84"></path>
<polygon fill="black" stroke="black" points="242.46,-232.9 252.81,-235.16 246.13,-226.94 242.46,-232.9"></polygon>
</g>
<!-- H1_3&#45;&gt;H2_1 -->
<g id="edge26" class="edge">
<title>H1_3-&gt;H2_1</title>
<path fill="none" stroke="black" d="M175.99,-51.17C186.14,-56.87 197.56,-64.89 205,-75 223.67,-100.37 206.44,-117.54 225,-143 230.2,-150.13 237.38,-156.25 244.68,-161.25"></path>
<polygon fill="black" stroke="black" points="242.85,-164.24 253.18,-166.59 246.58,-158.31 242.85,-164.24"></polygon>
</g>
<!-- H1_3&#45;&gt;H2_2 -->
<g id="edge27" class="edge">
<title>H1_3-&gt;H2_2</title>
<path fill="none" stroke="black" d="M175.08,-52.84C194.39,-63.7 223.2,-79.89 244.93,-92.1"></path>
<polygon fill="black" stroke="black" points="243.33,-95.21 253.76,-97.06 246.76,-89.11 243.33,-95.21"></polygon>
</g>
<!-- H1_3&#45;&gt;H2_3 -->
<g id="edge28" class="edge">
<title>H1_3-&gt;H2_3</title>
<path fill="none" stroke="black" d="M178.2,-41C196.08,-41 220.69,-41 240.62,-41"></path>
<polygon fill="black" stroke="black" points="240.85,-44.5 250.85,-41 240.85,-37.5 240.85,-44.5"></polygon>
</g>
<!-- O0 -->
<g id="node10" class="node">
<title>O0</title>
<ellipse fill="salmon" stroke="black" cx="393" cy="-143" rx="25" ry="25"></ellipse>
<text text-anchor="middle" x="393" y="-138.8" font-family="Times,serif" font-size="14.00">O0</text>
</g>
<!-- H2_0&#45;&gt;O0 -->
<g id="edge29" class="edge">
<title>H2_0-&gt;O0</title>
<path fill="none" stroke="black" d="M298.12,-232.45C307.61,-226.43 318.74,-218.83 328,-211 342.76,-198.52 357.66,-182.91 369.36,-169.8"></path>
<polygon fill="black" stroke="black" points="372.24,-171.83 376.21,-162 366.98,-167.21 372.24,-171.83"></polygon>
</g>
<!-- H2_1&#45;&gt;O0 -->
<g id="edge30" class="edge">
<title>H2_1-&gt;O0</title>
<path fill="none" stroke="black" d="M300.27,-170.12C317.18,-165.12 340.31,-158.28 359.11,-152.72"></path>
<polygon fill="black" stroke="black" points="360.17,-156.06 368.76,-149.87 358.18,-149.35 360.17,-156.06"></polygon>
</g>
<!-- H2_2&#45;&gt;O0 -->
<g id="edge31" class="edge">
<title>H2_2-&gt;O0</title>
<path fill="none" stroke="black" d="M300.27,-115.88C317.18,-120.88 340.31,-127.72 359.11,-133.28"></path>
<polygon fill="black" stroke="black" points="358.18,-136.65 368.76,-136.13 360.17,-129.94 358.18,-136.65"></polygon>
</g>
<!-- H2_3&#45;&gt;O0 -->
<g id="edge32" class="edge">
<title>H2_3-&gt;O0</title>
<path fill="none" stroke="black" d="M298.12,-53.55C307.61,-59.57 318.74,-67.17 328,-75 342.76,-87.48 357.66,-103.09 369.36,-116.2"></path>
<polygon fill="black" stroke="black" points="366.98,-118.79 376.21,-124 372.24,-114.17 366.98,-118.79"></polygon>
</g>
</g>
</svg>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<aside class="notes">
<p>This Graphviz diagram provides a more explicit visualization of the connections within a 3-layer fully-connected neural network. You can clearly see the distinct input, hidden (two layers), and output layers.</p>
<p>Each input <code>x0</code>, <code>x1</code>, <code>x2</code> connected to all neurons in the first hidden layer. Similarly, all neurons in Hidden Layer 1 connect to all neurons in Hidden Layer 2, and all neurons in Hidden Layer 2 connect to the single output neuron. This “all-to-all” connectivity <em>between</em> layers defines a fully-connected architecture. Notice the absence of connections <em>within</em> any given layer, confirming the layered structure. This diagram helps solidify the concept of how information flows through the network.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="naming-conventions-sizing-neural-networks" class="slide level2 scrollable">
<h2>Naming Conventions &amp; Sizing Neural Networks</h2>
<ul>
<li><strong>N-layer network</strong>: Number of <em>hidden layers</em> + output layer (input layer is not usually counted).
<ul>
<li>Single-layer network: No hidden layers (e.g., Logistic Regression).</li>
<li><strong>Artificial Neural Networks (ANN)</strong> or <strong>Multi-Layer Perceptrons (MLP)</strong> are common synonyms.</li>
<li>“Units” is a more general term than “neurons”.</li>
</ul></li>
<li><strong>Output Layer</strong>: Typically has no activation function (or linear identity) for class scores or regression targets.</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Sizing Metrics:</strong></p>
<ol type="1">
<li><strong>Number of neurons</strong> (excluding input).</li>
<li><strong>Number of parameters</strong> (weights + biases) - more common.</li>
</ol>
<p><strong>Example 1 (2-layer NN):</strong></p>
<ul>
<li>4 hidden, 2 output neurons.</li>
<li>Weights: \([3 ] + [4 ] = 12 + 8 = 20\)</li>
<li>Biases: \(4 + 2 = 6\)</li>
<li>Total Parameters: \(20 + 6 = 26\)</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Example 2 (3-layer NN):</strong></p>
<ul>
<li>4 hidden (H1), 4 hidden (H2), 1 output neuron.</li>
<li>Weights: \([3 ] + [4 ] + [4 ] = 12 + 16 + 4 = 32\)</li>
<li>Biases: \(4 + 4 + 1 = 9\)</li>
<li>Total Parameters: \(32 + 9 = 41\)</li>
</ul>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p>Modern Convolutional Networks can have 10-20 layers and &gt;100 million parameters (“deep learning”).</p>
</div>
</div>
</div>
</div></div>
<aside class="notes">
<p>Understanding how to talk about and size neural networks is fundamental. When we refer to an “N-layer neural network,” N typically refers to the number of <em>hidden layers plus the output layer</em>, excluding the input layer. So, a network with just an input and output layer (like logistic regression) is often called a one-layer network.</p>
<p>The most practical way to measure a network’s size is by its total number of <strong>learnable parameters</strong>, which include all the weights and biases. I’ve broken down the calculation for the two example networks from previous slides. This kind of calculation is crucial for estimating computational requirements and potential for overfitting.</p>
<p>Notice that modern deep learning models, especially Convolutional Neural Networks, can be orders of magnitude larger, with many more layers and parameters. This massive scale is what truly enables them to learn incredibly complex patterns, but also necessitates advanced optimization and regularization techniques.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="example-feed-forward-computation" class="slide level2">
<h2><a name="feedforward"></a>Example: Feed-Forward Computation</h2>
<p>The layered structure allows for efficient computation using <strong>matrix vector operations</strong>.</p>
<p>Consider a 3-layer network:</p>
<ul>
<li>Input \(x\): [3x1] vector.</li>
<li>First hidden layer weights \(W_1\): [4x3] matrix, biases \(b_1\): [4x1] vector.</li>
<li>Second hidden layer weights \(W_2\): [4x4] matrix, biases \(b_2\): [4x1] vector.</li>
<li>Output layer weights \(W_3\): [1x4] matrix, biases \(b_3\): [1x1] vector.</li>
</ul>
<p>The full forward pass:</p>
<div>
<div id="pyodide-3" class="exercise-cell">

</div>
<script type="pyodide-3-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWUsIm1heC1saW5lcyI6MTB9LCJjb2RlIjoiIyBmb3J3YXJkLXBhc3Mgb2YgYSAzLWxheWVyIG5ldXJhbCBuZXR3b3JrICh1c2luZyBzaWdtb2lkIGFjdGl2YXRpb24pOlxuaW1wb3J0IG51bXB5IGFzIG5wXG5cbmYgPSBsYW1iZGEgeDogMS4wLygxLjAgKyBucC5leHAoLXgpKSAjIGFjdGl2YXRpb24gZnVuY3Rpb24gKHNpZ21vaWQpXG5cbiMgLS0tIFNpbXVsYXRlIHJhbmRvbSBsZWFybmFibGUgcGFyYW1ldGVycyAtLS1cblcxID0gbnAucmFuZG9tLnJhbmRuKDQsIDMpIFxuYjEgPSBucC5yYW5kb20ucmFuZG4oNCwgMSlcblcyID0gbnAucmFuZG9tLnJhbmRuKDQsIDQpXG5iMiA9IG5wLnJhbmRvbS5yYW5kbig0LCAxKVxuVzMgPSBucC5yYW5kb20ucmFuZG4oMSwgNClcbmIzID0gbnAucmFuZG9tLnJhbmRuKDEsIDEpXG4jIC0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS1cblxueCA9IG5wLnJhbmRvbS5yYW5kbigzLCAxKSAjIHJhbmRvbSBpbnB1dCB2ZWN0b3Igb2YgdGhyZWUgbnVtYmVycyAoM3gxKVxuXG5oMSA9IGYobnAuZG90KFcxLCB4KSArIGIxKSAjIGNhbGN1bGF0ZSBmaXJzdCBoaWRkZW4gbGF5ZXIgYWN0aXZhdGlvbnMgKDR4MSlcbmgyID0gZihucC5kb3QoVzIsIGgxKSArIGIyKSAjIGNhbGN1bGF0ZSBzZWNvbmQgaGlkZGVuIGxheWVyIGFjdGl2YXRpb25zICg0eDEpXG5vdXQgPSBucC5kb3QoVzMsIGgyKSArIGIzICAjIG91dHB1dCBuZXVyb24gKDF4MSlcblxucHJpbnQoXCJJbnB1dCB4OlxcblwiLCB4KVxucHJpbnQoXCJcXG5GaXJzdCBoaWRkZW4gbGF5ZXIgb3V0cHV0IChoMSk6XFxuXCIsIGgxKVxucHJpbnQoXCJcXG5TZWNvbmQgaGlkZGVuIGxheWVyIG91dHB1dCAoaDIpOlxcblwiLCBoMilcbnByaW50KFwiXFxuRmluYWwgb3V0cHV0IChvdXQpOlxcblwiLCBvdXQpIn0=
</script>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>The forward pass of a fully-connected layer corresponds to one matrix multiplication followed by a bias offset and an activation function.</p>
</div>
</div>
</div>
<aside class="notes">
<p>This slide demonstrates the core computational process of a neural network: the forward pass. Thanks to its layered and fully-connected structure, this process can be highly optimized using linear algebra operations, specifically matrix multiplications.</p>
<p>The Python code snippet shows precisely how this works. Each layer’s computation involves <code>np.dot(W, input_from_prev_layer)</code> for the weighted sum, <code>+ b</code> for the bias, and then applying the non-linear activation function <code>f</code>. This is repeated for each layer until the final output is produced.</p>
<p>A crucial point for efficiency is that the input <code>x</code> can also be an entire batch of training data (where each example is a column in a matrix). This allows for parallel computation across multiple examples, a technique known as mini-batching, which is fundamental to modern deep learning training. Also, remember the last layer often doesn’t have an activation if it’s producing raw scores or regression values.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="representational-power-universal-approximators" class="slide level2">
<h2><a name="power"></a>Representational Power: Universal Approximators</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Are there functions a Neural Network cannot model?</strong></p>
<ul>
<li><strong>Universal Approximation Theorem:</strong>
<ul>
<li>A Neural Network with one hidden layer (and a reasonable non-linearity, e.g., sigmoid) can approximate <em>any continuous function</em> to an arbitrary degree of accuracy.</li>
<li>\( f(x), &gt; 0 g(x) f(x) - g(x) &lt; \)</li>
</ul></li>
</ul>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Mathematically, a single hidden layer is sufficient, but this doesn’t tell us about practical learning or performance.</p>
</div>
</div>
</div>
</div><div class="column" style="width:50%;">
<p><strong>Why go deeper then?</strong></p>
<ul>
<li>The “universal approximator” statement is theoretically comforting but <em>practically weak</em>.</li>
<li>Deeper networks (multiple hidden layers) often perform <strong>empirically better</strong>.</li>
<li>They learn more compact, hierarchical, and abstract representations.</li>
<li>Especially true for data with inherent hierarchical structure (e.g., images: edges <span class="math inline">\(\rightarrow\)</span> textures <span class="math inline">\(\rightarrow\)</span> objects).</li>
</ul>
<p><a href="http://www.dartmouth.edu/~gvc/Cybenko_MCSS.pdf">Cybenko (1989)</a> and <a href="http://neuralnetworksanddeeplearning.com/chap4.html">Michael Nielsen’s intuitive explanation</a> details this.</p>
</div></div>
<aside class="notes">
<p>The “Universal Approximation Theorem” is a powerful theoretical result that states, in essence, that a neural network with just <em>one</em> hidden layer is capable of approximating any continuous function to any desired degree of accuracy, provided it has enough neurons. This is a profound statement about the expressive power of neural networks.</p>
<p>However, theory often diverges from practice. While a single hidden layer <em>can</em> theoretically approximate any function, the number of neurons required might be astronomically large, making it impractical to train. Furthermore, deep networks (with multiple hidden layers) have been empirically shown to perform significantly better on many complex tasks.</p>
<p>Why? Deeper networks are hypothesized to learn more hierarchical and abstract representations of the data. For instance, in image recognition, a first layer might detect simple edges, a second might combine edges into textures, a third might combine textures into parts of objects, and so on. This hierarchical learning is more aligned with how meaningful features are structured in much of the real-world data we encounter, making deep networks not just theoretically capable, but practically powerful and efficient to learn.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="setting-number-of-layers-and-their-sizes" class="slide level2">
<h2><a name="arch"></a>Setting Number of Layers and Their Sizes</h2>
<p><strong>Network Capacity:</strong> The ability of a model to approximate complex functions. Increases with more layers and neurons.</p>
<div class="fig figcenter fighighlight">
<img src="https://cs231n.github.io/assets/nn1/layer_sizes.jpeg" width="90%">
<div class="figcaption">
Larger NNs can represent more complicated functions. Circles are data points, colors are classes, decision regions by trained NNs. (<a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html">ConvNetsJS demo</a>)
</div>
</div>
<ul>
<li><strong>Overfitting:</strong> When a high-capacity model learns noise in training data instead of underlying patterns.
<ul>
<li>Left: 1 hidden neuron - too low capacity, underfits.</li>
<li>Middle: 3 hidden neurons - good balance.</li>
<li>Right: 20 hidden neurons - very high capacity, potentially overfits by creating complex, disjoint decision regions.</li>
</ul></li>
</ul>
<aside class="notes">
<p>When designing a neural network, a core decision is its architecture: how many layers and how many neurons per layer. This directly impacts the network’s “capacity” – its ability to learn and represent complex functions.</p>
<p>The image vividly demonstrates this. A network with too few neurons (1 hidden neuron) has low capacity, and can’t even separate the two classes effectively, leading to <strong>underfitting</strong>. Conversely, a network with many neurons (20 hidden neurons) has very high capacity. While it can perfectly fit <em>all</em> training data, including noise (like the red points within the green cluster), this can lead to <strong>overfitting</strong>. The highly complex, jagged decision boundary on the right is a hallmark of overfitting; it may perform poorly on unseen data because it learned the peculiarities of the training set rather than the generalizable patterns. The goal is to find a balance, like the network with 3 hidden neurons, which provides a smoother, more generalizable decision boundary.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="controlling-overfitting-prioritizing-regularization" class="slide level2">
<h2>Controlling Overfitting: Prioritizing Regularization</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Counterintuitive Advice:</strong></p>
<ul>
<li><strong>Don’t use smaller networks to prevent overfitting.</strong></li>
<li>Smaller networks are harder to train effectively with gradient descent; they often converge to “bad” local minima.</li>
<li>Larger networks have many more local minima, but these tend to be <em>better</em> in terms of actual loss.</li>
</ul>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Always use as big of a neural network as your computational budget allows!</p>
</div>
</div>
</div>
</div><div class="column" style="width:50%;">
<p><strong>Preferred Strategy:</strong></p>
<ul>
<li>Use a large network to ensure high capacity.</li>
<li>Control overfitting with robust <strong>regularization techniques</strong>.</li>
</ul>
<div class="fig figcenter fighighlight">
<img src="https://cs231n.github.io/assets/nn1/reg_strengths.jpeg" width="90%">
<div class="figcaption">
Effects of regularization strength (20 hidden neurons each). Stronger regularization yields smoother decision regions. (<a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html">ConvNetsJS demo</a>)
</div>
</div>
</div></div>
<aside class="notes">
<p>This slide presents a crucial, often counterintuitive, piece of advice in neural network design. While small networks seem appealing to prevent overfitting, they are actually harder to train well. Their loss landscapes can be problematic, leading optimizers to get stuck in poor local minima, resulting in suboptimal performance.</p>
<p>Larger networks, despite their higher capacity, offer a more advantageous training landscape. While they have more local minima, these minima tend to yield much better actual loss values. This means a large network, if properly managed, is more likely to converge to a good solution.</p>
<p>Therefore, the recommended strategy is to “go big” with your network architecture (within computational limits) and then actively manage overfitting using <strong>regularization techniques</strong>. The image on the right illustrates how regularization, like increasing L2 weight decay, can smooth out the decision boundaries even for a large network, making it more generalizable to new data. We will explore various regularization methods like L2 regularization and dropout in detail in later modules.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="summary" class="slide level2">
<h2><a name="summary"></a>Summary</h2>
<ul>
<li>Introduced a coarse model of a <strong>biological neuron</strong> and its computational counterpart.</li>
<li>Explored various <strong>activation functions</strong> (Sigmoid, Tanh, ReLU, Leaky ReLU, Maxout), with ReLU being the most common choice today.</li>
<li>Defined <strong>Neural Networks</strong> with <strong>Fully-Connected layers</strong>, characterized by pairwise connections between adjacent layers.</li>
<li>Understood how this layered architecture allows for <strong>efficient feed-forward computation</strong> via matrix multiplications.</li>
<li>Discussed that Neural Networks are <strong>universal function approximators</strong>, and why deep layers are still empirically preferred in practice.</li>
<li>Emphasized using <strong>large networks</strong> and controlling overfitting with <strong>strong regularization</strong> rather than limiting network size.</li>
</ul>
</section>
<section id="additional-references" class="slide level2">
<h2><a name="add"></a>Additional References</h2>
<ul>
<li><a href="http://www.deeplearning.net/tutorial/mlp.html">deeplearning.net tutorial</a> with Theano</li>
<li><a href="http://cs.stanford.edu/people/karpathy/convnetjs/">ConvNetJS</a> demos for intuitions</li>
<li><a href="http://neuralnetworksanddeeplearning.com/chap1.html">Michael Nielsen’s</a> tutorials</li>
</ul>


<script type="pyodide-data">
eyJvcHRpb25zIjp7ImVudiI6eyJQTE9UTFlfUkVOREVSRVIiOiJwbG90bHlfbWltZXR5cGUifSwiaW5kZXhVUkwiOiJodHRwczovL2Nkbi5qc2RlbGl2ci5uZXQvcHlvZGlkZS92MC4yNy4wL2Z1bGwvIn0sInBhY2thZ2VzIjp7InBrZ3MiOlsicHlvZGlkZV9odHRwIiwibWljcm9waXAiLCJpcHl0aG9uIiwibnVtcHkiLCJwbG90bHkiLCJuYmZvcm1hdCJdfX0=
</script>
<script type="ojs-module-contents">
{"contents":[{"cellName":"pyodide-3","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_3 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-3-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-3-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_3 = pyodideOjs.process(_pyodide_editor_3, {});\n"},{"cellName":"pyodide-2","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_2 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-2-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-2-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_2 = pyodideOjs.process(_pyodide_editor_2, {x_input});\n"},{"cellName":"pyodide-1","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_1 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-1-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-1-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_1 = pyodideOjs.process(_pyodide_editor_1, {});\n"},{"cellName":"pyodide-prelude","inline":false,"methodName":"interpretQuiet","source":"pyodideOjs = {\n  const {\n    PyodideEvaluator,\n    PyodideEnvironmentManager,\n    setupPython,\n    startPyodideWorker,\n    b64Decode,\n    collapsePath,\n  } = window._exercise_ojs_runtime;\n\n  const statusContainer = document.getElementById(\"exercise-loading-status\");\n  const indicatorContainer = document.getElementById(\"exercise-loading-indicator\");\n  indicatorContainer.classList.remove(\"d-none\");\n\n  let statusText = document.createElement(\"div\")\n  statusText.classList = \"exercise-loading-details\";\n  statusText = statusContainer.appendChild(statusText);\n  statusText.textContent = `Initialise`;\n\n  // Hoist indicator out from final slide when running under reveal\n  const revealStatus = document.querySelector(\".reveal .exercise-loading-indicator\");\n  if (revealStatus) {\n    revealStatus.remove();\n    document.querySelector(\".reveal > .slides\").appendChild(revealStatus);\n  }\n\n  // Make any reveal slides with live cells scrollable\n  document.querySelectorAll(\".reveal .exercise-cell\").forEach((el) => {\n    el.closest('section.slide').classList.add(\"scrollable\");\n  })\n\n  // Pyodide supplemental data and options\n  const dataContent = document.querySelector(`script[type=\\\"pyodide-data\\\"]`).textContent;\n  const data = JSON.parse(b64Decode(dataContent));\n\n  // Grab list of resources to be downloaded\n  const filesContent = document.querySelector(`script[type=\\\"vfs-file\\\"]`).textContent;\n  const files = JSON.parse(b64Decode(filesContent));\n\n  let pyodidePromise = (async () => {\n    statusText.textContent = `Downloading Pyodide`;\n    const pyodide = await startPyodideWorker(data.options);\n\n    statusText.textContent = `Downloading package: micropip`;\n    await pyodide.loadPackage(\"micropip\");\n    const micropip = await pyodide.pyimport(\"micropip\");\n    await data.packages.pkgs.map((pkg) => () => {\n      statusText.textContent = `Downloading package: ${pkg}`;\n      return micropip.install(pkg);\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n    await micropip.destroy();\n\n    // Download and install resources\n    await files.map((file) => async () => {\n      const name = file.substring(file.lastIndexOf('/') + 1);\n      statusText.textContent = `Downloading resource: ${name}`;\n      const response = await fetch(file);\n      if (!response.ok) {\n        throw new Error(`Can't download \\`${file}\\`. Error ${response.status}: \"${response.statusText}\".`);\n      }\n      const data = await response.arrayBuffer();\n\n      // Store URLs in the cwd without any subdirectory structure\n      if (file.includes(\"://\")) {\n        file = name;\n      }\n\n      // Collapse higher directory structure\n      file = collapsePath(file);\n\n      // Create directory tree, ignoring \"directory exists\" VFS errors\n      const parts = file.split('/').slice(0, -1);\n      let path = '';\n      while (parts.length > 0) {\n        path += parts.shift() + '/';\n        try {\n          await pyodide.FS.mkdir(path);\n        } catch (e) {\n          if (e.name !== \"ErrnoError\") throw e;\n          if (e.errno !== 20) {\n            const errorTextPtr = await pyodide._module._strerror(e.errno);\n            const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n            throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n          }\n        }\n      }\n\n      // Write this file to the VFS\n      try {\n        return await pyodide.FS.writeFile(file, new Uint8Array(data));\n      } catch (e) {\n        if (e.name !== \"ErrnoError\") throw e;\n        const errorTextPtr = await pyodide._module._strerror(e.errno);\n        const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n        throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n      }\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n\n    statusText.textContent = `Pyodide environment setup`;\n    await setupPython(pyodide);\n\n    statusText.remove();\n    if (statusContainer.children.length == 0) {\n      statusContainer.parentNode.remove();\n    }\n    return pyodide;\n  })().catch((err) => {\n    statusText.style.color = \"var(--exercise-editor-hl-er, #AD0000)\";\n    statusText.textContent = err.message;\n    //indicatorContainer.querySelector(\".spinner-grow\").classList.add(\"d-none\");\n    throw err;\n  });\n\n  // Keep track of initial OJS block render\n  const renderedOjs = {};\n\n  const process = async (context, inputs) => {\n    const pyodide = await pyodidePromise;\n    const evaluator = new PyodideEvaluator(pyodide, context);\n    await evaluator.process(inputs);\n    return evaluator.container;\n  }\n\n  return {\n    pyodidePromise,\n    renderedOjs,\n    process,\n  };\n}\n"}]}
</script>
<div id="exercise-loading-indicator" class="exercise-loading-indicator d-none d-flex align-items-center gap-2">
<div id="exercise-loading-status" class="d-flex gap-2">

</div>
<div class="spinner-grow spinner-grow-sm">

</div>
</div>
<script type="vfs-file">
W10=
</script>
</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="qrjs_pics/unsoed_logo.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://imron-slide.vercel.app">irosyadi-2025</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script type="ojs-module-contents">
    eyJjb250ZW50cyI6W3sibWV0aG9kTmFtZSI6ImludGVycHJldCIsImNlbGxOYW1lIjoib2pzLWNlbGwtMSIsImlubGluZSI6ZmFsc2UsInNvdXJjZSI6InZpZXdvZiB4X2lucHV0ID0gSW5wdXRzLnJhbmdlKFstNSwgNV0sIHt2YWx1ZTogMCwgc3RlcDogMC4xLCBsYWJlbDogXCJJbnB1dCBYIFZhbHVlXCJ9KTtcbiJ9LHsibWV0aG9kTmFtZSI6ImludGVycHJldFF1aWV0Iiwic291cmNlIjoic2hpbnlJbnB1dCgneF9pbnB1dCcpIn1dfQ==
    </script>
    <script type="module">
    if (window.location.protocol === "file:") { alert("The OJS runtime does not work with file:// URLs. Please use a web server to view this document."); }
    window._ojs.paths.runtimeToDoc = "../../cs231n";
    window._ojs.paths.runtimeToRoot = "../..";
    window._ojs.paths.docToRoot = "..";
    window._ojs.selfContained = false;
    window._ojs.runtime.interpretFromScriptTags();
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>