<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/quarto-contrib/live-runtime/live-runtime.js" type="module"></script>
<link href="../site_libs/quarto-contrib/live-runtime/live-runtime.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.25">

  <meta name="author" content="Imron Rosyadi">
  <title>Machine Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-b0356e9119c1bdfd0db189d130feb51c.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script type="module" src="../site_libs/quarto-ojs/quarto-ojs-runtime.js"></script>
  <link href="../site_libs/quarto-ojs/quarto-ojs.css" rel="stylesheet">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Machine Learning</h1>
  <p class="subtitle">Image Classification: Data-driven Approach, k-Nearest Neighbor, train/val/test splits</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Imron Rosyadi 
</div>
</div>
</div>

</section>
<section>
<section id="introduction-to-nearest-neighbor-classifiers" class="title-slide slide level1 center">
<h1>Introduction to Nearest Neighbor Classifiers</h1>
<aside class="notes">
<p>Good morning everyone! Welcome to today’s lecture in Machine Learning for Electrical and Computer Engineering. Today, we’re diving into one of the simplest yet foundational algorithms: the Nearest Neighbor classifier. We’ll explore its concepts, dig into the crucial aspect of hyperparameter tuning, and discuss its practical implications within an ECE context. Our goal is to connect the theory with hands-on engineering challenges.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="what-is-image-classification" class="slide level2">
<h2>What is Image Classification?</h2>
<p><strong>Given:</strong> A set of images, each labeled with a single category (e.g., “cat”, “dog”, “car”).</p>
<p><strong>Goal:</strong> Train a model to predict the category of new, unseen images.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>ECE Relevance:</strong></p>
<ul>
<li><strong>Autonomous Systems:</strong> Object detection in self-driving cars.</li>
<li><strong>Medical Imaging:</strong> Diagnosing diseases from X-rays or MRI scans.</li>
<li><strong>Quality Control:</strong> Detecting defects in manufacturing.</li>
<li><strong>Signal Processing:</strong> Classifying radar or sonar signals into object types.</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/Image_classification.svg/640px-Image_classification.svg.png" style="width:100.0%"></p>
</div></div>
<aside class="notes">
<p>Before we jump into Nearest Neighbors, let’s set the stage. What exactly is image classification? Imagine you have a library of images, each already tagged with what’s inside – cats, dogs, cars, etc. The challenge for us is to build an intelligent system that can look at a <em>brand new image it has never seen before</em> and correctly assign it to one of these predefined categories.</p>
<p>This isn’t just an abstract computer science problem; it has profound implications in ECE. Think about autonomous vehicles needing to instantly recognize pedestrians, traffic signs, or other cars to ensure safety. In medical imaging, machine learning helps doctors identify anomalies in scans. In manufacturing, automated systems can ensure product quality by detecting tiny defects. Even in signal processing, we classify patterns in sensor data, like identifying different types of aircraft from radar signals. All these applications rely on effective classification.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="the-nearest-neighbor-nn-principle" class="slide level2">
<h2>The Nearest Neighbor (NN) Principle</h2>
<h3 id="birds-of-a-feather-flock-together.">“Birds of a feather flock together.”</h3>
<p>The core idea is simple: - <strong>Store all training data.</strong> - <strong>To classify a new point:</strong> Find the <strong>closest</strong> training example. - Assign the new point the label of its closest training example.</p>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>How do we define “closest”? We use <strong>Distance Metrics!</strong></p>
</div>
</div>
</div>
<aside class="notes">
<p>The Nearest Neighbor, or NN, classifier is beautifully simple. Its core principle is like the old saying: “birds of a feather flock together.” If you want to know what something is, look at what it’s most similar to from things you already know.</p>
<p>In practice, this means our classifier simply memorizes all the training examples. When a new, unknown data point comes in – let’s say a new image – the classifier compares it to <em>every single training image</em> it has. It then identifies which training image is the “closest” or “most similar” and simply assigns that training image’s label to our new, unknown image.</p>
<p>But this raises a critical question: how do we quantitatively define “closest” or “most similar”? This is where <strong>distance metrics</strong> come into play. They provide a mathematical way to measure the dissimilarity between two data points.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="defining-closeness-distance-metrics" class="slide level2 scrollable">
<h2>Defining “Closeness”: Distance Metrics</h2>
<p>Common choices for <code>d(I_1, I_2)</code> (distance between images <code>I_1</code> and <code>I_2</code>):</p>
<ol type="1">
<li><strong>L1 Distance (Manhattan Distance):</strong> <span class="math display">\[ d_1(I_1, I_2) = \sum_{p} |I_1(p) - I_2(p)| \]</span>
<ul>
<li>Sums absolute differences across all pixel values.</li>
<li>Represents distance if moving only horizontally or vertically.</li>
</ul></li>
<li><strong>L2 Distance (Euclidean Distance):</strong> <span class="math display">\[ d_2(I_1, I_2) = \sqrt{\sum_{p} (I_1(p) - I_2(p))^2} \]</span>
<ul>
<li>Sums squared differences, then takes the square root.</li>
<li>Represents the straight-line distance in a multi-dimensional space.</li>
</ul></li>
</ol>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p>In ECE, these distances are fundamental for comparing signals or data vectors.</p>
</div>
</div>
</div>
<aside class="notes">
<p>When we talk about images, each image can be thought of as a high-dimensional vector, where each pixel value is a dimension. So, subtracting two images means subtracting their pixel values.</p>
<p>The L1 distance, also known as Manhattan or city-block distance, is like navigating a grid. Imagine you’re in a city with perfectly laid-out streets; to get from point A to point B, you sum the horizontal and vertical distances. For images, we sum the absolute differences of each corresponding pixel.</p>
<p>The L2 distance, or Euclidean distance, is what you typically think of as “distance” in everyday life – the shortest straight line between two points. For images, we square the differences of corresponding pixels, sum them, and then take the square root. This emphasizes larger differences more than L1.</p>
<p>Both L1 and L2 distances are widely used in ECE not just for images, but for comparing sound signals, sensor readings, or any vectorized data. Understanding these basic metrics is crucial for many signal processing and machine learning tasks.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="k-nearest-neighbors-k-nn-classifier" class="slide level2">
<h2>k-Nearest Neighbors (k-NN) Classifier</h2>
<h3 id="beyond-a-single-neighbor">Beyond a single neighbor</h3>
<p>Instead of just one neighbor, k-NN considers the <strong><code>k</code> closest</strong> training examples. The label of the new point is determined by a <strong>majority vote</strong> among its <code>k</code> nearest neighbors.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Key Hyperparameter: <code>k</code></strong></p>
<ul>
<li><code>k = 1</code> is the basic Nearest Neighbor.</li>
<li>Choosing <code>k &gt; 1</code> provides a smoother decision boundary and can reduce sensitivity to noisy data points.</li>
</ul>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>A small <code>k</code> can be sensitive to noise. A large <code>k</code> can blur boundaries.</p>
</div>
</div>
</div>
</div><div class="column" style="width:50%;">
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code hidden" id="cb1" data-startfrom="131" data-source-offset="0"><pre class="sourceCode numberSource js number-lines code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 130;"><span id="cb1-131"><a></a>viewof k_value <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">1</span><span class="op">,</span> <span class="dv">15</span>]<span class="op">,</span> {<span class="dt">step</span><span class="op">:</span> <span class="dv">2</span><span class="op">,</span> <span class="dt">value</span><span class="op">:</span> <span class="dv">3</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Select k:"</span>})<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div id="ojs-cell-1" data-nodetype="declaration">

</div>
</div>
</div>
<div>
<div id="pyodide-1" class="exercise-cell">

</div>
<script type="pyodide-1-contents">
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
</script>
</div>
</div></div>
<aside class="notes">
<p>The concept of Nearest Neighbor is extended to <code>k</code>-Nearest Neighbors, or k-NN. Instead of just picking <em>one</em> closest example, k-NN looks at the <code>k</code> closest examples. The label for the new data point is then decided by a majority vote among these <code>k</code> neighbors.</p>
<p>The value of <code>k</code> is a crucial choice. If <code>k</code> is 1, it’s just the basic NN. If <code>k</code> is larger, like 3 or 5, the classifier tends to be more robust to noisy training data. A single outlier won’t sway the decision as much. However, if <code>k</code> is too large, it might smooth out the decision boundaries too much, potentially ignoring local patterns. This interactive plot on the right lets you play with the <code>k</code> value. Observe how increasing <code>k</code> changes the colored decision regions and even the classification of our ‘New Point’ by making the boundaries smoother. For ECE students, this directly relates to parameter tuning in filters or control systems, where a single parameter can dramatically alter system behavior.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="the-challenge-choosing-hyperparameters" class="slide level2">
<h2>The Challenge: Choosing Hyperparameters</h2>
<p>Hyperparameters are settings that control the learning process, not learned from data.</p>
<p>For k-NN, key hyperparameters include:</p>
<ul>
<li>The value of <code>k</code>.</li>
<li>The distance metric (L1, L2, etc.).</li>
</ul>
<div class="callout callout-caution callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Caution</strong></p>
</div>
<div class="callout-content">
<p><strong>Problem:</strong> If we choose <code>k</code> based on how well the model performs on the <em>test data</em>, we are essentially “cheating.” The model would seem to perform better than it would on truly unseen data. This is called <strong>overfitting to the test set.</strong></p>
</div>
</div>
</div>
<aside class="notes">
<p>So, we know we need to choose <code>k</code>, and we need to choose a distance metric. These are what we call <code>hyperparameters</code>. They are not learned directly from the training data, like the weights in a neural network; instead, they are set <em>before</em> the learning process begins and influence <em>how</em> the model learns.</p>
<p>The critical challenge here is figuring out the <em>best</em> values for these hyperparameters. It’s tempting to just try different <code>k</code> values and pick the one that gives the highest accuracy on our test dataset. However, this is a major pitfall in machine learning. If we do this, we are effectively using the test set <em>to train our model</em> – implicitly, by guiding our hyperparameter choices. This leads to an overly optimistic performance estimate. Our model would look great on <em>that specific test set</em>, but when deployed to truly new, unseen data, its performance would likely drop significantly. This is a form of <strong>overfitting to the test set</strong>, and it’s something we absolutely must avoid to build reliable ECE systems.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="data-splitting-the-holy-trinity" class="slide level2 scrollable">
<h2>Data Splitting: The Holy Trinity</h2>
<p>To correctly tune hyperparameters, we divide our data into three distinct sets:</p>
<div class="columns">
<div class="column" style="width:50%;">
<ol type="1">
<li><strong>Training Set (e.g., 60-80%):</strong>
<ul>
<li>Used to <strong>train</strong> the model (e.g., storage for k-NN).</li>
<li>The model <em>learns</em> from this data.</li>
</ul></li>
<li><strong>Validation Set (e.g., 10-20%):</strong>
<ul>
<li>A “fake test set” used to <strong>tune hyperparameters</strong>.</li>
<li>Provides an unbiased estimate of model performance <strong>during development</strong>.</li>
</ul></li>
<li><strong>Test Set (e.g., 10-20%):</strong>
<ul>
<li>Used for a <strong>single, final evaluation</strong> of the chosen model.</li>
<li>Provides an unbiased estimate of generalization performance on <strong>truly unseen data</strong>.</li>
</ul></li>
</ol>
</div><div class="column" style="width:50%;">
<p><img data-src="https://cs231n.github.io/assets/crossval.jpeg" style="width:100.0%"></p>
</div></div>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p><strong>Golden Rule:</strong> The Test Set is used <strong>only once, at the very end!</strong></p>
</div>
</div>
</div>
<aside class="notes">
<p>To address the hyperparameter challenge correctly, we partition our available dataset into three distinct, non-overlapping subsets: the training set, the validation set, and the test set.</p>
<p>The <strong>training set</strong> is the largest portion. This is the data our k-NN model “sees” and “memorizes.” All of the examples it will ever use for classification will come from here.</p>
<p>The <strong>validation set</strong> acts as our crucial bridge during development. It’s like a practice test. We use it to evaluate different hyperparameter settings (like varying <code>k</code>). By seeing how our model performs on the validation set with different <code>k</code> values, we can pick the best <code>k</code> without ever touching our final exam data. This gives us an honest estimate of how particular hyperparameter choices impact performance.</p>
<p>Finally, the <strong>test set</strong> is the most sacred. This set is kept entirely separate and unseen throughout the entire development and tuning process. Only <em>after</em> we have finalized our model and chosen all hyperparameters based on the validation set, do we run our model <em>once</em> on the test set. This single evaluation provides the most reliable measure of our model’s performance on truly unseen data – its ability to <code>generalize</code>.</p>
<p>The <strong>Golden Rule</strong> for all ECE machine learning projects is: The Test Set is used <strong>only once, at the very end!</strong> Never touch it for tweaking anything during development.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="tuning-hyperparameters-with-a-validation-set" class="slide level2">
<h2>Tuning Hyperparameters with a Validation Set</h2>
<h3 id="an-example-with-k">An example with <code>k</code></h3>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a></a><span class="co"># assume we have Xtr_rows, Ytr, Xte_rows, Yte (e.g., 50,000 CIFAR-10 images)</span></span>
<span id="cb2-2"><a></a><span class="co"># Xtr_rows is 50,000 x 3072 matrix (image data)</span></span>
<span id="cb2-3"><a></a><span class="co"># Ytr are the 50,000 corresponding labels</span></span>
<span id="cb2-4"><a></a></span>
<span id="cb2-5"><a></a><span class="co"># 1. Split training data into a smaller training set and a validation set</span></span>
<span id="cb2-6"><a></a>Xval_rows <span class="op">=</span> Xtr_rows[:<span class="dv">1000</span>, :] <span class="co"># Take first 1000 for validation</span></span>
<span id="cb2-7"><a></a>Yval <span class="op">=</span> Ytr[:<span class="dv">1000</span>]</span>
<span id="cb2-8"><a></a>Xtr_rows <span class="op">=</span> Xtr_rows[<span class="dv">1000</span>:, :] <span class="co"># Use remaining 49,000 for actual training</span></span>
<span id="cb2-9"><a></a>Ytr <span class="op">=</span> Ytr[<span class="dv">1000</span>:]</span>
<span id="cb2-10"><a></a></span>
<span id="cb2-11"><a></a><span class="co"># 2. Iterate through different k values and evaluate on the validation set</span></span>
<span id="cb2-12"><a></a>validation_accuracies <span class="op">=</span> []</span>
<span id="cb2-13"><a></a><span class="cf">for</span> k <span class="kw">in</span> [<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">50</span>, <span class="dv">100</span>]: <span class="co"># Try various 'k' values</span></span>
<span id="cb2-14"><a></a></span>
<span id="cb2-15"><a></a>    <span class="co"># (Imagine a NearestNeighbor class here with a 'train' and 'predict' method)</span></span>
<span id="cb2-16"><a></a>    <span class="co"># nn = NearestNeighbor()</span></span>
<span id="cb2-17"><a></a>    <span class="co"># nn.train(Xtr_rows, Ytr) # Model stores the (now smaller) training data</span></span>
<span id="cb2-18"><a></a></span>
<span id="cb2-19"><a></a>    <span class="co"># Yval_predict = nn.predict(Xval_rows, k = k) # Predict on validation data for current 'k'</span></span>
<span id="cb2-20"><a></a>    <span class="co"># acc = np.mean(Yval_predict == Yval) # Calculate accuracy</span></span>
<span id="cb2-21"><a></a></span>
<span id="cb2-22"><a></a>    <span class="co"># --- Simplified for demonstration using placeholder values ---</span></span>
<span id="cb2-23"><a></a>    <span class="co"># In a real scenario, the above commented lines would compute actual accuracy.</span></span>
<span id="cb2-24"><a></a>    <span class="co"># For this interactive demo, assume dummy accuracy based on 'k'.</span></span>
<span id="cb2-25"><a></a>    <span class="cf">if</span> k <span class="op">==</span> <span class="dv">1</span>: acc <span class="op">=</span> <span class="fl">0.38</span></span>
<span id="cb2-26"><a></a>    <span class="cf">elif</span> k <span class="op">==</span> <span class="dv">3</span>: acc <span class="op">=</span> <span class="fl">0.42</span></span>
<span id="cb2-27"><a></a>    <span class="cf">elif</span> k <span class="op">==</span> <span class="dv">5</span>: acc <span class="op">=</span> <span class="fl">0.44</span></span>
<span id="cb2-28"><a></a>    <span class="cf">elif</span> k <span class="op">==</span> <span class="dv">10</span>: acc <span class="op">=</span> <span class="fl">0.41</span></span>
<span id="cb2-29"><a></a>    <span class="cf">elif</span> k <span class="op">==</span> <span class="dv">20</span>: acc <span class="op">=</span> <span class="fl">0.35</span></span>
<span id="cb2-30"><a></a>    <span class="cf">elif</span> k <span class="op">==</span> <span class="dv">50</span>: acc <span class="op">=</span> <span class="fl">0.28</span></span>
<span id="cb2-31"><a></a>    <span class="cf">elif</span> k <span class="op">==</span> <span class="dv">100</span>: acc <span class="op">=</span> <span class="fl">0.20</span></span>
<span id="cb2-32"><a></a>    <span class="co"># --- End simplified part ---</span></span>
<span id="cb2-33"><a></a></span>
<span id="cb2-34"><a></a>    <span class="bu">print</span>(<span class="ss">f'k = </span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">: accuracy = </span><span class="sc">{</span>acc<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb2-35"><a></a>    validation_accuracies.append((k, acc))</span>
<span id="cb2-36"><a></a></span>
<span id="cb2-37"><a></a><span class="co"># 3. Choose the 'k' that gave the best accuracy on the validation set</span></span>
<span id="cb2-38"><a></a>best_k <span class="op">=</span> <span class="bu">max</span>(validation_accuracies, key<span class="op">=</span><span class="kw">lambda</span> item: item[<span class="dv">1</span>])[<span class="dv">0</span>]</span>
<span id="cb2-39"><a></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Best k on validation set: </span><span class="sc">{</span>best_k<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<aside class="notes">
<p>Let’s illustrate this process with a concrete example, just like you’d see in a typical ECE project dealing with sensor data or image classification.</p>
<p>Here, we start with our full training data. Our first step is to carve out a small portion – say, the first 1000 data points – to serve as our <code>validation set</code>. The remaining data then becomes our <em>actual</em> training set for the model.</p>
<p>Next, we loop through different possible values for <code>k</code>, our hyperparameter. For each <code>k</code>, we conceptually “train” our Nearest Neighbor model on the <em>reduced</em> training set and then evaluate its performance solely on the <code>validation set</code>. We calculate the accuracy and record it.</p>
<p>After testing all our desired <code>k</code> values, we look at the <code>validation_accuracies</code> and pick the <code>k</code> that yielded the highest performance on <em>that</em> validation set. This chosen <code>k</code> is now our optimal hyperparameter. Importantly, we still haven’t touched the final test set. In an ECE context, this careful tuning allows us to develop robust systems that perform predictably in the field.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="when-validation-data-is-small-cross-validation" class="slide level2 scrollable">
<h2>When Validation Data is Small: Cross-Validation</h2>
<ul>
<li><strong>Problem:</strong> If your dataset is small, a single validation split might not be representative (noisy estimate).</li>
<li><strong>Solution:</strong> <strong>Cross-validation</strong> provides a more robust estimate of performance for hyperparameter tuning.</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>How it works (e.g., 5-fold CV):</strong></p>
<ol type="1">
<li>Divide the training data into <code>N</code> equal “folds” (e.g., 5).</li>
<li>Iterate <code>N</code> times:
<ul>
<li>Use <code>N-1</code> folds for <strong>training</strong>.</li>
<li>Use the remaining 1 fold for <strong>validation</strong>.</li>
<li>Record performance for the current hyperparameter setting.</li>
</ul></li>
<li>Average the performance across all <code>N</code> iterations.</li>
</ol>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p>Common folds: 3, 5, or 10. More folds offer a better estimate but are more computationally expensive.</p>
</div>
</div>
</div>
</div><div class="column" style="width:50%;">
<p><img data-src="https://cs231n.github.io/assets/cvplot.png" style="width:100.0%"></p>
</div></div>
<aside class="notes">
<p>What if your dataset is quite small? If you carve out a significant portion for a validation set, your remaining training set might be too small to adequately learn patterns. Conversely, if your validation set is tiny, its performance might be very noisy and not a reliable indicator of the true quality of your hyperparameters.</p>
<p>This is where <strong>cross-validation</strong> steps in. It’s a more sophisticated and computationally intensive technique, but it gives a much more reliable estimate of hyperparameter effectiveness.</p>
<p>The idea is that instead of just one split, you perform multiple splits. For instance, in 5-fold cross-validation, you divide your entire training data into five equal segments. Then, in five separate rounds: in each round you designate one segment as the validation fold and the other four as the training folds. You train and evaluate for each hyperparameter setting for <em>each</em> of these five configurations. Finally, you average the performance results from these five rounds. This averaging reduces the variance in your performance estimate, making it more trustworthy. For ECE applications with limited data, such as specialized sensor readings, cross-validation is invaluable for ensuring robust model selection.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="practical-considerations-for-data-splits" class="slide level2">
<h2>Practical Considerations for Data Splits</h2>
<ul>
<li><strong>Typical Split Ratios:</strong>
<ul>
<li>Training: 50-90%</li>
<li>Validation: 10-20%</li>
<li>Test: 10-20%</li>
</ul></li>
<li><strong>When to favor Cross-Validation over a single split:</strong>
<ul>
<li>Small dataset size (validation set would be too small).</li>
<li>If a very accurate estimate of hyperparameter performance is crucial.</li>
</ul></li>
</ul>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Cross-validation is computationally more expensive. Choose between a single validation split and cross-validation based on dataset size, available computational resources, and the number of hyperparameters to tune.</p>
</div>
</div>
</div>
<aside class="notes">
<p>In practice, deciding on the exact proportions for your training, validation, and test sets often comes down to the specifics of your project. As a general rule of thumb, you’ll reserve the largest portion, typically 50-90%, for training your model. The validation and test sets usually make up the remaining percentages equally.</p>
<p>The decision to use a single validation split versus more complex cross-validation hinges on a few factors: If your dataset is large, a single validation split is often sufficient and computationally cheaper. However, if your data is limited, or if you have a wide range of hyperparameters that could interact in complex ways, and particularly if you need a very reliable performance estimate before deploying an ECE system, then cross-validation is the safer bet. It minimizes the risk of making hyperparameter choices that only look good on one particular arbitrary split of data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="pros-of-nearest-neighbor-classifiers-in-ece" class="slide level2 scrollable">
<h2>Pros of Nearest Neighbor Classifiers in ECE</h2>
<ol type="1">
<li><p><strong>Simplicity &amp; Interpretability:</strong></p>
<ul>
<li>Easy to understand and implement.</li>
<li>Can provide insight into why a decision was made (by looking at neighbors).</li>
<li>Useful for quick prototyping in ECE.</li>
</ul></li>
<li><p><strong>No Training Time:</strong></p>
<ul>
<li>The model merely stores the training data.</li>
<li>“Lazy learning” – computation happens during inference.</li>
<li>Can be beneficial for systems where initial training needs to be minimal or dynamic.</li>
</ul></li>
<li><p><strong>Non-parametric:</strong></p>
<ul>
<li>Makes no assumptions about the underlying data distribution.</li>
<li>Can model complex decision boundaries.</li>
</ul></li>
</ol>
<aside class="notes">
<p>Now, let’s consider where Nearest Neighbor classifiers shine, especially from an ECE perspective.</p>
<p>First, its simplicity is a huge plus. It’s easy to grasp, implement, and even explain. In engineering prototyping phases, a simple k-NN can provide a quick baseline without a lot of setup. Its interpretability is also valuable: if you want to know why an anomaly detector classified a sensor reading as ‘faulty’, you can inspect its nearest neighbors for clues.</p>
<p>Second, k-NN is a “lazy learner” – there’s virtually no explicit training phase. All it does during “training” is store the data. All the heavy lifting happens during prediction. This can be an advantage in scenarios where you need to quickly adapt to new data without retraining a complex model, or in resource-constrained embedded systems where intensive training might not be feasible on-device.</p>
<p>Finally, k-NN is non-parametric. It doesn’t assume your data fits a nice linear or Gaussian distribution, which is often a strong and incorrect assumption for real-world ECE data, especially from diverse sensors. It can adapt to very complex data patterns, making it surprisingly powerful in certain contexts.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="cons-of-nearest-neighbor-classifiers-in-ece" class="slide level2 scrollable">
<h2>Cons of Nearest Neighbor Classifiers in ECE</h2>
<ol type="1">
<li><strong>High Test-Time Cost:</strong>
<ul>
<li>Classifying a new point requires comparing it to <em>all</em> training points.</li>
<li><strong>Critical for ECE:</strong> Unsuitable for real-time applications or embedded systems with large datasets.</li>
</ul></li>
<li><strong>Storage Requirements:</strong>
<ul>
<li>Must store the entire training dataset.</li>
<li>Problematic for memory-constrained devices (e.g., IoT, edge AI).</li>
</ul></li>
<li><strong>Curse of Dimensionality:</strong>
<ul>
<li>Distances become less meaningful in high-dimensional spaces.</li>
<li>This is a major issue for image data, as we’ll see next.</li>
</ul></li>
</ol>
<aside class="notes">
<p>Despite its advantages, k-NN has significant drawbacks that often make it impractical for many real-world ECE applications.</p>
<p>The biggest hurdle is its <strong>high test-time computational cost</strong>. Each time you want to classify a <em>new</em> data point, the system needs to compare it against potentially tens of thousands or even millions of stored training examples. For real-time applications like autonomous driving, satellite imagery analysis, or high-speed communication systems, this latency is unacceptable. You can’t afford to take seconds or minutes to classify a single new input.</p>
<p>This also relates directly to <strong>storage requirements</strong>. k-NN needs to keep the entire training data readily accessible. For large datasets, this can mean Gigabytes or Terabytes of stored data, which is simply not feasible for memory-constrained embedded systems or many edge AI devices, where flash memory and RAM are precious.</p>
<p>Finally, and perhaps most critically for image data, we encounter the <strong>curse of dimensionality</strong>. As the number of features (like pixels in an image) increases, the concept of “distance” becomes less intuitive and less discriminative. All points tend to become equidistant from each other, making the “nearest neighbor” less meaningful. Let’s see this visually with images.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="the-curse-of-dimensionality-image-data" class="slide level2">
<h2>The Curse of Dimensionality &amp; Image Data</h2>
<h3 id="why-pixel-based-distances-fail-for-images">Why pixel-based distances fail for images</h3>
<p>Pixel-based L1 or L2 distances often correlate more with background and general color distribution than with semantic content.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="https://cs231n.github.io/assets/samenorm.png" style="width:100.0%"></p>
</div><div class="column" style="width:50%;">
<div class="callout callout-warning callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Warning</strong></p>
</div>
<div class="callout-content">
<p>The image on the left is the <code>original</code>. The three images next to it are all <strong>equally far away</strong> by L2 pixel distance. <strong>Notice:</strong> The L2 distance suggests they are equally similar, despite huge perceptual differences.</p>
</div>
</div>
</div>
<ul>
<li>A truck and a horse can be “closer” if they share a similar background or lighting.</li>
<li>Semantic meaning (“what the image <em>is</em>”) is lost in raw pixel comparisons.</li>
</ul>
</div></div>
<aside class="notes">
<p>Here’s a stark visual example of the curse of dimensionality affecting image classification. The image on the left is the original. The three images next to it – which depict a car, a deer, and a bird – are all <em>equally distant</em> from the original image based on a pixel-wise L2 distance calculation. This immediately tells you that raw pixel distances are a poor measure of perceptual or semantic similarity for images.</p>
<p>You can clearly see that, to human eyes, the car and the deer are <em>not</em> equally similar to the original image. This problem is pervasive: a truck might be computationally “closer” to a horse if both are photographed against a similar green field, rather than to another truck on a brown road. The raw pixel values prioritize low-level features like color and texture over high-level semantic content, which is what we actually want to classify. This highlights a fundamental limitation of simple distance metrics for rich, high-dimensional data like images.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="visualizing-failure-t-sne-embedding-of-cifar-10" class="slide level2">
<h2>Visualizing Failure: t-SNE Embedding of CIFAR-10</h2>
<p>t-SNE (t-Distributed Stochastic Neighbor Embedding) helps visualize high-dimensional data in 2D or 3D, preserving local neighborhood structures.</p>

<img data-src="https://cs231n.github.io/assets/pixels_embed_cifar10.jpg" style="width:100.0%" class="r-stretch"><div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p><strong>Observation:</strong> Images nearby in this embedding (meaning they are pixel-wise similar) are clustered by <strong>background/color</strong>, not by their semantic class (e.g., “dog”, “cat”, “car”).</p>
</div>
</div>
</div>
<aside class="notes">
<p>This image further drives home the point about pixel-based distances. Here, we see a t-SNE visualization of the CIFAR-10 dataset, which contains images from 10 different classes like cars, birds, cats, dogs. t-SNE is a technique that tries to map high-dimensional data into a lower-dimensional space (here, 2D) such that points that were close in the high-dimensional space remain close in the low-dimensional space.</p>
<p>What we observe is revealing: images that are clustered together here are not grouped by their actual semantic content (e.g., all <code>cars</code> together, all <code>birds</code> together). Instead, they are primarily grouped by factors like their background color, lighting conditions, or overall color distribution. For instance, you might see dogs and frogs clustered together if they were all photographed on a white background. This clearly demonstrates that raw pixel values are insufficient for capturing the complex, abstract features needed for robust image classification. This is a critical insight for ECE students: the raw sensor data isn’t always the best representation for ML; feature engineering or learning better representations is key.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="towards-smarter-features-introduction-to-convolution" class="slide level2">
<h2>Towards Smarter Features: Introduction to Convolution</h2>
<h3 id="why-ece-needs-better-feature-extraction-for-images">Why ECE needs better feature extraction for images</h3>
<p>The failure of pixel-wise distances indicates a need for more robust feature representations. <strong>ECE Connection:</strong> Image processing often involves <strong>filtering</strong> — a form of feature extraction.</p>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p><strong>Convolution</strong> is a fundamental operation for extracting meaningful features from image and signal data.</p>
</div>
</div>
</div>
<div class="columns">
<div class="column" style="width:40%;">
<p><strong>How it works:</strong></p>
<ul>
<li>A <strong>kernel</strong> (small matrix/filter) slides over the input image.</li>
<li>At each position, it computes element-wise products and sums them.</li>
<li>This generates a new “feature map” highlighting specific patterns (edges, textures).</li>
</ul>
</div><div class="column" style="width:60%;">
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code hidden" id="cb3" data-startfrom="521" data-source-offset="-0"><pre class="sourceCode numberSource js number-lines code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 520;"><span id="cb3-521"><a></a>viewof k11 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="op">-</span><span class="dv">1</span><span class="op">,</span> <span class="dv">1</span>]<span class="op">,</span> {<span class="dt">step</span><span class="op">:</span> <span class="fl">0.1</span><span class="op">,</span> <span class="dt">value</span><span class="op">:</span> <span class="dv">0</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"k[0,0]"</span>})<span class="op">;</span></span>
<span id="cb3-522"><a></a>viewof k12 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="op">-</span><span class="dv">1</span><span class="op">,</span> <span class="dv">1</span>]<span class="op">,</span> {<span class="dt">step</span><span class="op">:</span> <span class="fl">0.1</span><span class="op">,</span> <span class="dt">value</span><span class="op">:</span> <span class="op">-</span><span class="dv">1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"k[0,1]"</span>})<span class="op">;</span></span>
<span id="cb3-523"><a></a>viewof k13 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="op">-</span><span class="dv">1</span><span class="op">,</span> <span class="dv">1</span>]<span class="op">,</span> {<span class="dt">step</span><span class="op">:</span> <span class="fl">0.1</span><span class="op">,</span> <span class="dt">value</span><span class="op">:</span> <span class="dv">0</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"k[0,2]"</span>})<span class="op">;</span></span>
<span id="cb3-524"><a></a>viewof k21 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="op">-</span><span class="dv">1</span><span class="op">,</span> <span class="dv">1</span>]<span class="op">,</span> {<span class="dt">step</span><span class="op">:</span> <span class="fl">0.1</span><span class="op">,</span> <span class="dt">value</span><span class="op">:</span> <span class="op">-</span><span class="dv">1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"k[1,0]"</span>})<span class="op">;</span></span>
<span id="cb3-525"><a></a>viewof k22 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="op">-</span><span class="dv">1</span><span class="op">,</span> <span class="dv">1</span>]<span class="op">,</span> {<span class="dt">step</span><span class="op">:</span> <span class="fl">0.1</span><span class="op">,</span> <span class="dt">value</span><span class="op">:</span> <span class="dv">4</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"k[1,1]"</span>})<span class="op">;</span></span>
<span id="cb3-526"><a></a>viewof k23 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="op">-</span><span class="dv">1</span><span class="op">,</span> <span class="dv">1</span>]<span class="op">,</span> {<span class="dt">step</span><span class="op">:</span> <span class="fl">0.1</span><span class="op">,</span> <span class="dt">value</span><span class="op">:</span> <span class="op">-</span><span class="dv">1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"k[1,2]"</span>})<span class="op">;</span></span>
<span id="cb3-527"><a></a>viewof k31 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="op">-</span><span class="dv">1</span><span class="op">,</span> <span class="dv">1</span>]<span class="op">,</span> {<span class="dt">step</span><span class="op">:</span> <span class="fl">0.1</span><span class="op">,</span> <span class="dt">value</span><span class="op">:</span> <span class="dv">0</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"k[2,0]"</span>})<span class="op">;</span></span>
<span id="cb3-528"><a></a>viewof k32 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="op">-</span><span class="dv">1</span><span class="op">,</span> <span class="dv">1</span>]<span class="op">,</span> {<span class="dt">step</span><span class="op">:</span> <span class="fl">0.1</span><span class="op">,</span> <span class="dt">value</span><span class="op">:</span> <span class="op">-</span><span class="dv">1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"k[2,1]"</span>})<span class="op">;</span></span>
<span id="cb3-529"><a></a>viewof k33 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="op">-</span><span class="dv">1</span><span class="op">,</span> <span class="dv">1</span>]<span class="op">,</span> {<span class="dt">step</span><span class="op">:</span> <span class="fl">0.1</span><span class="op">,</span> <span class="dt">value</span><span class="op">:</span> <span class="dv">0</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"k[2,2]"</span>})<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-2-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-2-2" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-2-3" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-2-4" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-2-5" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-2-6" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-2-7" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-2-8" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-2-9" data-nodetype="declaration">

</div>
</div>
</div>
</div>
<div>
<div id="pyodide-2" class="exercise-cell">

</div>
<script type="pyodide-2-contents">
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
</script>
</div>
</div></div>
<aside class="notes">
<p>The limitations of pixel-wise distances for images highlight a major challenge, and it points us towards the need for more <code>intelligent feature extraction</code>. This is a concept ECE students are deeply familiar with through <code>digital signal processing</code> and <code>filtering</code>. Instead of just comparing raw pixel values, what if we could extract features that <em>do</em> capture semantic meaning or structural properties like edges, corners, or textures?</p>
<p>This is where <code>convolution</code> comes in. It’s a fundamental operation that applies a small filter, or <code>kernel</code>, across an input image. At each step, the kernel performs element-wise multiplication with the underlying image patch and sums the results, producing a single output value for that region. By sliding this kernel across the entire image, we generate a new “feature map” where each value represents the presence (or absence) of the pattern the kernel is designed to detect.</p>
<p>On the right, you can interact with a simple convolution. We have a basic input image. You can adjust the 3x3 kernel values using the sliders. As you change the kernel, observe how the “Convolved Output” image changes. Try to create kernels that detect edges or simply blur the image. For instance, a common edge detection kernel (like a sharpening filter) highlights changes. This interactive demo is a microcosm of how modern deep learning models learn to extract complex features from raw data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="summary" class="slide level2">
<h2>Summary</h2>
<ul>
<li><strong>Image Classification:</strong> Assigning labels to images.</li>
<li><strong>Nearest Neighbor (k-NN):</strong> Simple, non-parametric classifier that uses proximity in feature space. Key hyperparameters: <code>k</code> and distance metric.</li>
<li><strong>Hyperparameter Tuning:</strong> Critical for generalizing to new data. Avoid <code>test set overfitting</code>.</li>
<li><strong>Data Splits:</strong> Use <strong>Training</strong>, <strong>Validation</strong>, and <strong>Test</strong> sets. Test set is for <strong>final evaluation only</strong>.</li>
<li><strong>Cross-Validation:</strong> Robust tuning for smaller datasets.</li>
<li><strong>k-NN Limitations for Images:</strong> High test-time cost, storage, and <strong>curse of dimensionality</strong> (pixel-wise distances are inadequate).</li>
<li><strong>Future Direction:</strong> Need for better feature extraction (e.g., <strong>Convolution</strong>).</li>
</ul>
<aside class="notes">
<p>To quickly recap what we’ve covered:</p>
<ul>
<li>We started with <code>image classification</code> as a core machine learning problem with direct applications in ECE.</li>
<li>We then explored the <code>Nearest Neighbor (k-NN) classifier</code>. It’s simple, non-parametric, and works by finding the closest training examples. We identified its key hyperparameters: <code>k</code> and the distance metric.</li>
<li>A major focus was on the importance of <code>hyperparameter tuning</code>. We learned that improperly tuning these parameters can lead to <code>test set overfitting</code>, which gives a misleadingly optimistic view of our model’s performance.</li>
<li>The solution lies in proper <code>data splitting</code> into <code>training</code>, <code>validation</code>, and <code>test sets</code>. Remember the golden rule: the test set is reserved for a single, final evaluation.</li>
<li>For smaller datasets, we discussed <code>cross-validation</code> as a more robust method for hyperparameter tuning.</li>
<li>Finally, we critically examined the <code>limitations of k-NN</code>, especially for high-dimensional data like images. Its high test-time cost, storage needs, and the significant impact of the <code>curse of dimensionality</code> make simple pixel-wise comparisons inadequate.</li>
<li>This limitation highlighted the need for more sophisticated <code>feature extraction</code> techniques, paving our way to future topics like <code>convolutional neural networks</code>, which are very relevant in ECE for processing signals and images effectively.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="applying-k-nn-in-practice-ece-guidelines" class="slide level2 scrollable">
<h2>Applying k-NN in Practice (ECE Guidelines)</h2>
<p>If you consider k-NN (perhaps not for images, but for other sensor data):</p>
<ol type="1">
<li><strong>Preprocess Data:</strong> Normalize features (e.g., zero mean, unit variance). Critical for distance-based methods.</li>
<li><strong>Dimensionality Reduction:</strong> For very high-dimensional ECE data (e.g., spectral analysis, multi-sensor arrays), consider PCA, NCA, or Random Projections.</li>
<li><strong>Data Splitting:</strong> Robustly split data into train/validation/test. Use cross-validation if data is sparse or hyperparameters are complex.</li>
<li><strong>Hyperparameter Search:</strong> Systematically evaluate <code>k</code> and distance metrics on the validation set.</li>
<li><strong>Accelerate Retrieval:</strong> For speed-critical ECE applications, explore Approximate Nearest Neighbor (ANN) libraries like FLANN.</li>
<li><strong>Final Evaluation:</strong> After selecting the best hyperparameters, evaluate the model <em>once</em> on the untouched test set. Report this performance.</li>
</ol>
<aside class="notes">
<p>If you ever find yourself needing to apply k-NN in a practical ECE scenario, perhaps not for complex images but for other types of sensor data or feature vectors, here are some guidelines:</p>
<ol type="1">
<li><p><strong>Data Preprocessing:</strong> Always normalize your features. This means ensuring they have zero mean and unit variance. This prevents features with larger numerical ranges from disproportionately influencing the distance calculations. In ECE, scaling sensor readings is a common practice.</p></li>
<li><p><strong>Dimensionality Reduction:</strong> If your ECE data is inherently high-dimensional – for example, signals from many different channels or spectral data – consider techniques like Principal Component Analysis (PCA) or Neighborhood Components Analysis (NCA). These can project your data into a lower-dimensional space, mitigating the curse of dimensionality and making k-NN more effective. This is akin to feature extraction in signal processing.</p></li>
<li><p><strong>Robust Data Splitting:</strong> As discussed, meticulously split your data. Use cross-validation when dealing with limited datasets or when you need highly reliable hyperparameter selection, which is often the case for safety-critical ECE systems.</p></li>
<li><p><strong>Systematic Hyperparameter Search:</strong> Don’t just guess. Exhaustively search for the best <code>k</code> and appropriate distance metric.</p></li>
<li><p><strong>Accelerate Retrieval:</strong> If your ECE application demands real-time responses despite the inherent slowness of k-NN at test time, look into libraries for Approximate Nearest Neighbor (ANN) search, like FLANN. These algorithms provide a trade-off between exactness and speed.</p></li>
<li><p><strong>Final Evaluation:</strong> And always remember: one final, honest evaluation on your completely untouched test set to confirm your model’s real-world performance.</p></li>
</ol>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="further-reading" class="slide level2">
<h2>Further Reading</h2>
<ul>
<li><a href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf">A Few Useful Things to Know about Machine Learning</a>
<ul>
<li><em>Especially section 6, but the whole paper is highly recommended.</em></li>
</ul></li>
<li><a href="https://people.csail.mit.edu/torralba/shortCourseRLOC/index.html">Recognizing and Learning Object Categories</a>
<ul>
<li><em>A short course from ICCV 2005 on object categorization.</em></li>
</ul></li>
</ul>
<aside class="notes">
<p>For those eager to dive deeper, here are some excellent optional resources.</p>
<p>“A Few Useful Things to Know about Machine Learning” is a classic paper that offers profound insights into the field, particularly relevant for understanding the broader context of what we’ve discussed today. Section 6 directly relates to hyperparameter tuning and model evaluation, but I encourage you to read the entire paper.</p>
<p>“Recognizing and Learning Object Categories” provides a historical and foundational perspective on object classification, which is a key application area for many ECE disciplines, especially computer vision.</p>
<p>Thank you!</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>


<script type="pyodide-data">
eyJvcHRpb25zIjp7ImVudiI6eyJQTE9UTFlfUkVOREVSRVIiOiJwbG90bHlfbWltZXR5cGUifSwiaW5kZXhVUkwiOiJodHRwczovL2Nkbi5qc2RlbGl2ci5uZXQvcHlvZGlkZS92MC4yNy4wL2Z1bGwvIn0sInBhY2thZ2VzIjp7InBrZ3MiOlsicHlvZGlkZV9odHRwIiwibWljcm9waXAiLCJpcHl0aG9uIiwibnVtcHkiLCJwbG90bHkiLCJuYmZvcm1hdCJdfX0=
</script>
<script type="ojs-module-contents">
{"contents":[{"cellName":"pyodide-2","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_2 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-2-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-2-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_2 = pyodideOjs.process(_pyodide_editor_2, {k11, k12, k13, k21, k22, k23, k31, k32, k33});\n"},{"cellName":"pyodide-1","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_1 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-1-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-1-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_1 = pyodideOjs.process(_pyodide_editor_1, {k_value});\n"},{"cellName":"pyodide-prelude","inline":false,"methodName":"interpretQuiet","source":"pyodideOjs = {\n  const {\n    PyodideEvaluator,\n    PyodideEnvironmentManager,\n    setupPython,\n    startPyodideWorker,\n    b64Decode,\n    collapsePath,\n  } = window._exercise_ojs_runtime;\n\n  const statusContainer = document.getElementById(\"exercise-loading-status\");\n  const indicatorContainer = document.getElementById(\"exercise-loading-indicator\");\n  indicatorContainer.classList.remove(\"d-none\");\n\n  let statusText = document.createElement(\"div\")\n  statusText.classList = \"exercise-loading-details\";\n  statusText = statusContainer.appendChild(statusText);\n  statusText.textContent = `Initialise`;\n\n  // Hoist indicator out from final slide when running under reveal\n  const revealStatus = document.querySelector(\".reveal .exercise-loading-indicator\");\n  if (revealStatus) {\n    revealStatus.remove();\n    document.querySelector(\".reveal > .slides\").appendChild(revealStatus);\n  }\n\n  // Make any reveal slides with live cells scrollable\n  document.querySelectorAll(\".reveal .exercise-cell\").forEach((el) => {\n    el.closest('section.slide').classList.add(\"scrollable\");\n  })\n\n  // Pyodide supplemental data and options\n  const dataContent = document.querySelector(`script[type=\\\"pyodide-data\\\"]`).textContent;\n  const data = JSON.parse(b64Decode(dataContent));\n\n  // Grab list of resources to be downloaded\n  const filesContent = document.querySelector(`script[type=\\\"vfs-file\\\"]`).textContent;\n  const files = JSON.parse(b64Decode(filesContent));\n\n  let pyodidePromise = (async () => {\n    statusText.textContent = `Downloading Pyodide`;\n    const pyodide = await startPyodideWorker(data.options);\n\n    statusText.textContent = `Downloading package: micropip`;\n    await pyodide.loadPackage(\"micropip\");\n    const micropip = await pyodide.pyimport(\"micropip\");\n    await data.packages.pkgs.map((pkg) => () => {\n      statusText.textContent = `Downloading package: ${pkg}`;\n      return micropip.install(pkg);\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n    await micropip.destroy();\n\n    // Download and install resources\n    await files.map((file) => async () => {\n      const name = file.substring(file.lastIndexOf('/') + 1);\n      statusText.textContent = `Downloading resource: ${name}`;\n      const response = await fetch(file);\n      if (!response.ok) {\n        throw new Error(`Can't download \\`${file}\\`. Error ${response.status}: \"${response.statusText}\".`);\n      }\n      const data = await response.arrayBuffer();\n\n      // Store URLs in the cwd without any subdirectory structure\n      if (file.includes(\"://\")) {\n        file = name;\n      }\n\n      // Collapse higher directory structure\n      file = collapsePath(file);\n\n      // Create directory tree, ignoring \"directory exists\" VFS errors\n      const parts = file.split('/').slice(0, -1);\n      let path = '';\n      while (parts.length > 0) {\n        path += parts.shift() + '/';\n        try {\n          await pyodide.FS.mkdir(path);\n        } catch (e) {\n          if (e.name !== \"ErrnoError\") throw e;\n          if (e.errno !== 20) {\n            const errorTextPtr = await pyodide._module._strerror(e.errno);\n            const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n            throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n          }\n        }\n      }\n\n      // Write this file to the VFS\n      try {\n        return await pyodide.FS.writeFile(file, new Uint8Array(data));\n      } catch (e) {\n        if (e.name !== \"ErrnoError\") throw e;\n        const errorTextPtr = await pyodide._module._strerror(e.errno);\n        const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n        throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n      }\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n\n    statusText.textContent = `Pyodide environment setup`;\n    await setupPython(pyodide);\n\n    statusText.remove();\n    if (statusContainer.children.length == 0) {\n      statusContainer.parentNode.remove();\n    }\n    return pyodide;\n  })().catch((err) => {\n    statusText.style.color = \"var(--exercise-editor-hl-er, #AD0000)\";\n    statusText.textContent = err.message;\n    //indicatorContainer.querySelector(\".spinner-grow\").classList.add(\"d-none\");\n    throw err;\n  });\n\n  // Keep track of initial OJS block render\n  const renderedOjs = {};\n\n  const process = async (context, inputs) => {\n    const pyodide = await pyodidePromise;\n    const evaluator = new PyodideEvaluator(pyodide, context);\n    await evaluator.process(inputs);\n    return evaluator.container;\n  }\n\n  return {\n    pyodidePromise,\n    renderedOjs,\n    process,\n  };\n}\n"}]}
</script>
<div id="exercise-loading-indicator" class="exercise-loading-indicator d-none d-flex align-items-center gap-2">
<div id="exercise-loading-status" class="d-flex gap-2">

</div>
<div class="spinner-grow spinner-grow-sm">

</div>
</div>
<script type="vfs-file">
W10=
</script>
</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="../qrjs_pics/unsoed_logo.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://imron-slide.vercel.app">irosyadi-2025</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script type="ojs-module-contents">
    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
    </script>
    <script type="module">
    if (window.location.protocol === "file:") { alert("The OJS runtime does not work with file:// URLs. Please use a web server to view this document."); }
    window._ojs.paths.runtimeToDoc = "../../cs231n";
    window._ojs.paths.runtimeToRoot = "../..";
    window._ojs.paths.docToRoot = "..";
    window._ojs.selfContained = false;
    window._ojs.runtime.interpretFromScriptTags();
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>