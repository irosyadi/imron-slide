<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/quarto-contrib/live-runtime/live-runtime.js" type="module"></script>
<link href="../site_libs/quarto-contrib/live-runtime/live-runtime.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.25">

  <meta name="author" content="Imron Rosyadi">
  <title>Machine Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-b0356e9119c1bdfd0db189d130feb51c.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script type="module" src="../site_libs/quarto-ojs/quarto-ojs-runtime.js"></script>
  <link href="../site_libs/quarto-ojs/quarto-ojs.css" rel="stylesheet">
  <meta name="mermaid-theme" content="neutral">
  <script src="../site_libs/quarto-diagram/mermaid.min.js"></script>
  <script src="../site_libs/quarto-diagram/mermaid-init.js"></script>
  <link href="../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Machine Learning</h1>
  <p class="subtitle">1.6 Neural Networks Part 2: Setting up the Data and the Loss</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Imron Rosyadi 
</div>
</div>
</div>

</section>
<section>
<section id="understanding-neural-networks-from-neurons-to-layers" class="title-slide slide level1 center">
<h1>Understanding Neural Networks: From Neurons to Layers</h1>

</section>
<section id="sources" class="slide level2">
<h2>Sources</h2>
<p><a href="https://cs231n.stanford.edu/">Stanford University CS231n: Deep Learning for Computer Vision</a></p>
<p><a href="https://cs231n.github.io/">CS231n Deep Learning for Computer Vision</a></p>
</section>
<section id="table-of-contents" class="slide level2 scrollable">
<h2>Table of Contents</h2>
<ol type="1">
<li><a href="#/quick">Quick Intro: Linear vs NN</a></li>
<li><a href="#/intro">Modeling One Neuron</a>
<ul>
<li><a href="#/bio">Biological Motivation</a></li>
<li><a href="#/classifier">Single Neuron as Linear Classifier</a></li>
<li><a href="#/actfun">Activation Functions</a></li>
</ul></li>
<li><a href="#/nn">Neural Network Architectures</a>
<ul>
<li><a href="#/layers">Layer-wise Organization</a></li>
<li><a href="#/feedforward">Feed-Forward Computation</a></li>
<li><a href="#/power">Representational Power</a></li>
<li><a href="#/arch">Setting Layers &amp; Sizes</a></li>
</ul></li>
<li><a href="#/summary-p1">Summary P1</a></li>
</ol>
</section>
<section id="table-of-contents-continued" class="slide level2 scrollable">
<h2>Table of Contents (Continued)</h2>
<ol start="5" type="1">
<li><a href="#/intro-p2">Setting up the Data and Model</a>
<ul>
<li><a href="#/datapre">Data Preprocessing</a></li>
<li><a href="#/init">Weight Initialization</a></li>
<li><a href="#/batchnorm">Batch Normalization</a></li>
<li><a href="#/reg">Regularization</a></li>
</ul></li>
<li><a href="#/losses">Loss Functions</a></li>
<li><a href="#/summary-p2">Summary P2</a></li>
<li><a href="#/add">Additional References</a></li>
</ol>
</section>
<section id="setting-up-the-data-and-the-model" class="slide level2">
<h2><a name="intro-p2"></a>5. Setting up the Data and the Model</h2>
<p>Having introduced the basic neuron model and neural network architectures, we now delve into practical considerations for setting up a robust machine learning system.</p>
<p>These include:</p>
<ul>
<li><strong>Data Preprocessing</strong>: Preparing input data for optimal network performance.</li>
<li><strong>Weight Initialization</strong>: Setting initial values for network parameters.</li>
<li><strong>Batch Normalization</strong>: Stabilizing and accelerating training.</li>
<li><strong>Regularization</strong>: Techniques to prevent overfitting.</li>
</ul>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>A Neural Network performs a sequence of linear mappings with interwoven non-linearities. These design choices significantly impact training stability and final model performance.</p>
</div>
</div>
</div>
<aside class="notes">
<p>In the first part of this lecture, we established the fundamental building blocks of neural networks: the neuron, activation functions, and how neurons arrange into layers to form an architecture. Now, we shift our focus to the practical steps needed <em>before</em> we even begin the iterative training process.</p>
<p>The choices we make in data preprocessing, how we initialize weights, whether we use techniques like batch normalization, and applying regularization are not just minor details—they are critical engineering decisions that directly influence how quickly and effectively our network learns, and how well it generalizes to new, unseen data. Each of these steps plays a vital role in taking a theoretical model from concept to a high-performing solution.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="data-preprocessing-centering-and-scaling" class="slide level2">
<h2><a name="datapre"></a>5.1 Data Preprocessing: Centering and Scaling</h2>
<p>Three common forms of data preprocessing for a data matrix <code>X</code> of size <code>[N x D]</code> (N data, D dimensions).</p>
<h3 id="mean-subtraction">1. Mean Subtraction</h3>
<ul>
<li>Most common form; centers data around the origin.</li>
<li><code>X -= np.mean(X, axis = 0)</code> (subtract mean of each feature).</li>
<li>For images, can subtract global mean or per-channel mean.</li>
</ul>
<h3 id="normalization">2. Normalization</h3>
<ul>
<li>Scales data dimensions to approximately same range.</li>
<li><strong>Standardization</strong>: Divide by standard deviation after mean-centering: <code>X /= np.std(X, axis = 0)</code>.</li>
<li><strong>Min-Max Scaling</strong>: Normalize to range <code>[-1, 1]</code>.</li>
<li>Useful when features have different scales but similar importance.</li>
</ul>
<div class="fig figcenter fighighlight">
<img src="https://cs231n.github.io/assets/nn2/prepro1.jpeg" width="90%">
<div class="figcaption">
<b>Left</b>: Original data. <b>Middle</b>: Zero-centered. <b>Right</b>: Scaled by standard deviation.
</div>
</div>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p><strong>Pitfall</strong>: Preprocessing statistics must be computed <em>only on training data</em> and then applied to validation/test sets to avoid data leakage.</p>
</div>
</div>
</div>
<aside class="notes">
<p>Data preprocessing is a crucial first step. Imagine you have a dataset where one feature, like ‘age’, ranges from 0-100, and another feature, ‘income’, ranges from 0-1,000,000. If we feed these directly into a neural network, the income feature might disproportionately influence the weights and gradients simply because its numerical values are much larger.</p>
<p>Mean subtraction removes the average value from each feature, effectively centering the data around zero. This is geometrically equivalent to shifting the cloud of data points so its center aligns with the origin. Normalization then scales these features. Standardization, by dividing by the standard deviation, results in features with unit variance. This ensures that all features contribute roughly equally to the learning process, regardless of their original scale.</p>
<p>The image helps visualize these steps. The “Common pitfall” is extremely important: to prevent “data leakage” (where information from your test set inadvertently influences the training process), any statistics (like means or standard deviations) used for preprocessing must be calculated <em>only</em> from the training data. These calculated statistics are then used uniformly across the training, validation, and test datasets.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="interactive-data-preprocessing" class="slide level2">
<h2>Interactive Data Preprocessing</h2>
<p>Observe the effect of mean subtraction and normalization on a small dataset. Modify the <code>data</code> array and rerun the code.</p>
<div>
<div id="pyodide-1" class="exercise-cell">

</div>
<script type="pyodide-1-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWUsIm1heC1saW5lcyI6MTB9LCJjb2RlIjoiaW1wb3J0IG51bXB5IGFzIG5wXG5cbiMgLS0tIERhdGEgdG8gcHJlcHJvY2VzcyAoZmVlbCBmcmVlIHRvIGNoYW5nZSB0aGlzIGFycmF5KSAtLS1cbmRhdGEgPSBucC5hcnJheShbXG4gICAgWzEwLjAsIDEwMC4wLCAxLjBdLFxuICAgIFsxMi4wLCAxMTAuMCwgMi4wXSxcbiAgICBbOC4wLCA5NS4wLCAwLjVdLFxuICAgIFsxNS4wLCAxMjAuMCwgMy4wXVxuXSwgZHR5cGU9ZmxvYXQpXG4jIC0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS1cblxucHJpbnQoXCJPcmlnaW5hbCBEYXRhOlxcblwiLCBkYXRhKVxuXG4jIDEuIE1lYW4gU3VidHJhY3Rpb25cbm1lYW5fZGF0YSA9IG5wLm1lYW4oZGF0YSwgYXhpcz0wKSAjIE1lYW4gb2YgZWFjaCBjb2x1bW4vZmVhdHVyZVxuZGF0YV9jZW50ZXJlZCA9IGRhdGEgLSBtZWFuX2RhdGFcbnByaW50KFwiXFxuTWVhbiBTdWJ0cmFjdGVkIERhdGE6XFxuXCIsIGRhdGFfY2VudGVyZWQpXG5wcmludChcIk1lYW4gb2YgQ2VudGVyZWQgRGF0YSAoc2hvdWxkIGJlIH4wKTpcXG5cIiwgbnAubWVhbihkYXRhX2NlbnRlcmVkLCBheGlzPTApKVxuXG4jIDIuIE5vcm1hbGl6YXRpb24gKFN0YW5kYXJkaXphdGlvbilcbnN0ZF9kYXRhID0gbnAuc3RkKGRhdGFfY2VudGVyZWQsIGF4aXM9MCkgIyBTdGFuZGFyZCBkZXZpYXRpb24gb2YgZWFjaCBjb2x1bW5cbiMgQXZvaWQgZGl2aXNpb24gYnkgemVybyBmb3IgZmVhdHVyZXMgd2l0aCB6ZXJvIHN0ZF9kZXZcbnN0ZF9kYXRhW3N0ZF9kYXRhID09IDBdID0gMSAjIEhhbmRsZSBjb25zdGFudCBmZWF0dXJlcyBncmFjZWZ1bGx5XG5kYXRhX25vcm1hbGl6ZWQgPSBkYXRhX2NlbnRlcmVkIC8gc3RkX2RhdGFcbnByaW50KFwiXFxuU3RhbmRhcmRpemVkIERhdGE6XFxuXCIsIGRhdGFfbm9ybWFsaXplZClcbnByaW50KFwiTWVhbiBvZiBTdGFuZGFyZGl6ZWQgRGF0YSAoc2hvdWxkIGJlIH4wKTpcXG5cIiwgbnAubWVhbihkYXRhX25vcm1hbGl6ZWQsIGF4aXM9MCkpXG5wcmludChcIlN0ZCBEZXYgb2YgU3RhbmRhcmRpemVkIERhdGEgKHNob3VsZCBiZSB+MSk6XFxuXCIsIG5wLnN0ZChkYXRhX25vcm1hbGl6ZWQsIGF4aXM9MCkpIn0=
</script>
</div>
<aside class="notes">
<p>This interactive block allows you to directly manipulate a small data array and see the immediate effects of mean subtraction and standardization.</p>
<p>First, observe the <code>Original Data</code>. Note the different scales of the features (e.g., column 1 vs.&nbsp;column 2). After <code>Mean Subtracted Data</code>, you’ll see that the values are now centered around zero for each feature. The <code>np.mean(data_centered, axis=0)</code> call will show values very close to zero, validating the centering. Finally, the <code>Standardized Data</code> will have features with values typically ranging around -1 to 1, with means near zero and standard deviations near one. This ensures all features are on a comparable scale, preventing one feature from dominating the learning process purely due to its magnitude. Feel free to change the <code>data</code> array to include different values or even columns of constant values to see how the standardization handles them (e.g., if a feature has zero variance, it will be untouched or handled by the <code>std_data[std_data == 0] = 1</code> line).</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="data-preprocessing-pca-and-whitening" class="slide level2 scrollable">
<h2>5.1 Data Preprocessing: PCA and Whitening</h2>
<p>Advanced preprocessing techniques for decorrelation and isotropic representation.</p>
<ol type="1">
<li><strong>Zero-center</strong> the data.</li>
<li>Compute the <strong>covariance matrix</strong>: <code>cov = np.dot(X.T, X) / X.shape[0]</code>.
<ul>
<li>Reveals feature correlations.</li>
</ul></li>
<li>Perform <strong>Singular Value Decomposition (SVD)</strong> on <code>cov</code>: <code>U,S,V = np.linalg.svd(cov)</code>.
<ul>
<li><code>U</code>: Eigenvectors (new orthogonal basis).</li>
<li><code>S</code>: Singular values (related to variance along new axes).</li>
</ul></li>
<li><strong>Decorrelate (PCA)</strong>: Project data onto eigenbasis: <code>Xrot = np.dot(X, U)</code>.
<ul>
<li><code>Xrot_reduced = np.dot(X, U[:,:k])</code>: PCA dimensionality reduction, keeping <code>k</code> most variant dimensions.</li>
</ul></li>
<li><strong>Whitening</strong>: Scale decorrelated data by eigenvalues: <code>Xwhite = Xrot / np.sqrt(S + 1e-5)</code>.
<ul>
<li>Transforms data to have zero mean and identity covariance matrix (isotropic Gaussian blob).</li>
</ul></li>
</ol>
<div class="callout callout-warning callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Warning</strong></p>
</div>
<div class="callout-content">
<p>Whitening can exaggerate noise by scaling up low-variance dimensions. A small constant <code>1e-5</code> prevents division by zero.</p>
</div>
</div>
</div>
</section>
<section id="visualizing-pcawhitening-transformations" class="slide level2">
<h2>Visualizing PCA/Whitening Transformations</h2>
<div class="fig figcenter fighighlight">
<img src="https://cs231n.github.io/assets/nn2/prepro2.jpeg" width="90%">
<div class="figcaption">
<pre><code>&lt;b&gt;Left&lt;/b&gt;: Original toy 2D data.
&lt;b&gt;Middle&lt;/b&gt;: After PCA, data is zero-centered and rotated to its eigenbasis (decorrelated).
&lt;b&gt;Right&lt;/b&gt;: After Whitening, data dimensions are scaled to unit variance, making it an isotropic Gaussian blob.</code></pre>
</div>
</div>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p><strong>In practice:</strong> PCA/Whitening are less common for <strong>Convolutional Networks</strong>. However, <strong>zero-centering</strong> is always crucially important, and <strong>normalization</strong> (dividing by pixel range or standard deviation) is common for images.</p>
</div>
</div>
</div>
<aside class="notes">
<p>PCA and Whitening are more advanced preprocessing steps often used in classical machine learning or for specific types of data. The image shows a beautiful geometric intuition behind these transformations.</p>
<p>After zero-centering, PCA works by rotating the data. The new axes are the “principal components” that capture the most variance in the data. This effectively decorrelates the features. We can also use PCA for dimensionality reduction by keeping only the <code>k</code> principal components that explain most of the variance.</p>
<p>Whitening takes this a step further by scaling each of these decorrelated dimensions so that they all have unit variance. Geometrically, this transforms the data cloud into a spherical (isotropic) Gaussian distribution.</p>
<p>While powerful, these methods are computationally more intensive and generally not directly applied to raw images in modern Convolutional Neural Networks. For CNNs, zero-centering and simple scaling (e.g., dividing pixel values by 255 or subtracting channel means and dividing by channel standard deviations) are typically sufficient and more robust. The caution about Whitening exaggerating noise is also important, as it scales up even the smallest variances.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="pca-visualization-with-cifar-10-features" class="slide level2">
<h2>PCA Visualization with CIFAR-10 Features</h2>
<p>These visualizations demonstrate PCA’s effect on image features, reducing dimensionality while preserving information.</p>
<div class="fig figcenter fighighlight">
<img src="https://cs231n.github.io/assets/nn2/cifar10pca.jpeg" width="95%">
<div class="figcaption">
<pre><code>&lt;b&gt;Left:&lt;/b&gt; Sample CIFAR-10 images.
&lt;b&gt;2nd Left:&lt;/b&gt; Top 144 eigenvectors (basis images); capture lower frequencies.
&lt;b&gt;2nd Right:&lt;/b&gt; Images reconstructed from 144 PCA-reduced features (slightly blurrier, but preserved).
&lt;b&gt;Right:&lt;/b&gt; Whitened images; higher frequencies exaggerated.</code></pre>
</div>
</div>
<aside class="notes">
<p>This slide offers a qualitative understanding of PCA in the context of image data, specifically CIFAR-10. If we treat each image as a high-dimensional vector, we can apply PCA. The “eigenvectors” in this context can be visualized as synthetic images themselves. The top eigenvectors (second from left) represent dominant patterns or lower frequencies in the dataset.</p>
<p>When we reduce an image to its top 144 PCA components and then reconstruct it (second from right), you can see that despite using significantly fewer numbers (144 vs 3072), much of the visual information—the general shape and color—is retained, although some fine details (higher frequencies) are lost, resulting in blurriness.</p>
<p>The rightmost image, showing the “whitened” representation, further illustrates the impact of scaling by eigenvalues. Here, the low-frequency components, which naturally have high variance in images, are compressed, while the higher-frequency, often noisy components are amplified. This leads to a distinct, “edge-enhanced” appearance, which can be useful in some contexts but often problematic for noise sensitivity. This helps illustrate why simple normalization is preferred for CNNs in practice.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="weight-initialization" class="slide level2">
<h2><a name="init"></a>5.2 Weight Initialization</h2>
<p>Crucial for network stability and convergence. Poor initialization can lead to vanishing/exploding gradients or slow learning.</p>
<h3 id="pitfall-all-zero-initialization">Pitfall: All Zero Initialization</h3>
<ul>
<li><code>W = np.zeros((D, H))</code></li>
<li><strong>Problem:</strong> Every neuron computes the same output, gradients, and updates.</li>
<li>Leads to a symmetric network where all neurons learn the same features.</li>
<li><strong>Result:</strong> No symmetry breaking, network effectively becomes a single neuron per layer.</li>
</ul>
<h3 id="small-random-numbers-symmetry-breaking">Small Random Numbers (Symmetry Breaking)</h3>
<ul>
<li><code>W = 0.01 * np.random.randn(D, H)</code></li>
<li>Initialize weights to small, random values (e.g., from a Gaussian distribution).</li>
<li>Neurons start unique, compute distinct updates, and break symmetry.</li>
</ul>
<div class="callout callout-warning callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Warning</strong></p>
</div>
<div class="callout-content">
<p>Very small weights can lead to very small gradients during backpropagation, diminishing the “gradient signal” in deep networks.</p>
</div>
</div>
</div>
<aside class="notes">
<p>Weight initialization is far more important than it might seem at first glance. It sets the starting point for the optimization process, and a bad start can either prevent the network from learning anything or significantly slow down its convergence.</p>
<p>The “all zero initialization” is a classic example of what <em>not</em> to do. If all weights are zero, then every neuron in a given layer will produce the same output, and during backpropagation, they will all receive the identical gradients. This means they will all update their weights identically, and effectively become indistinguishable, failing to learn diverse features.</p>
<p>The simplest solution is to initialize with “small random numbers.” This breaks the symmetry, ensuring that each neuron starts in a unique state and thus learns different features. However, even this has a caveat: if values are <em>too</em> small, the gradients flowing back through the network can become very tiny, leading to the vanishing gradient problem, especially in deeper networks. This highlights a balancing act we need to achieve: not too uniform, not too small, and preferably calibrated.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="weight-initialization-calibrating-variances" class="slide level2">
<h2>5.2 Weight Initialization: Calibrating Variances</h2>
<p>Problem: Variance of a neuron’s output grows with the number of inputs (<code>n</code>).</p>
<p>Proposed Solution: Scale initial weights by <code>1/sqrt(n)</code> to normalize output variance.</p>
<p><strong>Heuristic</strong>: <code>w = np.random.randn(n) / np.sqrt(n)</code></p>
<ul>
<li>Ensures all neurons initially have approximately the same output distribution.</li>
<li>Empirically improves convergence rate.</li>
</ul>
<p><strong>Derivation Sketch:</strong></p>
<p>For a neuron’s raw activation s = _i^n w_i x_i with zero-mean inputs/weights: <span class="math display">\[
\text{Var}(s) = \left( n \text{Var}(w) \right) \text{Var}(x)
\]</span></p>
<p>To make <span class="math inline">\(\text{Var}(s) \approx \text{Var}(x)\)</span>, we need <span class="math inline">\(n \text{Var}(w) = 1\)</span>, so <span class="math inline">\(\text{Var}(w) = 1/n\)</span>.</p>
<p>If <span class="math inline">\(w_i \sim N(0, \sigma^2)\)</span>, then <span class="math inline">\(\sigma^2 = 1/n\)</span>, so <span class="math inline">\(\sigma = 1/\sqrt{n}\)</span>.</p>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p><strong>Current Recommendation (He et al.&nbsp;2015):</strong> For ReLU neurons, use <span class="math inline">\(\text{Var}(w) = 2/n\)</span>. Thus, <code>w = np.random.randn(n) * np.sqrt(2.0/n)</code>.</p>
</div>
</div>
</div>
<aside class="notes">
<p>A significant challenge with random initialization is that the variance of a neuron’s output can increase with the number of its inputs (<code>n</code>). If outputs get too large, they can cause activation functions (like sigmoid or tanh) to saturate, leading to vanishing gradients. If they’re too small, gradients also vanish.</p>
<p>The solution is to “calibrate” the variance. The core idea is to scale the initial weights such that the variance of the neuron’s output remains stable across layers, ideally similar to the variance of its inputs. The derivation shows how <code>1/sqrt(n)</code> comes about.</p>
<p>For ReLU activation functions, which are very common, a slightly different scaling factor was derived by He et al.&nbsp;(2015), pushing the variance to <code>2/n</code>, leading to the <code>np.sqrt(2.0/n)</code> scaling factor. This initialization is widely used and highly recommended for networks employing ReLU. This helps maintain healthy gradients throughout training.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="interactive-weight-initialization-variance" class="slide level2">
<h2>Interactive Weight Initialization Variance</h2>
<p>Observe how weight scaling affects the variance of a neuron’s output.</p>
<p>Adjust the <code>number_of_inputs</code> and <code>scaling_factor</code> to see their impact.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code hidden" id="cb3" data-startfrom="323" data-source-offset="-0"><pre class="sourceCode numberSource js number-lines code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 322;"><span id="cb3-323"><a></a>viewof number_of_inputs <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">1</span><span class="op">,</span> <span class="dv">100</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="dv">50</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="dv">1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Number of Inputs (n)"</span>})<span class="op">;</span></span>
<span id="cb3-324"><a></a>viewof scaling_factor_val <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="fl">0.01</span><span class="op">,</span> <span class="fl">2.0</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="fl">1.0</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.01</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Weight Scaling Factor"</span>})<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-2" data-nodetype="declaration">

</div>
</div>
</div>
</div>
<div>
<div id="pyodide-2" class="exercise-cell">

</div>
<script type="pyodide-2-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWUsImlucHV0IjpbIm51bWJlcl9vZl9pbnB1dHMiLCJzY2FsaW5nX2ZhY3Rvcl92YWwiXSwibWF4LWxpbmVzIjoxMH0sImNvZGUiOiJpbXBvcnQgbnVtcHkgYXMgbnBcbmltcG9ydCBwbG90bHkuZ3JhcGhfb2JqZWN0cyBhcyBnb1xuXG5uID0gbnVtYmVyX29mX2lucHV0c1xuc2NhbGluZ19mYWN0b3Jfd2VpZ2h0cyA9IHNjYWxpbmdfZmFjdG9yX3ZhbFxuXG4jIFNpbXVsYXRlIGlucHV0cyAoZS5nLiwgZnJvbSBwcmV2aW91cyBsYXllciwgemVyby1tZWFuLCB1bml0IHZhcmlhbmNlKVxueCA9IG5wLnJhbmRvbS5yYW5kbigxMDAwLCBuKSAjIDEwMDAgc2FtcGxlcywgbiBmZWF0dXJlc1xuIyB4IGhhcyBhcHByb3ggbWVhbiAwLCB2YXJpYW5jZSAxIGZvciBlYWNoIGZlYXR1cmVcblxuIyBTaW11bGF0ZSB3ZWlnaHRzIGJhc2VkIG9uIHNjYWxpbmcgZmFjdG9yXG53ID0gbnAucmFuZG9tLnJhbmRuKG4pICogc2NhbGluZ19mYWN0b3Jfd2VpZ2h0c1xuIyBJbml0aWFsIGJpYXMgKG9mdGVuIDApXG5iID0gMC4wXG5cbiMgQ2FsY3VsYXRlIG5ldXJvbidzIG91dHB1dCAoYmVmb3JlIG5vbi1saW5lYXJpdHkgZm9yIHZhcmlhbmNlIGNoZWNrKVxucyA9IG5wLmRvdCh4LCB3KSArIGJcblxuIyBDYWxjdWxhdGUgYW5kIHByaW50IHN0YXRpc3RpY3Ncbm1lYW5fcyA9IG5wLm1lYW4ocylcbnZhcl9zID0gbnAudmFyKHMpXG5cbnByaW50KGZcIk51bWJlciBvZiBJbnB1dHMgKG4pOiB7bn1cIilcbnByaW50KGZcIldlaWdodCBTY2FsaW5nIEZhY3Rvcjoge3NjYWxpbmdfZmFjdG9yX3dlaWdodHM6LjJmfVwiKVxucHJpbnQoZlwiTWVhbiBvZiBuZXVyb24gb3V0cHV0IChzKToge21lYW5fczouNGZ9XCIpXG5wcmludChmXCJWYXJpYW5jZSBvZiBuZXVyb24gb3V0cHV0IChzKToge3Zhcl9zOi40Zn1cIilcbnByaW50KGZcIlRoZW9yZXRpY2FsIFZhcihzKSA9IG4gKiBWYXIodykgKiBWYXIoeCkgPSB7biAqIChzY2FsaW5nX2ZhY3Rvcl93ZWlnaHRzKioyKSAqIDEuMDouNGZ9XCIpXG5wcmludChmXCIoRXhwZWN0ZWQgaWYgVmFyKHgpPTEuMCwgVmFyKHcpPXtzY2FsaW5nX2ZhY3Rvcl93ZWlnaHRzKioyOi40Zn0pXCIpXG5cbiMgUGxvdCBkaXN0cmlidXRpb25cbmZpZyA9IGdvLkZpZ3VyZShkYXRhPVtnby5IaXN0b2dyYW0oeD1zLCBuYmluc3g9NTAsIG5hbWU9J05ldXJvbiBPdXRwdXQgRGlzdHJpYnV0aW9uJyldKVxuZmlnLnVwZGF0ZV9sYXlvdXQoXG4gICAgdGl0bGU9ZlwiRGlzdHJpYnV0aW9uIG9mIE5ldXJvbiBPdXRwdXQgKG49e259LCBTY2FsZT17c2NhbGluZ19mYWN0b3Jfd2VpZ2h0czouMmZ9KVwiLFxuICAgIHhheGlzX3RpdGxlPVwiTmV1cm9uIE91dHB1dCAocylcIixcbiAgICB5YXhpc190aXRsZT1cIkNvdW50XCIsXG4gICAgd2lkdGg9OTAwLCBoZWlnaHQ9NDAwLFxuICAgIG1hcmdpbj1kaWN0KGw9MCwgcj0wLCBiPTAsIHQ9MzApXG4pXG5maWcifQ==
</script>
</div>
<aside class="notes">
<p>This interactive example allows you to explore the relationship between the number of inputs to a neuron (<code>n</code>) and the scaling of its initial weights on the variance of its output <code>s</code>.</p>
<ul>
<li><strong><code>number_of_inputs (n)</code></strong>: Represents the “fan-in” to the neuron.</li>
<li><strong><code>scaling_factor_val</code></strong>: This explicitly controls the <code>sigma</code> used to initialize the weights.</li>
</ul>
<p>Observe how <code>Variance of neuron output (s)</code> changes. If <code>scaling_factor_val</code> is <code>1.0</code> (default for <code>np.random.randn</code>), try increasing <code>n</code>. You’ll see <code>var_s</code> increase proportionally to <code>n</code>. This illustrates the problem: a deeper network with many inputs per neuron would quickly lead to very large outputs.</p>
<p>Now, try to manually set <code>scaling_factor_val</code> to <code>1/sqrt(n)</code> or <code>sqrt(2/n)</code> (where <code>n</code> is your current <code>number_of_inputs</code>) and observe if <code>var_s</code> approaches <code>1.0</code> or <code>2.0</code> respectively (reflecting <code>Var(x)</code> if <code>Var(x)=1</code>). For instance, if <code>n=50</code>, try <code>1/sqrt(50)</code> (approx <code>0.14</code>) or <code>sqrt(2/50)</code> (approx <code>0.2</code>). You should see the variance stabilize.</p>
<p>This demonstrates why proper scaling is critical for maintaining healthy signal magnitudes throughout the network, preventing activations from exploding or vanishing and facilitating stable training.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="weight-initialization-other-considerations" class="slide level2">
<h2>5.2 Weight Initialization: Other Considerations</h2>
<ul>
<li><strong>Sparse Initialization</strong>:
<ul>
<li>Set all weight matrices to zero, but randomly connect a fixed, small number of neurons (e.g., 10) with small Gaussian weights.</li>
<li>Addresses uncalibrated variance but less common than He/Xavier.</li>
</ul></li>
<li><strong>Initializing Biases</strong>:
<ul>
<li>Usually initialized to <strong>zero</strong>. Symmetry breaking is handled by weights.</li>
<li>For ReLU, sometimes a small constant (e.g., 0.01) is used to ensure units fire initially, but this is not consistently beneficial.</li>
</ul></li>
</ul>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p><strong>In practice:</strong> Current recommendation for ReLU is <code>w = np.random.randn(n) * np.sqrt(2.0/n)</code>. Biases are typically initialized to zero.</p>
</div>
</div>
</div>
</section>
<section id="batch-normalization-batchnorm" class="slide level2 scrollable">
<h2><a name="batchnorm"></a>5.3 Batch Normalization (BatchNorm)</h2>
<p>A technique to stabilize and accelerate deep network training.</p>
<p><strong>Core Idea</strong>:</p>
<p>Explicitly forces activations throughout the network to take on a unit Gaussian distribution at the beginning of training for each mini-batch.</p>
<p><strong>Mechanism</strong>:</p>
<p>For each feature map in a layer, normalize its activations:</p>
<ol type="1">
<li>Calculate mean <span class="math inline">\(\mu_B\)</span> and variance <span class="math inline">\(\sigma_B^2\)</span> for the current mini-batch B.</li>
<li>Normalize: <span class="math inline">\(\hat{x}_i = (x_i - \mu_B) / \sqrt{\sigma_B^2 + \epsilon}\)</span></li>
<li>Scale and Shift: <span class="math inline">\(y_i = \gamma \hat{x}_i + \beta\)</span>
<ul>
<li><span class="math inline">\(\gamma, \beta\)</span> are learnable parameters (scale and shift).</li>
<li>Allows network to restore original distribution if optimal.</li>
</ul></li>
</ol>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>Insert the BatchNorm layer immediately <strong>after</strong> fully connected/convolutional layers and <strong>before</strong> non-linearities.</p>
</div>
</div>
</div>
</section>
<section id="visualizing-batch-normalizations-placement" class="slide level2">
<h2>Visualizing Batch Normalization’s Placement</h2>
<div class="cell" data-reveal="true" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<pre class="mermaid mermaid-js">graph TD
    Input --&gt; FC_Layer(Fully Connected Layer)
    FC_Layer --&gt; BatchNorm_Layer(Batch Normalization Layer)
    BatchNorm_Layer --&gt; Activation_Function("Activation Function (e.g., ReLU)")
    Activation_Function --&gt; Next_Layer(...)

    style Input fill:#e0f7fa,stroke:#333,stroke-width:2px;
    style FC_Layer fill:#fff8e1,stroke:#333,stroke-width:2px;
    style BatchNorm_Layer fill:#e8f5e9,stroke:#333,stroke-width:2px;
    style Activation_Function fill:#ffebee,stroke:#333,stroke-width:2px;
    style Next_Layer fill:#e0f7fa,stroke:#333,stroke-width:2px;
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p><strong>Benefits of BatchNorm:</strong></p>
<ul>
<li>Greatly improves <strong>training speed</strong>.</li>
<li>Makes networks significantly more <strong>robust to bad initialization</strong>.</li>
<li>Acts as a form of <strong>regularization</strong>, reducing reliance on other techniques like Dropout.</li>
<li>Allows for <strong>higher learning rates</strong>.</li>
</ul>
<aside class="notes">
<p>Batch Normalization, introduced by Ioffe and Szegedy in 2015, is a major breakthrough in deep learning training. It addresses the “internal covariate shift” problem, where the distribution of activations changes throughout training as parameters in previous layers update. This constant shifting makes it difficult for deeper layers to learn effective representations.</p>
<p>The core idea is simple: for each mini-batch during training, normalize the activations of each feature to have zero mean and unit variance. This is done on a per-feature basis. However, simply normalizing might restrict the network’s representational power. To counteract this, BatchNorm introduces two learnable parameters, gamma (scale) and beta (shift), which allow the network to optimally re-scale and re-shift the normalized values. If it’s optimal for a layer to have a different mean or variance, the network can learn to restore that.</p>
<p>The Mermaid diagram clearly shows <code>BatchNorm</code>’s preferred placement: always <em>after</em> the linear transformation (FC or Conv layer) and <em>before</em> the non-linear activation function. This ensures that the inputs to the non-linearity are always in a stable, well-behaved range. The benefits are profound, leading to faster training, better robustness, and often improved generalization.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="regularization-preventing-overfitting" class="slide level2">
<h2><a name="reg"></a>5.4 Regularization: Preventing Overfitting</h2>
<p>Techniques to control network capacity and improve generalization to unseen data.</p>
<h3 id="l2-regularization-weight-decay">L2 Regularization (Weight Decay)</h3>
<ul>
<li>Most common form.</li>
<li>Adds <span class="math inline">\(\frac{1}{2}\lambda w^2\)</span> to the objective for each weight <span class="math inline">\(w\)</span>.</li>
<li><strong>Intuition:</strong> Penalizes large weights, preferring diffuse weight vectors.</li>
<li>Encourages the network to use all inputs a little, rather than some inputs a lot.</li>
<li>During gradient descent, causes weights to decay linearly towards zero: <code>W += -lambda * W</code>.</li>
</ul>
<h3 id="l1-regularization">L1 Regularization</h3>
<ul>
<li>Adds <span class="math inline">\(\lambda \mid w \mid\)</span> to the objective for each weight w.</li>
<li><strong>Property:</strong> Leads to sparse weight vectors (many weights become exactly zero).</li>
<li>Useful for feature selection; neurons rely on a sparse subset of inputs.</li>
<li>Can be combined with L2: <strong>Elastic Net Regularization</strong>.</li>
</ul>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>L2 regularization generally gives superior performance unless explicit feature selection (sparsity) is desired.</p>
</div>
</div>
</div>
</section>
<section id="regularization-max-norm-dropout" class="slide level2">
<h2>5.4 Regularization: Max Norm &amp; Dropout</h2>
<h3 id="max-norm-constraints">Max Norm Constraints</h3>
<ul>
<li>Enforces an absolute upper bound on the magnitude of each neuron’s weight vector.</li>
<li>Weight vector <span class="math inline">\(\vec{w}\)</span> is clamped to satisfy <span class="math inline">\(\Vert \vec{w} \Vert_2 &lt; c\)</span> after each update (e.g., <span class="math inline">\(c=3\)</span> or <span class="math inline">\(4\)</span>).</li>
<li><strong>Benefit:</strong> Prevents “exploding” network activations, even with high learning rates.</li>
</ul>
<h3 id="dropout">Dropout</h3>
<ul>
<li>Extremely effective and simple regularization technique.</li>
<li>During training, each neuron is kept active with probability <span class="math inline">\(p\)</span> (hyperparameter, e.g., 0.5) or set to zero otherwise.</li>
</ul>
<div class="fig figcenter fighighlight">
<img src="https://cs231n.github.io/assets/nn2/dropout.jpeg" width="70%">
<div class="figcaption">
Dropout can be seen as training an ensemble of neural networks.
</div>
</div>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>Dropout, L2, L1, and Max Norm address different aspects of overfitting and can often be combined effectively.</p>
</div>
</div>
</div>
</section>
<section id="dropout-implementation" class="slide level2">
<h2>Dropout: Implementation</h2>
<h3 id="vanilla-dropout-not-recommended">Vanilla Dropout (Not Recommended)</h3>
<p>Scales activations at test time.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a></a>p <span class="op">=</span> <span class="fl">0.5</span> <span class="co"># probability of keeping a unit active</span></span>
<span id="cb4-2"><a></a></span>
<span id="cb4-3"><a></a><span class="kw">def</span> train_step(X):</span>
<span id="cb4-4"><a></a>  H1 <span class="op">=</span> np.maximum(<span class="dv">0</span>, np.dot(W1, X) <span class="op">+</span> b1)</span>
<span id="cb4-5"><a></a>  U1 <span class="op">=</span> np.random.rand(<span class="op">*</span>H1.shape) <span class="op">&lt;</span> p <span class="co"># binary mask</span></span>
<span id="cb4-6"><a></a>  H1 <span class="op">*=</span> U1 <span class="co"># drop!</span></span>
<span id="cb4-7"><a></a>  <span class="co"># ... second layer ...</span></span>
<span id="cb4-8"><a></a></span>
<span id="cb4-9"><a></a><span class="kw">def</span> predict(X):</span>
<span id="cb4-10"><a></a>  H1 <span class="op">=</span> np.maximum(<span class="dv">0</span>, np.dot(W1, X) <span class="op">+</span> b1) <span class="op">*</span> p <span class="co"># </span><span class="al">NOTE</span><span class="co">: scale by p</span></span>
<span id="cb4-11"><a></a>  <span class="co"># ... second layer ...</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="dropout-implementation-1" class="slide level2">
<h2>Dropout: Implementation</h2>
<h3 id="inverted-dropout-recommended">Inverted Dropout (Recommended)</h3>
<p>Scales activations at train time, leaving test time untouched.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a></a>p <span class="op">=</span> <span class="fl">0.5</span> <span class="co"># probability of keeping a unit active</span></span>
<span id="cb5-2"><a></a></span>
<span id="cb5-3"><a></a><span class="kw">def</span> train_step(X):</span>
<span id="cb5-4"><a></a>  H1 <span class="op">=</span> np.maximum(<span class="dv">0</span>, np.dot(W1, X) <span class="op">+</span> b1)</span>
<span id="cb5-5"><a></a>  U1 <span class="op">=</span> (np.random.rand(<span class="op">*</span>H1.shape) <span class="op">&lt;</span> p) <span class="op">/</span> p <span class="co"># </span><span class="al">NOTE</span><span class="co">: scale by 1/p</span></span>
<span id="cb5-6"><a></a>  H1 <span class="op">*=</span> U1 <span class="co"># drop!</span></span>
<span id="cb5-7"><a></a>  <span class="co"># ... second layer ...</span></span>
<span id="cb5-8"><a></a></span>
<span id="cb5-9"><a></a><span class="kw">def</span> predict(X):</span>
<span id="cb5-10"><a></a>  H1 <span class="op">=</span> np.maximum(<span class="dv">0</span>, np.dot(W1, X) <span class="op">+</span> b1) <span class="co"># NO scaling needed</span></span>
<span id="cb5-11"><a></a>  <span class="co"># ... second layer ...</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p><strong>In practice:</strong> Use a single, global L2 regularization strength (cross-validated) with inverted dropout (<code>p=0.5</code> is a good default).</p>
</div>
</div>
</div>
<aside class="notes">
<p>Dropout is an ingenious and remarkably simple regularization technique. During training, it randomly “turns off” (<code>sets to zero</code>) a fraction of neurons in a layer. This forces the network to learn more robust features because no single neuron can rely too heavily on the presence of another. It prevents complex co-adaptations and effectively trains an “ensemble” of many smaller networks.</p>
<p>The crucial detail is how it’s handled during prediction. To maintain the same expected output magnitude as during training, we usually have to scale the activations. “Vanilla Dropout” scales at test time, which can complicate deployment. “Inverted Dropout,” the recommended approach, performs this scaling <em>during training</em> (by dividing by <code>p</code>), so that at test time, no modifications are needed. This makes the prediction code cleaner and more efficient.</p>
<p>The figure illustrates dropout as sampling a sub-network from the full network. The “In practice” tip is a strong guide for initial experiments: combine L2 regularization with inverted dropout.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="interactive-dropout-simulation" class="slide level2 scrollable">
<h2>Interactive Dropout Simulation</h2>
<p>Simulate inverted dropout on a small matrix. Adjust <code>dropout_probability_p</code> to see how many elements are dropped and scaled.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code hidden" id="cb6" data-startfrom="574" data-source-offset="0"><pre class="sourceCode numberSource js number-lines code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 573;"><span id="cb6-574"><a></a>viewof dropout_probability_p <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="fl">0.1</span><span class="op">,</span> <span class="fl">1.0</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="fl">0.5</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Dropout Probability (p)"</span>})<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div id="ojs-cell-2" data-nodetype="declaration">

</div>
</div>
</div>
<div>
<div id="pyodide-3" class="exercise-cell">

</div>
<script type="pyodide-3-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWUsImlucHV0IjpbImRyb3BvdXRfcHJvYmFiaWxpdHlfcCJdLCJtYXgtbGluZXMiOjEwfSwiY29kZSI6ImltcG9ydCBudW1weSBhcyBucFxuXG5wID0gZHJvcG91dF9wcm9iYWJpbGl0eV9wXG5cbiMgU2ltdWxhdGUgYSBzbWFsbCBoaWRkZW4gbGF5ZXIgYWN0aXZhdGlvbiBIXG5IX29yaWdpbmFsID0gbnAuYXJyYXkoW1xuICAgIFsxLjIsIDAuOCwgMi41XSxcbiAgICBbMC4xLCAzLjAsIDAuNV0sXG4gICAgWzEuNSwgMC40LCAyLjBdXG5dKVxuXG5wcmludChmXCJPcmlnaW5hbCBIaWRkZW4gTGF5ZXIgQWN0aXZhdGlvbnMgKEgpOlxcbntIX29yaWdpbmFsfVwiKVxucHJpbnQoZlwiRHJvcG91dCBQcm9iYWJpbGl0eSAocCk6IHtwOi4xZn1cIilcblxuIyBHZW5lcmF0ZSBhIHJhbmRvbSBiaW5hcnkgbWFza1xucmFuZG9tX21hc2sgPSBucC5yYW5kb20ucmFuZCgqSF9vcmlnaW5hbC5zaGFwZSkgPCBwXG5wcmludChmXCJcXG5SYW5kb20gTWFzayAoVHJ1ZSA9IEtlZXAsIEZhbHNlID0gRHJvcCk6XFxue3JhbmRvbV9tYXNrfVwiKVxuXG4jIEFwcGx5IGludmVydGVkIGRyb3BvdXQ6IGRyb3AgYW5kIHNjYWxlXG5VID0gcmFuZG9tX21hc2sgLyBwXG5IX2Ryb3BwZWRfc2NhbGVkID0gSF9vcmlnaW5hbCAqIFVcblxucHJpbnQoZlwiXFxuQWN0aXZhdGlvbnMgYWZ0ZXIgSW52ZXJ0ZWQgRHJvcG91dCAoSCAqIFUpOlxcbntIX2Ryb3BwZWRfc2NhbGVkfVwiKVxucHJpbnQoZlwiTnVtYmVyIG9mIGVsZW1lbnRzIGtlcHQ6IHtucC5zdW0ocmFuZG9tX21hc2spfVwiKVxucHJpbnQoZlwiTnVtYmVyIG9mIGVsZW1lbnRzIGRyb3BwZWQ6IHtucC5zaXplKHJhbmRvbV9tYXNrKSAtIG5wLnN1bShyYW5kb21fbWFzayl9XCIpIn0=
</script>
</div>
<aside class="notes">
<p>This Pyodide block provides a hands-on demonstration of Inverted Dropout. You can see how:</p>
<ol type="1">
<li>A random <code>random_mask</code> is generated based on the <code>dropout_probability_p</code>. Elements are <code>True</code> (kept) with probability <code>p</code>, and <code>False</code> (dropped) with probability <code>1-p</code>.</li>
<li>The mask <code>U</code> is created by dividing the <code>random_mask</code> by <code>p</code>. This <code>1/p</code> scaling is the core of inverted dropout, ensuring that the <em>expected</em> sum of activations remains the same after dropout.</li>
<li>The <code>H_original</code> activations are then multiplied by <code>U</code> to get <code>H_dropped_scaled</code>. Notice how some values become zero (dropped), and the values that are kept are scaled up by <code>1/p</code>.</li>
</ol>
<p>Experiment by changing the <code>dropout_probability_p</code>:</p>
<ul>
<li>A <code>p=1.0</code> means no dropout; all elements are kept and scaled by <code>1/1.0 = 1</code>, so <code>H_dropped_scaled</code> should be identical to <code>H_original</code>.</li>
<li>A smaller <code>p</code> (e.g., <code>0.1</code>) will result in more elements being dropped and the kept elements being scaled up by a larger factor (e.g., <code>1/0.1 = 10</code>).</li>
</ul>
<p>This interactive visualization helps solidify the understanding of how inverted dropout operates and why the scaling factor <code>1/p</code> is essential.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="loss-functions" class="slide level2">
<h2><a name="losses"></a>6. Loss Functions</h2>
<p>The “data loss” component of your objective function. Measures compatibility between prediction (<code>f</code>) and ground truth label (<code>y</code>). Total loss: <span class="math inline">\(L = \frac{1}{N} \sum_i L_i + \text{Regularization Loss}\)</span>.</p>
<h3 id="classification">6.1 Classification</h3>
<p>One correct label <span class="math inline">\(y_i\)</span> from a fixed set.</p>
<ul>
<li><strong>SVM Loss (Weston Watkins):</strong> <span class="math display">\[L_i = \sum_{j\neq y_i} \max(0, f_j - f_{y_i} + 1)\]</span>
<ul>
<li>Also common: squared hinge loss <span class="math inline">\(\max(0, f_j - f_{y_i} + 1)^2\)</span>.</li>
</ul></li>
<li><strong>Softmax Loss (Cross-Entropy):</strong> <span class="math display">\[L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right)\]</span>
<ul>
<li>Interprets scores <span class="math inline">\(f_j\)</span> as unnormalized log-probabilities.</li>
</ul></li>
</ul>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p><strong>Large Number of Classes Problem:</strong> For huge label sets (e.g., ImageNet 22k, NLP vocabularies), computing full Softmax is expensive. Solutions like <strong>Hierarchical Softmax</strong> approximate by structuring labels in a tree.</p>
</div>
</div>
</div>
<aside class="notes">
<p>Loss functions are fundamental in supervised learning; they quantify how “wrong” our model’s predictions are. The goal of training is to minimize this loss, along with the regularization loss discussed earlier.</p>
<p>For classification, where each example has one correct label, two major loss functions stand out. The SVM loss aims to ensure that the score of the correct class is at least a certain margin (typically 1) higher than the scores of all incorrect classes. If this margin is not met, a penalty is incurred.</p>
<p>The Softmax loss, also known as cross-entropy loss, is perhaps more intuitive. It transforms the raw scores into probabilities using the softmax function and then penalizes the model based on the (negative) log-probability assigned to the true class. The higher the probability of the <em>correct</em> class, the lower the loss.</p>
<p>When you have thousands or even millions of classes (like in large language models or image datasets with very fine-grained categories), computing the sum over all possible classes in Softmax can become computationally prohibitive. Hierarchical Softmax is one technique that tries to mitigate this by creating a tree structure for classes, where decisions are made at each node, dramatically reducing the number of computations.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="interactive-classification-loss-plot" class="slide level2">
<h2>Interactive Classification Loss Plot</h2>
<p>Compare SVM and Softmax loss for a single example prediction. Adjust the <code>correct_class_score</code> and <code>incorrect_class_score</code> to see their impact on loss.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code hidden" id="cb7" data-startfrom="667" data-source-offset="-0"><pre class="sourceCode numberSource js number-lines code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 666;"><span id="cb7-667"><a></a>viewof correct_score <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="op">-</span><span class="dv">3</span><span class="op">,</span> <span class="dv">5</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="dv">2</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Score for Correct Class"</span>})<span class="op">;</span></span>
<span id="cb7-668"><a></a>viewof incorrect_score <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="op">-</span><span class="dv">3</span><span class="op">,</span> <span class="dv">5</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="fl">0.5</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Score for Incorrect Class"</span>})<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-3-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-3-2" data-nodetype="declaration">

</div>
</div>
</div>
</div>
<div>
<div id="pyodide-4" class="exercise-cell">

</div>
<script type="pyodide-4-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWUsImlucHV0IjpbImNvcnJlY3Rfc2NvcmUiLCJpbmNvcnJlY3Rfc2NvcmUiXSwibWF4LWxpbmVzIjoxMH0sImNvZGUiOiJpbXBvcnQgbnVtcHkgYXMgbnBcbmltcG9ydCBwbG90bHkuZ3JhcGhfb2JqZWN0cyBhcyBnb1xuXG5mX2NvcnJlY3QgPSBjb3JyZWN0X3Njb3JlXG5mX2luY29ycmVjdCA9IGluY29ycmVjdF9zY29yZVxuXG4jIFNpbXVsYXRlIHNjb3JlcyBmb3IgYSBzaW5nbGUgZXhhbXBsZSB3aXRoIDIgY2xhc3NlcyAob25lIGNvcnJlY3QsIG9uZSBpbmNvcnJlY3QpXG5zY29yZXMgPSBucC5hcnJheShbZl9pbmNvcnJlY3QsIGZfY29ycmVjdF0pICMgQXNzdW1pbmcgY29ycmVjdCBjbGFzcyBpcyBpbmRleCAxXG5jb3JyZWN0X2lkeCA9IDFcblxuIyBTVk0gTG9zcyAoYXNzdW1lIG1hcmdpbiBvZiAxKVxuIyBGb3IgdGhlIGluY29ycmVjdCBzY29yZSAoaW5kZXggMClcbmxvc3Nfc3ZtX2luY29ycmVjdCA9IG1heCgwLCBmX2luY29ycmVjdCAtIGZfY29ycmVjdCArIDEpXG4jIFRoZSBzdW0gYmVsb3cgaXMganVzdCBvdmVyIGluY29ycmVjdCBjbGFzc2VzLCB3aGljaCBpbiB0aGlzIDItY2xhc3MgY2FzZSBpcyBqdXN0IHRoZSBmaXJzdCB0ZXJtXG5sb3NzX3N2bSA9IGxvc3Nfc3ZtX2luY29ycmVjdCBcblxuIyBTb2Z0bWF4IExvc3MgKENyb3NzLUVudHJvcHkpXG5leHBfc2NvcmVzID0gbnAuZXhwKHNjb3JlcylcbnN1bV9leHBfc2NvcmVzID0gbnAuc3VtKGV4cF9zY29yZXMpXG5wcm9iX2NvcnJlY3QgPSBleHBfc2NvcmVzW2NvcnJlY3RfaWR4XSAvIHN1bV9leHBfc2NvcmVzXG5sb3NzX3NvZnRtYXggPSAtbnAubG9nKHByb2JfY29ycmVjdClcblxucHJpbnQoZlwiQ29ycmVjdCBDbGFzcyBTY29yZToge2ZfY29ycmVjdDouMmZ9XCIpXG5wcmludChmXCJJbmNvcnJlY3QgQ2xhc3MgU2NvcmU6IHtmX2luY29ycmVjdDouMmZ9XCIpXG5wcmludChmXCJTVk0gTG9zczoge2xvc3Nfc3ZtOi40Zn1cIilcbnByaW50KGZcIlNvZnRtYXggTG9zczoge2xvc3Nfc29mdG1heDouNGZ9XCIpXG5cbiMgUGxvdHRpbmcgdGhlIGxvc3MgbGFuZHNjYXBlc1xuc2NvcmVfcmFuZ2UgPSBucC5saW5zcGFjZSgtMywgNSwgMTAwKVxuc3ZtX2xvc3NlcyA9IFttYXgoMCwgcyAtIGZfY29ycmVjdCArIDEpIGZvciBzIGluIHNjb3JlX3JhbmdlXVxuc29mdG1heF9sb3NzZXMgPSBbLW5wLmxvZyhucC5leHAoZl9jb3JyZWN0KSAvIChucC5leHAoZl9jb3JyZWN0KSArIG5wLmV4cChzKSkpIGZvciBzIGluIHNjb3JlX3JhbmdlXVxuXG5maWcgPSBnby5GaWd1cmUoKVxuZmlnLmFkZF90cmFjZShnby5TY2F0dGVyKHg9c2NvcmVfcmFuZ2UsIHk9c3ZtX2xvc3NlcywgbW9kZT0nbGluZXMnLCBuYW1lPSdTVk0gTG9zcyAoSW5jb3JyZWN0IFNjb3JlIHZzLiBDb3JyZWN0IFNjb3JlKScsIGxpbmU9ZGljdChjb2xvcj0nYmx1ZScpKSlcbmZpZy5hZGRfdHJhY2UoZ28uU2NhdHRlcih4PXNjb3JlX3JhbmdlLCB5PXNvZnRtYXhfbG9zc2VzLCBtb2RlPSdsaW5lcycsIG5hbWU9J1NvZnRtYXggTG9zcyAoSW5jb3JyZWN0IFNjb3JlIHZzLiBDb3JyZWN0IFNjb3JlKScsIGxpbmU9ZGljdChjb2xvcj0ncmVkJykpKVxuXG4jIEFkZCBtYXJrZXJzIGZvciBjdXJyZW50IHByZWRpY3Rpb25cbmN1cnJlbnRfc3ZtX2xvc3NfdmFsID0gbWF4KDAsIGZfaW5jb3JyZWN0IC0gZl9jb3JyZWN0ICsgMSlcbmN1cnJlbnRfc29mdG1heF9sb3NzX3ZhbCA9IC1ucC5sb2cobnAuZXhwKGZfY29ycmVjdCkgLyAobnAuZXhwKGZfY29ycmVjdCkgKyBucC5leHAoZl9pbmNvcnJlY3QpKSlcblxuZmlnLmFkZF90cmFjZShnby5TY2F0dGVyKFxuICAgIHg9W2ZfaW5jb3JyZWN0XSwgeT1bY3VycmVudF9zdm1fbG9zc192YWxdLFxuICAgIG1vZGU9J21hcmtlcnMnLCBtYXJrZXI9ZGljdChzaXplPTEwLCBjb2xvcj0nYmx1ZScpLFxuICAgIG5hbWU9ZidDdXJyZW50IFNWTSBMb3NzOiB7Y3VycmVudF9zdm1fbG9zc192YWw6LjJmfSdcbikpXG5maWcuYWRkX3RyYWNlKGdvLlNjYXR0ZXIoXG4gICAgeD1bZl9pbmNvcnJlY3RdLCB5PVtjdXJyZW50X3NvZnRtYXhfbG9zc192YWxdLFxuICAgIG1vZGU9J21hcmtlcnMnLCBtYXJrZXI9ZGljdChzaXplPTEwLCBjb2xvcj0ncmVkJyksXG4gICAgbmFtZT1mJ0N1cnJlbnQgU29mdG1heCBMb3NzOiB7Y3VycmVudF9zb2Z0bWF4X2xvc3NfdmFsOi4yZn0nXG4pKVxuXG5cbmZpZy51cGRhdGVfbGF5b3V0KFxuICAgIHRpdGxlPVwiU1ZNIHZzLiBTb2Z0bWF4IExvc3MgZm9yIEluY29ycmVjdCBDbGFzcyBTY29yZVwiLFxuICAgIHhheGlzX3RpdGxlPVwiSW5jb3JyZWN0IENsYXNzIFNjb3JlXCIsXG4gICAgeWF4aXNfdGl0bGU9XCJMb3NzXCIsXG4gICAgd2lkdGg9OTAwLCBoZWlnaHQ9NTAwLFxuICAgIG1hcmdpbj1kaWN0KGw9NTAsIHI9NTAsIGI9NTAsIHQ9NTApLFxuICAgIGxlZ2VuZD1kaWN0KHg9MC4wMSwgeT0wLjk5LCBiZ2NvbG9yPSdyZ2JhKDI1NSwyNTUsMjU1LDAuNyknLCBib3JkZXJjb2xvcj0ncmdiYSgwLDAsMCwwLjUpJylcbilcbmZpZyJ9
</script>
</div>
<aside class="notes">
<p>This interactive plot helps compare the behavior of SVM and Softmax loss functions. We simulate a scenario with two classes, where one is correct and the other is incorrect.</p>
<ul>
<li><strong>X-axis</strong>: Represents the score assigned by the model to the <em>incorrect</em> class.</li>
<li><strong>Y-axis</strong>: Represents the resulting loss.</li>
<li>The <code>correct_class_score</code> slider allows you to fix the score of the correct class.</li>
</ul>
<p><strong>Observations:</strong></p>
<ul>
<li><strong>SVM Loss (Blue):</strong> Notice that when the incorrect class score is much lower than the correct class score (by at least the margin of 1), the SVM loss becomes 0. This means the SVM is “satisfied” and no longer penalizes the model. It cares only about achieving this margin.</li>
<li><strong>Softmax Loss (Red):</strong> The Softmax loss <em>never</em> reaches 0, even when the incorrect score is very low. It continuously encourages the correct class score to be as high as possible and incorrect class scores to be as low as possible, pushing towards an ideal probability of 1 for the correct class.</li>
</ul>
<p>Adjust the sliders to explore different scenarios. For instance, make the <code>incorrect_class_score</code> higher than <code>correct_class_score</code> to see how both losses increase. This demonstrates their distinct behaviors in guiding the optimization process. SVM creates a “flat” loss landscape once the margin is met, while Softmax always provides a gradient to improve probabilities.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="attribute-classification" class="slide level2">
<h2>6.2 Attribute Classification</h2>
<h3 id="attribute-classification-1">Attribute Classification</h3>
<ul>
<li>For multi-label problems where an example can have multiple non-exclusive attributes (e.g., an image with multiple hashtags).</li>
<li>Approach: Build a <strong>binary classifier for each attribute independently</strong>.</li>
<li><strong>SVM-like Loss:</strong> <span class="math display">\[L_i = \sum_j \max(0, 1 - y_{ij} f_j)\]</span>
<ul>
<li><span class="math inline">\(y_{ij} \in \{+1, -1\}\)</span>, <span class="math inline">\(f_j\)</span> is the score for attribute <span class="math inline">\(j\)</span>.</li>
</ul></li>
<li><strong>Logistic Regression Loss:</strong> <span class="math display">\[L_i = -\sum_j y_{ij} \log(\sigma(f_j)) + (1 - y_{ij}) \log(1 - \sigma(f_j))\]</span>
<ul>
<li><span class="math inline">\(y_{ij} \in \{0, 1\}\)</span>, <span class="math inline">\(\sigma(\cdot)\)</span> is the sigmoid function.</li>
</ul></li>
</ul>
</section>
<section id="regression" class="slide level2">
<h2>6.3 Regression</h2>
<h3 id="regression-predicting-real-valued-quantities">Regression (Predicting Real-Valued Quantities)</h3>
<ul>
<li>Commonly uses L2 or L1 norm of the difference.</li>
<li><strong>L2 Loss (Squared Error):</strong> <span class="math display">\[L_i = \Vert f - y_i \Vert_2^2\]</span></li>
<li><strong>L1 Loss (Absolute Error):</strong> <span class="math display">\[L_i = \Vert f - y_i \Vert_1 = \sum_j \mid f_j - (y_i)_j \mid\]</span></li>
</ul>
<div class="callout callout-warning callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Warning</strong></p>
</div>
<div class="callout-content">
<p>L2 loss is much harder to optimize and less robust to outliers than Softmax. Consider quantizing outputs into bins and performing classification whenever possible for regression tasks.</p>
</div>
</div>
</div>
<aside class="notes">
<p>Beyond single-label classification, we encounter other types of problems. Attribute classification, also known as multi-label classification, is when an instance can belong to multiple categories simultaneously (e.g., an image of a dog <em>and</em> a cat). The most common approach is to treat each attribute as an independent binary classification problem, applying either an SVM-like hinge loss or a logistic regression (binary cross-entropy) loss to each attribute’s score.</p>
<p>Regression, on the other hand, deals with predicting continuous, real-valued quantities. The L2 (squared error) and L1 (absolute error) norms are the standard choices. L2 loss is differentiable everywhere and heavily penalizes large errors, pushing the model to be very accurate for all points. However, this also makes it sensitive to outliers. L1 loss is more robust to outliers as it scales linearly with the error.</p>
<p>Critically, the warning reminds us that L2 loss can be very fragile. Neural networks might struggle to output <em>exact</em> values. It is often a strong engineering heuristic to convert a regression problem into a classification problem by discretizing the output range into bins. For example, predicting house prices might become classifying into “price range” bins. This approach can be more stable and provide confidence estimates over the predicted range.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="summary-part-2" class="slide level2">
<h2><a name="summary-p2"></a>7. Summary (Part 2)</h2>
<ul>
<li><strong>Data Preprocessing:</strong> Crucial for model stability and performance.
<ul>
<li>Always <strong>zero-center</strong> data.</li>
<li><strong>Normalize</strong> data scale (e.g., by standard deviation).</li>
<li>Compute preprocessing statistics <em>only</em> on training data.</li>
</ul></li>
<li><strong>Weight Initialization:</strong>
<ul>
<li>Avoid zero initialization; use small random numbers.</li>
<li>Recommended for ReLU: <code>w = np.random.randn(n) * np.sqrt(2.0/n)</code>.</li>
<li>Biases typically initialized to zero.</li>
</ul></li>
<li><strong>Batch Normalization:</strong>
<ul>
<li>Stabilizes training and speeds up convergence.</li>
<li>Insert after FC/Conv layers, before non-linearities.</li>
<li>Makes networks robust to poor initialization.</li>
</ul></li>
<li><strong>Regularization:</strong> Prevents overfitting.
<ul>
<li>Commonly use <strong>L2 regularization</strong> and <strong>inverted Dropout</strong> (<code>p=0.5</code> is a good default).</li>
</ul></li>
<li><strong>Loss Functions:</strong>
<ul>
<li><strong>Classification:</strong> SVM loss, Softmax (Cross-entropy) loss.</li>
<li><strong>Attribute Classification:</strong> Per-attribute binary classifiers (SVM-like or Logistic Regression).</li>
<li><strong>Regression:</strong> L2 or L1 Loss. Prefer classification for regression when possible.</li>
</ul></li>
</ul>


<script type="pyodide-data">
eyJvcHRpb25zIjp7ImVudiI6eyJQTE9UTFlfUkVOREVSRVIiOiJwbG90bHlfbWltZXR5cGUifSwiaW5kZXhVUkwiOiJodHRwczovL2Nkbi5qc2RlbGl2ci5uZXQvcHlvZGlkZS92MC4yNy4wL2Z1bGwvIn0sInBhY2thZ2VzIjp7InBrZ3MiOlsicHlvZGlkZV9odHRwIiwibWljcm9waXAiLCJpcHl0aG9uIiwibnVtcHkiLCJwbG90bHkiLCJuYmZvcm1hdCJdfX0=
</script>
<script type="ojs-module-contents">
{"contents":[{"cellName":"pyodide-4","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_4 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-4-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-4-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_4 = pyodideOjs.process(_pyodide_editor_4, {correct_score, incorrect_score});\n"},{"cellName":"pyodide-3","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_3 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-3-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-3-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_3 = pyodideOjs.process(_pyodide_editor_3, {dropout_probability_p});\n"},{"cellName":"pyodide-2","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_2 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-2-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-2-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_2 = pyodideOjs.process(_pyodide_editor_2, {number_of_inputs, scaling_factor_val});\n"},{"cellName":"pyodide-1","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_1 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-1-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-1-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_1 = pyodideOjs.process(_pyodide_editor_1, {});\n"},{"cellName":"pyodide-prelude","inline":false,"methodName":"interpretQuiet","source":"pyodideOjs = {\n  const {\n    PyodideEvaluator,\n    PyodideEnvironmentManager,\n    setupPython,\n    startPyodideWorker,\n    b64Decode,\n    collapsePath,\n  } = window._exercise_ojs_runtime;\n\n  const statusContainer = document.getElementById(\"exercise-loading-status\");\n  const indicatorContainer = document.getElementById(\"exercise-loading-indicator\");\n  indicatorContainer.classList.remove(\"d-none\");\n\n  let statusText = document.createElement(\"div\")\n  statusText.classList = \"exercise-loading-details\";\n  statusText = statusContainer.appendChild(statusText);\n  statusText.textContent = `Initialise`;\n\n  // Hoist indicator out from final slide when running under reveal\n  const revealStatus = document.querySelector(\".reveal .exercise-loading-indicator\");\n  if (revealStatus) {\n    revealStatus.remove();\n    document.querySelector(\".reveal > .slides\").appendChild(revealStatus);\n  }\n\n  // Make any reveal slides with live cells scrollable\n  document.querySelectorAll(\".reveal .exercise-cell\").forEach((el) => {\n    el.closest('section.slide').classList.add(\"scrollable\");\n  })\n\n  // Pyodide supplemental data and options\n  const dataContent = document.querySelector(`script[type=\\\"pyodide-data\\\"]`).textContent;\n  const data = JSON.parse(b64Decode(dataContent));\n\n  // Grab list of resources to be downloaded\n  const filesContent = document.querySelector(`script[type=\\\"vfs-file\\\"]`).textContent;\n  const files = JSON.parse(b64Decode(filesContent));\n\n  let pyodidePromise = (async () => {\n    statusText.textContent = `Downloading Pyodide`;\n    const pyodide = await startPyodideWorker(data.options);\n\n    statusText.textContent = `Downloading package: micropip`;\n    await pyodide.loadPackage(\"micropip\");\n    const micropip = await pyodide.pyimport(\"micropip\");\n    await data.packages.pkgs.map((pkg) => () => {\n      statusText.textContent = `Downloading package: ${pkg}`;\n      return micropip.install(pkg);\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n    await micropip.destroy();\n\n    // Download and install resources\n    await files.map((file) => async () => {\n      const name = file.substring(file.lastIndexOf('/') + 1);\n      statusText.textContent = `Downloading resource: ${name}`;\n      const response = await fetch(file);\n      if (!response.ok) {\n        throw new Error(`Can't download \\`${file}\\`. Error ${response.status}: \"${response.statusText}\".`);\n      }\n      const data = await response.arrayBuffer();\n\n      // Store URLs in the cwd without any subdirectory structure\n      if (file.includes(\"://\")) {\n        file = name;\n      }\n\n      // Collapse higher directory structure\n      file = collapsePath(file);\n\n      // Create directory tree, ignoring \"directory exists\" VFS errors\n      const parts = file.split('/').slice(0, -1);\n      let path = '';\n      while (parts.length > 0) {\n        path += parts.shift() + '/';\n        try {\n          await pyodide.FS.mkdir(path);\n        } catch (e) {\n          if (e.name !== \"ErrnoError\") throw e;\n          if (e.errno !== 20) {\n            const errorTextPtr = await pyodide._module._strerror(e.errno);\n            const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n            throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n          }\n        }\n      }\n\n      // Write this file to the VFS\n      try {\n        return await pyodide.FS.writeFile(file, new Uint8Array(data));\n      } catch (e) {\n        if (e.name !== \"ErrnoError\") throw e;\n        const errorTextPtr = await pyodide._module._strerror(e.errno);\n        const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n        throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n      }\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n\n    statusText.textContent = `Pyodide environment setup`;\n    await setupPython(pyodide);\n\n    statusText.remove();\n    if (statusContainer.children.length == 0) {\n      statusContainer.parentNode.remove();\n    }\n    return pyodide;\n  })().catch((err) => {\n    statusText.style.color = \"var(--exercise-editor-hl-er, #AD0000)\";\n    statusText.textContent = err.message;\n    //indicatorContainer.querySelector(\".spinner-grow\").classList.add(\"d-none\");\n    throw err;\n  });\n\n  // Keep track of initial OJS block render\n  const renderedOjs = {};\n\n  const process = async (context, inputs) => {\n    const pyodide = await pyodidePromise;\n    const evaluator = new PyodideEvaluator(pyodide, context);\n    await evaluator.process(inputs);\n    return evaluator.container;\n  }\n\n  return {\n    pyodidePromise,\n    renderedOjs,\n    process,\n  };\n}\n"}]}
</script>
<div id="exercise-loading-indicator" class="exercise-loading-indicator d-none d-flex align-items-center gap-2">
<div id="exercise-loading-status" class="d-flex gap-2">

</div>
<div class="spinner-grow spinner-grow-sm">

</div>
</div>
<script type="vfs-file">
W10=
</script>
</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="../qrjs_pics/unsoed_logo.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://imron-slide.vercel.app">irosyadi-2025</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script type="ojs-module-contents">
    eyJjb250ZW50cyI6W3sibWV0aG9kTmFtZSI6ImludGVycHJldCIsImNlbGxOYW1lIjoib2pzLWNlbGwtMSIsImlubGluZSI6ZmFsc2UsInNvdXJjZSI6InZpZXdvZiBudW1iZXJfb2ZfaW5wdXRzID0gSW5wdXRzLnJhbmdlKFsxLCAxMDBdLCB7dmFsdWU6IDUwLCBzdGVwOiAxLCBsYWJlbDogXCJOdW1iZXIgb2YgSW5wdXRzIChuKVwifSk7XG52aWV3b2Ygc2NhbGluZ19mYWN0b3JfdmFsID0gSW5wdXRzLnJhbmdlKFswLjAxLCAyLjBdLCB7dmFsdWU6IDEuMCwgc3RlcDogMC4wMSwgbGFiZWw6IFwiV2VpZ2h0IFNjYWxpbmcgRmFjdG9yXCJ9KTtcbiJ9LHsibWV0aG9kTmFtZSI6ImludGVycHJldCIsImNlbGxOYW1lIjoib2pzLWNlbGwtMiIsImlubGluZSI6ZmFsc2UsInNvdXJjZSI6InZpZXdvZiBkcm9wb3V0X3Byb2JhYmlsaXR5X3AgPSBJbnB1dHMucmFuZ2UoWzAuMSwgMS4wXSwge3ZhbHVlOiAwLjUsIHN0ZXA6IDAuMSwgbGFiZWw6IFwiRHJvcG91dCBQcm9iYWJpbGl0eSAocClcIn0pO1xuIn0seyJtZXRob2ROYW1lIjoiaW50ZXJwcmV0IiwiY2VsbE5hbWUiOiJvanMtY2VsbC0zIiwiaW5saW5lIjpmYWxzZSwic291cmNlIjoidmlld29mIGNvcnJlY3Rfc2NvcmUgPSBJbnB1dHMucmFuZ2UoWy0zLCA1XSwge3ZhbHVlOiAyLCBzdGVwOiAwLjEsIGxhYmVsOiBcIlNjb3JlIGZvciBDb3JyZWN0IENsYXNzXCJ9KTtcbnZpZXdvZiBpbmNvcnJlY3Rfc2NvcmUgPSBJbnB1dHMucmFuZ2UoWy0zLCA1XSwge3ZhbHVlOiAwLjUsIHN0ZXA6IDAuMSwgbGFiZWw6IFwiU2NvcmUgZm9yIEluY29ycmVjdCBDbGFzc1wifSk7XG4ifSx7Im1ldGhvZE5hbWUiOiJpbnRlcnByZXRRdWlldCIsInNvdXJjZSI6InNoaW55SW5wdXQoJ251bWJlcl9vZl9pbnB1dHMnKSJ9LHsibWV0aG9kTmFtZSI6ImludGVycHJldFF1aWV0Iiwic291cmNlIjoic2hpbnlJbnB1dCgnc2NhbGluZ19mYWN0b3JfdmFsJykifSx7Im1ldGhvZE5hbWUiOiJpbnRlcnByZXRRdWlldCIsInNvdXJjZSI6InNoaW55SW5wdXQoJ2Ryb3BvdXRfcHJvYmFiaWxpdHlfcCcpIn0seyJtZXRob2ROYW1lIjoiaW50ZXJwcmV0UXVpZXQiLCJzb3VyY2UiOiJzaGlueUlucHV0KCdjb3JyZWN0X3Njb3JlJykifSx7Im1ldGhvZE5hbWUiOiJpbnRlcnByZXRRdWlldCIsInNvdXJjZSI6InNoaW55SW5wdXQoJ2luY29ycmVjdF9zY29yZScpIn1dfQ==
    </script>
    <script type="module">
    if (window.location.protocol === "file:") { alert("The OJS runtime does not work with file:// URLs. Please use a web server to view this document."); }
    window._ojs.paths.runtimeToDoc = "../../cs231n";
    window._ojs.paths.runtimeToRoot = "../..";
    window._ojs.paths.docToRoot = "..";
    window._ojs.selfContained = false;
    window._ojs.runtime.interpretFromScriptTags();
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>