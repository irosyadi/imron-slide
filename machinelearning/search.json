[
  {
    "objectID": "amli/09_05-07-career.html#introduction-the-dual-path",
    "href": "amli/09_05-07-career.html#introduction-the-dual-path",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Introduction: The Dual Path",
    "text": "Introduction: The Dual Path\n\n\nWelcome to a special session bridging foundational Machine Learning concepts with essential Professional Development skills for ECE careers.\n\nPart 2: Professional Development for ECE\n\nCommunication with recruiters.\nInterview preparation (technical & behavioral).\nPresenting with confidence.\n\n\n\n\n\n\n\n\n\nNote\n\n\nWhy both? Technical prowess is vital, but effective communication and career navigation are equally crucial for success in ECE and ML.\n\n\n\n\n\n\nImage of circuit board with ML overlay"
  },
  {
    "objectID": "amli/09_05-07-career.html#transition-to-professional-development",
    "href": "amli/09_05-07-career.html#transition-to-professional-development",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Transition to Professional Development",
    "text": "Transition to Professional Development\nWhile technical skills are paramount in ECE and ML, career success also hinges on strong professional abilities. The following slides, based on the provided content, focus on essential skills for navigating your career journey.\n\n\n\n\n\n\nImportant\n\n\nNote on Content Source: This section uses provided text on “Career Development” topics, which does not directly cover Machine Learning theory but is crucial for professional growth for ECE students."
  },
  {
    "objectID": "amli/09_05-07-career.html#career-development-communicating-with-recruiters",
    "href": "amli/09_05-07-career.html#career-development-communicating-with-recruiters",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Career Development: Communicating With Recruiters",
    "text": "Career Development: Communicating With Recruiters\nEffectively navigating conversations with recruiters is key to landing your desired role.\n\nWho has had experience working with a recruiter? What was the process like?\nDid you ever experience uncertainty about how often you should reach out to your recruiter or what you should/shouldn’t share?\nHopefully this discussion will clear up some of that uncertainty and provide you with some best practices for communicating with recruiters that you can apply when you work with a recruiter next. Whether you’ve never worked with a recruiter or have worked with many, this session is for you. Let’s get started!"
  },
  {
    "objectID": "amli/09_05-07-career.html#agenda",
    "href": "amli/09_05-07-career.html#agenda",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Agenda",
    "text": "Agenda\n\nRole of your recruiter\nEffectively communicating with your recruiter\nHow to navigate interview scheduling\nSample emails to recruiters\nQ&A\n\n\nFirst we’ll go over the role your recruiter serves in your hiring process. Their exact job responsibilities can vary by employer but are generally similar. Then we’ll talk about how to effectively communicate with your recruiter and how to navigate all the minor and major details of scheduling and participating in interviews. Then we’ll close out with some sample emails to bring more color to our discussion and Q&A."
  },
  {
    "objectID": "amli/09_05-07-career.html#role-of-your-recruiter",
    "href": "amli/09_05-07-career.html#role-of-your-recruiter",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Role of Your Recruiter",
    "text": "Role of Your Recruiter\n\nYour advocate and partner (they WANT to get you hired!)\nMain point of contact at the company\nGuide throughout the interview/hiring process\nBe friendly, gracious and thoughtful when talking to your recruiter\n\n\n\nYour recruiter is your advocate and partner. Be kind, respectful, friendly, and honest in your communications. They’re there to help you, provide you with resources, and guide you through the interview and hiring processes.\nWhile you may also be in touch with a hiring manager, your recruiter is typically your main point of contact during an interview process with a company. Make sure to keep them in the loop with any updates that affect your candidacy.\nYour recruiter will also keep you updated, so keep an eye on your inbox! You should always respond to your recruiter within 48 hours.\nTip: It’s always good to express your thanks and show your appreciation. Your recruiters work hard to ensure your process is as informative, timely, and smooth as possible. Show your positivity and gratitude. It can go a long way!"
  },
  {
    "objectID": "amli/09_05-07-career.html#effective-communication",
    "href": "amli/09_05-07-career.html#effective-communication",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Effective Communication",
    "text": "Effective Communication\n\nEffective Communication\nLet’s move on to some guidelines for effectively communicating with your recruiter."
  },
  {
    "objectID": "amli/09_05-07-career.html#tips-for-communicating-with-your-recruiter",
    "href": "amli/09_05-07-career.html#tips-for-communicating-with-your-recruiter",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Tips for Communicating With Your Recruiter",
    "text": "Tips for Communicating With Your Recruiter\n\nRead all emails from top to bottom\nAnswer all questions and provide all requested information\nSchedule time to speak with your recruiter as needed\nRespond to emails in a timely manner, within 48 hours is best\n\n\nIt is important to follow these tips when communicating with recruiters: * Read all emails from top to bottom: Recruiter emails often have a lot of information in them, and they don’t want you to miss anything. Be sure to scroll down and look for FAQs in case they’ve included them. Save your emails with recruiters to refer back to them as needed. Also, be sure to read through all links or attached materials. * Answer all the questions and provide all requested information to save time; a lot of inefficiency is caused by emailing back and forth unnecessarily, and you don’t want to slow down your process if you can avoid it. * Some recruiters will have a link to their calendars in their emails so you can choose a good time that works best for you. Otherwise, they will ask you for your availability. Be aware of times zones and make explicit plans. Make sure the contact details you provide are accurate and that it is clear who will be contacting whom if you connect over phone or video. * Recruiters try to respond to candidates as soon as possible. It’s important you do the same! Things can move pretty quickly, and you want to ensure you are keeping your recruiter posted with the most updated information possible. At the same time, you want to be mindful of work/life balance; recruiters won’t be responding to emails late at night or during weekends and holidays."
  },
  {
    "objectID": "amli/09_05-07-career.html#when-to-contact-your-recruiter",
    "href": "amli/09_05-07-career.html#when-to-contact-your-recruiter",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "When to Contact Your Recruiter",
    "text": "When to Contact Your Recruiter\n\nYour university’s recruiting deadlines (if any)\nOffer deadlines with other companies\nMajor update to your resume\nSustained period without updates\n\n\n\n\n\n\n\nTip\n\n\nAlways be truthful, open, and honest with your recruiter. If they have the full picture of your status and options, they can guide your process with their company more comprehensively.\n\n\n\n\nStudents often ask, “Should I update my recruiter about that?” Here are some guidelines for when you definitely want to communicate with your recruiter:\n\nUniversity deadlines: Some universities have specific guidelines on when its students are permitted to accept internship or full-time job offers. These guidelines exist to serve you and make sure you have sufficient time to make the best decision for yourself. Check to see if your university has NACE guidelines, and mark your calendar accordingly.\nCompany deadlines: If you’re interviewing with another company and receive an offer, let your recruiter know. You’ll hear recruiters refer to this as a “competing offer.” Having a competing offer can sometimes speed up the offer process at the other places where you’re interviewing. It would be a shame if you had to accept an offer with other possibilities on the table, so it’s important to be transparent with your recruiter to let them know if you have other offers or may soon.\nMajor update on your resume (e.g., new skill, award, conference, publication, etc.). Update your resume and send it as a PDF.\nWhen you haven’t heard from your recruiter in a while (2+ weeks), you can reach out to check in and see if any progress has been made on their end or if they need anything from you.\n\nTIP: Always be truthful, open, and honest with your recruiter. If they have the full picture of your status and options, they can guide your process with their company more comprehensively."
  },
  {
    "objectID": "amli/09_05-07-career.html#interviews",
    "href": "amli/09_05-07-career.html#interviews",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Interviews",
    "text": "Interviews\n\nInterviews\nNow let’s move on to scheduling and completing interviews at a company."
  },
  {
    "objectID": "amli/09_05-07-career.html#scheduling-your-interviews",
    "href": "amli/09_05-07-career.html#scheduling-your-interviews",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Scheduling Your Interviews",
    "text": "Scheduling Your Interviews\n\nProvide a range of times that work for you around class, when you will be relaxed and most focused\nEnsure that for phone interviews, you are sure of who is contacting whom and what the medium is\nEnsure you know the difference between an interview and informational chat\n\n\n\nAt most companies, recruiters will schedule your interviews, or they will introduce you to a recruiting coordinator. Oftentimes recruiting coordinators partner with recruiters and candidates to schedule your interviews.\nGenerally companies interview between 10am-4pm PST, Monday through Friday.\n\nEveryone wants you to feel comfortable and at your best.\nProvide times that work for you and your class schedule.\nMake sure you know the format of any interview, as well as who will be contacting whom and when. If you’ll be using a format you’re not that familiar with, like coding on a Google Doc, then you should practice coding in a Google Doc.\n\nSometimes, depending on the company, a recruiter or someone else in the company may want to start with an informational chat before jumping into official interviews. These will usually consist of talking about the role, company, and your resume. Ensure you have your resume in front of you and can speak about the experiences you want to highlight most. Also, it is so important that you do some basic research on the company. It’s common in an informational session to be asked about your knowledge about the company, your willingness to relocate, and what interests you about working there. Be ready to answer very general questions meant to show you are serious about the job hunt."
  },
  {
    "objectID": "amli/09_05-07-career.html#asking-for-help",
    "href": "amli/09_05-07-career.html#asking-for-help",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Asking for Help",
    "text": "Asking for Help\n\nLet your recruiter know if you require interview accommodations.\nNot sure what kind you need? Say so.\nDisclosing a disability is a personal decision.\nDon’t be shy. Ask for help at any point during the process.\n\n\n\nCompanies should be committed to providing equal opportunities for everyone throughout their hiring process.\nIf you’re not sure what type of accommodations you might need, that’s ok. Many companies have a specialist or team that will work directly to identify what types of accommodations you might require. If a company is too small to have a designated person or team for this important task, they should have a contractor or consultant who can assist.\nDisclosing a disability or need for an accommodation is a personal decision. Ultimately the decision of whether or not to disclose is up to you, but you want to do everything you can to perform well in your interview and get the job or internship. Recruiters want that for you, too."
  },
  {
    "objectID": "amli/09_05-07-career.html#once-interviews-are-scheduled",
    "href": "amli/09_05-07-career.html#once-interviews-are-scheduled",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Once Interviews Are Scheduled",
    "text": "Once Interviews Are Scheduled\n\nRescheduling an interview\nInterviewer running late\nFollow up with a thank you\n\n\nRescheduling an interview * If an interview absolutely needs to be rescheduled, let your recruiter and recruiting coordinator know on one email as soon as possible. It’s best practice to reply all to the email where your interview was originally scheduled. * Keep rescheduling to a minimum. If you’re sick or there’s an emergency, rescheduling is warranted. But once your interview date is set, make sure to prepare yourself for that day. * Rescheduling multiple times might make you more anxious, so it’s generally best to stick with your originally planned interview date.\nInterviewer is late * You’re waiting to start your interview and your interviewer is late. What do you do? Wait about 5-10 minutes. Interviewers are often going from meeting to meeting, and conference rooms may not be booked near each other. After 5-10 minutes, email your recruiter and recruiting coordinator (once again, on the same email thread), and let them know you haven’t heard from your interviewer yet. They’ll sort out what’s going on.\nThank you * Most recruiters can’t share interviewers’ contact information with you directly, but feel free to send a note to your recruiter to pass along to your interviewer. * It’s always nice to thank your recruiting coordinator and recruiter for their help prepping you and getting you scheduled."
  },
  {
    "objectID": "amli/09_05-07-career.html#your-turn",
    "href": "amli/09_05-07-career.html#your-turn",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Your Turn",
    "text": "Your Turn\n\nYour Turn\nNow that we’ve gone over some important information to consider when working with your recruiter, let’s put these best practices to use!"
  },
  {
    "objectID": "amli/09_05-07-career.html#example-email-from-recruiter-to-candidate",
    "href": "amli/09_05-07-career.html#example-email-from-recruiter-to-candidate",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Example Email From Recruiter to Candidate",
    "text": "Example Email From Recruiter to Candidate\n\nExample Email From Recruiter to Candidate\nHave 1-2 student volunteers read this email from a recruiter out loud to the class."
  },
  {
    "objectID": "amli/09_05-07-career.html#your-response",
    "href": "amli/09_05-07-career.html#your-response",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Your Response",
    "text": "Your Response\nYou have six minutes to draft an appropriate response to the recruiter’s initial email.\n\nNow, take six minutes to draft an appropriate response to your recruiter, Clarissa. I’ll switch the screen back so you can reference it while you type.\nRevert to previous slide.\nGive students six minutes to write and then ask for 1-2 student volunteers to read their versions. Highlight 1-2 things students did well in their response."
  },
  {
    "objectID": "amli/09_05-07-career.html#candidate-response-to-recruiter-example",
    "href": "amli/09_05-07-career.html#candidate-response-to-recruiter-example",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Candidate Response to Recruiter Example",
    "text": "Candidate Response to Recruiter Example\n\nCandidate Response to Recruiter Example\nNow that we’ve read an email exchange between a recruiter and candidate, take a look at this candidate’s response to their recruiter. Can you point out what’s wrong with the candidate’s response or what’s missing in this scenario?\nResponse should include:  The initial email from the recruiter states that there will be two technical interviews - remember to read all emails from top to bottom! * Availability for interviews is given in EST, not PST like the recruiter requested. * Availability for interviews is outside of general workings hours (late in the evening and on a Saturday). * Screenshot of transcript is pasted in body of email. Recruiter requested a PDF of both transcript and resume * Availability should be specific: for example, list dates as Tuesday, May 29 * The initial email from the recruiter says that the project questionnaire will be sent out within 24-48 hours. This usually means business hours. If you still haven’t received it within 48 business hours, you should reach out to your recruiter then. * Don’t expect an immediate response especially when it’s outside of our general working hours. Notice that this person was replying to their recruiter very early hours in EST. Remember, Googlers try to get back to everyone within 24 hours. * This person is sending multiple emails back to back. While it’s not necessarily a bad thing, it isn’t preferred for most recruiters. Keep your emails clear and concise. * It’s always nice to sign off with a thank you and your name! Be friendly!"
  },
  {
    "objectID": "amli/09_05-07-career.html#sample-response",
    "href": "amli/09_05-07-career.html#sample-response",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Sample Response",
    "text": "Sample Response\n\nSample Response\nHere’s a good example of a positive response from a candidate to a recruiter. What are some things Jane did well here?"
  },
  {
    "objectID": "amli/09_05-07-career.html#what-if-youre-not-interested-in-a-role",
    "href": "amli/09_05-07-career.html#what-if-youre-not-interested-in-a-role",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "What If You’re NOT Interested in a Role?",
    "text": "What If You’re NOT Interested in a Role?\n\nHear the recruiter out, and ask questions\n\nDo some homework and follow up with them promptly\n\nBe honest, but professional\nKeep them in your network\n\nWhat would be some things to include in an email to a recruiter for a role you are NOT interested in?\n\n\nIf a recruiter contacts you out of the blue, you should respond at least once. Ask clarifying questions and do some homework to see if the role is at all a good match for you. You don’t want them to waste your time, but don’t waste theirs, either.\nIt may not be the perfect role, but if it is at all interesting to you, you should go forward. You’ll learn a lot through the application process.\nIf an opportunity truly is not for you, you should respond honestly and courteously. You don’t have to write a long email or provide too much detail, but you can definitely feel free to give some feedback and you should follow up promptly.\nIt is not good professional etiquette to not respond, nor to lie. For example, don’t say you have a job if you don’t.\nRemember you are still looking for a role and that people move around a lot. This recruiter could end up in a role or company years from now that could be your dream job, so you don’t want to burn bridges. Be polite, and if it seems like a good opportunity, you can ask to connect on LinkedIn or to be kept in mind for future roles that may be a better fit.\n\nWhat would be some things to include in an email to a recruiter for a role you are NOT interested in?"
  },
  {
    "objectID": "amli/09_05-07-career.html#the-no-thank-you-email-example",
    "href": "amli/09_05-07-career.html#the-no-thank-you-email-example",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "The “No, Thank You” Email Example",
    "text": "The “No, Thank You” Email Example\n\nThe “No, Thank You” Email Example\nThis could be sent as an email or copied and pasted into LinkedIn messages. Just remember best practices of spelling the recruiter’s name correctly and making sure details are correct. If you want to connect over LinkedIn or otherwise, keep this person in your contacts (even if this role is not a good fit at this time), and then follow up promptly on doing so. Keep it brief. Even if you give a specific detail on why this is not a fit, you don’t have to get too specific. Remember that it is not okay to lie, so keep it honest and courteous."
  },
  {
    "objectID": "amli/09_05-07-career.html#lets-recap",
    "href": "amli/09_05-07-career.html#lets-recap",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Let’s Recap",
    "text": "Let’s Recap\n\nYour recruiter is your partner and main contact within a company.\nRead all emails from top to bottom and respond within 24-48 hours.\nBe open and honest with your recruiter. Share deadlines and any important information. Ask for help.\nProofread, be professional and thankful.\n\n\nDon’t forget you and your recruiter both want you to have a clear, efficient, and overall great hiring process, regardless of whether or not the job is a best fit for you. They are your advocate. Be sure you read all aspects of the emails they send you, links included, and respond to their emails within 48 hours at the latest. Be transparent with them about your offers and ask for help when you need it or when you feel uncertain about something. Be professional and be grateful for their time and work with you."
  },
  {
    "objectID": "amli/09_05-07-career.html#questions",
    "href": "amli/09_05-07-career.html#questions",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Questions?",
    "text": "Questions?\n\nThank you all so much for participating today. What remaining questions do you have?"
  },
  {
    "objectID": "amli/09_05-07-career.html#career-development-preparing-for-interviews",
    "href": "amli/09_05-07-career.html#career-development-preparing-for-interviews",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Career Development: Preparing for Interviews",
    "text": "Career Development: Preparing for Interviews\nInterview preparation is essential for success in securing your ECE & ML roles.\n\nToday we’ll go over a basic overview of interview preparation and the differences between behavioral and technical interviews, both of which you’re likely to face as you job search. Technical interviews can be a very stressful experience for students starting their technical careers and seasoned professionals alike. These interviews should be stressful and require significant preparation, as you can be asked questions from a range of topics. Behavioral interviews also require preparation, but with the helpful framework we give you today, hopefully they will become manageable. And as with most things, both get easier with practice. Let’s dig in."
  },
  {
    "objectID": "amli/09_05-07-career.html#agenda-1",
    "href": "amli/09_05-07-career.html#agenda-1",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Agenda",
    "text": "Agenda\n\nInterview prep essentials\nTechnical vs. behavioral interviews\nTechnical interview basics & tips\nBehavioral interview basics & tips\nQ&A\n\n\nWe’ll start by going over some general best practices for interviewing for any full-time role. Then we’ll get on the same page about the differences between behavioral and technical interviews and go over tips for both. Then we’ll discuss the types of questions you’re likely to be asked in data science/machine learning-related technical interviews and some common behavioral interview questions. We’ll close out with Q&A."
  },
  {
    "objectID": "amli/09_05-07-career.html#interview-prep-essentials",
    "href": "amli/09_05-07-career.html#interview-prep-essentials",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Interview Prep Essentials",
    "text": "Interview Prep Essentials\n\nBefore →\n\nResearch company’s history, mission, and the job description fully.\nCome prepared with 2-3 things you like about the opportunity.\nBe able to clearly articulate why you’re a good fit.\n\nDuring →\n\nDress up! Even if the work place is casual, dress a notch above that.\nShake hands, make eye contact, skew answers toward the positive.\nYou are also evaluating the company. Think, is this a place you want to work?\n\nAfter →\n\nThank your interviewer(s).\nSend a same-day thank you email that’s tailored to the interview.\n\n\n\nFirst, it’s extremely important to come prepared to an interview having researched the company, its history, the role, and why you’re a good fit for the position. It’s possible that you don’t meet all the requirements they’re looking for the role; this is common! But you can stand out as an especially desirable candidate by being passionate about the organization and the work it’s doing.\nAlways dress up for an interview. Don’t take a guess about whether or not a workplace is casual. If the recruiter or interviewer tells you in advance of the interview that attire is casual, still wear business casual to the interview. If the workplace is NOT casual, definitely wear business attire - a business suit.\nAdditionally, while they are evaluating your skills and fit for the role, interviews are a time for you to evaluate if this is a place you want to work.\nMuch like in your work with recruiters, show gratitude to your interviewers. Thank them during the interview and then in a same-day thank you email that mentions something you specifically enjoyed about the interview and that excites you about the role or the company. Sometimes you may not have access to the interviewer’s contact information. In that case, you should forward personalized thank you emails to your recruiter or whomever set up your interviews and ask them to pass your notes along. It’s important to remember that some interviewers could be your future boss, but other interviewers can be employees at the company whose job involves interviewing from time to time. It’s because people make time for that in their work that you’re able to interview in a timely way, so it’s important to show appreciation for their time."
  },
  {
    "objectID": "amli/09_05-07-career.html#behavioral-interviews-vs.-technical-interviews",
    "href": "amli/09_05-07-career.html#behavioral-interviews-vs.-technical-interviews",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Behavioral Interviews vs. Technical Interviews",
    "text": "Behavioral Interviews vs. Technical Interviews\nWhat’s the difference?\n\nWhat do you think? What’s the difference between a behavioral and a technical interview?"
  },
  {
    "objectID": "amli/09_05-07-career.html#technical-vs.-behavioral-interviews",
    "href": "amli/09_05-07-career.html#technical-vs.-behavioral-interviews",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Technical vs. Behavioral Interviews",
    "text": "Technical vs. Behavioral Interviews\n\n\nTechnical interviews assess:\n\nRole-specific technical knowledge\nProblem-solving abilities\nHow you think\nCommunication skills\nHow you get to your answer\n\nPrevalent with employers hiring for engineering, science, and IT roles\n\nBehavioral interviews assess:\n\nLikelihood of future behavior based on past behavior\nHow you’ve responded in specific situations\nThe measurable skills and value you brought to those situations\n\nPrevalent with employers hiring for any role\n\n\nTechnical interviews assess your technical knowledge for a given role, your problem-solving abilities, and what the process of getting to your answer entailed. Your skills are on display in real time.\nBehavioral interviews, on the other hand, assess your likelihood of future behavior based on past behavior. The interviewer wants to know how you’ve responded in specific situations and the measurable skills you’ve brought to those situations. Behavioral interview questions often start with, “Tell me about a time when….” or “Describe a time when…”\nNote that many interviews may involve both technical and behavioral questions."
  },
  {
    "objectID": "amli/09_05-07-career.html#technical-interview-basics",
    "href": "amli/09_05-07-career.html#technical-interview-basics",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Technical Interview Basics",
    "text": "Technical Interview Basics\nThere are five types of technical interview questions you’re likely to encounter:\n\nEstimation questions\nKnowledge questions\nProbability questions\nCoding questions\nOpen-ended questions\n\n\nNow let’s get into the basics of technical interviews. Technical interviews vary depending on the kind of technical work you’ll be asked to do in the job. For technical roles related to data science and machine learning, you’re likely going to see questions of these five types:\n1.) Estimation: These kinds of questions used to be popular in high tech in the late 90’s and have still not completely died out. They are significantly less common than they used to be though. Examples include, “Why are manhole covers round?” “What numbers would you put on each face of 2 D6s to represent dates?” These questions don’t really give a valuable signal to the employer.\n2.) Knowledge: Knowledge questions can include anything within the field that you’re applying to and have been studying. Throughout this program we’ve tried to cover the topics that are likely to come up in the interview. It’s worth reviewing the list of topics that have been covered in the course.\n3.) Probability: The interviewer is also likely to assess your knowledge of probability. This might be rolled into one of the other questions (e.g., coding or open-ended) but could be a separate question as well.\n4.) Coding: There are likely to be coding questions as well. If you have a CS degree, check out the Cracking the Coding interview book and use it to practice. If you don’t, you’re still likely to get a coding question but hopefully one that is tailored to your abilities. Whatever question you get, this is definitely a place to practice active listening. Make sure you understand the question, ask clarifying questions, verify sample inputs and outputs. Also, coding on a whiteboard is weird and you need to practice it. And once you’re done writing the code, you’re not done. Verify that it works as intended by doing careful testing.\n5.) Open-ended: Open-ended questions are often last because they can shrink or expand to fit the time that is left. They sound like knowledge questions but don’t have a single quick answer, and it’s more about the discussion that ensues than getting the “right” answer."
  },
  {
    "objectID": "amli/09_05-07-career.html#technical-interview-example-questions",
    "href": "amli/09_05-07-career.html#technical-interview-example-questions",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Technical Interview Example Questions",
    "text": "Technical Interview Example Questions\nBelow is an example of each kind of technical question you’re likely to see:\n\nEstimation: How many ping pong balls does it take to fill a 747 airplane?\nKnowledge: What is selection bias? Why does it matter?\nProbability: There is a research on two medical cures: A, B. 200 people tried the cure A, it helped 110 people and did not help 90 people. 210 people tried the cure B, it helped 120 people and did not help 90 people. Compute success percentages for each drug.\nCoding: Reverse all the words in a string of words while preserving the word order and spacing.\nOpen-ended: Is it better to have too many false negatives or too many false positives?\n\n\nHere’s an example of each of these kinds of technical questions. Estimation questions, as stated previously, are not commonly used anymore.\nNext is knowledge questions. Knowledge questions might include, “What is selection bias? Why does it matter” or “What is linear regression?”\nThird is probability questions. This is just one example. Other questions may include something like Bertrand’s box paradox or Penney’s game.\nFourth is coding. You could be presented with a lot of different coding questions. Again, Cracking the Coding Interview is a great resource for technical interview prep.\nFifth is open-ended. These are often reserved for the end of an interview because they can shrink and expand to meet the time that’s left in the interview."
  },
  {
    "objectID": "amli/09_05-07-career.html#technical-interview-tips",
    "href": "amli/09_05-07-career.html#technical-interview-tips",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Technical Interview Tips",
    "text": "Technical Interview Tips\n\nThey typically last for 45-50 minutes\n\nReview the fundamentals\n\nAsk your recruiter how you’ll be coding.\n\nWhiteboard on video?\nPhone and with a shared document?\n\nThe interviewer is not a judge; they are your advocate.\nEngage in active listening throughout your interview.\n\nAsk questions.\nVerbalize your thought process.\nJustify and vocalize your assumptions.\nListen carefully. Are they giving you hints or providing directives?\n\nThere are usually multiple correct answers. Take time to think and choose carefully.\n\n\nFirst, interviews are not like school work. They’re not like the work you’re doing in this program. They’re not even like the work you’re going to be doing with these employers. Therefore, you really need to prepare for them.\nSecond, the interviewer is not a judge. Think of them as your advocate. Can’t guarantee that you won’t get an interviewer who’s trying to trick you, but in most cases they are trying to help you.\nRemember that you’re not just being evaluated; you’re also evaluating the company. That doesn’t mean you can ask questions for 20 minutes in a 45 minute interview, but come prepared with a few questions and take note of aspects of the application and interview process that reflect a workplace culture you’d want to join or not want to join.\nListen to your interviewer. Questions can sound similar, so make sure you understand what you’re being asked to do. Ask questions to clarify the problem and verbalize your thought process. It’s better to say something wrong than to say nothing. Make sure you’re justifying (and vocalizing) your assumptions.\nLast, there are usually multiple correct answers. Take the time to think. Pick carefully."
  },
  {
    "objectID": "amli/09_05-07-career.html#behavioral-interview-example-questions",
    "href": "amli/09_05-07-career.html#behavioral-interview-example-questions",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Behavioral Interview Example Questions",
    "text": "Behavioral Interview Example Questions\n\nDescribe a situation that required you to consider a different perspective when exploring an issue.\nDescribe a project that required input from people at different levels in the organization.\nTell me about a time you went above and beyond the call of duty to get things done.\nTell me about two improvements you have made in the last six months.\nTell me about a time that you took the lead on a difficult project.\nDescribe a time when you found it difficult to work with someone from a different background.\nDescribe a time when you anticipated potential problems and developed preventive measures.\nTell me about a time when you had to analyze information and make a recommendation.\n\n\nNow let’s transition into behavioral interviews. Remember that in behavioral interviews, employers are assessing your likelihood of future behavior based on past behavior. How have you responded in specific situations that will help employers to know how you’ll respond in future, similar situations?\nHere are some sample behavioral interview questions they may use to help do that. Can you think of some others you’ve heard?\nAs you can see, there are a lot of different directions you could take your answers to these questions. The best way to prepare for behavioral interviews is to have a bank of stories lined up that align with the role’s expectations and required/preferred qualifications. You might be wondering how to create that bank of stories, which is what we’ll discuss in the next two slides."
  },
  {
    "objectID": "amli/09_05-07-career.html#behavioral-interview-basics-star-method",
    "href": "amli/09_05-07-career.html#behavioral-interview-basics-star-method",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Behavioral Interview Basics: STAR Method",
    "text": "Behavioral Interview Basics: STAR Method\n\nBehavioral Interview Basics: STAR Method\nThe best way to approach creating a bank of behavioral interview stories is to follow the STAR method. Take a few moments to read this slide over before we get started. [Provide students 1-2 minutes of silence to read the slide.]\nThe STAR method is a tried and true technique for preparing for behavioral interviews and successfully answering behavioral interview questions. In the 1-2 minutes in which you’re responding to a behavioral interview question, you should format your answer using the STAR method: describe the situation, the task or tasks, the actions you took, and the result. Remember, most behavioral interview questions start with “Tell me about a time when…” or “Describe a time when…” so interviewers are explicitly looking for you to tell a true story about your past academic or professional experiences. Practicing your stories in this order and keeping this framework in your head will help you deliver more effective, organized answers in interviews.\nLet’s look at an example."
  },
  {
    "objectID": "amli/09_05-07-career.html#behavioral-interview-basics-star-method-example",
    "href": "amli/09_05-07-career.html#behavioral-interview-basics-star-method-example",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Behavioral Interview Basics: STAR Method Example",
    "text": "Behavioral Interview Basics: STAR Method Example\nTell me about a time you didn’t meet a goal and how you handled it.\n\nBehavioral Interview Basics: STAR Method Example\nHere’s an example that will hopefully bring the STAR method to life a bit. Tell me about a time you didn’t meet a goal and how you handled it.\nCould I get a few volunteers to read this STAR method example please? [Have four students read each of the different parts of the STAR example, beginning with the Situation.]\nWhat stands out to you here? What has the interviewee done well? [Ask for student contributions and respond to each. Things to point out: gave appropriate background information in the Situation, quantified the situation and the result to show impact and provide specifics, showed initiative, displayed humility, continued improving even after meeting the goal, etc.]\nWhat questions do you have looking at this example or about the STAR method generally? [Ask for student contributions and respond to each.]"
  },
  {
    "objectID": "amli/09_05-07-career.html#behavioral-interview-basics-star-method-example-1",
    "href": "amli/09_05-07-career.html#behavioral-interview-basics-star-method-example-1",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Behavioral Interview Basics: STAR Method Example",
    "text": "Behavioral Interview Basics: STAR Method Example\n\nBehavioral Interview Basics: STAR Method Example\nAs you prepare for behavioral interviews, it might be helpful to set up a table like this to prepare for behavioral interview questions. After you write out these different stories according to the STAR method, use this sheet to practice saying them aloud."
  },
  {
    "objectID": "amli/09_05-07-career.html#recap",
    "href": "amli/09_05-07-career.html#recap",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Recap",
    "text": "Recap\n\nInterviews require preparation. Prep for technical and behavioral questions and research the company and position.\nEngage in active listening throughout your interview.\n\nAsk questions.\nVerbalize your thought process.\nJustify and vocalize your assumptions.\nListen closely for hints or additional information\n\nYou’re also evaluating the employer. Come prepared with questions about the company.\nUse the STAR method to create a bank of behavioral stories that reflect what the position requires.\n\n\nLet’s recap! Please don’t forget that interviews require a lot of preparation. They are not like school work. They’re not like the work you’re doing in this course. They’re not even like the work you’re going to be doing with an employer. Therefore, you really need to prepare for them.\nRemember that you’re not just being evaluated; you’re also evaluating the company. Both before, during, and after the interview, you should pay attention to what’s a good fit for you at this company and for this role, and what may not be.\nListen to your interviewer. Questions can sound similar, so make sure you understand what you’re being asked to do. Ask questions to clarify the problem and verbalize your thought process. It’s better to say something wrong than to say nothing. Make sure you’re justifying (and vocalizing) your assumptions.\nLast, take some time to create a bank of STAR method stories. Your examples can be from internships, volunteer work, extracurriculars, school work, or jobs. Making a list of these stories and practicing them will help you think and answer more quickly, completely, and effectively in your behavioral interviews."
  },
  {
    "objectID": "amli/09_05-07-career.html#questions-1",
    "href": "amli/09_05-07-career.html#questions-1",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Questions?",
    "text": "Questions?\n\nThank you all so much for participating! What questions do you have?"
  },
  {
    "objectID": "amli/09_05-07-career.html#career-development-presenting-with-confidence",
    "href": "amli/09_05-07-career.html#career-development-presenting-with-confidence",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Career Development: Presenting with Confidence",
    "text": "Career Development: Presenting with Confidence\nEffective communication is crucial for ECE professionals, especially when presenting technical work.\n\nToday we’re talking about everyone’s favorite topic: public speaking. That was my attempt at a joke. I’m sure some of you enjoy public speaking and maybe others avoid it like the plague.\nPresenting with confidence is a critical skill, though. Your career will hopefully provide opportunities to give presentations to your team, at professional conferences, and/or in academic settings. Being an effective presenter gives you credibility and visibility across all of those environments. Furthermore, hard work can be overlooked or under-appreciated if presenters don’t convey their ideas and work products in clear, understandable, and compelling ways.\nShare any anecdote or personal story about why presenting with confidence is such an important skill.\nBut being a strong presenter takes time to learn and practice. So I hope no matter where you are in your experience with presenting with confidence, you can take a few tips and some extra experience from today’s session."
  },
  {
    "objectID": "amli/09_05-07-career.html#agenda-2",
    "href": "amli/09_05-07-career.html#agenda-2",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Agenda",
    "text": "Agenda\n\nFormula for a Successful Presentation\n\nContent + Delivery\nThe 4 P’s\nCreating the Visual\nHabits to Avoid\n\nActivity + Practice\n\nValues Activity\nPractice Presenting Confidently\nFeedback\n\nQ&A\n\n\nWe’ll start by going over what makes a successful presentation: content, delivery, creating helpful visuals, and avoiding certain presentation pitfalls. Then we’ll do a brief activity and practice presenting with confidence. We’ll close with feedback and Q&A."
  },
  {
    "objectID": "amli/09_05-07-career.html#content-delivery-success",
    "href": "amli/09_05-07-career.html#content-delivery-success",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Content + Delivery = Success",
    "text": "Content + Delivery = Success\nWhat you say and how you say it\n\nThe two main components of giving a presentation are the content (what you say) and the delivery (how you say it)."
  },
  {
    "objectID": "amli/09_05-07-career.html#content-go-from-good-to-great",
    "href": "amli/09_05-07-career.html#content-go-from-good-to-great",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Content: Go From Good to Great!",
    "text": "Content: Go From Good to Great!\n\nKnow your audience\nTell a story\nMake your ideas flow in an organized fashion\nSummarize main points\n\n\nFirst let’s discuss content, which is everything you say, display, or show in your presentation.\nNo matter what information you’re sharing, it’s important you tailor it to your audience. If the information you’re presenting to a non-technical or mixed audience is too technical or not well-explained, your audience is likely to tune out. Increase the likelihood that your presentation will be engaging and well-received by tailoring your slide design, verbiage, and content to them.\nSecond, using examples is a great way to tell a story that keeps your audience engaged. People remember how stories illuminated a particular point better than they will if just the point is presented.\nThird, try to maintain a logical progression of ideas, keeping in mind the main purpose of the presentation. When possible, provide an agenda. With long presentations, it’s a good idea to benchmark your progress or potentially create different sections so the audience knows where you are in the presentation.\nFinally, it’s important to summarize the main points of your presentation at the end of any presentation and throughout different sections if the presentation is 30 or more minutes."
  },
  {
    "objectID": "amli/09_05-07-career.html#delivery-the-four-ps-of-presenting",
    "href": "amli/09_05-07-career.html#delivery-the-four-ps-of-presenting",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Delivery: The Four Ps of Presenting",
    "text": "Delivery: The Four Ps of Presenting\n\nDelivery: The Four Ps of Presenting\nNext up is delivery, or, how you get the content across to your audience.\nPitch or ‘musicality’ has to do with the note with which we pitch a word or phrase. Steering away from a monotone and varying your pitch when appropriate grabs attention and emphasizes what you want to express. It also helps convey the urgency or novelty of what you’re saying.\nPace is the speed at which you talk. Pace can also be helpful to drive emphasis. For instance, when you want to highlight an important point, slow down and speak distinctly.\nNext is power. Regardless of whether you speak quietly or loudly in day-to-day conversation, being able to project your voice while presenting is a clear sign of confidence. It is impossible to ‘whisper’ and still be heard.\nPausing allows you time to think, and it can be effectively used to create anticipation. Sometimes silence can get people’s attention more than speaking. It also allows sufficient time for an audience to fully absorb your message.\nWhich of these is something you think your presentation skills should improve on? Or what do you feel like you do well?"
  },
  {
    "objectID": "amli/09_05-07-career.html#content-delivery-creation",
    "href": "amli/09_05-07-career.html#content-delivery-creation",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Content, Delivery, Creation:",
    "text": "Content, Delivery, Creation:\n\nContent, Delivery, Creation\nNow that we’ve covered what makes up a strong presentation, let’s go over some suggestions for how to achieve it. These are some general rules of thumb when making any presentation.\nIt’s important that your slides, tone, and overall presentation are consistent.\nA presentation should be a simple representation of pieces of information. It only acts as a visual aid to the broader story, project, or conversation. Therefore, the key points in this deck would be explained verbally in a presentation rather than all piled onto a slide.\nFor all slides, apply the 3-second rule. People should be able to glean the point you’re trying to convey on a slide within three seconds or less."
  },
  {
    "objectID": "amli/09_05-07-career.html#tips",
    "href": "amli/09_05-07-career.html#tips",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Tips",
    "text": "Tips\nAvoid these common habits * Not practicing * Not recording practice * Using filler words * Fidgeting * Avoiding eye contact or engagement * Rushing\n\nThese are common presentation quirks that usually stem from nervousness or lack of preparedness. It’s important to be aware and try to avoid them, as they can detract from the impact of your presentation.\nFirst, know your content, and do not read directly from the slides. Even if you feel you know your content well, practicing is essential. You may pick up on a slide or order that needs revising.\nRecord yourself when you practice so you can see what the audience will see. Look for ways to improve. Were there any awkward transitions? Did something not make sense?\nAvoid using comfort/filler words like “um, like, y’know.” This is often subconscious. We don’t always realize when or how often we use fillers, which is another great reason to record yourself practicing.\nFidgeting, like touching your hair, fiddling with your clothes, etc., is very distracting for the audience. Some people roll up a piece of paper or pull on their work badge. Work on it if you know you have something like that.\nA key part of presenting with confidence is engaging with your audience. You don’t want to exclude them from the presentation by avoiding eye contact or looking at the floor. Rotate who you make eye contact with to help keep people engaged. Ask them questions, and engage them in what you’re doing.\nDon’t speed through your presentation. Your audience wants to hear what you have to say, and it is more inclusive of different kinds of learners and people with varying levels of English language acquisition to speak clearly and not too fast.\nWhich of these do you fall victim to?"
  },
  {
    "objectID": "amli/09_05-07-career.html#recap-1",
    "href": "amli/09_05-07-career.html#recap-1",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Recap",
    "text": "Recap\n\nConsider your audience, be organized, tell a story\n4 Ps of persuasive presenting: Pitch. Pace. Power. Pausing.\nBe consistent with theme, colors, fonts, spacing, style\n3-second rule of slide development\nUse eye contact and engage your audience with questions\nRecord your practice\n\n\nTake a moment to read over some main points thus far. Also, here’s me putting to use the suggestion to recap for the audience!\nWhat questions or comments do you have about the content we’ve covered so far?"
  },
  {
    "objectID": "amli/09_05-07-career.html#activity-practice",
    "href": "amli/09_05-07-career.html#activity-practice",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Activity + Practice",
    "text": "Activity + Practice\n\nLet’s do a brief activity and then put these presentation suggestions into practice!"
  },
  {
    "objectID": "amli/09_05-07-career.html#activity",
    "href": "amli/09_05-07-career.html#activity",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Activity",
    "text": "Activity\n\nRead the list of values on the next slide\nWrite down the eight values that resonate with you most\nStar four of those eight values that are most important to you\nFinally, circle two of those four values that feel like your most core values, those you can’t separate yourself from"
  },
  {
    "objectID": "amli/09_05-07-career.html#activity-1",
    "href": "amli/09_05-07-career.html#activity-1",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Activity",
    "text": "Activity\n\nActivity: List of Values\nTake a moment to read over the list of values. When you’re done, write down the eight values that resonate with you most or add other one-word values that resonate with you. (This is list not exhaustive.)\nLooking at this list of eight, star the four values that are most important to you.\nNow, this part might be the most difficult, but the final task here is to circle two of those four values that are your core values. You may find that some of the other values that resonate with you fall under those two. Think of these values as the ones you hold yourself to in your everyday behavior and future planning, the values you aspire to embody - maybe so much so that you do so unconsciously.\nAllow students one minute to find those two values.\nWhy two values, you may ask? Researcher, public speaker, social worker, and famous author Brene Brown writes extensively about the importance of determining your two core values. If we have too many, they lose meaning and we aren’t able to hold ourselves to them. The two values that feel inextricably intertwined with who you are as a person are those two values.\nFacilitator should feel free to share their own two values here.\nDoes anyone want to share what 1 or 2 of their values are?"
  },
  {
    "objectID": "amli/09_05-07-career.html#practice",
    "href": "amli/09_05-07-career.html#practice",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Practice",
    "text": "Practice\n\nPick a partner\nTake 3-4 minutes to prepare a 1½ -2 minute presentation on this prompt:\n\nWhat are the two values that are most important to you?\n\nTake 6 minutes to present to one another and give feedback\n\n3 minutes each\n1½ - 2 minutes to present\n1 minute of feedback\n\nLet’s not film it for now, so your partner can be fully present\nShare one strength and one area of improvement for your partner after they present\n\nRemember, helpful feedback is clear, specific, kind, future-focused, and starts with “I”\n\n\n\nNow, we’re going to share these values in a brief presentation to a partner. (We’ll also revisit these values in our next career development session, so don’t forget what you chose!)\nAsk a student to read the instructions aloud.\nQuestions before we get started?"
  },
  {
    "objectID": "amli/09_05-07-career.html#practice-1",
    "href": "amli/09_05-07-career.html#practice-1",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Practice",
    "text": "Practice\n\nIntroduce yourself and your topic\n\nHello, my name is ____ and today I’ll be talking about ____.\n\nWhat do you want your audience to know?\n\nMain Point 1 - support with evidence, data, or story\nMain Point 2 - support with evidence, data, or story\n\nConclude\n\nEmphasize the most important points\nThank the audience for their time\n\n\n\nHere’s one way you can structure your presentation, but feel free to be creative. Note that this isn’t likely a place for data, but I wanted to include it as a method of supporting a main point.\nIf desired, the facilitator can share an example presentation before releasing students.\nFlip back to the previous slide so those instructions are displayed for students as they work. Give students 10 minutes total for this entire exercise, letting them know when they should transition from preparing the presentations to giving them and switching between partners."
  },
  {
    "objectID": "amli/09_05-07-career.html#any-volunteers",
    "href": "amli/09_05-07-career.html#any-volunteers",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Any Volunteers?",
    "text": "Any Volunteers?\n\nAfter students have presented to their partners and given and received feedback, ask if anyone would like to present to the class. Thank students who volunteer. Move on to closing if no one volunteers or if running short on time.\nWhat was this experience like for you? Which of the best practices we went over were difficult to apply? What did you or your partner do particularly well? What would you work to improve if you gave this presentation again?"
  },
  {
    "objectID": "amli/09_05-07-career.html#qa",
    "href": "amli/09_05-07-career.html#qa",
    "title": "Machine Learning for ECE: Professional & Technical Skills",
    "section": "Q&A",
    "text": "Q&A\n\nThank you all so much for engaging in this session and sharing about yourselves in these presentations.\nAs I mentioned in the beginning of this presentation, becoming a presenter takes time and involves significant, frequent practice. I hope you’ve taken something from today’s session that you can apply to your project presentations and other presentations you give in the future. What remaining questions do you have?"
  },
  {
    "objectID": "amli/07-capstone.html#capstone-project-introduction",
    "href": "amli/07-capstone.html#capstone-project-introduction",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Capstone Project Introduction",
    "text": "Capstone Project Introduction\n\nToday we’ll begin discussing the culminating assignment for this course: the capstone project. We’ll begin by going over the project goals, guidelines, deliverables, and timeline in this presentation. Please feel free to ask questions as we go through. Later on we’ll move into discussing project topic ideation and begin the process of forming teams."
  },
  {
    "objectID": "amli/07-capstone.html#project-goals",
    "href": "amli/07-capstone.html#project-goals",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Project Goals",
    "text": "Project Goals\n\nApply technical ML concepts and data skills learned in the first half of the course on a sizeable dataset and challenging problem\nDemonstrate professional development skills, such as working in a group with different personalities, giving and receiving feedback, communicating results, and project management\nCreate and facilitate a recorded demo or presentation of capstone project work\n\n\nA significant amount of this course and class time are intentionally devoted to the capstone project.\nWe do this because employers want to see and hear about times you have worked collaboratively with other people in pursuit of a shared goal. Truthfully, that’s much of professional life. You are better equipped to make your case to employers that you have experience working with others toward a technical goal if you have specific projects you can speak to, both listed on your resume and in your interview answers. The capstone project is one of those experiences, and it also allows you to apply the skills you’ve learned so far to address a specific question and present your results."
  },
  {
    "objectID": "amli/07-capstone.html#capstone-project-deliverables",
    "href": "amli/07-capstone.html#capstone-project-deliverables",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Capstone Project Deliverables",
    "text": "Capstone Project Deliverables\n\nDesign doc\nEthical consideration worksheet\nNotebook\nProject demo or presentation\n\n\nHere are the deliverables for the capstone project. We’ll go into each of them in more detail now."
  },
  {
    "objectID": "amli/07-capstone.html#capstone-project-deliverables-1",
    "href": "amli/07-capstone.html#capstone-project-deliverables-1",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Capstone Project Deliverables",
    "text": "Capstone Project Deliverables\nDeliverables 1-2 of 4\n\n\n1. Design doc * Goal(s) for the project - ideally long term, and realistic for the project duration * Who will play what roles (program manager, note taker) during each project phase * Describe your plan for data acquisition and data preparation * Explain problem space and motivated questions * Approach and list of tasks\n\n2. Ethical consideration worksheet * Submit with design docs, then submit an updated version with the final product\n\n\nFirst your team will complete a design doc that lays out the goals for your team’s unique project, which team members will play what roles during each of the project phases, data-related information, what problem your project seeks to solve, and a list of tasks you’ll complete in pursuit of that solution.\nNext you’ll complete an ethical consideration worksheet that facilitates thought and discussion about the ethical implications of your project idea. Again, a template will be provided. You will submit this worksheet as part of the design docs. Then after you’ve worked more with your data and trained your model, you will submit an updated version of this worksheet with your final product. It is important to consider the ethical implications before you start a project, but also during each phase of development as things change and new information surfaces."
  },
  {
    "objectID": "amli/07-capstone.html#capstone-project-deliverables-2",
    "href": "amli/07-capstone.html#capstone-project-deliverables-2",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Capstone Project Deliverables",
    "text": "Capstone Project Deliverables\nDeliverables 3 of 4\n3. Notebook * The notebook should read like a formal report and follow a linear flow from start to finish with both narrative and code blocks. * The notebook should be internally complete in the sense that a “reader” should understand the motivating question, goals, dataset, model, and ethical considerations. * Markdown cells with narrative should precede and follow each code block. * For each piece of code, describe the purpose, an overview of how it works, and how to interpret results. * Discuss current limitations and future improvements (if applicable).\n\nIn addition to the design doc and ethical consideration exercise, your team will submit a notebook of your project’s narrative and code blocks that reads like a formal report. For each piece of code, you should describe the purpose, an overview of how it works, and how to interpret the results. If applicable, include a discussion of current limitations and future improvements."
  },
  {
    "objectID": "amli/07-capstone.html#capstone-project-deliverables-3",
    "href": "amli/07-capstone.html#capstone-project-deliverables-3",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Capstone Project Deliverables",
    "text": "Capstone Project Deliverables\nDeliverables 4 of 4\n4. Project demo or presentation * Overview of the project and the problem it seeks to solve * Conclusions and findings * Things that went well, things that did not go well, and lessons learned * Next steps (if applicable)\n\nLast is the project demo or presentation. The presentation is 15 minutes in length with 5 minutes for questions. Every member of your team is expected to speak and contribute. We’ll get into the specific requirements for the project presentation later on in this discussion."
  },
  {
    "objectID": "amli/07-capstone.html#example-datasets",
    "href": "amli/07-capstone.html#example-datasets",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Example Datasets",
    "text": "Example Datasets\n\nIMDB Movie reviews sentiment classification Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative).\nBoston housing price regression dataset Dataset taken from the StatLib library, which is maintained at Carnegie Mellon University. Samples contain 13 attributes of houses at different locations around the Boston suburbs in the late 1970s.\nNOAA GSOD dataset Daily global surface summary from over 9000 weather stations from 1929 to 2016 maintained by National Oceanic and Atmospheric Administration (NOAA)\n\n\nLinked here are some datasets your team can consider using for your project. This is not an exhaustive list or rulebook. Instead, this list is intended to help you think about a few options, and hopefully spark more/different ideas!"
  },
  {
    "objectID": "amli/07-capstone.html#example-datasets-continued",
    "href": "amli/07-capstone.html#example-datasets-continued",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Example Datasets (continued)",
    "text": "Example Datasets (continued)\n\nUC Irvine ML Repository A collection of datasets including Life Sciences, Physical Sciences, Business, CS/Engineering, Social Sciences, etc.\nGoogle BigQuery Public dataset Readily available copies of public datasets including Github actions, US births, Shakespeare, Wikipedia revision history, etc.\nGoogle AI Datasets A collection of datasets with wide-ranging topics.\ndataCommons Constructed by synthesizing a single Knowledge Graph from many different data sources. It currently contains data from Wikipedia, the US Census, NOAA, FBI, etc.\n\nAlternatively, you can use Kaggle or Google Public Data Explorer to search for an interesting dataset for your project.\n\nMore examples.\nAlternatively, Kaggle and Google Public Data Explorer have datasets to look through, too."
  },
  {
    "objectID": "amli/07-capstone.html#project-timeline",
    "href": "amli/07-capstone.html#project-timeline",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Project Timeline",
    "text": "Project Timeline\nPhases 1-2 of 4\n\n\nProject Phase 1 * Form teams * Explore and pick project topics * Write a design doc on how you approach building the ML model * Create a high level project plan of who’s doing what tasks by when\n\nProject Phase 2 * Acquire & prep dataset * Start defining and training your ML model * Beginning project presentation: discuss acquiring data, exploratory data analysis, and justification for first model\n\n\nThere are four distinct phases of the capstone project. The first involves forming teams, exploring and picking project topics, and completing the design doc and project plan to guide your work. Please note a template will NOT be provided for the project plan. The project plan format will be up to your team to decide. You’re welcome to use one from the project management session or create a new one.\nThe second project phase includes acquiring and prepping your team’s dataset, defining and training your ML model, and beginning designing your project presentation or demo."
  },
  {
    "objectID": "amli/07-capstone.html#project-timeline-continued",
    "href": "amli/07-capstone.html#project-timeline-continued",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Project Timeline (continued)",
    "text": "Project Timeline (continued)\nPhases 3-4 of 4\n\n\nProject Phase 3 * Use data to tune model and validate * Mid project presentation: share iterations of model tuning with a focus on challenges and questions, welcome feedback from peers and instructors * Group and individual meetings with the instructional team\n\nProject Phase 4 * Final iterations on model predictions * Review results and prepare conclusions * Practice final presentation and demo, collect, and incorporate final feedback\n\n\nThe third project phase involves using data to tune and validate your model and continuing to work on your final presentation with peer and instructor feedback.\nThe fourth phase includes final iterations on model predictions, reviewing results and preparing conclusions. And then, finally, you’ll present your final presentations to the class. Now we’ll transition into the expectations for your final presentations."
  },
  {
    "objectID": "amli/07-capstone.html#project-phases-flow",
    "href": "amli/07-capstone.html#project-phases-flow",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Project Phases Flow",
    "text": "Project Phases Flow\n\n\n\n\n\nflowchart LR\n    A[Phase 1: Setup & Planning] --&gt; B[Phase 2: Data & Initial Model]\n    B --&gt; C[Phase 3: Tuning & Validation]\n    C --&gt; D[Phase 4: Finalization & Presentation]\n\n\n\n\n\n\n\nThis diagram visually summarizes the progression of the project through its four distinct phases. Each phase builds upon the previous one, leading towards the final presentation. This flow helps in understanding the project lifecycle at a glance."
  },
  {
    "objectID": "amli/07-capstone.html#final-presentations",
    "href": "amli/07-capstone.html#final-presentations",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Final Presentations",
    "text": "Final Presentations\n\nPresentations should be 15 minutes each plus 5 minutes for questions. Each member of the team should present for roughly the same amount of time.\nPresentations should be visually interesting but also follow best practices for verbal presentation skills. Namely, it is important for all points to be conveyed clearly.\nYou will have an opportunity to present to the instructional team during the 2-3 days leading up to final presentations.\n\n\nFinal presentations should include an overview of the project and the problem it seeks to solve, conclusions and findings, lessons learned, and a brief discussion of what went well and what didn’t go well. It should include any next steps your team has as you consider future developments for your project.\nEach team’s presentation should last 15 minutes, and there will be an additional five minutes allowed for Q&A. Every team member is expected to present for roughly the same amount of time. Please consider presentation best practices and make a presentation that is visually interesting and one that conveys your points clearly.\nIt is important to practice your presentation several times to ensure that it is very high-quality. It will also help you prepare your timings and transitions. Students often underestimate the amount of practice that goes into a professional presentation, so practice practice practice!! You will also have opportunities to practice your presentation for instructional staff, and we recommend taking advantage of those opportunities to refine your team’s presentation."
  },
  {
    "objectID": "amli/07-capstone.html#capstone-project-team-roles",
    "href": "amli/07-capstone.html#capstone-project-team-roles",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Capstone Project Team Roles",
    "text": "Capstone Project Team Roles\nRotating with each of the four project phases, team members will take turns working in different capacities.\nProgram Manager: Everyone will have a turn in the role of “program manager,” or the person who is purposefully directing the team and assigning action items. This role builds useful professional skills, and rotating ensures no one person gets stuck with all of the administrative tasks to keep the team running.\nNote Taker: Each student team member will be responsible for maintaining a “decision log” during at least one phase of the project. Entries in decision logs may include why the team went forward with one thing over another, how a problem got resolved, unresolved tasks or questions, etc. Decision logs will be housed in a common repo and accessible to students and instructional staff.\n\nNext we’ll talk briefly about team roles. In order to share the leadership and administrative responsibilities required for such a project, as well as gain experience in both capacities, teams will rotate who is the program manager and who is the note taker.\nProgram managers will assign tasks and keep the group on track during their respective turn in this role.\nNote takers will be in charge of keeping a decision log that will be accessible to students and instructional staff. This log will include why certain courses of action were taken and not others, along with any notes from discussion, points that the team needs to return to, etc."
  },
  {
    "objectID": "amli/07-capstone.html#daily-schedule",
    "href": "amli/07-capstone.html#daily-schedule",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Daily Schedule",
    "text": "Daily Schedule\nProject teams work synchronously or asynchronously, but they must have daily meetings as a whole group.\nSample daily schedule * Morning team stand-ups * Project work (multi-hour block) * (Optional) ad hoc topic session (at most 2-3 times per phase) * Lunch * Project work (multi-hour block) * (Optional) professional skills session or guest speaker (at most 1 per phase) * End of day team meetings and stand-ups\n\nDuring the capstone project phases, we will have morning and afternoon “standups.” This is a very common industry practice, and for good reason! They allow group members an opportunity to raise issues or ask for help regarding anything that’s blocking their work.\nEvery morning we’ll take 10 minutes or less to share how your group will prioritize your time for the day and what you will feasibly get accomplished. Then at the end of the day, groups will come back together as a class to share what you accomplished during the day, anything you need help with, and what your focus will be the following day. This bookend standup structure will keep us on track and allow instructors to help as needed."
  },
  {
    "objectID": "amli/07-capstone.html#questions",
    "href": "amli/07-capstone.html#questions",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Questions?",
    "text": "Questions?\n\nWe’ve covered our capstone project goals, deliverables, phases, team roles, final presentation expectations, and how our daily schedule will shift for the remainder of the course. Now that we have covered these basics, our next capstone-related session will get into project topic ideation and forming groups. Before we move on, however, what questions do you have on what we’ve gone over here?"
  },
  {
    "objectID": "amli/07-capstone.html#objective",
    "href": "amli/07-capstone.html#objective",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Objective",
    "text": "Objective\nGive a brief description of the goals of your project.\n\nWhat questions do you hope to address? What value will you be adding to the space?"
  },
  {
    "objectID": "amli/07-capstone.html#background",
    "href": "amli/07-capstone.html#background",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Background",
    "text": "Background\nDescribe the background and context for this project.\n\nWhat is the motivation for studying this topic? What other work has been done: what was good and where does it fall short? If your project is interdisciplinary, give an overview of the key ideas from the connecting disciplines."
  },
  {
    "objectID": "amli/07-capstone.html#data-collection",
    "href": "amli/07-capstone.html#data-collection",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Data Collection",
    "text": "Data Collection\nDescribe your preliminary ideas for data collection and cleaning.\n\nAre you planning to use datasets that are already compiled? Will you need to pull supplemental information from additional sources? What plans do you have to clean and prepare your data? How much time are you budgeting for the data collection and cleaning process?\n\n\n\n\n\n\nImportant\n\n\nImportant Note: Students frequently underestimate this step. It is crucial to remember that sometimes 90 percent of this work is getting the data in a useable format."
  },
  {
    "objectID": "amli/07-capstone.html#detail-designideas",
    "href": "amli/07-capstone.html#detail-designideas",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Detail Design/Ideas",
    "text": "Detail Design/Ideas\nProvide a detailed overview of the project design.\n\nHow will you approach the problem? What ideas/experiments do you have planned?\n\n\n\n\n\n\nTip\n\n\nTips for Design: * Focus on design, not implementation details. * A picture is worth a thousand words – use diagrams or drawings. * If describing alternatives (e.g., approach, platform, algorithm), explain why it’s NOT part of your chosen design. * If you take a different approach than originally designed, update your design documents accordingly."
  },
  {
    "objectID": "amli/07-capstone.html#project-plan",
    "href": "amli/07-capstone.html#project-plan",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Project Plan",
    "text": "Project Plan\nProvide a project plan that includes a list of intermediate goals, milestones, and timelines. Also include the schedule of when different team members will take on the program manager and note taker roles.\n\nYou will update this project plan throughout the project. It will be a guide that keeps you on track and accountable. It may be helpful to think about the project management session as you create your plan."
  },
  {
    "objectID": "amli/07-capstone.html#interactive-project-planning-example",
    "href": "amli/07-capstone.html#interactive-project-planning-example",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Interactive Project Planning Example",
    "text": "Interactive Project Planning Example\nAdjust the sliders below to explore how the number of tasks and average complexity might influence simulated project effort over time. This can help in early-stage project planning and resource allocation.\n\nviewof num_tasks = Inputs.range([1, 20], {value: 10, step: 1, label: \"Number of Tasks\"});\nviewof avg_complexity = Inputs.range([0.5, 5.0], {value: 2.0, step: 0.1, label: \"Avg Task Complexity (units)\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive visualization helps in understanding how varying project parameters can affect overall workload and effort distribution. As you increase the number of tasks or their average complexity, you’ll see the cumulative effort curve rise, indicating a greater overall project commitment. This can inform decisions about project scope, team size, and estimated timelines, directly aligning with the “Project Plan” section. In a real-world engineering scenario, similar interactive tools are often used for resource planning and risk assessment."
  },
  {
    "objectID": "amli/07-capstone.html#fairness-considerations",
    "href": "amli/07-capstone.html#fairness-considerations",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Fairness Considerations",
    "text": "Fairness Considerations\nComplete the ethical consideration activity.\n\nYou will also complete this again as part of your final project, as it is critical to consider ethical implications early and often throughout any project."
  },
  {
    "objectID": "amli/07-capstone.html#references",
    "href": "amli/07-capstone.html#references",
    "title": "ECE Machine Learning Capstone Project",
    "section": "References",
    "text": "References\nProvide an annotated list of reference materials that you used in preparing these design documents, as well as any additional references that you plan to use in your project.\n\nFor each reference provide a 1-2 sentence summary of the content and a brief description of how it was (or will be used)."
  },
  {
    "objectID": "amli/07-capstone.html#brainstorming",
    "href": "amli/07-capstone.html#brainstorming",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Brainstorming",
    "text": "Brainstorming\n\nGroup Brainstorming Session\nWe’ll do two rounds of brainstorming. The first is very general, capturing as many ideas as possible. The second will be more specific, helping us narrow in on our exact idea.\nEveryone will write down individual ideas for the final project. Then we will discuss as a group all the ideas that people came up with. It might be helpful at that time to start grouping similar ideas, considering feasibility, and getting slightly more detailed. Then each of you will “pitch” your favorite idea. After that the most-liked ideas will become the project topics, and we’ll form groups."
  },
  {
    "objectID": "amli/07-capstone.html#brainstorming-1",
    "href": "amli/07-capstone.html#brainstorming-1",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Brainstorming",
    "text": "Brainstorming\n\nDefer judgment\nEncourage wild ideas\nBuild on the ideas of others\nStay focused on the topic\nOne idea and one conversation at a time\nBe visual\nGo for quantity\n\n\nWe will take five minutes to write down as many ideas as possible about goals/questions/topics for your capstone project.\nIn order to have a fun and productive exercise, we all need to agree on some norms: * Brainstorming should be completely judgement free. * Crazy ideas are more than welcome! Even if it’s infeasible that they’d get accomplished in the next few weeks, they may inspire other great and more realistic ideas. So write down EVERY idea. No matter how well-thought out it is. * Each idea gets its own post-it note. We want to ensure every idea gets its fair shake. * Let’s have one conversation and share one idea at a time, so people don’t get spoken over or have their ideas minimized. * Feel free to make your ideas visual, with a drawing instead of text.\nCan you think of any other norms we should all agree upon before diving into the initial phase?\nPass out markers and post-it notes. Set a timer for three minutes."
  },
  {
    "objectID": "amli/07-capstone.html#group-ideas-by-theme",
    "href": "amli/07-capstone.html#group-ideas-by-theme",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Group Ideas by Theme",
    "text": "Group Ideas by Theme\n\nGrouping Ideas by Theme\nNow we’ll share out our ideas. Once everyone has shared, we’ll start to group the ideas together by theme.\nAsk students to come up and share their ideas, one by one, sticking their post-it notes on the whiteboard. Make sure there’s a large, clean area devoted to this activity. If your classroom is very large, you may consider asking all students to get up and gather around the whiteboard, so it’s easier to hear each other and so it’s more dynamic. Make sure to enforce the “one conversation at a time” rule so students feel heard. Anyone can start, and others should jump in and share similar ideas or build on the ideas of one another. As they share out, ask them to start roughly clustering similar ideas, if possible.\nAfter everyone shares their ideas, give them 10 minutes to group the ideas by theme. Encourage them to create or revise clusters and use markers to identify each cluster."
  },
  {
    "objectID": "amli/07-capstone.html#narrow-your-problem-and-brainstorm-again",
    "href": "amli/07-capstone.html#narrow-your-problem-and-brainstorm-again",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Narrow Your Problem and Brainstorm Again",
    "text": "Narrow Your Problem and Brainstorm Again\nHow Might We…?\n\nPick an idea or problem that’s been presented so far today.\nWrite a sentence that considers what project you could take on to address that idea.\nBegin the sentence with, “How might we…”\n\n\nNow it’s time to narrow down a problem and brainstorm a project idea. Everyone take a new post-it and write down ONE idea or problem that has been presented so far today. It doesn’t need to be one that you originally came up with.\nNow we’ll work on brainstorming how to frame a project around this idea. For example, suppose one of the ideas was “identify skin cancer,” which was grouped in the “healthcare” cluster. One might frame this problem as “How might we use machine learning to identify photos of skin cancer?” or yet “How might we make it easier for people to detect skin cancer early?” This can then eventually become the idea for a mobile app that allows you to take pictures or moles or skin tags, and get a confidence interval of how likely that is to be skin cancer. (This is actually a past project done by a previous student in this course.\nEveryone should have the problem statement written down on a post-it note in front of them (one for each student, as this portion is also individual).\n*Then, set the timer for another five minutes and ask them to come up with ways to frame solutions to that problem. It’s important to stay focused on the problem that they wrote down and not get distracted by other ideas. They will use these ideas to create a pitch. The best ideas will be selected, and we will only form groups at the very end.\nThe next step we’ll take is pitching your idea."
  },
  {
    "objectID": "amli/07-capstone.html#pitch",
    "href": "amli/07-capstone.html#pitch",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Pitch",
    "text": "Pitch\nThe WHO * Always start by introducing yourself.\nThe WHAT * Describe your idea. * Tell people what makes your idea stand out from other, maybe similar, ideas.\nThe WHY * What’s the value proposition of your idea? Why should people consider working on this idea? * Why is this a challenging yet feasible project that will be enjoyable to work on?\n\nNow you will pitch your idea to the class. Each person will have two minutes to convey their idea and convince others to consider working on it as their capstone project. A pitch must have:\n\nWho: In one or two sentences, describe who you are with a focus on what makes you qualified to propose the project you’re proposing.\nWhat: This is the meat of the pitch. Describe your idea in one sentence.\nWhy: This is all about the value proposition. What are you adding, what are users getting they wouldn’t get otherwise? How will “what you’re pitching” achieve the value proposition. What is the reason for you to be offering them this value proposition. What are your reasons? Why should others care?"
  },
  {
    "objectID": "amli/07-capstone.html#create-your-pitch",
    "href": "amli/07-capstone.html#create-your-pitch",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Create Your Pitch",
    "text": "Create Your Pitch\n\nCreating a Pitch\nYou will have 15 minutes and a piece of flip-chart paper to create your two-minute pitch and then share with the class. The who/what/why should be clearly stated on each paper, including your name. Feel free to use text but also drawings and visual representations if you’d like."
  },
  {
    "objectID": "amli/07-capstone.html#pitch-1",
    "href": "amli/07-capstone.html#pitch-1",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Pitch",
    "text": "Pitch\nTime to pitch!\n\n*Each pitch is two minutes. Assign a person to keep time. Assign someone to put the posters up after each presentation. The posters should be clustered by themes, so try to put all posters of healthcare related ideas on one wall or corner and all posters with education related ideas on another, etc."
  },
  {
    "objectID": "amli/07-capstone.html#finalizing-project-ideas",
    "href": "amli/07-capstone.html#finalizing-project-ideas",
    "title": "ECE Machine Learning Capstone Project",
    "section": "Finalizing Project Ideas",
    "text": "Finalizing Project Ideas\n\nEveryone take out a piece of paper and rank order the top five ideas that you’re interested in.\nAfter every student has ranked their top five ideas, the instructional team will collect the votes. It is then up to the instructional team how to choose the final project topics and form the groups. Many of the pitches may have been for similar projects and it may be easy to form groups that way. The instructional team may also consider running a simple optimization scheme using the students’ rankings to maximize overall happiness of the class. This is left to the discretion of the instructor, as they know the projects on the table, the students in the room, and the class dynamics."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#the-biological-inspiration-visual-cortex",
    "href": "amli/05_00-02-deeplearning.html#the-biological-inspiration-visual-cortex",
    "title": "Machine Learning",
    "section": "The Biological Inspiration: Visual Cortex",
    "text": "The Biological Inspiration: Visual Cortex\n\n\nIn the 1960s, research by David Hubel and Torsten Wiesel revealed how the visual cortex processes visual information.\n\nNeurons respond to specific regions of the visual field.\n\nEach neuron has a “receptive field.”\n\nSpatially close neurons have similar, overlapping receptive fields.\n\n\n\n\n\nLike neural networks, convolutional neural networks were inspired by biology.\nIn the 1960s, David Hubel and Torsten Wiesel showed that the visual cortex in cats and monkeys contain neurons that fire individually in response to small regions in the field of view."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#receptive-fields-and-image-formation",
    "href": "amli/05_00-02-deeplearning.html#receptive-fields-and-image-formation",
    "title": "Machine Learning",
    "section": "Receptive Fields and Image Formation",
    "text": "Receptive Fields and Image Formation\n\n\nOur visual system integrates information from these small receptive fields.\nThis process forms the complete images we perceive.\nThis biological mechanism provided a key inspiration for Convolutional Neural Networks (CNNs).\n\n\n\nFor a given neuron, the visual space that affects whether or not that neuron will fire is known as its “receptive field.”\nNeurons that are spatially close together often have similar and overlapping receptive fields.\nOur eyes and brains then take the information from each of these small receptive fields and meld them together into the images that we see."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#introduction-to-convolutional-neural-networks-cnns",
    "href": "amli/05_00-02-deeplearning.html#introduction-to-convolutional-neural-networks-cnns",
    "title": "Machine Learning",
    "section": "Introduction to Convolutional Neural Networks (CNNs)",
    "text": "Introduction to Convolutional Neural Networks (CNNs)\n\n\nInspired by the visual cortex, CNNs emerged in the 1980s.\nCNNs are specialized neural networks for processing structured grid data, like images.\nThey incorporate unique layer types:\n\nConvolutional Layers\n\nDownsampling Layers\n\nPooling Layers\n\n\n\n\n\nIn the 1980s researchers were inspired by the visual cortex and used these ideas to create convolutional neural networks.\nA convolutional neural network is simply a neural network with additional (or different) types of layers. There are convolutional layers, downsampling layers, and pooling layers."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#cnn-architecture-flexibility",
    "href": "amli/05_00-02-deeplearning.html#cnn-architecture-flexibility",
    "title": "Machine Learning",
    "section": "CNN Architecture Flexibility",
    "text": "CNN Architecture Flexibility\n\n\nThe power of CNNs lies in their flexible architecture.\nDifferent combinations and ordering of layers lead to varied results.\nThis allows for optimization based on the specific task.\n\n\n\nYou can stack different numbers of these layers in various orders to achieve different results during training."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#recall-the-perceptron",
    "href": "amli/05_00-02-deeplearning.html#recall-the-perceptron",
    "title": "Machine Learning",
    "section": "Recall: The Perceptron",
    "text": "Recall: The Perceptron\nThe simplest building block of a typical neural network.\nA basic computational unit that processes inputs to produce an output.\n\n\nRecall the simplest building block for a typical neural network: the perceptron."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#issues-with-multi-layer-perceptrons-plain-anns-for-images",
    "href": "amli/05_00-02-deeplearning.html#issues-with-multi-layer-perceptrons-plain-anns-for-images",
    "title": "Machine Learning",
    "section": "Issues with Multi-Layer Perceptrons (Plain ANNs) for Images",
    "text": "Issues with Multi-Layer Perceptrons (Plain ANNs) for Images\n\n\nSensitivity to Input Changes:\n\nSmall shifts in image data can drastically alter learned parameters.\n\nE.g., a cat’s position in an image should not change its recognition.\n\n\n\n\n\nIf we are dealing with image data, small and often insignificant changes to the training data can yield large and often incorrect changes to the learned parameters in the model.\nFor example, consider a problem where you want to identify a cat in an image. If the cat is translated to a different part of the image, then the model will adjust different weights to recognize the cat. But the cat being on the left or right of an image isn’t really a defining feature of a cat, right? We’d prefer to recognize things like ears, fur, etc.\nCNNs help us solve this problem."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#cnns-a-solution-for-image-data",
    "href": "amli/05_00-02-deeplearning.html#cnns-a-solution-for-image-data",
    "title": "Machine Learning",
    "section": "CNNs: A Solution for Image Data",
    "text": "CNNs: A Solution for Image Data\nCNNs introduce specialized layers to address the limitations of plain ANNs for image processing.\nThe processed output then feeds into a fully connected neural network.\n\n\nIn a convolutional neural network, we first feed our data into convolutional, downsampling, and pooling layers. The results are then fed into a fully connected neural network like we have seen before."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#convolution-analyzing-pixel-influence",
    "href": "amli/05_00-02-deeplearning.html#convolution-analyzing-pixel-influence",
    "title": "Machine Learning",
    "section": "Convolution: Analyzing Pixel Influence",
    "text": "Convolution: Analyzing Pixel Influence\nA fundamental operation in CNNs.\nIt uses a filter (also called a kernel, mask, or convolution matrix) to analyze the influence of nearby pixels."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#convolution-example-input-image",
    "href": "amli/05_00-02-deeplearning.html#convolution-example-input-image",
    "title": "Machine Learning",
    "section": "Convolution Example: Input Image",
    "text": "Convolution Example: Input Image\n\n\nConsider a simple image: a rectangle with two shaded halves.\nPixel intensities are represented numerically.\n\n\n\nLet’s look at a simple example. Imagine we have the image on the left. It’s just a rectangle with two halves shaded different colors.\nThe intensity of each pixel is recorded on the right. This is how we typically work with image data."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#convolution-example-blurring-filter",
    "href": "amli/05_00-02-deeplearning.html#convolution-example-blurring-filter",
    "title": "Machine Learning",
    "section": "Convolution Example: Blurring Filter",
    "text": "Convolution Example: Blurring Filter\n\n\nWe apply a 3x3 filter designed to create a blurring effect.\nEach element in the filter has a specific weight.\n\n\n\nWe’ll apply this 3x3 filter to the image.\nIt’s a filter that adds a blurring effect."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#convolution-example-filter-application",
    "href": "amli/05_00-02-deeplearning.html#convolution-example-filter-application",
    "title": "Machine Learning",
    "section": "Convolution Example: Filter Application",
    "text": "Convolution Example: Filter Application\nThe filter is applied by sliding it over the image.\nAt each position, it computes a new pixel value.\n\n\nWe’ll apply this 3x3 filter to the image.\nIt’s a filter that adds a blurring effect."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#convolution-example-calculating-a-new-pixel-value",
    "href": "amli/05_00-02-deeplearning.html#convolution-example-calculating-a-new-pixel-value",
    "title": "Machine Learning",
    "section": "Convolution Example: Calculating a New Pixel Value",
    "text": "Convolution Example: Calculating a New Pixel Value\n\n\nTo calculate the new value for a pixel:\n\nCenter the filter on the target pixel.\n\nMultiply corresponding values from the filter and the image.\n\nSum the products.\n\n\n\n\n\nLet’s use the 3x3 filter to calculate the new value for this pixel."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#convolution-example-result-of-blurring",
    "href": "amli/05_00-02-deeplearning.html#convolution-example-result-of-blurring",
    "title": "Machine Learning",
    "section": "Convolution Example: Result of Blurring",
    "text": "Convolution Example: Result of Blurring\n\n\nThe new pixel value is an average of its neighbors.\nThis averaging effect reduces sharp intensity changes, leading to blurring.\n\n\n\nFirst we think of centering the filter on the pixel. Then we multiply the values in the filter by the values in the image. And finally, we add up the result.\nAs you can see, the new pixel value is slightly lower than 100, but it’s higher than 50. So the intensity is getting muted a little. This is because our filter is averaging the intensity of all the pixels around the center point. That is why this filter results in a blurring effect."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#handling-edges-padding",
    "href": "amli/05_00-02-deeplearning.html#handling-edges-padding",
    "title": "Machine Learning",
    "section": "Handling Edges: Padding",
    "text": "Handling Edges: Padding\n\n\nWhen the filter reaches the image edge, padding is often used.\nCommonly, the original image is padded with zeros around its borders.\n\n\n\nYou may be wondering what happens if we’re at the edge. There are different ways to handle this. But it’s common to pad the original image with 0’s around the edges. That way, those values drop out in the average."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#edge-handling-relevant-filter-area",
    "href": "amli/05_00-02-deeplearning.html#edge-handling-relevant-filter-area",
    "title": "Machine Learning",
    "section": "Edge Handling: Relevant Filter Area",
    "text": "Edge Handling: Relevant Filter Area\nThe padding ensures the filter can be centered on edge pixels.\nOnly the part of the filter overlapping the original image contributes to the sum.\n\n\nHere you can see that we only used the part of the filter that is relevant to the image."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#interactive-convolution-demo",
    "href": "amli/05_00-02-deeplearning.html#interactive-convolution-demo",
    "title": "Machine Learning",
    "section": "Interactive Convolution Demo",
    "text": "Interactive Convolution Demo\nExplore how different filter values can affect a simple 3x3 image.\nAdjust the filter weights and observe the output.\n\nviewof p1 = Inputs.range([0, 1], {value: 0.11, step: 0.01, label: \"Top-Left Weight\"});\nviewof p2 = Inputs.range([0, 1], {value: 0.11, step: 0.01, label: \"Top-Middle Weight\"});\nviewof p3 = Inputs.range([0, 1], {value: 0.11, step: 0.01, label: \"Top-Right Weight\"});\nviewof p4 = Inputs.range([0, 1], {value: 0.11, step: 0.01, label: \"Mid-Left Weight\"});\nviewof p5 = Inputs.range([0, 1], {value: 0.11, step: 0.01, label: \"Center Weight\"});\nviewof p6 = Inputs.range([0, 1], {value: 0.11, step: 0.01, label: \"Mid-Right Weight\"});\nviewof p7 = Inputs.range([0, 1], {value: 0.11, step: 0.01, label: \"Bottom-Left Weight\"});\nviewof p8 = Inputs.range([0, 1], {value: 0.11, step: 0.01, label: \"Bottom-Middle Weight\"});\nviewof p9 = Inputs.range([0, 1], {value: 0.11, step: 0.01, label: \"Bottom-Right Weight\"});"
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#line-detectors-gx-and-gy-kernels",
    "href": "amli/05_00-02-deeplearning.html#line-detectors-gx-and-gy-kernels",
    "title": "Machine Learning",
    "section": "Line Detectors: Gx and Gy Kernels",
    "text": "Line Detectors: Gx and Gy Kernels\n\n\nThese kernels are designed to detect sharp intensity changes, indicating lines or edges.\n\n\\(G_x\\): Detects vertical lines.\n\n\\(G_y\\): Detects horizontal lines.\n\n\n\n\n\nHere are two very common kernels that can be used to detect lines in an image.\nOverall the goal is to detect sharp changes in intensity. Let’s see how this works by doing an example with G\\(_{x}\\)."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#line-detector-example-input-image",
    "href": "amli/05_00-02-deeplearning.html#line-detector-example-input-image",
    "title": "Machine Learning",
    "section": "Line Detector Example: Input Image",
    "text": "Line Detector Example: Input Image\n\n\nAn image with a vertical line where shading changes.\nWe will apply the \\(G_x\\) kernel to detect this line.\n\n\n\nOn the left we have an image that is similar to the previous example. There is a line down the center, where the shading changes color. Let’s see if the kernel G\\(_{x}\\) can detect this line.\nCalculate the pixel on the right."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#line-detector-example-first-pixel",
    "href": "amli/05_00-02-deeplearning.html#line-detector-example-first-pixel",
    "title": "Machine Learning",
    "section": "Line Detector Example: First Pixel",
    "text": "Line Detector Example: First Pixel\n\n\nApplying \\(G_x\\) to the first 3x3 block yields 0.\nNo intensity change within this block.\n\n\n\nWe get 0. There are no changes in intensity in the 3x3 block that is highlighted in the original image."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#line-detector-example-shifting-right",
    "href": "amli/05_00-02-deeplearning.html#line-detector-example-shifting-right",
    "title": "Machine Learning",
    "section": "Line Detector Example: Shifting Right",
    "text": "Line Detector Example: Shifting Right\n\n\nShifting the \\(G_x\\) kernel one pixel to the right.\nThe kernel now straddles the intensity change.\n\n\n\nNow let’s move one pixel to the right."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#line-detector-example-detecting-the-edge",
    "href": "amli/05_00-02-deeplearning.html#line-detector-example-detecting-the-edge",
    "title": "Machine Learning",
    "section": "Line Detector Example: Detecting the Edge",
    "text": "Line Detector Example: Detecting the Edge\n\n\nThe calculation results in a non-zero value (\\(200/9\\)).\nThis indicates the presence of a vertical edge.\n\n\n\nWe get 200/9."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#line-detector-example-further-shift",
    "href": "amli/05_00-02-deeplearning.html#line-detector-example-further-shift",
    "title": "Machine Learning",
    "section": "Line Detector Example: Further Shift",
    "text": "Line Detector Example: Further Shift\n\n\nShifting the \\(G_x\\) kernel one more pixel to the right.\nThe kernel is now fully on the right side of the edge.\n\n\n\nAgain move one pixel to the right."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#line-detector-example-another-non-zero",
    "href": "amli/05_00-02-deeplearning.html#line-detector-example-another-non-zero",
    "title": "Machine Learning",
    "section": "Line Detector Example: Another Non-Zero",
    "text": "Line Detector Example: Another Non-Zero\n\n\nThe calculation results in \\(300/9\\).\nThis continues to highlight the edge region.\n\n\n\nWe get 300/9."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#line-detector-example-past-the-edge",
    "href": "amli/05_00-02-deeplearning.html#line-detector-example-past-the-edge",
    "title": "Machine Learning",
    "section": "Line Detector Example: Past the Edge",
    "text": "Line Detector Example: Past the Edge\n\n\nShifting the \\(G_x\\) kernel one final pixel to the right.\nThe kernel is now past the intensity change.\n\n\n\nFinally, let’s move one more pixel to the right."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#line-detector-example-back-to-zero",
    "href": "amli/05_00-02-deeplearning.html#line-detector-example-back-to-zero",
    "title": "Machine Learning",
    "section": "Line Detector Example: Back to Zero",
    "text": "Line Detector Example: Back to Zero\n\n\nThe calculation returns 0 again.\nThis demonstrates that \\(G_x\\) effectively detects vertical lines by identifying sharp horizontal intensity transitions.\n\n\n\nAnd again we get 0.\nThus, we see that a vertical line was detected when the intensity changed in the original image."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#cnns-learn-features",
    "href": "amli/05_00-02-deeplearning.html#cnns-learn-features",
    "title": "Machine Learning",
    "section": "CNNs Learn Features",
    "text": "CNNs Learn Features\n\n\nIn CNNs, the kernel values are learned during training.\nThe network automatically identifies important features.\nWe don’t explicitly tell the model what to look for (e.g., “vertical lines”).\n\n\n\nThis type of convolution happens in the convolutional layers of a neural network. The values in the kernels are parameters that will be learned during training. Thus, the specific features in the images that the kernels are testing for is something that the model “learns.” In other words, you don’t say “Hey model, test for vertical lines.” Instead, the model identifies the features that are important to test for."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#pooling",
    "href": "amli/05_00-02-deeplearning.html#pooling",
    "title": "Machine Learning",
    "section": "Pooling",
    "text": "Pooling\nA downsampling technique often applied after convolution.\nObjective: Reduce data size without losing critical information.\n\nWindow Size: Select a region (e.g., 2x2 or 3x3).\n\nStride: Define movement step (e.g., 2 pixels).\n\nWindow Movement: Slide the window across filtered images.\n\nValue Selection: Take the maximum (Max Pooling) or average (Average Pooling) value within each window.\n\n\nPooling is a type of downsampling that often occurs after convolution. The goal is, without losing much information, to reduce the size of the training data before it goes into the fully connected network."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#hyperparameters-in-cnns",
    "href": "amli/05_00-02-deeplearning.html#hyperparameters-in-cnns",
    "title": "Machine Learning",
    "section": "Hyperparameters in CNNs",
    "text": "Hyperparameters in CNNs\nWhile CNNs learn many parameters, users define several key hyperparameters:\nConvolution:\n- Number of filters (features)\n- Size of filters\nPooling:\n- Window size\n- Stride\nFully Connected Layers:\n- Number of nodes\nAlso, the number and order of each layer type.\n\nWhile a convolutional neural network learns MANY parameters, there are also several hyperparameters that are chosen by the user. Here are the main ones. But as with our previous neural networks, the user can also choose the optimizer, activation function, etc."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#your-turn-cnns-in-practice",
    "href": "amli/05_00-02-deeplearning.html#your-turn-cnns-in-practice",
    "title": "Machine Learning",
    "section": "Your Turn: CNNs in Practice",
    "text": "Your Turn: CNNs in Practice\nIn the lab, you will build and experiment with a Convolutional Neural Network.\nYou’ll apply these concepts to a practical image classification task.\n\nNow it’s your turn to build a CNN in the lab."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#beyond-feedforward-introducing-rnns",
    "href": "amli/05_00-02-deeplearning.html#beyond-feedforward-introducing-rnns",
    "title": "Machine Learning",
    "section": "Beyond Feedforward: Introducing RNNs",
    "text": "Beyond Feedforward: Introducing RNNs\n\n\nPrevious deep neural networks were primarily feedforward.\n\nData flows in one direction.\n\nWeights adjusted via backpropagation.\n\nRecurrent Neural Networks (RNNs) introduce a new dynamic.\n\nNot strictly feedforward.\n\nDesigned for sequential data.\n\n\n\n\n\n\n\n\n\nNote\n\n\nRNNs excel in tasks where the order of data points is crucial, such as time series or natural language.\n\n\n\n\n\nWe have encountered numerous deep neural networks throughout this course. In previous tracks we’ve covered, training data flowed through the network (feedforward), and then adjustments were made to the weights in the network from the last layer through the first (backpropagation).\nJust to name a few, we’ve used dense layers, dropout layers, convolutional layers, and pooling layers. In this unit we will learn about recurrent neural networks, which are not strictly feedforward networks."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#the-feedforward-neuron",
    "href": "amli/05_00-02-deeplearning.html#the-feedforward-neuron",
    "title": "Machine Learning",
    "section": "The Feedforward Neuron",
    "text": "The Feedforward Neuron\n\n\n\nReceives inputs from the previous layer.\n\nMultiplies inputs by weights, adds bias.\n\nPasses sum through an activation function.\n\nOutput goes to the next layer.\n\n\n\n\n\nHere we see a typical feedforward neuron. Depending on the size of the layers before and after, one or more weights feed into the neuron. These are multiplied by the bias, summed, and then passed through an activation function. The resultant value is then passed to the nodes in the next layer of the network."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#the-recurrent-neuron",
    "href": "amli/05_00-02-deeplearning.html#the-recurrent-neuron",
    "title": "Machine Learning",
    "section": "The Recurrent Neuron",
    "text": "The Recurrent Neuron\n\n\nA feedforward neuron with a crucial addition:\n\nIts output feeds back into its own inputs.\n\nThis creates a “memory” over time.\n\nAllows processing of sequential data.\n\n\n\n\n\nHere is a recurrent neuron. You can see that the recurrent neuron looks a lot like a feedforward neuron, except that it also feeds its output back into its inputs.\nImagine we have a fully connected layer that is 10 nodes wide before this neuron. In a typical feedforward, fully-connected network, we would expect 10 inputs, one for each of the nodes in the previous layer. In this case we’ll actually have 11 inputs: one for each of the nodes in the previous layer and the output of the node itself.\nWhat does this do? This gives the neuron memory over time. It allows you to pass a series of data points into the network over time."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#unrolling-a-recurrent-neuron-over-time",
    "href": "amli/05_00-02-deeplearning.html#unrolling-a-recurrent-neuron-over-time",
    "title": "Machine Learning",
    "section": "Unrolling a Recurrent Neuron Over Time",
    "text": "Unrolling a Recurrent Neuron Over Time\n\n\nVisualizing the data flow:\n\nStarts with a seeded input (often zero).\n\nAt each time step, it processes current input and its own previous output.\n\nPasses data to the next layer and forward in time to itself.\n\n\n\n\n\nThis is what it would look like to “unroll” the flow of data through a recurrent neuron over time. You can see that it typically starts out with a seeded input value of zero for its backfeed. At each point in the series, the neuron both passes data to the next layer of neurons and passes data forward in time to itself the next time it fires.\nAlso note that we’re looking at a single neuron in a layer with one input and one output. In reality you’ll have wide layers, so imagine multiple recurrent nodes side by side, each with multiple inputs and outputs."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#the-long-short-term-memory-lstm-neuron",
    "href": "amli/05_00-02-deeplearning.html#the-long-short-term-memory-lstm-neuron",
    "title": "Machine Learning",
    "section": "The Long Short-Term Memory (LSTM) Neuron",
    "text": "The Long Short-Term Memory (LSTM) Neuron\n\n\nAddresses the “short memory” problem of standard RNNs.\n\nPasses two weights back to itself:\n\nLong-term memory\n\nShort-term memory\n\n\nUses “gates” (forget, input, output) to control information flow.\n\n\n\n\n\nWith a typical recurrent neural network, the network tends to have a very short memory. As the sequences passing through the network get longer, the network forgets what it first saw. There have been a few strategies to get around this, one of which is the “long short term memory” (LSTM) neuron.\nOn this slide you can see a very simplified LSTM cell. If you look at the horizontal center, you can see the standard neuron: X-in, y-out. However, instead of having a single feedback like a standard recurrent neuron, this neuron passes two weights back to itself. One represents the long-term member, and the other represents the short-term member.\nYou can see that the short-term state gets mixed with the weights in a set of activation functions labelled A1 through A4. The outputs of these functions, as well as the long-term state, then get passed through a series of gates that ultimately lead to the output of a new y, c, and h value.\nThe numbered gates in order are:\n1. The forget gate\n2. The input gate\n3. Addition of the forget and input gate\n4. The output gate\nLSTM cells are often higher-performing than standard recurrent cells. They also often train faster than standard recurrent cells."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#other-recurrent-neuron-types",
    "href": "amli/05_00-02-deeplearning.html#other-recurrent-neuron-types",
    "title": "Machine Learning",
    "section": "Other Recurrent Neuron Types",
    "text": "Other Recurrent Neuron Types\n\nGated Recurrent Unit (GRU) Neuron:\n\nSimpler than LSTM, with a single feedback channel managing both short- and long-term state.\n\nOften performs comparably to LSTMs with fewer parameters.\n\n\nConvolutional Neurons:\n\nCan also be adapted for sequence tasks.\n\nEffective in identifying local patterns within sequences.\n\n\n\nThe LSTM cell is pretty complex. There is an alternative called the “Gated Recurrent Unit” (GRU) neuron. The GRU has a single feedback channel that manages both short- and long-term state.\nAnother type of neuron that performs very well for sequence tasks alongside, or in place of, LSTM and GRU neurons are convolutional neurons. We’ll see a convolutional neuron in action in our lab."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#what-are-rnns-good-for",
    "href": "amli/05_00-02-deeplearning.html#what-are-rnns-good-for",
    "title": "Machine Learning",
    "section": "What Are RNNs Good For?",
    "text": "What Are RNNs Good For?\nRNNs excel in tasks involving sequential data:\n\nLanguage Translation\n\nSequence Prediction (e.g., stock prices, weather)\n\nSequence Generation (e.g., music, text)\n\nTagging (e.g., video annotation)\n\nSummarization (e.g., text summarization)\n\n… and many more!\n\nRecurrent neural networks are useful for solving a variety of problems. They are commonly used on problems where there is a sequence of data that needs to be processed. For example, they can convert from one language to another. They are also useful in sequence prediction. For example, you might use an RNN to try to predict stock prices or temperatures over time. Since they also work so well with sequences, they can be used to generate sequences such as musical notes or strokes on a canvas. They can also process data such as video and “tag” those videos. Another application is summarization. A RNN can ingest a large amount of text and create a summary about that text.\nThere are many many more applications of RNNs."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#sequence-prediction",
    "href": "amli/05_00-02-deeplearning.html#sequence-prediction",
    "title": "Machine Learning",
    "section": "Sequence Prediction",
    "text": "Sequence Prediction\n\n\nRNNs are particularly strong in sequence prediction.\nUnlike earlier models, RNNs inherently consider the temporal dependency of data.\nPrevious models often assumed time-independent data.\n\n\n\n\n\n\n\nImportant\n\n\nFor example, predicting future sensor readings from past measurements in an ECE system.\n\n\n\n\n\nOne application that RNNs are particularly good at is sequence prediction. Our lab will actually be a sequence prediction lab.\nSo far in this course, all of our predictions have assumed the data are time-independent. That is, we could shuffle around all the data points and prediction points in time, and nothing would change.\nFor example, consider the model we used earlier to predict height from shoe size. The dataset we used was all the students in the class. Now, if we shuffle around all those students, and take the data points in a different order, the model doesn’t change."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#time-series-data",
    "href": "amli/05_00-02-deeplearning.html#time-series-data",
    "title": "Machine Learning",
    "section": "Time Series Data",
    "text": "Time Series Data\n\n\nTime series data is an ordered set of data points indexed by time.\nThe inherent ordering makes it ideal for RNNs.\n\n\n\nIn this unit, we will look at sequence prediction. In sequence prediction the input data is an ordered set of data, most commonly a time series. A time series is a set of data where the index is a date. Since dates have an inherent ordering, time series are ordered data."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#what-are-we-predicting",
    "href": "amli/05_00-02-deeplearning.html#what-are-we-predicting",
    "title": "Machine Learning",
    "section": "What Are We Predicting?",
    "text": "What Are We Predicting?\n\n\nSequence prediction aims to forecast future values based on historical data.\nExample: Predicting the next quarter’s performance from a year of data.\n\n\n\nSequence prediction is about predicting what happens next in a sequence. For example, if we have a year’s worth of data, we may want to know what happens in Q1 of the next year."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#rnns-for-sequence-prediction-a-new-tool",
    "href": "amli/05_00-02-deeplearning.html#rnns-for-sequence-prediction-a-new-tool",
    "title": "Machine Learning",
    "section": "RNNs for Sequence Prediction: A New Tool",
    "text": "RNNs for Sequence Prediction: A New Tool\n\nTraditional Approach: Statistical methods (e.g., ARIMA, Markov chains)\n\nOften involve numerous assumptions.\n\n\nMachine Learning & RNNs: A largely non-parametric approach.\n\nData “speaks for itself,” fewer assumptions.\n\nRequires more data for optimal performance.\n\n\n\nThe standard approach to sequence prediction for several years was a statistical one (like Markov chains or ARIMA).\nThere may not be a need to go into detail, but you could mention Markov Chains or ARIMA time series forecasting. Suffice to say, these approaches often require a lot of assumptions, such as a transition matrix of probabilities, or a normal distribution of noise.\nRNNs allow the data to “speak for themselves.” Using an RNN is a largely non-parametric approach. The downside is that RNNs usually need more data to make good predictions."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#examples-of-sequence-prediction",
    "href": "amli/05_00-02-deeplearning.html#examples-of-sequence-prediction",
    "title": "Machine Learning",
    "section": "Examples of Sequence Prediction",
    "text": "Examples of Sequence Prediction\n\nWalk students through the following examples. These are just a few examples. There are many more, so feel free to elaborate on these and/or add others."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#example-stock-price-prediction",
    "href": "amli/05_00-02-deeplearning.html#example-stock-price-prediction",
    "title": "Machine Learning",
    "section": "Example: Stock Price Prediction",
    "text": "Example: Stock Price Prediction\n\n\nPredicting future stock prices based on historical market data.\nA complex task due to market volatility, but RNNs can capture trends.\n\n\n\nOne of the most common examples of sequence prediction is predicting stock prices. Stock prices are notoriously volatile, but a lot of people are involved in the practice of trying to predict them. There are entire industries based on this practice."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#example-weather-forecasting",
    "href": "amli/05_00-02-deeplearning.html#example-weather-forecasting",
    "title": "Machine Learning",
    "section": "Example: Weather Forecasting",
    "text": "Example: Weather Forecasting\n\n\nForecasting weather patterns based on past meteorological data.\nWhile complex, RNNs can identify subtle temporal dependencies.\n\n\n\nPredicting the weather based on previous days of weather could also be an important application of sequence prediction. While most meteorological systems use a parametric approach based on input data (such as air pressure, cloud cover, etc.), a sequence prediction model can go surprisingly far."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#example-predicting-passenger-traffic",
    "href": "amli/05_00-02-deeplearning.html#example-predicting-passenger-traffic",
    "title": "Machine Learning",
    "section": "Example: Predicting Passenger Traffic",
    "text": "Example: Predicting Passenger Traffic\n\n\nPredicting daily traveler numbers at a train station.\nRNNs can learn seasonality (weekdays vs. weekends, holidays).\nRequires sufficient historical data to capture these patterns.\n\n\n\nYou may want to predict the number of travelers at a train station on a given day, given the previous data of how many travelers were there each day. RNNs pick up on things like varieties of seasonality (e.g., weekday vs weekend, holiday season) and noise.\nHowever, especially for time series with seasonality, we need to have enough data. For example, if we only have data for October and November, we won’t do very well at predicting December because it is a holiday month; we would ideally have data for December of the previous year (if not multiple years)."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#your-turn-rnns-for-vibration-prediction",
    "href": "amli/05_00-02-deeplearning.html#your-turn-rnns-for-vibration-prediction",
    "title": "Machine Learning",
    "section": "Your Turn: RNNs for Vibration Prediction",
    "text": "Your Turn: RNNs for Vibration Prediction\nIn the lab, you will use an RNN to predict a sequence of vibration readings from an engine.\nYou’ll apply TensorFlow with Keras to build, test, and tune your model.\n\nIn the lab we’ll use a recurrent neural network to predict a sequence of vibration readings from an engine. We’ll see how to apply TensorFlow with Keras to build, test, and tune your model."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#what-is-natural-language-processing",
    "href": "amli/05_00-02-deeplearning.html#what-is-natural-language-processing",
    "title": "Machine Learning",
    "section": "What is Natural Language Processing?",
    "text": "What is Natural Language Processing?\n\n\nThe interaction between computers and human (natural) language.\nEnables computers to understand, interpret, and generate human language.\n\n\n\nSo what is natural language processing?\nWhat are some applications of NLP in your everyday life? Prompt the group to respond."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#nlp-applications-in-everyday-life",
    "href": "amli/05_00-02-deeplearning.html#nlp-applications-in-everyday-life",
    "title": "Machine Learning",
    "section": "NLP Applications in Everyday Life",
    "text": "NLP Applications in Everyday Life\n\nAutocorrect & Predictive Text\n\nTranslation Services (e.g., Google Translate)\n\nParsing Text (e.g., extracting information)\n\nChatbots & Virtual Assistants\n\nQuestion Answering Systems\n\nSpeech Recognition\n\n… and so much more!\n\nHere are some examples of what is considered natural language processing. You have likely interacted with systems that perform these tasks before.\nThere is some argument regarding whether speech recognition should actually be considered NLP. It is possible to convert sound waves into words without actually understanding what those words are. This is technically “processing” of natural language, but it falls short of “Natural Language Understanding.” However, many speech recognition systems actually attempt to understand the speech in order to correctly predict ambiguous words like “there,” “their,” and “they’re.”"
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#character-vs.-word-level-models",
    "href": "amli/05_00-02-deeplearning.html#character-vs.-word-level-models",
    "title": "Machine Learning",
    "section": "Character vs. Word-Level Models",
    "text": "Character vs. Word-Level Models\n\n\nCharacter-Level Models:\n- Process text one character at a time.\n- Can handle out-of-vocabulary words and typos.\n\n\n\nModels can process text at different levels. For example, you’ll see some language generation models that use a character-by-character approach such as the RNN shown in this slide."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#character-vs.-word-level-models-cont.",
    "href": "amli/05_00-02-deeplearning.html#character-vs.-word-level-models-cont.",
    "title": "Machine Learning",
    "section": "Character vs. Word-Level Models (Cont.)",
    "text": "Character vs. Word-Level Models (Cont.)\n\n\nWord-Level Models:\n- Process text one word at a time.\n- More common for English and similar languages.\n- Often faster to train and perform well.\nWhich is better? Depends on the language and use case.\n\n\n\nHere is a word-based model. It looks structurally like the character-based model except that it works at the word level.\nWhich is better?\nIt depends. For some languages and use cases, the character-based approach works well. In practice, you see more word-based models, especially for English and similar languages. The models typically perform well and are quicker to train than character-based models."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#text-processing-regular-expressions-regex",
    "href": "amli/05_00-02-deeplearning.html#text-processing-regular-expressions-regex",
    "title": "Machine Learning",
    "section": "Text Processing: Regular Expressions (Regex)",
    "text": "Text Processing: Regular Expressions (Regex)\nA powerful tool for pattern matching in strings.\nUsed for extracting, validating, or modifying text.\n\n\n\nregex\nmatches\n\n\n\n\n[wW]ood\nwood, Wood\n\n\nbeg.n\nbegin, begun, beg3n\n\n\no+h\noh, oooooh\n\n\n[^a-zA-Z]\na single non-alpha character\n\n\n\n\nBefore machine learning, NLP problems were usually solved by pattern matching. Even now, these text processing techniques can be very important in processing messy natural language.\nRegular expressions are widely used in text processing. Imagine needing to extract all the email addresses from a block of text or remove prefixes/suffixes from a word. A regex defines a pattern that is used to match certain character combinations, following a set of rules. In this table we show a few examples of pattern matching rules:\n* “.” matches any single character\n* “+” matches 1 or more of the previous character\n* “[^...]” negates the rest of the pattern in the brackets\nRegex rules can be very powerful but also very complex. Many guides exist for effectively using regexes: https://www.rexegg.com/regex-quickstart.html"
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#text-processing-minimum-edit-distance",
    "href": "amli/05_00-02-deeplearning.html#text-processing-minimum-edit-distance",
    "title": "Machine Learning",
    "section": "Text Processing: Minimum Edit Distance",
    "text": "Text Processing: Minimum Edit Distance\nAlso known as Levenshtein distance.\n\nMeasures the minimum number of single-character edits (insertions, deletions, substitutions) required to change one string into another.\n\nCrucial for autocorrect, spell checkers, and evaluating text generation systems.\n\n\n\nAnother important concept for text processing is minimum edit distance, also called Levenshtein distance. This is especially useful for autocorrect tools and evaluating systems that generate language, e.g., translation. There are many open source Python implementations of this metric that you can use."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#feature-extraction-in-nlp",
    "href": "amli/05_00-02-deeplearning.html#feature-extraction-in-nlp",
    "title": "Machine Learning",
    "section": "Feature Extraction in NLP",
    "text": "Feature Extraction in NLP\n\n\nTransforming raw text into informative numerical features.\n\nN-grams: Consider sequences of n words.\n\nCaptures context beyond single words (e.g., “not horrible”).\n\n\nTFIDF (Term Frequency-Inverse Document Frequency):\n\nDetermines word importance in a document.\n\nDiscounts common words like “the” or “and.”\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nN-grams help capture local word order, while TFIDF helps identify unique and important terms.\n\n\n\n\n\nBefore neural networks, the first step in NLP was “feature extraction,” or transforming raw text into informative features. The idea is that just the individual words in a text do not fully capture the meaning of the text.\nOne very common feature extraction technique is n-grams, which consider n-word sequences instead of just individual words. In the original sentence “that movie was not horrible,” the word “horrible” may cause a model to predict very strong negative emotion. But, if we extract bigrams (2-grams), then we would correctly pair “not horrible,” which is a much milder emotion.\nAnother common technique is TFIDF, which calculates how important a word is to a text. This often has the effect of ignoring more common words like “the” and letting the model focus on more unique words in the text."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#language-modeling-bag-of-words",
    "href": "amli/05_00-02-deeplearning.html#language-modeling-bag-of-words",
    "title": "Machine Learning",
    "section": "Language Modeling: Bag-of-Words",
    "text": "Language Modeling: Bag-of-Words\n\n\nSimplest language modeling approach.\nTreats a sentence as an unordered collection of words.\nExample: “I love love loved it!” and “I HATED it :-(”\n- Meaning can often be inferred without word order.\n\n\n\nTo build models for NLP tasks, we must have some notion of how words fit together into sentences and text. Language modeling refers to determining how likely a certain sentence is. The simplest language modeling approach is a bag-of-words: treat a sentence like an unordered collection (set) of words.\nTake an example movie review, “I love love loved it!”, and another, “I HATED it :-(”. As humans, we could deduce which review corresponded to a positive sentiment and which review corresponded to a negative sentiment, even if we looked at these sentences out of order (e.g., “it! I loved love love” and “HATED :-( I it”). So bag-of-words is like saying, “I’m pretty sure I can glean the meaning of sentences, with words in any order, so why bother keeping track of the order? Sounds like more work to me.”\nBut can you think of an example or two where this strategy would fail? Especially consider if you’re trying to predict more than just two sentiments (“good” and “bad”). Prompt class for discussion."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#language-modeling-sequential-words",
    "href": "amli/05_00-02-deeplearning.html#language-modeling-sequential-words",
    "title": "Machine Learning",
    "section": "Language Modeling: Sequential Words",
    "text": "Language Modeling: Sequential Words\n\n\nWhile bag-of-words is effective for some tasks (e.g., spam filtering), word order is crucial for complex NLP.\nSequential approaches preserve word order.\nThis is where Recurrent Neural Networks (RNNs) become indispensable.\n\n\n\nBag-of-words approaches are surprisingly successful on many tasks (email spam filter, sentiment analysis) and are less computationally intensive.\nBut fundamentally we know that the order of words matters. Harder NLP tasks build upon sequential approaches, which preserve the order of words in a text. This is exactly what RNNs are useful for. Recurrent Neural Networks handle this well."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#the-nlp-processing-pipeline",
    "href": "amli/05_00-02-deeplearning.html#the-nlp-processing-pipeline",
    "title": "Machine Learning",
    "section": "The NLP Processing Pipeline",
    "text": "The NLP Processing Pipeline\nUnderstanding the typical flow for NLP tasks:\n\n\n\n\n\ngraph TD\n    A[\"Raw Text\"] --&gt; B{\"Feature Extraction / Embeddings\"};\n    B --&gt; C[\"Machine Learning Model\"];\n    C --&gt; D[\"Supervised Task (e.g., Classification, Generation)\"];\n\n\n\n\n\n\n\nThe typical process for an NLP task is:\n1. Raw text\n2. Transform to feature vectors (either through feature extraction or embeddings)\n3. Run through some model\n4. Perform supervised task"
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#your-turn-nlp-lab",
    "href": "amli/05_00-02-deeplearning.html#your-turn-nlp-lab",
    "title": "Machine Learning",
    "section": "Your Turn: NLP Lab",
    "text": "Your Turn: NLP Lab\nIn the lab, you will perform sentiment analysis on reviews.\nYou will then build a classifier to determine authorship (e.g., Jane Austen vs. Charles Dickens).\n\nIn this lab we will perform sentiment analysis on reviews as an example. After that we’ll write a classifier that determines if a piece of text was written by Jane Austen or Charles Dickens."
  },
  {
    "objectID": "amli/05_00-02-deeplearning.html#conclusion-and-qa",
    "href": "amli/05_00-02-deeplearning.html#conclusion-and-qa",
    "title": "Machine Learning",
    "section": "Conclusion and Q&A",
    "text": "Conclusion and Q&A\nWe’ve explored Convolutional Neural Networks for image processing, Recurrent Neural Networks for sequential data, and key concepts in Natural Language Processing.\nThese deep learning techniques are fundamental to many modern ECE applications.\nWhat questions do you have about these powerful tools and their applications in engineering?"
  },
  {
    "objectID": "amli/05_03-05-deeplearning.html#what-is-an-autoencoder",
    "href": "amli/05_03-05-deeplearning.html#what-is-an-autoencoder",
    "title": "Machine Learning",
    "section": "What Is an Autoencoder?",
    "text": "What Is an Autoencoder?\nA neural network designed to learn an efficient data encoding (compression)\nand then reconstruct the input data from that encoding (decompression).\n\n\n\n\n\n\nTip\n\n\nCore Idea: Learn a compressed, “latent” representation of data without supervision.\n\n\n\n\nFirst things first, what is an autoencoder? It is a model that learns how to encode data and then decode data.\nLet’s look at a visualization."
  },
  {
    "objectID": "amli/05_03-05-deeplearning.html#encoder",
    "href": "amli/05_03-05-deeplearning.html#encoder",
    "title": "Machine Learning",
    "section": "Encoder",
    "text": "Encoder\n\n\nThe encoder is a neural network that transforms input data into a smaller,\nlower-dimensional representation.\n\nGoal: Reduce data dimensionality while retaining essential information.\n\nLayers: Can use dense, convolutional, pooling layers, etc.\n\nOutput: A compressed “latent space” representation.\n\n\n\n\n\n\nEncoding looks a whole lot like the neural networks that we have already seen, and that’s because it is. The encoder is a neural network that starts out with some input data and outputs a smaller form of that data. The encoder can use dense layers, convolutional layers, pooling layers, and more.\nThe goal of the encoder is to take some form of input data and reduce it down to a smaller representation.\nBut there has to be some way to know if this smaller representation is useful. We do that with the decoder."
  },
  {
    "objectID": "amli/05_03-05-deeplearning.html#decoder",
    "href": "amli/05_03-05-deeplearning.html#decoder",
    "title": "Machine Learning",
    "section": "Decoder",
    "text": "Decoder\n\n\nThe decoder performs the reverse operation of the encoder.\n\nInput: The compressed latent representation from the encoder.\n\nGoal: Reconstruct an approximation of the original input data.\n\nMethod: Expands the data back to its original dimensions.\n\n\n\n\n\n\nAs you might expect, the decoder does the opposite of the encoder. The decoder starts with a compressed representation of the data and inflates it back to the original size.\nWe haven’t really seen this before. The networks we have built tend to get narrower as data flows through them. This widening is less common. Sure, we could add wider and wider dense layers in a deep neural network, but it isn’t common to see outside of this context.\nHow do we do this?"
  },
  {
    "objectID": "amli/05_03-05-deeplearning.html#decoder-upsampling",
    "href": "amli/05_03-05-deeplearning.html#decoder-upsampling",
    "title": "Machine Learning",
    "section": "Decoder: Upsampling",
    "text": "Decoder: Upsampling\n\n\nTo expand the data in the decoder, we use upsampling techniques.\n\nConcept: Reverse of pooling; expands input dimensions.\n\nUpSampling2D: Commonly used in TensorFlow Keras for image data.\n\nExample: Doubles spatial dimensions (e.g., 4x4 to 8x8).\n\n\n\ntf.keras.layers.UpSampling2D(\n    size=(2, 2)\n)\nconv2d_3 (Conv2D)            (None, 4, 4, 16)          2320      \n_________________________________________________________________\nup_sampling2d (UpSampling2D) (None, 8, 8, 16)          0      \n\n\nWe use upsampling to add wider dense layers and create the decoder. You can think of upsampling as the reverse of the pooling layers we used in the convolutional neural networks we created for classification. While a pooling layer shrinks its input, the upsampling layer expands its input.\nIn TensorFlow Keras we’ll use the UpSampling2D layer to decode our encoded data.\nIn the example on this slide, you can see a convolutional layer that outputs a 4x4x16 matrix. The upsampling layer doubles the first two dimensions to 8x8x16."
  },
  {
    "objectID": "amli/05_03-05-deeplearning.html#autoencoder",
    "href": "amli/05_03-05-deeplearning.html#autoencoder",
    "title": "Machine Learning",
    "section": "Autoencoder",
    "text": "Autoencoder\n\n\nCombining the encoder and decoder forms an autoencoder.\n\nEncoder: Learns efficient data representation.\n\nDecoder: Reconstructs data from the encoded representation.\n\n“Lossy” Compression: Output is an approximation, not exact replica.\n\n\n\n\n\n\n\nImportant\n\n\nThe autoencoder aims to reconstruct its own input.\n\n\n\n\n\n\n\nWhat do you get when you mix an encoder and a decoder? An autoencoder!\nThe encoder finds an efficient representation for the data. The decoder is able to revive some approximation of the original data from the encoded data.\nThis is “lossy” compression. The output of the model is not typically exactly what was put in, but is hopefully a reasonable approximation."
  },
  {
    "objectID": "amli/05_03-05-deeplearning.html#what-are-autoencoders-good-for",
    "href": "amli/05_03-05-deeplearning.html#what-are-autoencoders-good-for",
    "title": "Machine Learning",
    "section": "What Are Autoencoders Good For?",
    "text": "What Are Autoencoders Good For?\n\nLossy Data Compression: Efficiently reduce data size.\n\nNon-linear Principal Component Analysis (PCA): Discover underlying data structure.\n\nData Denoising/Cleaning: Remove noise or artifacts from data.\n\nFeature Learning: Extract meaningful features for other ML tasks.\n\nAnomaly Detection: Identify data points that deviate from learned patterns.\n\n\nObviously autoencoders are good at lossy data compression. Once trained, the encoder part of the model can be used to compress our input data. The decoder can then later be used to expand that data to a version that is close to the original.\nAnother application is principal component analysis. If you think about what an autoencoder is doing, it is reducing input data down into the minimal amount of information required to then revive that data. It is finding principal components using a neural network. You can train the model and then use the encoder to reduce the dimensionality of your data before feeding it into another model.\nAnother interesting application is data cleaning. Autoencoders can be used to remove noise from data. In our lab we’ll remove static and watermarks from images. Admittedly, there is some data loss, but it is still an interesting application."
  },
  {
    "objectID": "amli/05_03-05-deeplearning.html#keras-model-building-the-encoder",
    "href": "amli/05_03-05-deeplearning.html#keras-model-building-the-encoder",
    "title": "Machine Learning",
    "section": "Keras Model: Building the Encoder",
    "text": "Keras Model: Building the Encoder\nUtilize the Keras Model class for flexible architecture.\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, UpSampling2D, Input\nfrom tensorflow.keras.models import Model\n\n# Define input layer\ninput_layer = Input(shape=(28, 28, 1), name='input_image')\n\n# Encoder path\nconv_layer = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\nlatent_layer = MaxPool2D((2, 2), padding='same')(conv_layer)\n\n# Define the encoder model\nencoder = Model(input_layer, latent_layer, name='encoder')\n\nprint(encoder.summary())\n\n\n\n\n\n\nNote\n\n\nThe latent_layer represents the compressed output of the encoder.\n\n\n\n\nYou could build an autoencoder with a standard Sequential model, but often you’ll want to use the encoder and decoder separately. In order to do this, we can use the Keras Model class.\nIn this example we build an input layer and pass it to a convolutional layer, which is then passed to a pooling layer. The input and output layers are then passed to the Model.\nYou might also notice that we called the output of the encoder the “latent layer.” This is a common term used to identify the intermediate data representation that is output by the encoder and input to the decoder."
  },
  {
    "objectID": "amli/05_03-05-deeplearning.html#keras-model-assembling-the-autoencoder",
    "href": "amli/05_03-05-deeplearning.html#keras-model-assembling-the-autoencoder",
    "title": "Machine Learning",
    "section": "Keras Model: Assembling the Autoencoder",
    "text": "Keras Model: Assembling the Autoencoder\nConnect encoder and decoder using the Keras Functional API.\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, UpSampling2D, Input\nfrom tensorflow.keras.models import Model\n\n# Example placeholder for decoder output (in a real model, this would be built from latent_layer)\n# For this example, let's just make a simple decoder structure\nlatent_input = Input(shape=(14, 14, 32), name='latent_input')\nconv_decoder = Conv2D(32, (3, 3), activation='relu', padding='same')(latent_input)\nup_sample = UpSampling2D((2, 2))(conv_decoder)\noutput_layer = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up_sample) # Assuming 1-channel output like original input\n\n# Define the encoder model (as on previous slide)\ninput_autoencoder = Input(shape=(28, 28, 1), name='input_image_autoencoder')\nencoder_conv = Conv2D(32, (3, 3), activation='relu', padding='same')(input_autoencoder)\nlatent_encoded = MaxPool2D((2, 2), padding='same')(encoder_conv)\nencoder = Model(input_autoencoder, latent_encoded, name='encoder')\n\n# Define the decoder model\ndecoder = Model(latent_input, output_layer, name='decoder')\n\n# Connect them to form the autoencoder\nautoencoder = Model(\n    input_autoencoder,\n    decoder(encoder(input_autoencoder)),\n    name=\"autoencoder\"\n)\n\nprint(autoencoder.summary())\n\n\n\n\n\n\nTip\n\n\nTraining the autoencoder model simultaneously trains both the encoder and decoder sub-models.\n\n\n\n\nTo build an autoencoder, you create an encoder and a decoder. The encoder accepts an input layer and outputs a latent layer. The decoder accepts a latent layer and outputs an output layer.\nThe encoder and decoder are stitched together into a third model, the autoencoder. Notice that the autoencoder accepts the input layer and passes it directly to the encoder. The encoder is the input to the decoder (via the latent layer).\nWhen the autoencoder is trained, the encoder and decoder are also trained and can be used separately."
  },
  {
    "objectID": "amli/05_03-05-deeplearning.html#interactive-convolution-visualization",
    "href": "amli/05_03-05-deeplearning.html#interactive-convolution-visualization",
    "title": "Machine Learning",
    "section": "Interactive Convolution Visualization",
    "text": "Interactive Convolution Visualization\nExplore how a 2D convolution filter processes an input matrix.\nAdjust filter size and visualize the output.\n\n\n\nviewof input_val = Inputs.range([0, 10], {step: 1, value: 5, label: \"Input Matrix Base Value\"});\nviewof filter_size = Inputs.select([\"3x3\", \"5x5\"], {value: \"3x3\", label: \"Filter Size\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nThis visualization shows a single convolution operation with a simple filter.\n\n\n\n\n\n\n\n\n\n\n\n\nHere we visualize a simplified 2D convolution. You can adjust the base value of the input matrix and the size of the filter. The input matrix is a simple gradient. The filter matrix is a basic edge detection or blur kernel. The output feature map shows the result of applying the filter across the input.\nConvolution involves sliding the filter over the input, performing element-wise multiplication, and summing the results for each position. This process helps detect features like edges or textures in the input data."
  },
  {
    "objectID": "amli/05_03-05-deeplearning.html#your-turn",
    "href": "amli/05_03-05-deeplearning.html#your-turn",
    "title": "Machine Learning",
    "section": "Your Turn!",
    "text": "Your Turn!\nNow it is your turn to apply what you’ve learned about autoencoders.\nIn the lab, you will work through examples of:\n\nUsing an autoencoder for image compression.\n\nApplying autoencoders for denoising (e.g., removing static from images).\n\nAn exercise to remove a watermark from a video or image.\n\nWhat specific ECE applications do you envision for autoencoders, beyond those discussed?\n\nNow it is your turn. In this lab we will walk through examples of using an autoencoder for compression and for removing static. Our exercise will be to remove a watermark from a video."
  },
  {
    "objectID": "amli/05_03-05-deeplearning.html#identifying-pneumonia-from-x-rays",
    "href": "amli/05_03-05-deeplearning.html#identifying-pneumonia-from-x-rays",
    "title": "Machine Learning",
    "section": "Identifying Pneumonia from X-rays",
    "text": "Identifying Pneumonia from X-rays\nThis project consolidates your knowledge of image classification.\nYou will apply various techniques learned throughout the course.\n\n\n\n\n\n\nNote\n\n\nFocus on building, evaluating, and tuning models for a real-world medical imaging task.\n\n\n\n\nWe are nearing the end of the classification track. We’ve learned quite a bit. In the last few labs we’ve created binary and multiclass classifiers. We’ve used scikit-learn and TensorFlow to create various models we then evaluated and tuned.\nIn this final project, you’ll get to show off what you’ve learned in one large project."
  },
  {
    "objectID": "amli/05_03-05-deeplearning.html#image-classification-project-chest-x-rays",
    "href": "amli/05_03-05-deeplearning.html#image-classification-project-chest-x-rays",
    "title": "Machine Learning",
    "section": "Image Classification Project: Chest X-rays",
    "text": "Image Classification Project: Chest X-rays\n\n\nYou will work with a dataset of chest X-ray images.\n\nTask: Classify images as either NORMAL or PNEUMONIA.\n\nSource: Dataset from Kaggle, pre-divided for training, testing, and validation.\n\nRelevance: Demonstrates ML application in medical diagnostics.\n\n\n\n\n\n\nIn the lab we’ll download a dataset from Kaggle. The dataset contains images of x-rays of patient lungs. Some of the images are classified as having pneumonia, while others are classified as normal."
  },
  {
    "objectID": "amli/05_03-05-deeplearning.html#review-what-is-convolution",
    "href": "amli/05_03-05-deeplearning.html#review-what-is-convolution",
    "title": "Machine Learning",
    "section": "Review: What is Convolution?",
    "text": "Review: What is Convolution?\n\n\n\nDefinition: A mathematical operation involving two functions to produce a third.\n\nImage Processing: A “filter” (kernel) slides over an input image.\n\nProcess: Element-wise multiplication of filter with image region, then sum.\n\nGoal: Detect specific features (e.g., edges, textures) in the image.\n\nParameters: Filter size, stride, padding.\n\n\n\nflowchart LR\n    subgraph \"Input Image\"\n        A[\"Pixel Grid\"]\n    end\n\n    subgraph \"Filter (Kernel)\"\n        B[\"Small Matrix\"]\n    end\n\n    subgraph \"Operation\"\n        C((\"Slide & Multiply-Add\"))\n    end\n\n    subgraph \"Output\"\n        D[\"Feature Map\"]\n    end\n\n    A -- \"Apply Filter\" --&gt; C\n    B -- \"Over Region\" --&gt; C\n    C --&gt; D\n\n    style A fill:#cef,stroke:#333,stroke-width:2px\n    style B fill:#fec,stroke:#333,stroke-width:2px\n    style C fill:#ccf,stroke:#333,stroke-width:2px\n    style D fill:#cfc,stroke:#333,stroke-width:2px\n\n\n@Exercise(5 minutes) {\nHave students discuss convolution. It is a process of passing a filter (kernel) over an image and computing new pixel values. This process involves multiplying the values in the image by those in the filter and adding them up. You need to know the size of your filter and the stride. The goal is to detect features in the image. Remind students that we saw simple kernels that were line detectors.\n}"
  },
  {
    "objectID": "amli/05_03-05-deeplearning.html#review-convolutional-neural-network-cnn-architecture",
    "href": "amli/05_03-05-deeplearning.html#review-convolutional-neural-network-cnn-architecture",
    "title": "Machine Learning",
    "section": "Review: Convolutional Neural Network (CNN) Architecture",
    "text": "Review: Convolutional Neural Network (CNN) Architecture\n\nKey Components:\n\nConvolutional Layers: Feature extraction using filters.\n\nPooling Layers: Downsampling to reduce dimensionality and computational cost.\n\nActivation Functions: Introduce non-linearity (e.g., ReLU).\n\nFully Connected Layers: Classification based on extracted features.\n\n\nTunable “Knobs”:\n\nNumber of layers, layer order.\n\nFilter size, number of filters, stride.\n\nPooling size, type of pooling.\n\nActivation functions (e.g., ReLU, Sigmoid for output).\n\nDropout rates, learning rate, batch size.\n\n\n\n\n\n\n\n\n\nCNN_Architecture\n\n\n\nInput\n\nInput Image\n(e.g., 256x256x3)\n\n\n\nConv1\n\n\nConv Layer 1\n(Filters, Kernel, Stride)\n\n\n\nInput-&gt;Conv1\n\n\nFeature Extraction\n\n\n\nPool1\n\nPooling Layer 1\n(Max/Avg Pool)\n\n\n\nConv1-&gt;Pool1\n\n\nDownsampling\n\n\n\nConv2\n\nConv Layer 2\n\n\n\nPool1-&gt;Conv2\n\n\n\n\n\nPool2\n\nPooling Layer 2\n\n\n\nConv2-&gt;Pool2\n\n\n\n\n\nFlatten\n\nFlatten Layer\n\n\n\nPool2-&gt;Flatten\n\n\nVectorization\n\n\n\nDense1\n\nDense Layer 1\n(ReLU)\n\n\n\nFlatten-&gt;Dense1\n\n\nClassification\n\n\n\nOutput\n\nOutput Layer\n(Softmax/Sigmoid)\n\n\n\nDense1-&gt;Output\n\n\n\n\n\n\n\n\n\n\n\nExercise(5 minutes) {\nHave students discuss CNNs. In general, there are convolutional layers and pooling layers. Then the information is fed into a typical fully-connected neural network. Furthermore, an important choice the user needs to make is which activation function to use. Since this is a binary classification task, it is useful to use the sigmoid function on the final output layer. Relu works well on the other layers.\n}"
  },
  {
    "objectID": "amli/05_03-05-deeplearning.html#image-classification-project-dataset-structure",
    "href": "amli/05_03-05-deeplearning.html#image-classification-project-dataset-structure",
    "title": "Machine Learning",
    "section": "Image Classification Project: Dataset Structure",
    "text": "Image Classification Project: Dataset Structure\nThe dataset is pre-organized into standard machine learning splits.\nchest_xray/\n     ├── test/\n     │       ├── NORMAL/\n     │       └── PNEUMONIA/\n     ├── train/\n     │       ├── NORMAL/\n     │       └── PNEUMONIA/\n     └── val/\n             ├── NORMAL/\n             └── PNEUMONIA/\n\ntrain set: Used for model training.\n\ntest set: For hyperparameter tuning and model selection.\n\nval set: Final, unbiased evaluation of model generalization.\n\n\nThe images in the dataset are already divided into test, train, and validation sets. The training set is, of course, used for training your model. The testing dataset should be used to adjust model hyperparameters, shape, etc. Once you have found a model that tests well, check it against the validation dataset. That will serve as one final test for the ability for your model to generalize."
  },
  {
    "objectID": "amli/05_03-05-deeplearning.html#image-classification-project-tips-for-success",
    "href": "amli/05_03-05-deeplearning.html#image-classification-project-tips-for-success",
    "title": "Machine Learning",
    "section": "Image Classification Project: Tips for Success",
    "text": "Image Classification Project: Tips for Success\n\nEnable GPU: Significantly speeds up training for image models.\n\nIn Google Colab: Runtime &gt; Change runtime type &gt; Hardware accelerator &gt; GPU.\n\n\nPerform Exploratory Data Analysis (EDA):\n\nVerify dataset integrity and structure.\n\nCheck for image dimensions, class balance, and potential anomalies.\n\n\nData Augmentation: Consider techniques like rotation, flipping, zooming to expand training data.\n\nTransfer Learning: Explore pre-trained models for faster convergence and better performance.\n\nWhat challenges do you anticipate when working with medical image data?\n\nFirst tip: enable GPU in Google Colab. This dataset tends to train significantly faster if you enable GPU in the runtime.\nAlso, perform EDA on your dataset. The dataset may have duplication, undocumented folders, etc."
  },
  {
    "objectID": "amli/05_03-05-deeplearning.html#your-turn-1",
    "href": "amli/05_03-05-deeplearning.html#your-turn-1",
    "title": "Machine Learning",
    "section": "Your Turn!",
    "text": "Your Turn!\nNow, apply your skills to the Image Classification Project.\nGood luck building a robust pneumonia detection model!\nWhat strategies will you prioritize for model evaluation and fine-tuning?\n\nAnd with that, it is your turn to work on the lab."
  },
  {
    "objectID": "amli/05_03-05-deeplearning.html#leveraging-pre-trained-models",
    "href": "amli/05_03-05-deeplearning.html#leveraging-pre-trained-models",
    "title": "Machine Learning",
    "section": "Leveraging Pre-trained Models",
    "text": "Leveraging Pre-trained Models\n\n\nConcept: Reusing a pre-trained model as a starting point for a new task.\n\nTraditional ML: Models trained from scratch on specific datasets.\n\nTransfer Learning: Utilizes knowledge gained from a large, general dataset.\n\nBenefit: Accelerates training, improves performance with limited data.\n\n\n\n\n\n\n\n\n\nImportant\n\n\nWhy train from scratch when you can stand on the shoulders of giants?\n\n\n\n\n\nMost of the models we have trained so far have been trained from scratch. We start with a randomly-weighted model and then use large amounts of data and many, many epochs over that data in order to build a reasonable model.\nBut is that how we learn?"
  },
  {
    "objectID": "amli/05_03-05-deeplearning.html#transferring-knowledge-human-analogy",
    "href": "amli/05_03-05-deeplearning.html#transferring-knowledge-human-analogy",
    "title": "Machine Learning",
    "section": "Transferring Knowledge: Human Analogy",
    "text": "Transferring Knowledge: Human Analogy\n\n\nJust as humans learn by building upon existing knowledge,\nML models can benefit from “transferred” insights.\n\nDirect Learning: Observing examples directly.\n\nKnowledge Transfer: Gaining insights from others’ experiences or related domains.\n\nEfficiency: Speeds up the learning process for new, related tasks.\n\n\n\n\n\n\nWell, yes and no.\nWe do indeed learn in self-guided ways by looking at examples. But we also learn through the transfer of knowledge. Others can provide insights that can be used to accelerate our learning process."
  },
  {
    "objectID": "amli/05_03-05-deeplearning.html#transferring-knowledge-the-zebra-example",
    "href": "amli/05_03-05-deeplearning.html#transferring-knowledge-the-zebra-example",
    "title": "Machine Learning",
    "section": "Transferring Knowledge: The Zebra Example",
    "text": "Transferring Knowledge: The Zebra Example\n\n\nImagine identifying a zebra:\n\nKnowns: Horse shape, tiger stripes, penguin colors.\n\nTransferred Knowledge: Combining these known features accelerates zebra identification.\n\nML Parallel: A model good at general image recognition can quickly adapt to new categories.\n\n\n\n\n\n\nLet’s say we already know what a horse, tiger, and penguin are. If we wanted to learn how to identify a zebra, we could look at pictures of zebras. Or, someone might tell us that a zebra is the shape of a horse, has the coat patterns of a tiger, and the colors of a penguin. This would greatly accelerate our ability to identify zebras, even if we only had a handful of pictures of zebras to study.\nTransfer learning is a similar idea. A model that can already identify some classes of data can be extended to fit a different problem that we are trying to solve. The base model is already good at finding key features. The new model can utilize this ability and perform better and faster than if it was trained from scratch."
  },
  {
    "objectID": "amli/05_03-05-deeplearning.html#transfer-learning-high-level-overview",
    "href": "amli/05_03-05-deeplearning.html#transfer-learning-high-level-overview",
    "title": "Machine Learning",
    "section": "Transfer Learning: High-Level Overview",
    "text": "Transfer Learning: High-Level Overview\n\n\nTypically involves attaching new layers to a pre-trained base model.\n\nPre-trained Model: Base model with learned weights (e.g., ImageNet classifier).\n\nCustomization Model: New, untrained layers added on top.\n\nData Flow: Input goes through pre-trained model, then into new layers.\n\nOutput: The final prediction from the new layers.\n\n\n\n\n\n\nAt a very high level, transfer learning can look a lot like adding an extra few layers to the end of a pre-trained model.\nIn this diagram the pre-trained model is an existing model that has been trained and performs acceptably well. This model has persisted weights that are packaged and loaded with the model.\nThe customizations model is a new set of untrained layers. They have random, or at least naive, initial weights. These weights still need to be learned through training.\nAs you can decipher from the data flow arrow, data typically still enters the model through the pre-trained input layer. However, the output layer of the pre-trained model then feeds the new model. The final output is the output layer of the new model."
  },
  {
    "objectID": "amli/05_03-05-deeplearning.html#do-you-retrain-the-pre-trained-model",
    "href": "amli/05_03-05-deeplearning.html#do-you-retrain-the-pre-trained-model",
    "title": "Machine Learning",
    "section": "Do You Retrain the Pre-Trained Model?",
    "text": "Do You Retrain the Pre-Trained Model?\nThe decision to retrain (fine-tune) the pre-trained model’s weights depends on several factors:\n\n“Freezing” Weights:\n\nUsually “No”: If new data is small or classes don’t largely overlap.\n\nBenefit: Prevents overfitting, faster training of new layers.\n\n\nFine-tuning Weights:\n\nSometimes “Yes”: If new data is large and similar to original training data.\n\nMethod: Unfreeze some or all layers and train with a very small learning rate.\n\n\nLayer-Specific Freezing: Often, layers closer to the input are frozen, while later layers are fine-tuned.\n\n\nThis begs the question: Do you re-train the pre-trained model?\nThe answer is usually “no” but not always.\nIf the data you have to train your new model is similar in size or larger than the data used to train the pre-trained model, and if the classes that they identify largely overlap, then it may be worthwhile. Otherwise it is advisable to “freeze” the pre-trained model and not update the weights.\nThis freezing can be for the whole model or for only a few specific layers. Typically those layers closer to the input layer are frozen."
  },
  {
    "objectID": "amli/05_03-05-deeplearning.html#which-output-layer-to-use",
    "href": "amli/05_03-05-deeplearning.html#which-output-layer-to-use",
    "title": "Machine Learning",
    "section": "Which Output Layer to Use?",
    "text": "Which Output Layer to Use?\n\n\nWhen using a pre-trained model, we typically don’t use its final classification layer.\n\nProblem: Final layers usually flatten data into class-specific vectors.\n\nSolution: Use an intermediate high-dimensional layer as the output from the pre-trained model.\n\nBenefit: Provides rich feature representations for the new custom layers.\n\n\n\n\n\n\nWe also need to think about which layer is actually the output layer from a pre-trained model.\nIn most classification problems, we have multiple layers of high-dimensional matrices. But then at the very end of the model, we flatten the data down to a vector of class-estimates.\nWe don’t want to flatten the data before feeding it to our extended model, so instead we need to use an intermediate high-dimensional layer."
  },
  {
    "objectID": "amli/05_03-05-deeplearning.html#model-terminology-bottom-and-top",
    "href": "amli/05_03-05-deeplearning.html#model-terminology-bottom-and-top",
    "title": "Machine Learning",
    "section": "Model Terminology: “Bottom” and “Top”",
    "text": "Model Terminology: “Bottom” and “Top”\n\n\nUnderstanding these terms helps when configuring pre-trained models.\n\n“Bottom”: Refers to the input layers of the model.\n\n“Top”: Refers to the output layers of the model.\n\nThis convention often comes from how models are diagrammed, with input at the bottom and output at the top.\n\n\n\n\nWe need to introduce a little modelling terminology at this point. You sometimes hear about the “bottom” or “top” of a model. Which end is which?\nMany papers illustrate models with the input layer at the bottom and the output layer at the top of diagrams. For this reason, culture now dictates that the “bottom” of a model is the input, and the “top” of a model is the output."
  },
  {
    "objectID": "amli/05_03-05-deeplearning.html#include_top-a-key-parameter",
    "href": "amli/05_03-05-deeplearning.html#include_top-a-key-parameter",
    "title": "Machine Learning",
    "section": "include_top: A Key Parameter",
    "text": "include_top: A Key Parameter\n\n\nMany pre-trained models (e.g., in Keras) offer an include_top parameter.\n\ninclude_top=True (default): Includes the original classification layers.\n\ninclude_top=False: Excludes the original classification layers.\n\nBenefit: Provides a high-dimensional feature extractor.\n\nUse Case: Ideal for building custom classifiers on top.\n\n\n\n\n\n\n\nThis terminology is important because some models allow you to choose to include the “top” of the model or not. If you leave out the top, then you get a higher-dimensional input for your custom model, which is typically a good thing."
  },
  {
    "objectID": "amli/05_03-05-deeplearning.html#your-turn-2",
    "href": "amli/05_03-05-deeplearning.html#your-turn-2",
    "title": "Machine Learning",
    "section": "Your Turn!",
    "text": "Your Turn!\nNow, let’s put transfer learning into practice.\nYou will use MobileNetV2, a powerful pre-trained model,\nto build a network capable of classifying cats and dogs.\nWhat advantages do you expect from using MobileNetV2 compared to training from scratch for this task?\n\nNow we’ll attempt a little transfer learning of our own. We’ll use MobileNetV2, which can classify 1,000 classes, to build a network that can reliably classify cats and dogs."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "This is Machine Learning Lecture Notes"
  },
  {
    "objectID": "index.html#week-8",
    "href": "index.html#week-8",
    "title": "Machine Learning",
    "section": "Week 8",
    "text": "Week 8\n\nAMLI 3.1: Regression"
  },
  {
    "objectID": "index.html#week-9",
    "href": "index.html#week-9",
    "title": "Machine Learning",
    "section": "Week 9",
    "text": "Week 9\n\nAMLI 3.1: Regression"
  },
  {
    "objectID": "index.html#week-10",
    "href": "index.html#week-10",
    "title": "Machine Learning",
    "section": "Week 10",
    "text": "Week 10\n\nAMLI 3.2: Regression"
  },
  {
    "objectID": "index.html#week-11",
    "href": "index.html#week-11",
    "title": "Machine Learning",
    "section": "Week 11",
    "text": "Week 11\n\nAMLI 4.1: Classification\nAMLI 4.2: Classification\nGoogle-1: Tensor Flow Keras Deep Learning 1"
  },
  {
    "objectID": "index.html#google",
    "href": "index.html#google",
    "title": "Machine Learning",
    "section": "Google",
    "text": "Google\n\nMachine Learning: Tensor Flow Keras Deep Learning 1\nMachine Learning: Tensor Flow Keras Deep Learning 2"
  },
  {
    "objectID": "index.html#amli",
    "href": "index.html#amli",
    "title": "Machine Learning",
    "section": "AMLI",
    "text": "AMLI\n\nMachine Learning 3.1: Regression\nMachine Learning 3.2: Regression\nMachine Learning 4.1: Classification\nMachine Learning 4.2: Classification\nMachine Learning 5.1: Classification\nMachine Learning 5.2: Classification"
  },
  {
    "objectID": "index.html#cs231n",
    "href": "index.html#cs231n",
    "title": "Machine Learning",
    "section": "CS231n",
    "text": "CS231n\n\nCS231n: Classification\nCS231n: Neural Network 1\nCS231n: Neural Network 2\nCS231n: Neural Network 3"
  },
  {
    "objectID": "google/tfkdl_01.html#introduction-to-cnns",
    "href": "google/tfkdl_01.html#introduction-to-cnns",
    "title": "Machine-Learning-03",
    "section": "Introduction to CNNs",
    "text": "Introduction to CNNs\nBridging Theory and ECE Applications\n\n\nWhat is a CNN?\n\nA type of neural network specialized in processing data with a grid-like topology.\nPrimarily used for image recognition, computer vision, and signal processing.\nInspired by the human visual cortex.\n\n\n\n\n\nFiltering an image with two successive filters, each made of 4x4x3=48 learnable weights.\n\n\n\nCNNs are a cornerstone of modern AI, especially for tasks involving spatial data. For ECE students, it’s not just about theoretical understanding but also about their practical implementation in hardware and software for real-world applications. This introductory slide sets the stage for diving into the core mechanisms."
  },
  {
    "objectID": "google/tfkdl_01.html#core-concept-the-convolution-operation",
    "href": "google/tfkdl_01.html#core-concept-the-convolution-operation",
    "title": "Machine-Learning-03",
    "section": "Core Concept: The Convolution Operation",
    "text": "Core Concept: The Convolution Operation\nA Localized Filtering Approach\nLocalized Receptive Fields\n\nUnlike dense layers where each neuron sees the entire input, a CNN neuron processes only a small, local region of the input (its receptive field).\nThis significantly reduces the number of learnable parameters.\n\nWeight Sharing (Filtered Operations)\n\nThe same set of weights (a filter or kernel) is applied across the entire image.\nThis operation is known as convolution.\n\nIt acts like a digital filter, detecting specific features (edges, textures, patterns).\n\n\n\n\n\n\n\n\nNote\n\n\nThis operation is analogous to applying FIR/IIR filters in digital signal processing, but here the filter coefficients are learned."
  },
  {
    "objectID": "google/tfkdl_01.html#core-concept-the-convolution-operation-1",
    "href": "google/tfkdl_01.html#core-concept-the-convolution-operation-1",
    "title": "Machine-Learning-03",
    "section": "Core Concept: The Convolution Operation",
    "text": "Core Concept: The Convolution Operation\nA Localized Filtering Approach\n\nAnalogy: Each filter creates a “feature map” or “channel” highlighting specific patterns.Why Multiple Filters/Channels?\n\nA single filter might detect one type of feature (e.g., horizontal edges).\nTo detect a variety of features, we use multiple filters.\nEach filter produces a separate output channel or feature map.\nThese multiple channels are stacked to form a new “data cube”."
  },
  {
    "objectID": "google/tfkdl_01.html#visualizing-the-convolution-operation",
    "href": "google/tfkdl_01.html#visualizing-the-convolution-operation",
    "title": "Machine-Learning-03",
    "section": "Visualizing the Convolution Operation",
    "text": "Visualizing the Convolution Operation\nAn Interactive Example\nInput: A simple 2D grid representing an image or signal.\nKernel: A smaller 2D filter that slides over the input.\nOperation: At each position, the kernel’s values are multiplied element-wise by the corresponding input values within its receptive field, and the products are summed to form a single output pixel.\nKey Parameters:\n\nKernel Size: Dimensions of the filter (e.g., 3x3, 5x5).\nStride: Number of pixels the filter shifts at each step.\nPadding: Adding zeros to the input edges to preserve spatial dimensions."
  },
  {
    "objectID": "google/tfkdl_01.html#visualizing-the-convolution-operation-1",
    "href": "google/tfkdl_01.html#visualizing-the-convolution-operation-1",
    "title": "Machine-Learning-03",
    "section": "Visualizing the Convolution Operation",
    "text": "Visualizing the Convolution Operation\nAn Interactive Example\n\nviewof input_size = Inputs.range([3, 10], {value: 5, step: 1, label: \"Input Size (N x N)\"});\nviewof kernel_size = Inputs.range([1, 3], {value: 3, step: 2, label: \"Kernel Size (K x K)\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive slide allows students to dynamically adjust input and kernel sizes to see how the convolution operation changes. It visually demonstrates the concept of a sliding window. Emphasize that in actual CNNs, the kernel values are learned through backpropagation, not randomly assigned. This interactivity helps bridge the abstract concept with a concrete visual."
  },
  {
    "objectID": "google/tfkdl_01.html#building-blocks-of-a-cnn",
    "href": "google/tfkdl_01.html#building-blocks-of-a-cnn",
    "title": "Machine-Learning-03",
    "section": "Building Blocks of a CNN",
    "text": "Building Blocks of a CNN\nLayer by Layer\nConvolutional Layer (Conv2D)\n\nApplies a set of learnable filters to the input image / feature maps.\nGenerates new feature maps that highlight specific patterns.\nParameters: filters, kernel_size, strides, padding, activation.\n\nActivation Function (ReLU)\n\nIntroduces non-linearity to the model.\nReLU (Rectified Linear Unit) is popular due to its computational efficiency.\n\n\\(f(x) = \\max(0, x)\\)\n\n\nPooling Layer (Max Pooling, GlobalAveragePooling2D)\n\nReduces the spatial dimensions of the feature maps.\nDecreases computation and helps control overfitting.\nMax Pooling: Takes the maximum value from each patch.\nGlobalAveragePooling2D: Averages entire feature maps, often used before the final classification."
  },
  {
    "objectID": "google/tfkdl_01.html#building-blocks-of-a-cnn-1",
    "href": "google/tfkdl_01.html#building-blocks-of-a-cnn-1",
    "title": "Machine-Learning-03",
    "section": "Building Blocks of a CNN",
    "text": "Building Blocks of a CNN\nLayer by Layer\nExample Keras Model Snippet:\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Reshape(input_shape=(28*28,), target_shape=(28, 28, 1)),\n    tf.keras.layers.Conv2D(kernel_size=3, filters=12, activation='relu'),\n    tf.keras.layers.Conv2D(kernel_size=6, filters=24, strides=2, activation='relu'),\n    tf.keras.layers.Conv2D(kernel_size=6, filters=32, strides=2, activation='relu'),\n    tf.keras.layers.Flatten(), # or GlobalAveragePooling2D()\n    tf.keras.layers.Dense(10, activation='softmax')\n])\nDense Layer (Dense)\n\nA fully connected layer, typically at the end of the network.\nUsed for classification, after the spatial features have been extracted and flattened.\n\nFlatten Layer (Flatten)\n\nConverts the multi-dimensional output of convolutional/pooling layers into a 1D vector.\nPrepares the data for input to a dense layer."
  },
  {
    "objectID": "google/tfkdl_01.html#architectural-view-of-a-cnn",
    "href": "google/tfkdl_01.html#architectural-view-of-a-cnn",
    "title": "Machine-Learning-03",
    "section": "Architectural View of a CNN",
    "text": "Architectural View of a CNN\nData Cubes in Motion\nData Transformation:\n\nA CNN can be visualized as transforming “cubes” of data.\nEach operation (convolution, pooling) manipulates the dimensions of these cubes.\n\nInput Layer:\n\nStarts with an input image (e.g., 28x28x1 for grayscale MNIST, 224x224x3 for RGB images).\n\nIntermediate Layers:\n\nConvolutional layers increase the number of channels (filters) while possibly reducing spatial dimensions (if strides &gt; 1).\nPooling layers reduce spatial dimensions, preserving the number of channels.\n\nOutput Layer:\n\nTypically, a Flatten or GlobalAveragePooling2D layer converts the cube into a vector.\nFollowed by a Dense layer with a softmax activation for classification probabilities.\n\n\n\n\n\n\n\nImportant\n\n\nThe precise management of spatial dimensions (height, width) and channels is a key ECE consideration for memory and computational efficiency.\n\n\n\n\nThis diagram visually reinforces the concept of data flowing as “cubes” through different layers, changing their dimensions. Explain how each layer type manipulates the height, width, and channel dimensions. For ECE, also consider how these dimension changes impact memory bandwidth and computational load on custom hardware accelerators."
  },
  {
    "objectID": "google/tfkdl_01.html#strided-convolutions-max-pooling",
    "href": "google/tfkdl_01.html#strided-convolutions-max-pooling",
    "title": "Machine-Learning-03",
    "section": "Strided Convolutions & Max Pooling",
    "text": "Strided Convolutions & Max Pooling\nDownsampling Strategies\nPurpose of Downsampling:\n\nReduce computational load: Fewer pixels to process in subsequent layers.\nExtract more abstract features: Larger receptive fields for higher-level features.\nIncrease robustness: Make the network less sensitive to small shifts or distortions in the input.\n\n1. Strided Convolution:\n\nThe convolution filter moves by stride pixels at each step (e.g., stride=2).\nDirectly reduces the output feature map size.\nExample: tf.keras.layers.Conv2D(..., strides=2, ...)\n\n2. Max Pooling:\n\nA non-parametric operation.\nSlides a window (e.g., 2x2) across the feature map.\nOutputs the maximum value within that window.\nCommonly used with pool_size = strides (e.g., 2x2 window with stride 2).\nExample: tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))"
  },
  {
    "objectID": "google/tfkdl_01.html#strided-convolutions-max-pooling-1",
    "href": "google/tfkdl_01.html#strided-convolutions-max-pooling-1",
    "title": "Machine-Learning-03",
    "section": "Strided Convolutions & Max Pooling",
    "text": "Strided Convolutions & Max Pooling\nDownsampling Strategies\n\nStrided convolutions or max pooling reduce spatial dimensions.\n\n\n\n\n\nNote\n\n\nIn ECE, choice between strided convolution and pooling impacts hardware architecture. Strided convo uses multiply-accumulate units. Max pooling requires comparators and a maximum selector.\n\n\n\nComparison:\n\nStrided Conv: Learns how to downsample, can adapt filters for optimal feature reduction.\nMax Pooling: Simpler, fixed operation; preserves only the most salient features. Reduces overfitting by taking only the maximum."
  },
  {
    "objectID": "google/tfkdl_01.html#the-final-layer-classification-head",
    "href": "google/tfkdl_01.html#the-final-layer-classification-head",
    "title": "Machine-Learning-03",
    "section": "The Final Layer: Classification Head",
    "text": "The Final Layer: Classification Head\nConnecting Features to Decisions\nGoal: Transform the extracted features into class probabilities.\n\n\nOption 1: Flatten and Dense Layers\n\nFlatten: Converts the 3D feature cube into a 1D vector.\n\ntf.keras.layers.Flatten()\n\nDense Layer(s): One or more fully connected layers.\n\nCan be computationally expensive.\nHigh number of weights, especially for large feature maps.\n\nSoftmax Activation: Last dense layer outputs probabilities for each class.\n\n\n\n\n\n\n\nWarning\n\n\nA large Flatten output connected to a Dense layer can lead to a parameter explosion, a major concern for memory and computation in embedded ECE systems.\n\n\n\n\nOption 2: Global Average Pooling (GAP) and Dense/Softmax\n\nGlobalAveragePooling2D:\n\nAverages each feature map across its spatial dimensions (height x width).\nConverts a [H x W x C] cube into a [C] vector.\nExample: tf.keras.layers.GlobalAveragePooling2D()\n\nDense Layer (optional) / Softmax:\n\nOften, this is fed directly into a final Dense layer with softmax.\n0 weights if directly connected to output softmax layer (no intermediate Dense layer).\nSignificantly reduces parameters and helps prevent overfitting.\n\n\n\n\n\n\n\n\nTip\n\n\nFor ECE applications, especially on resource-constrained devices, GlobalAveragePooling2D is often preferred for its parameter efficiency."
  },
  {
    "objectID": "google/tfkdl_01.html#the-final-layer-classification-head-1",
    "href": "google/tfkdl_01.html#the-final-layer-classification-head-1",
    "title": "Machine-Learning-03",
    "section": "The Final Layer: Classification Head",
    "text": "The Final Layer: Classification Head\nConnecting Features to Decisions\n\nTwo options for the final layer."
  },
  {
    "objectID": "google/tfkdl_01.html#conclusion",
    "href": "google/tfkdl_01.html#conclusion",
    "title": "Machine-Learning-03",
    "section": "Conclusion",
    "text": "Conclusion\nKey Takeaways for ECE\nRecap of CNN Fundamentals:\n\nLocalized Receptive Fields: Neurons respond to small regions.\nWeight Sharing: Filters slide across the input, making CNNs efficient.\nFeature Hierarchy: Layers progressively learn more complex features.\nDownsampling: Strided convolutions and pooling reduce data dimensions, manage computation.\nClassification Head: Flatten + Dense or GlobalAveragePooling2D for final prediction."
  },
  {
    "objectID": "google/tfkdl_01.html#conclusion-1",
    "href": "google/tfkdl_01.html#conclusion-1",
    "title": "Machine-Learning-03",
    "section": "Conclusion",
    "text": "Conclusion\nKey Takeaways for ECE\nRelevance for Electrical and Computer Engineers:\n\nHardware Acceleration: Designing custom chips (ASICs, FPGAs) for efficient CNN inference.\nEmbedded AI: Deploying CNNs on low-power, edge devices (e.g., IoT, drones, automotive ECUs).\nSensor Fusion: Processing data from multiple ECE sensors (cameras, LiDAR, radar) using CNNs.\nReal-time Processing: Optimizing CNN architectures and implementations for latency-critical applications.\nResource Optimization: Balancing accuracy with computational cost, power consumption, and memory footprint.\n\n\nReinforce the practical implications of CNN knowledge. Emphasize that ECE professionals don’t just use these models, they build the underlying hardware and software tools that make them possible, and they face unique challenges related to resource constraints and real-time performance."
  },
  {
    "objectID": "google/tfkdl_01.html#regularization-dropout",
    "href": "google/tfkdl_01.html#regularization-dropout",
    "title": "Machine-Learning-03",
    "section": "Regularization: Dropout",
    "text": "Regularization: Dropout\nMitigating Overfitting in Machine Learning Models"
  },
  {
    "objectID": "google/tfkdl_01.html#understanding-overfitting",
    "href": "google/tfkdl_01.html#understanding-overfitting",
    "title": "Machine-Learning-03",
    "section": "Understanding Overfitting",
    "text": "Understanding Overfitting\nA Critical Challenge in ML\nWhat is Overfitting?\n\nA model that performs exceptionally well on the training data but poorly on unseen data (validation/test data).\nThe model essentially “memorizes” the training examples, including noise and specific patterns, rather than learning generalizable features.\n\nSigns of Overfitting:\n\nTraining Loss Decreases: Continues to go down during training.\nValidation Loss Increases: Starts to rise after an initial decrease, indicating the model is losing generalization ability.\nHigh Training Accuracy, Low Validation Accuracy: A significant gap between performance on seen and unseen data.\n\nCauses of Overfitting:\n\nModel Complexity: Too many parameters or layers relative to data.\nInsufficient Data: Not enough training examples to learn general patterns.\nNoisy Data: The model learns to fit the noise in the training set.\n\n\n\n\n\n\n\nWarning\n\n\nFor ECE applications, overfitting can lead to unreliable system performance in real-world scenarios, which is unacceptable for safety-critical systems like autonomous vehicles or medical devices."
  },
  {
    "objectID": "google/tfkdl_01.html#understanding-overfitting-1",
    "href": "google/tfkdl_01.html#understanding-overfitting-1",
    "title": "Machine-Learning-03",
    "section": "Understanding Overfitting",
    "text": "Understanding Overfitting\nA Critical Challenge in ML\n\nValidation loss flattening and training loss decreasing, a positive sign with dropout."
  },
  {
    "objectID": "google/tfkdl_01.html#introduction-to-dropout",
    "href": "google/tfkdl_01.html#introduction-to-dropout",
    "title": "Machine-Learning-03",
    "section": "Introduction to Dropout",
    "text": "Introduction to Dropout\nA Simple Yet Powerful Regularization Technique\nWhat is Dropout?\n\nA regularization technique introduced by Hinton et al. in 2012.\nRandomly sets a fraction of neuron outputs to zero at each training step.\n“Dropping out” neurons means they temporarily do not contribute to the forward pass and do not participate in backpropagation.\n\nAnalogy:\n\nImagine a team where, for each task, a random subset of members is absent. The remaining members must learn to perform their roles more robustly without relying on any single individual."
  },
  {
    "objectID": "google/tfkdl_01.html#introduction-to-dropout-1",
    "href": "google/tfkdl_01.html#introduction-to-dropout-1",
    "title": "Machine-Learning-03",
    "section": "Introduction to Dropout",
    "text": "Introduction to Dropout\nA Simple Yet Powerful Regularization Technique\nHow it Works (Training Phase):\n\nA dropout rate (e.g., 0.2 to 0.5) specifies the probability of a neuron being dropped.\nEach neuron has its state (active/inactive) sampled independently.\nThe weights of the remaining active neurons are scaled up by 1 / (1 - dropout_rate) to maintain the same expected sum of outputs.\n\nInference/Testing Phase:\n\nAll neurons are active and contribute to the output.\nTheir weights are not scaled (or equivalently, they are pre-scaled by the dropout rate during training).\n\n\nExplain that dropout is like training an ensemble of neural networks, where each epoch a different “thinned” network is trained. This forces the network to learn more robust features that are not reliant on specific co-adaptations of neurons. Emphasize the difference between training (random drops and scaling) and inference (all neurons active, no scaling)."
  },
  {
    "objectID": "google/tfkdl_01.html#dropout-in-keras",
    "href": "google/tfkdl_01.html#dropout-in-keras",
    "title": "Machine-Learning-03",
    "section": "Dropout in Keras",
    "text": "Dropout in Keras\nImplementation and Placement\nAdding Dropout Layers:\n\nIn Keras, dropout is implemented as a layer: tf.keras.layers.Dropout(rate).\nThe rate is the fraction of inputs to drop (e.g., 0.2 for 20%).\n\nTypical Placement:\n\nOften applied after activation functions (or after convolutional and pooling layers, before Flatten or Dense).\nCommonly used after convolutional blocks and before fully connected (dense) layers to prevent overfitting on extracted features.\n\nExample Keras Model with Dropout:\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Reshape(input_shape=(28*28,), target_shape=(28, 28, 1)),\n    tf.keras.layers.Conv2D(kernel_size=3, filters=12, activation='relu'),\n    tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)), # Added pooling\n    tf.keras.layers.Dropout(0.25), # Dropout after pooling\n    tf.keras.layers.Conv2D(kernel_size=6, filters=24, strides=1, activation='relu'),\n    tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)), # Another pooling\n    tf.keras.layers.Dropout(0.25), # Dropout after second pooling\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'), # Intermediate Dense layer\n    tf.keras.layers.Dropout(0.5), # High dropout for dense layers\n    tf.keras.layers.Dense(10, activation='softmax')\n])"
  },
  {
    "objectID": "google/tfkdl_01.html#dropout-in-keras-1",
    "href": "google/tfkdl_01.html#dropout-in-keras-1",
    "title": "Machine-Learning-03",
    "section": "Dropout in Keras",
    "text": "Dropout in Keras\nImplementation and Placement\nPractical Considerations:\n\nDropout Rate:\n\nUsually between 0.2 and 0.5. Needs tuning.\nHigher rates for larger models or smaller datasets.\n\nPlacement: Experiment with placement.\n\nOften effective before dense layers due to their high parameter count.\nCan be used after convolutional layers too.\n\nTraining Time: Dropout can sometimes increase training time as the network needs more iterations to converge.\n\n\n\n\n\n\n\nTip\n\n\nIn ECE, understanding dropout’s impact on model robustness is critical. A robust model performs reliably even with minor input variations, which is vital for real-world environmental noise or sensor inaccuracies."
  },
  {
    "objectID": "google/tfkdl_01.html#dropout-in-keras-2",
    "href": "google/tfkdl_01.html#dropout-in-keras-2",
    "title": "Machine-Learning-03",
    "section": "Dropout in Keras",
    "text": "Dropout in Keras\nDropout Effect on Training\nThis Python code block does not execute a full training loop to demonstrate dropout, but illustrates how the dropout layer works by modifying input tensors.\n#| max-lines: 10\nimport tensorflow as tf\nimport numpy as np\n\n# Create a sample input tensor (e.g., output from a previous layer)\ninput_tensor = tf.constant(np.arange(1, 11, dtype=np.float32).reshape(1, 10))\nprint(\"Original Input Tensor:\")\nprint(input_tensor.numpy())\n\n# Create a Dropout layer with a rate of 0.5\ndropout_layer = tf.keras.layers.Dropout(rate=0.5)\n\n# Simulate training mode (where dropout is active)\n# The is_training=True argument is crucial for Dropout to be active\ndropped_output = dropout_layer(input_tensor, training=True)\nprint(\"\\nOutput with Dropout (training=True):\")\nprint(dropped_output.numpy())\n\n# Simulate inference mode (where dropout is inactive, and scaling is applied - though handled internally)\nregular_output = dropout_layer(input_tensor, training=False)\nprint(\"\\nOutput without Dropout (training=False - all values present, no explicit scaling shown here as it's for weights):\")\nprint(regular_output.numpy())\n\nThe interactive example shows how Dropout literally sets some values to zero in training. Emphasize that in inference, no values are dropped, and all neurons contribute. The scaling factor applied during training (or to weights during inference, depending on implementation) ensures the expected output magnitude remains consistent."
  },
  {
    "objectID": "google/tfkdl_01.html#when-dropout-helps-and-when-it-doesnt",
    "href": "google/tfkdl_01.html#when-dropout-helps-and-when-it-doesnt",
    "title": "Machine-Learning-03",
    "section": "When Dropout Helps (and When It Doesn’t)",
    "text": "When Dropout Helps (and When It Doesn’t)\nDiagnosing Overfitting\nWhen Dropout is Effective:\n\nOverfitting is the primary problem: When validation loss starts increasing while training loss continues to decrease.\nComplex Models: Helps prevent co-adaptation of neurons in deep architectures.\nSufficiently Large Datasets: Dropout works best when the model has enough data to learn diverse features across different “sub-networks.”\n\nWhen Dropout Might Not Help (or Even Hurt):\n\nUnderfitting: If the model is too simple or data is too scarce, dropout can make it even harder for the model to learn.\nWrong Architecture: If the model itself is fundamentally unsuited for the task (e.g., fully connected layers for image data without convolutions).\n\nIn such cases, dropout cannot compensate for a poor architectural design.\n\nToo High Dropout Rate: Can lead to underfitting if too many neurons are dropped, effectively simplifying the network too much."
  },
  {
    "objectID": "google/tfkdl_01.html#when-dropout-helps-and-when-it-doesnt-1",
    "href": "google/tfkdl_01.html#when-dropout-helps-and-when-it-doesnt-1",
    "title": "Machine-Learning-03",
    "section": "When Dropout Helps (and When It Doesn’t)",
    "text": "When Dropout Helps (and When It Doesn’t)\nDiagnosing Overfitting\nAnalyzing Loss Curves:\n\nTraining loss decreasing, Validation loss increasing: Classic sign of overfitting \\(\\rightarrow\\) Use Dropout.\nBoth training and validation loss decreasing but high: Underfitting or insufficient capacity \\(\\rightarrow\\) Adjust architecture, add layers, remove dropout.\nBoth training and validation loss erratic: Learning rate too high, batch size too small, or bad initialization \\(\\rightarrow\\) Adjust hyperparameters.\n\n\n\n\n\n\n\nNote\n\n\nFor ECE systems, debugging model performance requires a systematic approach. Distinguishing between architectural issues, data limitations, and overfitting is key to efficient resource allocation in hardware design and algorithm tuning.\n\n\n\nBeyond Dropout: Other regularization techniques include L1/L2 regularization, data augmentation, batch normalization, and early stopping."
  },
  {
    "objectID": "google/tfkdl_01.html#conclusion-2",
    "href": "google/tfkdl_01.html#conclusion-2",
    "title": "Machine-Learning-03",
    "section": "Conclusion",
    "text": "Conclusion\nDropout Summary & ECE Relevance\nKey Learnings about Dropout:\n\nMechanism: Randomly deactivates neurons during training to prevent co-adaptation.\nGoal: Mitigates overfitting, leading to better generalization on unseen data.\nImplementation: Simple Keras Dropout layer.\nDiagnosis: Effective when validation loss deviates upwards from training loss."
  },
  {
    "objectID": "google/tfkdl_01.html#conclusion-3",
    "href": "google/tfkdl_01.html#conclusion-3",
    "title": "Machine-Learning-03",
    "section": "Conclusion",
    "text": "Conclusion\nDropout Summary & ECE Relevance\nECE Relevance:\n\nRobustness: Dropout improves model robustness, crucial for deployment in noisy or varying real-world environments (e.g., sensor data processing).\nDeployment Reliability: Reduces the chance of models failing on edge cases not perfectly represented in the training data, enhancing system reliability.\nModel Selection: ECE engineers need to select appropriate regularization techniques to optimize the trade-off between model accuracy, complexity, and deployment constraints (memory, power).\nDebugging: Understanding loss curves and the role of regularization is vital for efficient troubleshooting and performance tuning of hardware-accelerated ML systems.\n\n\nConclude by reiterating the direct benefits of dropout for ECE students: not just better theoretical understanding, but practical tools for building more reliable and robust intelligent systems. Emphasize that regularization is a critical skill for any ECE professional working with AI/ML."
  },
  {
    "objectID": "google/tfkdl_01.html#batch-normalization",
    "href": "google/tfkdl_01.html#batch-normalization",
    "title": "Machine-Learning-03",
    "section": "Batch Normalization",
    "text": "Batch Normalization\nStabilizing and Accelerating Neural Network Training"
  },
  {
    "objectID": "google/tfkdl_01.html#the-challenge-of-internal-covariate-shift",
    "href": "google/tfkdl_01.html#the-challenge-of-internal-covariate-shift",
    "title": "Machine-Learning-03",
    "section": "The Challenge of Internal Covariate Shift",
    "text": "The Challenge of Internal Covariate Shift\nWhy Normalization is Needed in Deep Networks\nInternal Covariate Shift (ICS):\n\nDuring neural network training, the distribution of inputs to hidden layers changes as the parameters of the preceding layers change.\nThis continuous shift means that a layer constantly has to adapt to a new input distribution.\nAnalogy: Imagine trying to learn to ride a bike where the ground underneath is constantly shifting.\n\nProblems Caused by ICS:\n\nSlower Training: Requires lower learning rates and careful initialization.\nVanishing/Exploding Gradients: Can make activations too small or too large.\nDifficulty in Optimization: Makes it harder for the optimizer to find good weights.\nReduced Generalization: Can negatively impact model performance.\n\n\n\n\n\n\n\nWarning\n\n\nUnderstanding and mitigating ICS is crucial for ECE, especially in designing custom hardware for deep learning, as it impacts convergence speed and resource utilization."
  },
  {
    "objectID": "google/tfkdl_01.html#the-challenge-of-internal-covariate-shift-1",
    "href": "google/tfkdl_01.html#the-challenge-of-internal-covariate-shift-1",
    "title": "Machine-Learning-03",
    "section": "The Challenge of Internal Covariate Shift",
    "text": "The Challenge of Internal Covariate Shift\nWhy Normalization is Needed in Deep Networks\n\nConceptual representation of the impact of Batch Normalization, allowing for smoother training curves.How Batch Normalization Addresses ICS:\n\nStandardizes the inputs to hidden layers.\nIt normalizes the activations of each previous layer at each mini-batch, making the distribution more stable.\nFor each feature/channel across the mini-batch, it computes the mean and variance, then normalizes the data to have zero mean and unit variance."
  },
  {
    "objectID": "google/tfkdl_01.html#what-is-batch-normalization",
    "href": "google/tfkdl_01.html#what-is-batch-normalization",
    "title": "Machine-Learning-03",
    "section": "What is Batch Normalization?",
    "text": "What is Batch Normalization?\nThe Mechanism\nPer-Batch, Per-Feature Normalization:\nFor each mini-batch during training, and for each feature (or channel in CNNs) independently:\n\nCalculate Mean (\\(\\mu_B\\)): Compute the mean of the activations for that feature across all samples in the mini-batch.\nCalculate Variance (\\(\\sigma_B^2\\)): Compute the variance of the activations for that feature across all samples in the mini-batch.\nNormalize: Scale the activations to have zero mean and unit variance. \\[ \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\] (\\(\\epsilon\\) is a small constant for numerical stability)\nScale and Shift: Apply learnable parameters \\(\\gamma\\) (scale) and \\(\\beta\\) (shift) to restore representation power. \\[ y_i = \\gamma \\hat{x}_i + \\beta \\]\n\n\\(\\gamma\\) and \\(\\beta\\) allow the network to learn optimal mean/variance for each layer, not just forcing it to be 0 and 1.\nThese are trained parameters, just like weights and biases."
  },
  {
    "objectID": "google/tfkdl_01.html#what-is-batch-normalization-1",
    "href": "google/tfkdl_01.html#what-is-batch-normalization-1",
    "title": "Machine-Learning-03",
    "section": "What is Batch Normalization?",
    "text": "What is Batch Normalization?\nThe Mechanism\nTraining vs. Inference:\n\nTraining: Mean and variance are calculated per mini-batch.\nInference: Moving averages of mean and variance (collected during training) are used.\n\nEnsures deterministic output for a single input, as batch statistics are unavailable.\n\n\n\nDetail the four steps of Batch Norm. Emphasize that \\(\\gamma\\) and \\(\\beta\\) are learnable parameters, making Batch Norm a flexible transformation, not just a rigid standardization. Clarify the different behavior during training (batch statistics) and inference (running averages). This distinction is critical for ECE in deployment."
  },
  {
    "objectID": "google/tfkdl_01.html#benefits-of-batch-normalization",
    "href": "google/tfkdl_01.html#benefits-of-batch-normalization",
    "title": "Machine-Learning-03",
    "section": "Benefits of Batch Normalization",
    "text": "Benefits of Batch Normalization\nWhy it’s a Game Changer\n\nFaster Training (Higher Learning Rates):\n\nStabilizes gradients, allowing for much higher learning rates without risk of divergence.\nThis significantly reduces training time.\n\nReduces Impact of Initialization:\n\nLess sensitive to initial weights.\nThe normalization process acts as a buffer against poor initial conditions.\n\nRegularization Effect:\n\nAdds a slight amount of noise to the network’s activations, much like dropout.\nCan sometimes reduce or negate the need for dropout, as seen in the text.\n\nImproved Gradient Flow:\n\nPrevents vanishing/exploding gradients in very deep networks by keeping activation distributions stable.\n\n\n\n\n\n\n\n\nTip\n\n\nFor ECE, faster training means shorter design cycles and more iterations for experimentation, while improved stability leads to more robust and deployable ML models."
  },
  {
    "objectID": "google/tfkdl_01.html#benefits-of-batch-normalization-1",
    "href": "google/tfkdl_01.html#benefits-of-batch-normalization-1",
    "title": "Machine-Learning-03",
    "section": "Benefits of Batch Normalization",
    "text": "Benefits of Batch Normalization\nWhy it’s a Game Changer\nImpact on Training Curves:\n\nBatch Norm often leads to smoother, faster convergence to higher accuracy.Practical Rules of Thumb from the Reference:\n\nBatch Norm helps neural networks converge and usually allows you to train faster.\nBatch Norm is a regularizer. You can usually decrease the amount of dropout you use, or even not use dropout at all."
  },
  {
    "objectID": "google/tfkdl_01.html#implementing-batch-normalization-in-keras",
    "href": "google/tfkdl_01.html#implementing-batch-normalization-in-keras",
    "title": "Machine-Learning-03",
    "section": "Implementing Batch Normalization in Keras",
    "text": "Implementing Batch Normalization in Keras\nPractical Integration\nKeras Layer:\n\nImplemented as tf.keras.layers.BatchNormalization().\n\nPlacement:\n\nTypically placed after a convolutional or dense layer and before its activation function.\nWhy use_bias=False? The beta parameter in Batch Norm effectively acts as a bias term, so the biases from the preceding Conv2D or Dense layer are redundant.\nWhy scale=False for ReLU? For ReLU, scale=False can sometimes be used because ReLU sets negative values to zero, and the learned \\(\\gamma\\) parameter might not be strictly necessary for scaling. However, scale=True (default) is generally fine and allowing \\(\\gamma\\) to be learned provides more flexibility.\n\nExample Keras Snippet:\n# Original approach with activation in the layer\n# tf.keras.layers.Conv2D(kernel_size=3, filters=12, activation='relu')\n\n# With Batch Normalization:\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(kernel_size=3, filters=12, use_bias=False), # No bias\n    tf.keras.layers.BatchNormalization(scale=True), # Default scale=True\n    tf.keras.layers.Activation('relu'), # Activation after Batch Norm\n  \n    tf.keras.layers.Conv2D(kernel_size=6, filters=24, strides=2, use_bias=False),\n    tf.keras.layers.BatchNormalization(scale=True),\n    tf.keras.layers.Activation('relu'),\n  \n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, use_bias=False),\n    tf.keras.layers.BatchNormalization(scale=True),\n    tf.keras.layers.Activation('relu'),\n    tf.keras.layers.Dense(10, activation='softmax') # No Batch Norm here\n])"
  },
  {
    "objectID": "google/tfkdl_01.html#implementing-batch-normalization-in-keras-1",
    "href": "google/tfkdl_01.html#implementing-batch-normalization-in-keras-1",
    "title": "Machine-Learning-03",
    "section": "Implementing Batch Normalization in Keras",
    "text": "Implementing Batch Normalization in Keras\nPractical Integration\nInteractive Example: Batch Norm Effect on Activation Distribution\n\nviewof mean_val = Inputs.range([-2, 2], {value: 0.5, step: 0.1, label: \"Input Mean\"});\nviewof std_val = Inputs.range([0.1, 3], {value: 1.5, step: 0.1, label: \"Input Std Dev\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe interactive demo clearly shows how Batch Normalization transforms an arbitrary input distribution to one with approximately zero mean and unit variance. This visual “stabilization” helps students intuitively grasp the core idea of reducing internal covariate shift. Explain that the gamma and beta parameters (set to 1 and 0 here for simplicity) allow the network to ‘undo’ this if needed, but the normalization acts as a helpful processing step."
  },
  {
    "objectID": "google/tfkdl_01.html#conclusion-4",
    "href": "google/tfkdl_01.html#conclusion-4",
    "title": "Machine-Learning-03",
    "section": "Conclusion",
    "text": "Conclusion\nBatch Normalization’s Impact on Modern ML\nKey Takeaways on Batch Normalization:\n\nAddresses Internal Covariate Shift: Stabilizes the input distribution to layers.\nAccelerates Training: Allows higher learning rates and faster convergence.\nActs as a Regularizer: Reduces the need for dropout.\nImproves Gradient Flow: Aids optimization in deep networks.\nImplementation: tf.keras.layers.BatchNormalization() typically placed between a layer and its activation, with use_bias=False in the preceding layer."
  },
  {
    "objectID": "google/tfkdl_01.html#conclusion-5",
    "href": "google/tfkdl_01.html#conclusion-5",
    "title": "Machine-Learning-03",
    "section": "Conclusion",
    "text": "Conclusion\nBatch Normalization’s Impact on Modern ML\nSignificance for ECE Professionals:\n\nHardware Efficiency: Designing efficient hardware accelerators for Batch Normalization (e.g., dedicated arithmetic units for mean/variance, fixed-point implementations).\nEmbedded Systems: Crucial for deploying deep models on resource-constrained devices, as faster training means quicker model updates and fewer training epochs.\nPower Optimization: Faster convergence contributes to less power consumption during training.\nRobust Model Design: Leads to more stable and robust models, essential for mission-critical ECE applications in real-world deployments.\nState-of-the-Art Models: Batch Normalization is a fundamental component of almost all modern, high-performing deep learning architectures.\n\n\nConclude with a strong emphasis on why Batch Normalization is indispensable for ECE students. It’s not just an algorithmic trick but a critical component influencing hardware design, deployment strategies, and overall system performance in AI-driven ECE applications. Highlight its role in enabling the development of deeper, more complex, and more effective neural networks."
  },
  {
    "objectID": "google/tfkdl_00.html#overview-recognizing-handwritten-digits",
    "href": "google/tfkdl_00.html#overview-recognizing-handwritten-digits",
    "title": "Machine-Learning-03",
    "section": "Overview: Recognizing Handwritten Digits",
    "text": "Overview: Recognizing Handwritten Digits\nToday, we’ll dive into building a neural network to recognize handwritten digits.\nWe’ll achieve ~99% accuracy using fewer than 100 lines of Python/Keras code.\nThis is a classic problem in Machine Learning, often tackled with the MNIST dataset.\n\nWelcome to this lecture on Deep Learning for ECE! Today, we’re going to tackle a fascinating problem: teaching a computer to recognize handwritten digits. This isn’t just a theoretical exercise; it has real-world applications in areas like postal code recognition, check processing, and even automated data entry.\nWe’ll be using one of the most famous datasets in machine learning, MNIST, which consists of tens of thousands of handwritten digits. Our goal is ambitious: to achieve around 99% accuracy. And the best part? We’ll do it with remarkably concise code using Python’s TensorFlow and Keras libraries. This means focusing on the concepts and building blocks rather than getting bogged down in low-level details."
  },
  {
    "objectID": "google/tfkdl_00.html#what-youll-learn",
    "href": "google/tfkdl_00.html#what-youll-learn",
    "title": "Machine-Learning-03",
    "section": "What You’ll Learn",
    "text": "What You’ll Learn\nThis session will cover key concepts and practical techniques:\n\nWhat a neural network is and how it learns.\nBuilding basic 1-layer neural networks with tf.keras.\nAdding more layers for improved performance.\nImplementing learning rate schedules.\nIntroduction to Convolutional Neural Networks (CNNs).\nRegularization techniques: Dropout and Batch Normalization.\nUnderstanding and mitigating overfitting.\n\n\nHere’s a roadmap of what we’ll cover. We’ll start with the fundamentals of neural networks, then progressively build more complex models. We’ll explore techniques to optimize training, such as learning rate schedules, and introduce you to Convolutional Neural Networks, which are particularly powerful for image data like the MNIST digits. We’ll also discuss critical techniques like dropout and batch normalization to ensure our models are robust, and we’ll address the common pitfall of overfitting."
  },
  {
    "objectID": "google/tfkdl_00.html#understanding-the-training-data-mnist",
    "href": "google/tfkdl_00.html#understanding-the-training-data-mnist",
    "title": "Machine-Learning-03",
    "section": "Understanding the Training Data: MNIST",
    "text": "Understanding the Training Data: MNIST\nThe MNIST dataset contains 60,000 labeled images of handwritten digits (0-9).\n\n\nEach image is associated with its correct numerical label.\nThis “labeled dataset” is crucial for training.\nOur neural network learns to classify these images into 10 classes (0 through 9).\n\nExample MNIST Digits:\n\n\n\nThe core of any supervised machine learning task is the data, and for image classification, especially with neural networks, we need labeled data. The MNIST dataset is a classic example. It comprises 60,000 images, each a 28x28 pixel grayscale representation of a handwritten digit, and crucially, each image comes with a label indicating the actual digit it represents. For instance, an image of a handwritten ‘5’ is accompanied by the label ‘5’. This labeled data is what allows our neural network to learn by example."
  },
  {
    "objectID": "google/tfkdl_00.html#training-vs.-validation-datasets",
    "href": "google/tfkdl_00.html#training-vs.-validation-datasets",
    "title": "Machine-Learning-03",
    "section": "Training vs. Validation Datasets",
    "text": "Training vs. Validation Datasets\nHow do we assess our model’s “real-world” performance?\n\nTraining Dataset: Used to update the model’s internal parameters. The model sees this data multiple times.\nValidation Dataset: A separate, unseen labeled dataset to evaluate performance and prevent cheating. It reflects how well the model generalizes to new data.\n\n\n\n\n\n\n\nImportant\n\n\nUsing “unseen” data for validation is fundamental for robust model evaluation.\n\n\n\n\nIt’s vital to know if our trained neural network can generalize its knowledge to new, unseen examples. If we only tested it on the data it was trained on, it would likely perform exceptionally well, but that wouldn’t tell us if it’s truly learned the underlying patterns or just memorized the training examples. This is where the distinction between training and validation datasets comes in.\nThe training dataset is like the practice problems you do to learn a subject. The model sees these examples, adjusts its internal parameters, and tries to get better.\nThe validation dataset is like a surprise pop quiz. The model has never seen these examples before. Its performance on this dataset gives us an unbiased estimate of how well it will perform on new, real-world data. A good validation score indicates the model is truly learning generalized features, not just memorizing."
  },
  {
    "objectID": "google/tfkdl_00.html#monitoring-training-progress",
    "href": "google/tfkdl_00.html#monitoring-training-progress",
    "title": "Machine-Learning-03",
    "section": "Monitoring Training Progress",
    "text": "Monitoring Training Progress\nDuring training, we track two key metrics: accuracy and loss.\n\n\nAccuracy (Right Plot):\n\nPercentage of correctly recognized digits.\nShould increase as training progresses.\n\nLoss (Left Plot):\n\nMeasures how “badly” the model performs.\nThe goal is to minimize this value.\nShould decrease on both training and validation data.\n\n\n\nX-axis: Epochs (iterations over entire dataset)\n\n\nAs our neural network trains, we want to monitor its progress. Neural network training is an iterative process where the model continually adjusts its internal weights and biases to improve its performance. The two primary metrics we observe during this process are accuracy and loss.\nAccuracy is straightforward: it’s the proportion of correctly classified examples. Ideally, this should increase over time, indicating the model is getting better at its task.\nLoss, on the other hand, is a numerical value that quantifies how “wrong” the model’s predictions are. Our training objective is to minimize this loss function. A decreasing loss on both the training and validation datasets is a strong indicator that the model is learning effectively and generalizing well. If the training loss continues to decrease but the validation loss starts to increase, that’s a sign of overfitting, which we’ll discuss later.\nThe X-axis in these plots represents epochs, where one epoch signifies one complete pass through the entire training dataset."
  },
  {
    "objectID": "google/tfkdl_00.html#making-predictions",
    "href": "google/tfkdl_00.html#making-predictions",
    "title": "Machine-Learning-03",
    "section": "Making Predictions",
    "text": "Making Predictions\nAfter training, the model can predict digits it hasn’t seen.\n\nThis initial model reaches ~90% validation accuracy, meaning it still misclassifies 1000 out of 10,000 validation digits.\n\n\n\n\n\n\nCaution\n\n\nEven 90% accuracy leaves room for improvement, especially in critical ECE applications like medical imaging or autonomous systems.\n\n\n\n\nOnce training is complete, the true test of our model is its ability to make accurate predictions on new, unseen data. The visualization shows how our initial model performs on sample digits. Correct predictions are shown in black, and incorrect ones in red.\nAt 90% validation accuracy, the model is certainly respectable for a basic setup. However, in Many ECE applications, 90% might not be enough. For instance, if this were an autonomous driving system recognizing stop signs, a 10% error rate would be catastrophic. Even for digit recognition, if you’re processing millions of checks, a 10% error rate means a lot of manual corrections. This highlights why we constantly strive for higher accuracy and robustness in deep learning models."
  },
  {
    "objectID": "google/tfkdl_00.html#understanding-tensors-the-language-of-data",
    "href": "google/tfkdl_00.html#understanding-tensors-the-language-of-data",
    "title": "Machine-Learning-03",
    "section": "Understanding Tensors: The Language of Data",
    "text": "Understanding Tensors: The Language of Data\nIn deep learning, data is represented as tensors. Tensors are multi-dimensional arrays, analogous to vectors and matrices.\n\nGrayscale Image (28x28 pixels): A 2D tensor (matrix) with shape [28, 28].\nColor Image (28x28 pixels, RGB): A 3D tensor with shape [28, 28, 3]. (Height, Width, Color Channels)\nBatch of Color Images (e.g., 128 images): A 4D tensor with shape [128, 28, 28, 3]. (Batch Size, Height, Width, Color Channels)\n\n\n\n\n\n\n\nNote\n\n\nThe list of dimensions is called the “shape” of the tensor.\nUnderstanding tensor shapes is crucial for building and debugging neural networks.\n\n\n\n\nBefore we dive into building models, it’s critical to understand how data is represented in deep learning. The fundamental data structure is the tensor. Think of a tensor as a generalization of a scalar (a single number), a vector (a 1D array), and a matrix (a 2D array) to an arbitrary number of dimensions.\nFor example, a simple grayscale handwritten digit image, which is 28x28 pixels, can be represented as a 2D tensor, or a matrix, with a shape of [28, 28]. If we had a color image, with red, green, and blue channels, it would need a third dimension for the color channels, giving it a shape like [28, 28, 3]. And often, we process images in batches to make training more efficient. So, a batch of 128 color images would become a 4D tensor with a shape of [128, 28, 28, 3].\nThe shape of a tensor tells you its dimensions, and understanding these shapes is absolutely crucial when designing neural network architectures and debugging issues. Incorrect tensor shapes are a very common source of errors."
  },
  {
    "objectID": "google/tfkdl_00.html#interactive-example-image-compression-analogy",
    "href": "google/tfkdl_00.html#interactive-example-image-compression-analogy",
    "title": "Machine-Learning-03",
    "section": "Interactive Example: Image Compression Analogy",
    "text": "Interactive Example: Image Compression Analogy\nLet’s visualize how much information we retain when we reduce the “dimensions” of an image. This is analogous to how neural networks extract features.\n\nviewof originalWidth = Inputs.range([100, 500], {value: 300, step: 10, label: \"Original Width\"});\nviewof compressedWidth = Inputs.range([10, 100], {value: 50, step: 5, label: \"Compressed Width\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive example uses an image compression analogy to illustrate how much information is retained as we reduce an image’s dimensions. In neural networks, particularly convolutional neural networks, we often downsample images (e.g., through pooling layers) to extract higher-level features and reduce computational load. By adjusting the “Compressed Width” slider, you can observe how a simplified representation of the image still captures its essence, much like how a neural network learns to represent complex patterns in a condensed form. This helps us understand the trade-offs between detail and efficiency in feature extraction."
  },
  {
    "objectID": "google/tfkdl_00.html#introduction-to-neural-networks",
    "href": "google/tfkdl_00.html#introduction-to-neural-networks",
    "title": "Machine-Learning-03",
    "section": "Introduction to Neural Networks",
    "text": "Introduction to Neural Networks\nNeural Networks are powerful computational models inspired by the human brain. They are used to learn complex patterns from data.\nFor ECE, neural networks are crucial in:\n\nSignal Processing: Noise reduction, feature extraction.\nImage Recognition: Object detection, medical imaging analysis.\nControl Systems: Adaptive control, robotics.\n\n\nWelcome to this lecture on Neural Networks! In ECE, understanding how these networks work is becoming increasingly vital. We’ll explore their fundamental concepts and see how they apply directly to engineering problems you’ll encounter. Think of them as sophisticated tools for processing and understanding complex data, much like how you design circuits to process signals."
  },
  {
    "objectID": "google/tfkdl_00.html#the-keras-sequential-api",
    "href": "google/tfkdl_00.html#the-keras-sequential-api",
    "title": "Machine-Learning-03",
    "section": "The Keras Sequential API",
    "text": "The Keras Sequential API\nWhen building neural networks with TensorFlow and Keras, the Sequential API is a straightforward way to stack layers. This is ideal for models where layers have exactly one input tensor and one output tensor.\nExample: Image Classifier using Dense Layers\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=[28, 28, 1]), # Flattens input images\n    tf.keras.layers.Dense(200, activation=\"relu\"),    # Hidden layer with ReLU\n    tf.keras.layers.Dense(60, activation=\"relu\"),     # Another hidden layer\n    tf.keras.layers.Dense(10, activation='softmax')   # Output layer for 10 classes\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n# Train the model\n# model.fit(dataset, ...)\n\nThe Keras Sequential API is your go-to for building simple, layer-by-layer models. It’s like stacking different circuit components one after another. In ECE, you might think of each layer as a stage in a signal processing pipeline. The Flatten layer converts your 2D image data into a 1D vector, which is necessary for the Dense layers. Dense layers are fully connected, meaning every neuron in one layer connects to every neuron in the next. We’ll discuss activation functions like “relu” and “softmax” shortly."
  },
  {
    "objectID": "google/tfkdl_00.html#what-is-a-neuron-the-basic-building-block",
    "href": "google/tfkdl_00.html#what-is-a-neuron-the-basic-building-block",
    "title": "Machine-Learning-03",
    "section": "What is a Neuron? The Basic Building Block",
    "text": "What is a Neuron? The Basic Building Block\nThe fundamental unit of a neural network is the neuron, a concept analogous to a processing unit in digital circuits.\n\n\nEach neuron performs three main operations:\n\nWeighted Sum: Multiplies each input by a corresponding weight and sums them up.\nBias Addition: Adds a bias constant to the weighted sum.\nActivation: Passes the result through a non-linear activation function.\n\nThe weights (W) and biases (b) are the parameters learned during training. Initially, they are random and get adjusted to minimize error.\n\n\n\n\nneuron.png\n\n\n\n\n\n\n\n\n\nTip\n\n\nThink of weights as variable resistors and biases as constant voltage offsets in an analog circuit. The activation function is like a threshold detector or a non-linear amplifier.\n\n\n\n\nThe neuron is the basic computational element. It’s not unlike an operational amplifier with multiple inputs, where each input has a gain (weight) and an offset (bias), and the output is then shaped by a non-linear transfer function. These weights and biases are the “knobs” we turn during training to make the network learn."
  },
  {
    "objectID": "google/tfkdl_00.html#single-dense-layer-mnist-example",
    "href": "google/tfkdl_00.html#single-dense-layer-mnist-example",
    "title": "Machine-Learning-03",
    "section": "Single Dense Layer: MNIST Example",
    "text": "Single Dense Layer: MNIST Example\nLet’s consider classifying handwritten digits from the MNIST dataset. Each image is 28x28 pixels grayscale.\n\n\nThe simplest neural network for this task uses 784 pixels (28x28) as inputs to a single dense layer.\nThis layer has 10 output neurons, one for each digit class (0-9).\nEach of these 10 output neurons takes all 784 pixel values as input, performs a weighted sum, adds a bias, and applies an activation.\n\n\n\n\n\n\n\n\nG\n\n\n\n\npixel_1\n\nPixel 1\n\n\n\npixel_2\n\nPixel 2\n\n\n\n\ndigit_0\n\nDigit 0\n\n\n\npixel_1-&gt;digit_0\n\n\n\n\ndigit_9\n\nDigit 9\n\n\n\npixel_1-&gt;digit_9\n\n\n\n\npixel_3\n\n...\n\n\n\n\npixel_784\n\nPixel 784\n\n\n\n\npixel_784-&gt;digit_0\n\n\n\n\npixel_784-&gt;digit_9\n\n\n\n\ndigit_1\n\nDigit 1\n\n\n\n\ndigit_2\n\n...\n\n\n\n\n\n\n\n\n\n\n\n\nFor MNIST, each 28x28 pixel image contains 784 individual pixel values. These 784 values become the direct inputs to the first layer of our neural network. Since we want to classify digits from 0 to 9, we need 10 output neurons, each corresponding to a specific digit. This diagram illustrates the “dense” or fully connected nature of this layer, where every input pixel influences every output neuron."
  },
  {
    "objectID": "google/tfkdl_00.html#matrix-multiplication-for-a-single-layer",
    "href": "google/tfkdl_00.html#matrix-multiplication-for-a-single-layer",
    "title": "Machine-Learning-03",
    "section": "Matrix Multiplication for a Single Layer",
    "text": "Matrix Multiplication for a Single Layer\nA dense layer’s operations can be efficiently represented using matrix multiplication.\nIf X is a matrix of 100 images (each flattened to 784 pixels), and W is the weight matrix (784 inputs x 10 outputs), then:\n\\[ \\text{Weighted Sums} = X \\cdot W \\]\n\\[ \\text{Output} = \\text{Activation}(X \\cdot W + b) \\]\nWhere b is the bias vector (10 elements), broadcasted across the 100 images."
  },
  {
    "objectID": "google/tfkdl_00.html#matrix-multiplication-for-a-single-layer-1",
    "href": "google/tfkdl_00.html#matrix-multiplication-for-a-single-layer-1",
    "title": "Machine-Learning-03",
    "section": "Matrix Multiplication for a Single Layer",
    "text": "Matrix Multiplication for a Single Layer\n\nmatmul.gifIn Keras, this is simplified:tf.keras.layers.Dense(10, activation='softmax')\n\nThe magic of linear algebra allows us to express thousands, or even millions, of neuron calculations as simple matrix multiplications. This is incredibly efficient for modern hardware like GPUs, which are optimized for these operations. X would be a 100x784 matrix, and W would be a 784x10 matrix. The product X \\cdot W would then be a 100x10 matrix, where each row represents the 10 neuron outputs for one of the 100 images."
  },
  {
    "objectID": "google/tfkdl_00.html#going-deep-chaining-layers",
    "href": "google/tfkdl_00.html#going-deep-chaining-layers",
    "title": "Machine-Learning-03",
    "section": "Going Deep: Chaining Layers",
    "text": "Going Deep: Chaining Layers\n“Deep learning” refers to using multiple hidden layers. Each layer computes weighted sums of the outputs of the previous layer.\nThis architecture allows the network to learn progressively more complex and abstract features from the raw input data.\nFor example, early layers might detect edges or simple shapes, while later layers combine these to recognize parts of objects or entire objects.\nThe choice of activation function is critical and typically changes only for the very last layer in a classifier."
  },
  {
    "objectID": "google/tfkdl_00.html#going-deep-chaining-layers-1",
    "href": "google/tfkdl_00.html#going-deep-chaining-layers-1",
    "title": "Machine-Learning-03",
    "section": "Going Deep: Chaining Layers",
    "text": "Going Deep: Chaining Layers\n\nfba0638cc213a29.png\n“Going deep” is like adding more stages to a complex signal processing pipeline, where each stage refines the information from the previous one. In image processing, the first layer might learn to identify basic lines or corners, while the second layer might combine these to detect ears or eyes, and final layers assemble these into a full face recognition. This hierarchical learning is a key advantage of deep networks."
  },
  {
    "objectID": "google/tfkdl_00.html#activation-functions-relu-and-softmax",
    "href": "google/tfkdl_00.html#activation-functions-relu-and-softmax",
    "title": "Machine-Learning-03",
    "section": "Activation Functions: ReLU and Softmax",
    "text": "Activation Functions: ReLU and Softmax\nActivation functions introduce non-linearity, allowing neural networks to learn complex, non-linear relationships in data.\n\n\nSigmoid\n\nThe most classical\nUsed on intermediate layers\n\nRectified Linear Unit (ReLU)\n\\(f(x) = \\max(0, x)\\)\n\nMost popular activation for hidden layers.\nSimple and computationally efficient.\nHelps prevent vanishing gradients.\n\nSoftmax\n\\[ \\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}} \\]\n\nUsed in the output layer of multi-class classifiers.\nConverts logits into probabilities that sum to 1.\n\n\n\n\n\nsigmoid.png\n\n\n\n\n\nrelu.png\n\n\n\n\nNon-linearity is crucial. Without it, stacking layers would just result in another linear transformation, essentially a single layer. ReLU is like a simple switch: if the input is positive, it passes it; otherwise, it stops it. Softmax is a smart way to get probabilities out of our network for classification. For instance, if you have ten possible digit classes, Softmax ensures that the network’s output is a set of ten probabilities that add up to one, indicating the likelihood of the input belonging to each class."
  },
  {
    "objectID": "google/tfkdl_00.html#softmax-in-action-interactive-example",
    "href": "google/tfkdl_00.html#softmax-in-action-interactive-example",
    "title": "Machine-Learning-03",
    "section": "Softmax in Action: Interactive Example",
    "text": "Softmax in Action: Interactive Example\nAdjust the Logit Value for a single class and observe how Softmax normalizes probabilities. Here, we simulate 10 classes, with one Logit Value adjusted at a time.\n\nviewof logit_val = Inputs.range([0, 10], {step: 0.1, value: 5, label: \"Logit Value for Class 0 (others fixed at 1.0)\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive example demonstrates the power of Softmax. Notice how even a small increase in one logit value can significantly boost its corresponding probability, while simultaneously reducing the probabilities of other classes to ensure their sum remains 1. This is critical for making clear, probabilistic predictions in classification tasks."
  },
  {
    "objectID": "google/tfkdl_00.html#loss-function-cross-entropy",
    "href": "google/tfkdl_00.html#loss-function-cross-entropy",
    "title": "Machine-Learning-03",
    "section": "Loss Function: Cross-Entropy",
    "text": "Loss Function: Cross-Entropy\nTo train a neural network, we need to measure how “wrong” its predictions are compared to the true labels. This measure is called the loss function.\nFor multi-class classification, Cross-Entropy Loss is the standard.\n\\[ H(p, q) = - \\sum_{i=1}^{K} p_i \\log(q_i) \\]\nWhere: * \\(p_i\\) is the true probability for class \\(i\\) (1 for the correct class, 0 otherwise). * \\(q_i\\) is the predicted probability for class \\(i\\) (output of softmax). * \\(K\\) is the number of classes.\n\n\n\n\n\n\nNote\n\n\nCross-entropy loss heavily penalizes incorrect high-confidence predictions, guiding the network to both be correct and confident."
  },
  {
    "objectID": "google/tfkdl_00.html#loss-function-cross-entropy-1",
    "href": "google/tfkdl_00.html#loss-function-cross-entropy-1",
    "title": "Machine-Learning-03",
    "section": "Loss Function: Cross-Entropy",
    "text": "Loss Function: Cross-Entropy\n\ncross_entropy.png\nThe loss function is our feedback mechanism. It tells us how far off our network’s predictions are from the ground truth. Cross-entropy is particularly effective because it’s not just about getting the right answer; it’s also about how confident the network is when it’s right or wrong. Misclassifying an image with high confidence leads to a very large penalty, forcing the network to learn better."
  },
  {
    "objectID": "google/tfkdl_00.html#training-gradient-descent",
    "href": "google/tfkdl_00.html#training-gradient-descent",
    "title": "Machine-Learning-03",
    "section": "Training: Gradient Descent",
    "text": "Training: Gradient Descent\n“Training” a neural network means iteratively adjusting its weights and biases to minimize the loss function. This is achieved using an optimization algorithm called Gradient Descent.\n\nCompute Gradient: Calculate the partial derivatives of the loss function with respect to every weight and bias. This “gradient” vector points in the direction of the steepest increase of the loss.\nUpdate Parameters: Adjust weights and biases in the opposite direction of the gradient, typically by a small step size called the learning rate.\n\n\\[ W_{new} = W_{old} - \\alpha \\frac{\\partial L}{\\partial W} \\\\ b_{new} = b_{old} - \\alpha \\frac{\\partial L}{\\partial b} \\]\nThis process is repeated over many epochs (passes through the entire dataset)."
  },
  {
    "objectID": "google/tfkdl_00.html#training-gradient-descent-1",
    "href": "google/tfkdl_00.html#training-gradient-descent-1",
    "title": "Machine-Learning-03",
    "section": "Training: Gradient Descent",
    "text": "Training: Gradient Descent\n\ngradient_descent.png\nGradient descent is like a guided search. Imagine you’re blindfolded on a mountainous landscape (the loss function surface) and you want to find the lowest point. You feel the slope around you (compute the gradient) and take a small step downhill. You repeat this until you reach a valley. The ‘learning rate’ determines how big a step you take. Too large, and you might overshoot the minimum; too small, and it takes too long to converge."
  },
  {
    "objectID": "google/tfkdl_00.html#mini-batching-and-momentum",
    "href": "google/tfkdl_00.html#mini-batching-and-momentum",
    "title": "Machine-Learning-03",
    "section": "Mini-Batching and Momentum",
    "text": "Mini-Batching and Momentum\nTo improve training efficiency and stability:\nMini-Batching\n\nInstead of computing the gradient for one image at a time, we use a batch (e.g., 32, 64, or 128 images).\nProvides a more stable and representative gradient estimate.\nLeverages highly optimized matrix operations on GPUs/TPUs.\n\nMomentum\n\nAdds a fraction of the previous update vector to the current update.\nHelps overcome local minima, saddle points, and speeds up convergence in relevant directions."
  },
  {
    "objectID": "google/tfkdl_00.html#mini-batching-and-momentum-1",
    "href": "google/tfkdl_00.html#mini-batching-and-momentum-1",
    "title": "Machine-Learning-03",
    "section": "Mini-Batching and Momentum",
    "text": "Mini-Batching and Momentum\n\nsaddle_point.pngIllustration: A saddle point, where the gradient is zero but not a true minimum in all directions.\n\nMini-batching is a practical optimization. Imagine getting directions from one person versus getting an averaged direction from a group of people; the latter is usually more reliable. Momentum, on the other hand, gives the optimization process inertia. It prevents the optimizer from getting stuck in “saddle points”—flat regions in the loss landscape that are not true minima. This is crucial for navigating high-dimensional spaces effectively."
  },
  {
    "objectID": "google/tfkdl_00.html#glossary-of-key-terms",
    "href": "google/tfkdl_00.html#glossary-of-key-terms",
    "title": "Machine-Learning-03",
    "section": "Glossary of Key Terms",
    "text": "Glossary of Key Terms\n\nBatch/Mini-batch: A subset of the training data used in one iteration of gradient descent.\nCross-Entropy Loss: A common loss function for classification tasks, measuring dissimilarity between predicted and true probability distributions.\nDense Layer: A layer where each neuron is connected to every neuron in the preceding layer.\nFeatures: The input attributes or data points fed into a neural network.\nLabels: The correct outputs or target values in supervised learning.\nLearning Rate: A hyperparameter controlling the step size during gradient descent.\nLogits: The raw, unnormalized outputs of a neural network layer before being passed through an activation function like softmax.\nLoss Function: A function that quantifies the error between predicted outputs and true labels.\nNeuron: The fundamental computational unit of a neural network.\nOne-Hot Encoding: A categorical variable representation where each category is a binary vector (e.g., [0,0,1,0] for class 3 of 4).\nReLU (Rectified Linear Unit): A popular activation function, \\(f(x) = \\max(0, x)\\).\nSigmoid: An S-shaped activation function, \\(f(x) = 1 / (1 + e^{-x})\\).\nSoftmax: An activation function that converts a vector of numbers into a probability distribution.\nTensor: A generalization of vectors and matrices to an arbitrary number of dimensions.\n\n\nThis glossary provides a quick reference for the key terms we’ve covered. Understanding these terms is foundational for further study and application of machine learning in ECE."
  },
  {
    "objectID": "google/tfkdl_00.html#diving-into-the-code-part-1",
    "href": "google/tfkdl_00.html#diving-into-the-code-part-1",
    "title": "Machine-Learning-03",
    "section": "Diving into the Code (Part 1)",
    "text": "Diving into the Code (Part 1)\nNow, let’s dissect the code from the keras_01_mnist.ipynb notebook. Understanding each section is key to building and modifying models.\n\n\n\n\n\n\nNote\n\n\nFollow along in the Colab notebook if you can!\n\n\n\nWe’ll cover core components: - Model Parameters and Imports - Data Preparation with tf.data.Dataset - Building a Keras Sequential Model - Training and Validation - Visualizing Predictions"
  },
  {
    "objectID": "google/tfkdl_00.html#model-parameters-and-imports",
    "href": "google/tfkdl_00.html#model-parameters-and-imports",
    "title": "Machine-Learning-03",
    "section": "Model Parameters and Imports",
    "text": "Model Parameters and Imports\nThese initial cells set up the environment and define global constants.\n\n\nParameters Cell:\nSets values for:\n\nBATCH_SIZE: Number of samples processed per gradient update.\nEPOCHS: Number of complete passes through the training dataset.\nGCS_PATTERN: Location of MNIST data files on Google Cloud Storage.\n\nImports Cell:\nImports necessary libraries:\n\ntensorflow (tf): Core Deep Learning framework.\nnumpy (np): For numerical operations (especially tensor manipulation).\nmatplotlib.pyplot (plt): For plotting and visualization.\n\n\n# Example of Parameters\nBATCH_SIZE = 64\nEPOCHS = 5\nGCS_PATTERN = \"gs://cloud-tpu-datasets/mnist/mnist_{}.tfrec\"\n\nprint(f\"Batch Size: {BATCH_SIZE}\")\nprint(f\"Epochs: {EPOCHS}\")\n# Example of Imports\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"NumPy version:\", np.__version__)\n\n\nThe first crucial step in any machine learning project is to set up our environment and define key hyper-parameters.\nThe “Parameters” cell defines values like BATCH_SIZE, which controls how many training examples are processed together before the model’s weights are updated. EPOCHS determines how many full iterations the model makes over the entire training dataset. The GCS_PATTERN indicates where our MNIST dataset is stored, in this case, on Google Cloud Storage.\nThe “Imports” cell brings in all the necessary Python libraries. tensorflow is our backbone for deep learning. numpy is essential for numerical operations and efficient array manipulation, which is fundamental to working with tensors. Lastly, matplotlib.pyplot is indispensable for visualizing our data, training progress, and results."
  },
  {
    "objectID": "google/tfkdl_00.html#data-preparation-with-tf.data.dataset",
    "href": "google/tfkdl_00.html#data-preparation-with-tf.data.dataset",
    "title": "Machine-Learning-03",
    "section": "Data Preparation with tf.data.Dataset",
    "text": "Data Preparation with tf.data.Dataset\nThe tf.data.Dataset API is powerful for building efficient data pipelines. It handles loading, parsing, and preprocessing data, especially at scale.\nKey Steps:\n\nLoad Fixed-Length Records: Images and labels are stored in tfrec files. We decode raw byte strings into images (float32, normalized 0-1) and flatten them.\nimagedataset = tf.data.FixedLengthRecordDataset(image_filename, 28*28, header_bytes=16)\nread_image Function: Parses byte strings into float32, normalizes pixels (0-1), and reshapes to [28*28] (flattened for initial dense layer).\ndef read_image(tf_bytestring):\n    image = tf.io.decode_raw(tf_bytestring, tf.uint8)\n    image = tf.cast(image, tf.float32)/256.0\n    image = tf.reshape(image, [28*28])\n    return image\n\n\nPreparing data efficiently is as important as designing the neural network itself. TensorFlow’s tf.data.Dataset API is specifically designed for this, allowing us to build highly optimized input pipelines.\nThe MNIST dataset images and labels are stored in a specialized format known as tfrec (TFRecord) files. The FixedLengthRecordDataset is used to read these files, assuming each record (image) has a fixed length. We also skip a header of 16 bytes.\nThe read_image function is crucial. Raw image data comes as bytes (tf.uint8). We cast this to tf.float32 because neural networks generally prefer floating-point inputs. We then normalize the pixel values from the [0, 255] range to [0, 1] by dividing by 256. Finally, we reshape the 2D 28x28 image into a 1D vector of 28*28 = 784 pixels because our initial single-layer model expects a flattened input."
  },
  {
    "objectID": "google/tfkdl_00.html#data-preparation-cont.-pipeline-operations",
    "href": "google/tfkdl_00.html#data-preparation-cont.-pipeline-operations",
    "title": "Machine-Learning-03",
    "section": "Data Preparation (Cont.): Pipeline Operations",
    "text": "Data Preparation (Cont.): Pipeline Operations\nAfter parsing, we apply various transformations to optimize the dataset for training:\n1. Mapping & Zipping:\n\nApply read_image to all images using .map().\nDo similar steps for labels.\nCombine images and labels using .zip(): dataset = tf.data.Dataset.zip((imagedataset, labelsdataset))\n\n2. Optimizations:\n\n.cache(): Store dataset in RAM for faster epoch transitions (for small datasets).\n.shuffle(buffer_size): Randomize training order to prevent batch order biases.\n.repeat(): Loop the dataset indefinitely for multiple epochs.\n.batch(batch_size): Group samples into mini-batches for efficient processing.\n.prefetch(tf.data.experimental.AUTOTUNE): Overlap data preprocessing and model execution to keep the GPU busy.\n\n\nOnce individual images and labels are read and decoded, we build the full data pipeline using a sequence of tf.data.Dataset operations.\nFirst, we use .map() to apply our read_image function across the entire image dataset. We perform analogous steps for the labels, then use .zip() to pair each image with its corresponding label, forming a dataset of (image, label) pairs.\nNext come the crucial optimizations for training:\n\n.cache() stores the prepared dataset in RAM after the first epoch, eliminating redundant processing in subsequent epochs, which is excellent for smaller datasets like MNIST.\n.shuffle() randomizes the order of examples within a specified buffer. This prevents the model from learning spurious patterns related to the order of appearance in the dataset.\n.repeat() ensures that the dataset loops indefinitely, so we can train for multiple epochs without explicitly re-initializing the data pipeline.\n.batch() groups individual examples into mini-batches. This is fundamental for efficient training, as gradient updates are typically computed over batches, not individual examples.\n.prefetch(tf.data.experimental.AUTOTUNE) is a performance booster. It allows the data pipeline to prepare the next batch of data on the CPU while the current batch is being processed by the GPU, maximizing hardware utilization."
  },
  {
    "objectID": "google/tfkdl_00.html#building-a-keras-sequential-model-the-1-layer-network",
    "href": "google/tfkdl_00.html#building-a-keras-sequential-model-the-1-layer-network",
    "title": "Machine-Learning-03",
    "section": "Building a Keras Sequential Model: The 1-Layer Network",
    "text": "Building a Keras Sequential Model: The 1-Layer Network\nOur first model is a simple, single-layer dense neural network.\n\n\nModel Definition:\n\nWe use tf.keras.Sequential for a linear stack of layers.\ntf.keras.layers.Input(shape=(28*28,)): Defines the input shape (784-element flattened vector).\ntf.keras.layers.Dense(10, activation='softmax'):\n\n10 neurons: One for each digit class (0-9).\n'softmax' activation: Outputs a probability distribution over the 10 classes, summing to 1. (The highest probability indicates the predicted class.)\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nInput\n\nInput Layer\n(Shape: 784)\n\n\n\nDense\n\nDense Layer\n(10 neurons, Softmax)\n\n\n\nInput-&gt;Dense\n\n\n\n\n\nOutput\n\nOutput\n(10 probabilities)\n\n\n\nDense-&gt;Output\n\n\n\n\n\n\n\n\n\n\n\n\nNow, let’s get to the heart of the neural network: the model itself. Keras provides a very intuitive API for building models. For simple, layer-by-layer architectures, tf.keras.Sequential is perfect.\nOur first model is remarkably simple:\n\nInput Layer: We explicitly define the input shape using tf.keras.layers.Input. Since we flattened our 28x28 images, the input to this layer is a 1D vector of 784 pixels.\nDense Layer: This is the core of our model. A “Dense” layer (also known as a fully connected layer) means every neuron in this layer is connected to every neuron in the previous layer.\n\nIt has 10 neurons, one for each output class (digits 0 through 9).\nThe activation='softmax' is critical for classification tasks. Softmax converts the raw output of the neurons into a probability distribution, ensuring that all 10 output values are positive and sum up to 1. The class with the highest probability is our model’s prediction."
  },
  {
    "objectID": "google/tfkdl_00.html#compiling-the-model",
    "href": "google/tfkdl_00.html#compiling-the-model",
    "title": "Machine-Learning-03",
    "section": "Compiling the Model",
    "text": "Compiling the Model\nBefore training, the model needs to be compiled. Compilation configures key aspects of the training process.\nmodel.compile(optimizer='sgd',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\noptimizer='sgd' (Stochastic Gradient Descent): The algorithm used to update the model’s weights based on the loss. SGD is a foundational optimizer for neural networks.\nloss='categorical_crossentropy': The loss function measures the discrepancy between predicted and true class probabilities. Categorical crossentropy is standard for multi-class classification when labels are one-hot encoded.\nmetrics=['accuracy']: Additional metrics to monitor during training and evaluation. 'accuracy' measures the percentage of correct predictions.\n\n\nCompiling the model is like setting up the engine and navigation system before going on a trip. It’s where we define how the model will learn.\n\nOptimizer: The optimizer is the algorithm that adjusts the weights of the neural network during training to minimize the loss function. ‘SGD’, or Stochastic Gradient Descent, is the simplest and a very common optimizer. It makes small, iterative adjustments to the weights based on the gradient of the loss function with respect to the weights.\nLoss Function: The loss function quantifies how ‘bad’ our model’s predictions are compared to the true labels. For multi-class classification problems like MNIST where each input belongs to exactly one class out of many, categorical_crossentropy is the standard choice. It penalizes incorrect predictions more heavily when the model was very confident in its wrong answer.\nMetrics: Metrics are used to monitor the training and validation process. While the loss function guides the optimization, metrics provide a more human-interpretable measure of performance. ‘Accuracy’ is intuitive: it’s the proportion of correctly classified examples."
  },
  {
    "objectID": "google/tfkdl_00.html#model-summary-training-utility",
    "href": "google/tfkdl_00.html#model-summary-training-utility",
    "title": "Machine-Learning-03",
    "section": "Model Summary & Training Utility",
    "text": "Model Summary & Training Utility\nAfter compilation, we can inspect the model’s architecture.\n\n\nmodel.summary():\nPrints a detailed overview of the model:\n\nLayers (type, output shape).\nNumber of trainable parameters in each layer.\nTotal parameters in the model.\n\nThis is invaluable for debugging and understanding model complexity.\nPlotTraining Callback:\nA custom utility (from the notebook) to visualize training curves dynamically. It shows loss and accuracy for both training and validation sets in real-time.\n\nimport tensorflow as tf\n# Define a simple model for demonstration\nmodel_summary_demo = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(28*28,)), # For MNIST, inputs are 784-element vectors\n    tf.keras.layers.Dense(10, activation='softmax') # 10 output classes\n])\n\n# Simulate compile for summary to show expected parameters\nmodel_summary_demo.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Print the model summary\nmodel_summary_demo.summary()\n\n\nBefore we start training, it’s good practice to inspect the model’s structure.\nmodel.summary() is an extremely useful Keras utility. It outputs a table showing each layer, its output shape, and the number of parameters it has.\n\nOutput Shape: This tells us the shape of the tensor outputted by each layer. For our Dense layer with 10 neurons, the output shape is (None, 10), where None indicates the batch size, which can vary.\nTrainable parameters: For the Dense layer, this is (input_features + 1) * output_neurons. In our case (784 + 1) * 10 = 7850. The +1 accounts for the bias term for each neuron.\n\nThe PlotTraining callback, though custom, is typical of utilities used to provide visual feedback during training. It helps identify issues like overfitting early on."
  },
  {
    "objectID": "google/tfkdl_00.html#training-and-validation",
    "href": "google/tfkdl_00.html#training-and-validation",
    "title": "Machine-Learning-03",
    "section": "Training and Validation",
    "text": "Training and Validation\nThe model.fit() function is where the actual learning takes place.\nmodel.fit(training_dataset,\n          steps_per_epoch=steps_per_epoch,\n          epochs=EPOCHS,\n          validation_data=validation_dataset,\n          validation_steps=1,\n          callbacks=[plot_training])\n\ntraining_dataset: The preprocessed dataset used for learning.\nsteps_per_epoch: Number of batches per epoch (total training samples / batch size).\nepochs: Total number of times the model iterates over the entire training data.\nvalidation_data: The unseen dataset used to evaluate performance after each epoch.\nvalidation_steps: Number of batches from validation_data to run per validation round.\ncallbacks: List of custom functions executed at various stages of training (e.g., plot_training).\n\n\nWith the model defined and compiled, we’re ready for the most exciting part: training! This is done using the model.fit() method.\n\ntraining_dataset: This is our tf.data.Dataset pipeline for the training data.\nsteps_per_epoch: Since we used dataset.repeat(), the data pipeline could theoretically run forever. steps_per_epoch tells Keras how many batches constitute one “epoch” or one full pass through our conceptual dataset. It is typically calculated as total_training_samples / BATCH_SIZE.\nepochs: As defined in our parameters, this is the total number of epochs the model will train for.\nvalidation_data: Here, we pass our tf.data.Dataset pipeline for the validation data. Keras automatically evaluates the model on this data at the end of each epoch.\nvalidation_steps: Similar to steps_per_epoch, this specifies how many validation batches to process during each validation run.\ncallbacks: This is a list where we can include custom functions that execute during training. Our plot_training callback is a great example, providing real-time visualization of metrics."
  },
  {
    "objectID": "google/tfkdl_00.html#visualizing-predictions",
    "href": "google/tfkdl_00.html#visualizing-predictions",
    "title": "Machine-Learning-03",
    "section": "Visualizing Predictions",
    "text": "Visualizing Predictions\nAfter training, we use model.predict() to evaluate the model on new inputs.\nprobabilities = model.predict(font_digits, steps=1)\npredicted_labels = np.argmax(probabilities, axis=1)\n\nmodel.predict(input_data): Generates output predictions for the input samples. For a classification model with softmax activation, it returns a 2D array where each row is a probability distribution over the classes for one input. (e.g., [[0.01, 0.05, ..., 0.90, ..., 0.02], ...])\nnp.argmax(probabilities, axis=1): Converts the probability distributions into a single predicted class label.\n\nnp.argmax(): Returns the index of the maximum value.\naxis=1: Specifies to find the maximum along the “class” dimension (i.e., for each image, find the class with the highest probability)."
  },
  {
    "objectID": "google/tfkdl_00.html#visualizing-predictions-1",
    "href": "google/tfkdl_00.html#visualizing-predictions-1",
    "title": "Machine-Learning-03",
    "section": "Visualizing Predictions",
    "text": "Visualizing Predictions\n\n\n\n\n\n\n\nNote\n\n\nThis simple 1-layer model already achieves ~90% accuracy! But we can do much better."
  },
  {
    "objectID": "google/tfkdl_00.html#adding-layers-going-deeper",
    "href": "google/tfkdl_00.html#adding-layers-going-deeper",
    "title": "Machine-Learning-03",
    "section": "Adding Layers: Going Deeper",
    "text": "Adding Layers: Going Deeper\nTo improve our model’s accuracy beyond 90%, we need to add more layers. This allows the network to learn more complex, hierarchical features."
  },
  {
    "objectID": "google/tfkdl_00.html#the-concept-of-depth",
    "href": "google/tfkdl_00.html#the-concept-of-depth",
    "title": "Machine-Learning-03",
    "section": "The Concept of Depth",
    "text": "The Concept of Depth\n\nA deeper network can model non-linear relationships more effectively.\nEach hidden layer learns increasingly abstract representations of the input data.\n\n\nOur single-layer neural network achieved about 90% accuracy, which is a good baseline, but not yet sufficient for many real-world ECE applications. To push this accuracy higher, the most intuitive next step is to make our neural network deeper by adding more layers.\nWhy go deeper? Deep networks are powerful because they can learn hierarchies of features. For image recognition, the first layer might learn simple edges or lines. A second layer might combine these to recognize shapes like circles or corners. A third layer might combine shapes to recognize parts of digits, and so on. This hierarchical learning allows deep networks to model very complex, non-linear relationships in the data, far beyond what a single layer can achieve."
  },
  {
    "objectID": "google/tfkdl_00.html#activation-functions-revisited",
    "href": "google/tfkdl_00.html#activation-functions-revisited",
    "title": "Machine-Learning-03",
    "section": "Activation Functions Revisited",
    "text": "Activation Functions Revisited\nWhile softmax is for the output layer of a classifier, hidden layers need different activation functions."
  },
  {
    "objectID": "google/tfkdl_00.html#sigmoid-activation-function",
    "href": "google/tfkdl_00.html#sigmoid-activation-function",
    "title": "Machine-Learning-03",
    "section": "Sigmoid Activation Function",
    "text": "Sigmoid Activation Function\nFor intermediate (hidden) layers, the sigmoid function is a classical choice:\n\\[ \\sigma(x) = \\frac{1}{1 + e^{-x}} \\]\n\n\n\nOutput Range: Maps any input to a value between 0 and 1.\nInterpretation: Can be seen as a “soft” switch, where values close to 0 or 1 indicate strong decisions.\nHistorical Significance: Widely used in early neural networks.\n\n\n\nPlot of the Sigmoid Function\n\n\nWhen we add more layers, we introduce hidden layers between the input and output. Each neuron in these hidden layers needs an activation function to introduce non-linearity into the network. Without non-linearity, no matter how many layers you add, the neural network would only be able to learn linear relationships, limiting its power.\nWhile softmax is perfect for the final classification layer, where we need probabilities for multiple classes, sigmoid is a classic choice for hidden layers. The sigmoid function squashes any input value into a range between 0 and 1. This can be interpreted as the “activation” strength of a neuron. Historically, sigmoid was one of the most popular activation functions, but we will soon see more effective alternatives for deeper networks."
  },
  {
    "objectID": "google/tfkdl_00.html#designing-a-deeper-model",
    "href": "google/tfkdl_00.html#designing-a-deeper-model",
    "title": "Machine-Learning-03",
    "section": "Designing a Deeper Model",
    "text": "Designing a Deeper Model\nLet’s expand our simple model by adding two hidden Dense layers with sigmoid activation.\n\n\nmodel = tf.keras.Sequential(\n    [\n        tf.keras.layers.Input(shape=(28*28,)),        # Input Layer\n        tf.keras.layers.Dense(200, activation='sigmoid'), # Hidden Layer 1\n        tf.keras.layers.Dense(60, activation='sigmoid'),  # Hidden Layer 2\n        tf.keras.layers.Dense(10, activation='softmax')   # Output Layer\n    ]\n)\n# (Compilation and training code follows)\n\nHidden Layer 1: 200 neurons with sigmoid activation.\nHidden Layer 2: 60 neurons with sigmoid activation.\nOutput Layer: Remains 10 neurons with softmax for classification.\n\n\n\n\n\n\n\n\n\nG\n\n\n\nInput\n\nInput Layer\n(Shape: 784)\n\n\n\nDense1\n\nDense Layer 1\n(200 neurons, Sigmoid)\n\n\n\nInput-&gt;Dense1\n\n\n\n\n\nDense2\n\nDense Layer 2\n(60 neurons, Sigmoid)\n\n\n\nDense1-&gt;Dense2\n\n\n\n\n\nOutput\n\nOutput Layer\n(10 neurons, Softmax)\n\n\n\nDense2-&gt;Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nNotice the increase in the number of parameters with multiple layers. Run this model in the Colab notebook.\n\n\n\n\nHere’s how we’d implement a deeper network in Keras. We simply add more tf.keras.layers.Dense components to our Sequential model.\n\nWe start with our Input layer, still expecting the flattened 784-pixel vector.\nThen we add tf.keras.layers.Dense(200, activation='sigmoid'). This layer has 200 neurons, processing the 784 inputs and producing 200 outputs. The sigmoid activation squashes these outputs between 0 and 1.\nNext, another hidden layer: tf.keras.layers.Dense(60, activation='sigmoid'). This layer takes the 200 outputs from the previous layer as its input, processes them with 60 neurons, and outputs 60 values.\nFinally, our output layer remains the same: tf.keras.layers.Dense(10, activation='softmax'), producing the 10 probabilities for digit classification.\n\nBy running model.summary() again, you’ll see a significant increase in the total number of trainable parameters. This increased capacity should, in theory, allow the a model to learn more intricate patterns."
  },
  {
    "objectID": "google/tfkdl_00.html#unexpected-behavior-what-happened",
    "href": "google/tfkdl_00.html#unexpected-behavior-what-happened",
    "title": "Machine-Learning-03",
    "section": "Unexpected Behavior: What Happened?",
    "text": "Unexpected Behavior: What Happened?\nDespite adding layers and parameters, the model doesn’t always improve as expected.\n\n\nHigh Loss: The training loss and validation loss are extremely high.\nLow Accuracy: Accuracy barely increases above random guessing (around 10%).\n\n\n\n\n\n\n\nWarning\n\n\nMore parameters don’t automatically mean better performance. Deeper networks introduce new challenges!\n\n\n\n\nAfter running the deeper model in the Colab notebook, you might observe a surprising and discouraging result: the model performs worse than our single-layer network. The training and validation loss remain prohibitively high, and the accuracy barely moves beyond 10%, which is what you’d expect from random guessing among 10 classes.\nThis outcome clearly illustrates an important point in deep learning: simply adding more layers and parameters isn’t a silver bullet. Deeper networks come with their own set of challenges. This particular issue with sigmoid activations in deep networks is a classic problem known as the vanishing gradient problem, which we will discuss next. The sigmoid function, especially for inputs far from zero, has very small gradients. In a deep network, these small gradients are multiplied together as they propagate backward through the layers, causing them to “vanish” by the time they reach earlier layers. This prevents the weights in those earlier layers from being updated effectively, stalling learning."
  },
  {
    "objectID": "google/tfkdl_00.html#why-did-the-deeper-model-fail-the-vanishing-gradient-problem",
    "href": "google/tfkdl_00.html#why-did-the-deeper-model-fail-the-vanishing-gradient-problem",
    "title": "Machine-Learning-03",
    "section": "Why Did the Deeper Model Fail? The Vanishing Gradient Problem",
    "text": "Why Did the Deeper Model Fail? The Vanishing Gradient Problem\nThe sigmoid activation function can hinder learning in deep networks.\nThe Problem:\n\nThe gradient (derivative) of the sigmoid function is very small for inputs far from 0.\nIn a deep network, these small gradients are multiplied together during backpropagation.\nThis causes gradients to “vanish” as they propagate back to earlier layers.\n\nConsequence for ECE:\n\nEarly layers’ weights are hardly updated.\nThe network struggles to learn useful features from the input.\nTraining stalls, leading to poor performance.\n\n\n\n\n\n\n\nImportant\n\n\nThis is a common issue with traditional activation functions like sigmoid and tanh in deep architectures."
  },
  {
    "objectID": "google/tfkdl_00.html#special-care-for-deep-networks",
    "href": "google/tfkdl_00.html#special-care-for-deep-networks",
    "title": "Machine-Learning-03",
    "section": "Special Care for Deep Networks",
    "text": "Special Care for Deep Networks\nThe “AI winter” of the 80s and 90s was partly due to the challenges of training deep networks. Modern deep learning thrives due to “dirty tricks” that ensure convergence."
  },
  {
    "objectID": "google/tfkdl_00.html#overcoming-deep-network-challenges",
    "href": "google/tfkdl_00.html#overcoming-deep-network-challenges",
    "title": "Machine-Learning-03",
    "section": "Overcoming Deep Network Challenges",
    "text": "Overcoming Deep Network Challenges\n\nRELU Activation: A simple yet powerful non-linearity.\nBetter Optimizers: Algorithms that navigate complex loss landscapes.\nCareful Initialization: Setting initial weights to facilitate learning.\nNumerical Stability: Ensuring calculations don’t crash.\n\n\nAs we saw, simply adding more layers to a network with sigmoid activations often leads to poor performance, a problem that significantly contributed to the “AI winter.” However, thanks to a few crucial mathematical and algorithmic advancements, deep neural networks, even with 20, 50, or hundreds of layers, can now be trained effectively. These “dirty tricks,” as the source calls them, address the fundamental challenges of deep network convergence and have been instrumental in the recent resurgence of deep learning. We’ll explore these techniques now."
  },
  {
    "objectID": "google/tfkdl_00.html#relu-activation-the-modern-choice",
    "href": "google/tfkdl_00.html#relu-activation-the-modern-choice",
    "title": "Machine-Learning-03",
    "section": "RELU Activation: The Modern Choice",
    "text": "RELU Activation: The Modern Choice\nThe sigmoid function’s vanishing gradients made it problematic for deep networks. The Rectified Linear Unit (RELU) is the de-facto standard activation today.\n\\[ \\text{ReLU}(x) = \\max(0, x) \\]\n\n\n\nSimplicity: Returns x for positive inputs, 0 for negative inputs.\nGradient: Has a constant gradient of 1 for positive inputs.\nBenefits for ECE:\n\nMitigates vanishing gradient problem.\nSpeeds up convergence.\nComputationally much cheaper than sigmoid/tanh.\n\n\n\n\nPlot of the ReLU Function\n\n\n\n\n\n\n\nNote\n\n\nReplace activation='sigmoid' with activation='relu' in hidden layers. The output layer retains softmax for classification.\n\n\n\n\nThe sigmoid activation function was historically significant, but its propensity to squash values between 0 and 1 resulted in very small gradients, leading to the vanishing gradient problem in deep networks. The solution that revolutionized deep learning is surprisingly simple: the Rectified Linear Unit, or ReLU.\nReLU(x) simply outputs x if x is positive, and 0 if x is negative.\n\nSimplicity: This piece-wise linear function is incredibly simple to compute.\nGradient: Crucially, its gradient is 1 for positive inputs and 0 for negative inputs. This 1 gradient for a large portion of its domain means that gradients can flow much more effectively through the network without vanishing, thereby addressing the core problem. This allows deeper networks to learn much faster and more effectively.\nComputational Efficiency: ReLU is also much faster to compute than exponentials (in sigmoid) or hyperbolic tangents, which speeds up training significantly.\n\nFor our ECE applications where efficiency and convergence are critical, ReLU is the go-to choice for hidden layers. Remember to keep softmax for the final output layer in classification tasks."
  },
  {
    "objectID": "google/tfkdl_00.html#better-optimizers-beyond-sgd",
    "href": "google/tfkdl_00.html#better-optimizers-beyond-sgd",
    "title": "Machine-Learning-03",
    "section": "Better Optimizers: Beyond SGD",
    "text": "Better Optimizers: Beyond SGD\nStochastic Gradient Descent (SGD) can get stuck in “saddle points” in high-dimensional spaces.\nModern optimizers are more robust and efficient.\nSaddle Points:\n\nPoints in the loss landscape where the gradient is zero, but it’s not a true minimum.\nSGD can get stuck here, preventing further learning.\n\nAdaptive Optimizers:\n\nUse concepts like “momentum” and “adaptive learning rates” for each parameter.\nHelp the model “sail past” saddle points and converge faster.\nExamples: Adam, RMSprop, Adagrad.\n\nKeras Implementation:\nUpdate the optimizer in model.compile:\nmodel.compile(optimizer='adam', # Use Adam optimizer\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n\n\n\n\n\nTip\n\n\nAdam is widely considered a good default choice for most deep learning tasks.\n\n\n\n\nWhile SGD is foundational, its simplicity can be a drawback in complex, high-dimensional loss landscapes, which are typical for deep neural networks. One significant issue is saddle points. Imagine a mountain pass where the terrain flattens out. The gradient at this point is zero, just like at a local minimum. SGD, looking only at the immediate gradient, would mistakenly think it’s reached a minimum and stop, even if there are lower points to explore.\nModern optimizers like Adam, RMSprop, and Adagrad address this by incorporating more sophisticated mechanics. They often include:\n\nMomentum: This helps the optimizer build up speed in a consistent direction and overshoot small local minima or saddle points, like a ball rolling down a hill that doesn’t just stop at the first dip.\nAdaptive Learning Rates: Instead of using a single learning rate for all parameters, these optimizers maintain a separate learning rate for each network weight, adapting it based on the historical gradients. This allows for faster learning in some dimensions and more cautious steps in others.\n\nBy simply changing our optimizer from 'sgd' to 'adam' in Keras, we leverage these advanced techniques immediately, leading to faster and more reliable convergence."
  },
  {
    "objectID": "google/tfkdl_00.html#weight-initialization-numerical-stability",
    "href": "google/tfkdl_00.html#weight-initialization-numerical-stability",
    "title": "Machine-Learning-03",
    "section": "Weight Initialization & Numerical Stability",
    "text": "Weight Initialization & Numerical Stability\nTwo critical, often hidden, factors for stable deep network training.\n1. Random Initializations:\n\nHow the network’s weights and biases are set before training begins.\nPoor initialization can lead to slow convergence or vanishing/exploding gradients.\nKeras Default: Uses 'glorot_uniform' (also known as Xavier uniform).\n\nDesigned to keep activation values and gradients roughly in the same scale across layers.\nNo action needed: Keras handles this optimally by default.\n\n\n2. Numerical Stability (NaNs):\n\nCategorical crossentropy involves log(). If input to log is 0, it’s NaN (Not a Number).\nsoftmax output (probabilities) can be numerically 0 in float32 despite being mathematically non-zero.\nKeras Solution: tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n\nComputes softmax and crossentropy together in a numerically stable way.\nNo action needed: Keras handles this automatically when softmax is the last activation and categorical_crossentropy is the loss.\n\n\n\nBeyond activations and optimizers, two more subtle but profoundly important aspects of deep learning convergence are weight initialization and numerical stability.\n\nWeight Initialization: The starting values of a neural network’s weights and biases are surprisingly critical. If they are too large, activations might saturate; if too small, gradients might vanish. The goal is to initialize weights such that the activations and gradients flowing through the network remain in a reasonable range. Keras, by default, uses clever initialization schemes like 'glorot_uniform' (also known as Xavier uniform initialization), which attempts to balance the variance of activations and gradients across layers. The good news for us is that Keras usually “does the right thing” here, so we often don’t need to specify it manually.\nNumerical Stability (NaNs): Our categorical_crossentropy loss function involves a logarithm. Mathematically, the softmax activation function produces probabilities that are strictly greater than zero (since it uses exponentials). However, in the finite precision world of float32 computer arithmetic, a very small positive number can be represented as 0. If log(0) occurs, the result is NaN (Not a Number), which crashes the training process. Keras smartly handles this by offering a version of categorical_crossentropy that optimally combines the softmax calculation with the cross-entropy loss in a numerically stable way, often internally using a from_logits=True argument when you specify 'softmax' and 'categorical_crossentropy'. This prevents those dreaded NaNs. Again, Keras typically manages this under the hood, so usually, no explicit action is required from us."
  },
  {
    "objectID": "google/tfkdl_00.html#success-so-far-97-accuracy",
    "href": "google/tfkdl_00.html#success-so-far-97-accuracy",
    "title": "Machine-Learning-03",
    "section": "Success So Far: ~97% Accuracy!",
    "text": "Success So Far: ~97% Accuracy!\nWith ReLU activation and the Adam optimizer, our deeper model should now converge effectively.\n\n\nYou should observe:\n\nTraining and validation loss decreasing steadily.\nTraining and validation accuracy climbing to around 97%.\n\nThis marks a significant improvement over the initial 90% and the failed deep sigmoid model.\nWe’re approaching our goal of “significantly above 99% accuracy!”\n\n (Example of ~97% accuracy training curves)\n\n\n\n\n\n\n\nTip\n\n\nIf you’re stuck, refer to keras_02_mnist_dense.ipynb in the Colab repo.\n\n\n\n\nAfter implementing ReLU activations in the hidden layers and switching to the Adam optimizer, you should see a dramatic improvement in your model’s performance. The training and validation curves for both loss and accuracy will now look much healthier, steadily converging. We’re now hitting around 97% accuracy, which is a major leap from our initial 90% and the failed sigmoid model. This demonstrates the critical importance of these “dirty tricks” in making deep networks viable. We’re definitely on the right track towards our ultimate goal!"
  },
  {
    "objectID": "google/tfkdl_00.html#learning-rate-decay-fine-tuning-convergence",
    "href": "google/tfkdl_00.html#learning-rate-decay-fine-tuning-convergence",
    "title": "Machine-Learning-03",
    "section": "Learning Rate Decay: Fine-Tuning Convergence",
    "text": "Learning Rate Decay: Fine-Tuning Convergence\nTraining too fast can lead to noisy convergence or even divergence. A learning rate decay schedule starts fast and slows down over time.\nThe Problem with High Learning Rates:\n\nTraining curves become noisy.\nValidation metrics jump erratically (d4fd66346d7c480e.png).\nModel might jump over optimal solutions or oscillate.\n\nThe Solution:\n\nStart with a higher learning rate to explore the loss landscape quickly.\nGradually decrease the learning rate as training progresses, allowing for finer adjustments and more stable convergence.\nOften exponential decay: \\(LR = LR_0 \\times \\text{decay_rate}^{\\text{epoch}}\\)\n\n\nEven with a powerful optimizer like Adam, simply using a fixed learning rate throughout training isn’t always optimal. Imagine searching for the lowest point in a valley. You might want to take large strides initially to cover ground quickly, but as you approach the bottom, you’d need smaller, more precise steps to find the exact lowest point without overshooting it.\nIn deep learning, the learning rate controls the step size of our optimizer. A very high learning rate can cause the model to bounce around the loss landscape without converging or even diverge. A very low learning rate can make training painfully slow.\nThe solution is learning rate decay. We start with a relatively high learning rate to make quick progress early on, and then gradually decrease it over epochs. This allows the model to fine-tune its weights as it gets closer to an optimal solution. Exponential decay is a common and effective schedule, where the learning rate shrinks by a constant factor after each epoch."
  },
  {
    "objectID": "google/tfkdl_00.html#implementing-learning-rate-decay-with-keras",
    "href": "google/tfkdl_00.html#implementing-learning-rate-decay-with-keras",
    "title": "Machine-Learning-03",
    "section": "Implementing Learning Rate Decay with Keras",
    "text": "Implementing Learning Rate Decay with Keras\nKeras makes it easy to add a learning rate scheduler using a callback.\n1. Define the Decay Function: A Python function that calculates the learning rate for a given epoch.\nimport math\ndef lr_decay(epoch):\n    return 0.01 * math.pow(0.6, epoch) # Exponential decay\nThis function starts at 0.01 and reduces it by 0.6 (60%) each epoch.\n2. Create the Callback:\nlr_decay_callback = tf.keras.callbacks.LearningRateScheduler(lr_decay, verbose=True)\nverbose=True prints the learning rate at the start of each epoch.\n3. Add to model.fit():\nInclude the lr_decay_callback in the list of callbacks.\nmodel.fit(..., callbacks=[plot_training, lr_decay_callback])"
  },
  {
    "objectID": "google/tfkdl_00.html#implementing-learning-rate-decay-with-keras-1",
    "href": "google/tfkdl_00.html#implementing-learning-rate-decay-with-keras-1",
    "title": "Machine-Learning-03",
    "section": "Implementing Learning Rate Decay with Keras",
    "text": "Implementing Learning Rate Decay with Keras\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\nThe lr_decay_callback must be added to the callbacks list for it to take effect.\n\n\n\n\nKeras provides a flexible way to implement custom learning rate schedules using the tf.keras.callbacks.LearningRateScheduler.\n\nDefine the Decay Function: First, you define a Python function (like lr_decay here) that takes the current epoch number as input and returns the desired learning rate for that epoch. Our example uses an exponential decay, starting at 0.01 and multiplying by 0.6 (a 40% reduction) at each subsequent epoch.\nCreate the Callback: You then instantiate tf.keras.callbacks.LearningRateScheduler, passing your decay function to it. Setting verbose=True is helpful for debugging, as it prints the calculated learning rate at the beginning of each epoch.\nAdd to model.fit(): The final step is to include this lr_decay_callback in the callbacks list when you call model.fit(). Keras will then automatically call your function at the start of each epoch and update the optimizer’s learning rate accordingly.\n\nThe interactive plot generated by the plot_learning_rate utility demonstrates how the learning rate will decrease over the epochs according to our defined function. This visual confirms our strategy."
  },
  {
    "objectID": "google/tfkdl_00.html#impact-of-learning-rate-decay-cleaner-convergence",
    "href": "google/tfkdl_00.html#impact-of-learning-rate-decay-cleaner-convergence",
    "title": "Machine-Learning-03",
    "section": "Impact of Learning Rate Decay: Cleaner Convergence",
    "text": "Impact of Learning Rate Decay: Cleaner Convergence\nThe effect of learning rate decay is often dramatic.\n\n\n\nNoise Reduction: Training curves become much smoother.\nStable Validation: Validation accuracy and loss fluctuate less, showing cleaner convergence.\nImproved Accuracy: Sustained test accuracy can now be observed above 98%.\n\nThis technique is crucial for pushing models to higher performance ceilings.\n\n\n(Training curves with learning rate decay)\n\n\nThe impact of implementing learning rate decay is usually quite significant and immediately visible in the training plots. You’ll notice that the training and validation curves, which might have been noisy and erratic before, become much smoother. This indicates that the model is making more stable and directed progress towards minimizing the loss.\nMore importantly, the validation accuracy, which is our true measure of generalization, becomes more stable and can push higher, often achieving levels above 98% in a consistent manner. This clearly shows that learning rate decay is not just a tweak but a fundamental technique for achieving robust and high-performing deep learning models. It helps the optimizer fine-tune the weights effectively as the model approaches optimal solutions."
  },
  {
    "objectID": "google/tfkdl_00.html#overfitting-and-dropout",
    "href": "google/tfkdl_00.html#overfitting-and-dropout",
    "title": "Machine-Learning-03",
    "section": "7. Overfitting and Dropout",
    "text": "7. Overfitting and Dropout\nEven with a deeper network, ReLU, Adam, and learning rate decay, we often hit a wall around 98% accuracy. This is frequently due to overfitting."
  },
  {
    "objectID": "google/tfkdl_00.html#the-signs-of-overfitting",
    "href": "google/tfkdl_00.html#the-signs-of-overfitting",
    "title": "Machine-Learning-03",
    "section": "The Signs of Overfitting",
    "text": "The Signs of Overfitting\n\nValidation loss increases: While training loss continues to decrease.\nValidation accuracy plateaus or drops: While training accuracy keeps improving.\n\nThis means the model is learning details specific to the training data, but failing to generalize to new, unseen data (like our validation set).\n\nExample: Validation loss rising while training loss drops\n\nWe’ve made significant progress, reaching around 97-98% accuracy. However, pushing beyond this often reveals a common problem in machine learning: overfitting. Overfitting occurs when a model learns the training data too well, including its noise and specific quirks, but fails to capture the underlying patterns that generalize to new, unseen data.\nThe tell-tale signs of overfitting, as seen in the provided image, are when the training loss continues to decrease, indicating the model is still learning on the training set, but the validation loss either plateaus or, more critically, starts to increase. Similarly, validation accuracy might stop improving even as training accuracy climbs higher. This divergence signifies that the model is no longer effectively learning features that help it generalize."
  },
  {
    "objectID": "google/tfkdl_00.html#dropout-a-regularization-technique",
    "href": "google/tfkdl_00.html#dropout-a-regularization-technique",
    "title": "Machine-Learning-03",
    "section": "Dropout: A Regularization Technique",
    "text": "Dropout: A Regularization Technique\nDropout is a powerful and widely used technique to combat overfitting.\nHow it Works:\n\nDuring each training iteration, a random subset of neurons (and their connections) are temporarily “dropped out” (set to zero).\nThis means the network cannot rely on any single neuron to be present.\nIt forces the network to learn more robust and redundant features.\n\nAnalogy:\nLike training multiple smaller, “thinner” networks simultaneously, combined into one.\nKeras Implementation:\nAdd tf.keras.layers.Dropout to your model:\nmodel = tf.keras.Sequential([\n    # ... previous layers\n    tf.keras.layers.Dense(200, activation='relu'),\n    tf.keras.layers.Dropout(0.2), # Dropout layer\n    tf.keras.layers.Dense(60, activation='relu'),\n    tf.keras.layers.Dropout(0.2), # Another Dropout layer\n    tf.keras.layers.Dense(10, activation='softmax')\n])\nThe 0.2 indicates dropping out 20% of neurons.\n\nWhen confronted with overfitting, one of the first and most effective regularization techniques to try is Dropout.\nHere’s how it works: During each training step, for every hidden layer that has a dropout layer, a randomly selected percentage of the neurons (along with their incoming and outgoing connections) are temporarily ignored – effectively “dropped out” – for that specific forward and backward pass. For example, a dropout rate of 0.2 means 20% of the neurons are randomly deactivated.\nThe critical insight here is that the network can no longer rely on any single neuron or specific configuration of neurons to be active. This forces the network to learn more robust features and less interdependent representations. It’s like having multiple experts (neurons) for a task, but during training, you randomly remove some experts, forcing the remaining ones to learn to pick up the slack, making the entire team more resilient.\nIn Keras, you simply add a tf.keras.layers.Dropout layer after a dense layer. The argument to Dropout is the fraction of neurons to drop. Common values are between 0.1 and 0.5. Note that dropout is only applied during training; during inference (when making predictions), all neurons are active, but their weights are scaled down appropriately to account for the dropout rate used during training."
  },
  {
    "objectID": "google/tfkdl_00.html#dropouts-initial-impact-a-complex-picture",
    "href": "google/tfkdl_00.html#dropouts-initial-impact-a-complex-picture",
    "title": "Machine-Learning-03",
    "section": "Dropout’s Initial Impact: A Complex Picture",
    "text": "Dropout’s Initial Impact: A Complex Picture\nApplying dropout can lead to a mixed initial response.\n\n(Training curves with Dropout)\n\nIncreased Noise: The training curves show more fluctuation due to the random dropping of neurons.\nHigher Overall Loss: Both training and validation loss might be higher than without dropout.\nSlight Accuracy Drop: Validation accuracy might initially decrease.\n\n\n\n\n\n\n\nNote\n\n\nThis doesn’t mean dropout failed; it indicates the model is being forced to learn differently.\nWe are pushing it to generalize better, not just memorize.\n\n\n\n\nWhen you first apply dropout and observe the training curves, the results might seem counterintuitive or even disappointing. You’ll likely see:\n\nIncreased Noise: The curves will become noisier because of the inherent randomness introduced by dropping neurons at each step.\nHigher Overall Loss: Both training and validation loss might be higher than before. This is because we are intentionally hindering the network’s ability to perfectly fit the training data, forcing it to generalize.\nSlight Accuracy Drop: Validation accuracy might even dip slightly.\n\nIt’s important not to conclude that dropout has “failed” at this point. These initial observations are actually consistent with dropout’s intended effect: it’s making the network’s learning process harder in a structured way, forcing it to discover more resilient and generalizable features. The goal with dropout isn’t necessarily to immediately boost accuracy, but to prevent the validation loss from creeping up and ultimately achieve better generalization on truly unseen data, even if it means slightly lower peak training performance. The long-term benefits typically outweigh the short-term perceived drawbacks."
  },
  {
    "objectID": "google/tfkdl_00.html#deeper-roots-of-overfitting-the-nature-of-the-problem",
    "href": "google/tfkdl_00.html#deeper-roots-of-overfitting-the-nature-of-the-problem",
    "title": "Machine-Learning-03",
    "section": "Deeper Roots of Overfitting: The Nature of the Problem",
    "text": "Deeper Roots of Overfitting: The Nature of the Problem\nOverfitting isn’t always easily fixed by dropout alone; it stems from fundamental issues.\n1. “Too Many Degrees of Freedom”:\n\nIf a network is too large for the complexity of the data, it can simply “memorize” training examples.\nIt fails to extract underlying patterns, resulting in poor generalization.\n\nAnalogy for ECE:\n\nImagine fitting a 10th-order polynomial to only three data points. It will perfectly hit those points but be wild everywhere else.\n\n2. Insufficient Training Data:\n\nNeural networks are data-hungry.\nWith too little data, even a reasonably sized network can overfit because there isn’t enough variety to learn robust patterns.\n\n3. Inadequate Network Architecture:\n\nSometimes, the chosen network type isn’t suitable for the data’s structure.\nOur current Dense (fully-connected) only network struggles with image spatial relationships.\n\n\nWhile dropout is a great tool, it’s essential to understand that overfitting has deeper, more fundamental causes. Simply applying dropout might not fully solve the problem if these underlying issues are present.\n\nToo Many Degrees of Freedom: This happens when your neural network is disproportionately complex for the problem it’s trying to solve. If a network has an excessive number of neurons and parameters, it gains the capacity to essentially memorize the training data points rather than learning the generalized rules that govern them. It’s like fitting a very high-degree polynomial to a small number of data points; it will pass through all of them perfectly (low training loss) but will be wildly inaccurate for any new point. A well-designed network needs a kind of constraint that forces it to extract meaningful, generalizable features.\nInsufficient Training Data: Deep neural networks are notoriously data-hungry. If you don’t have enough diverse training examples, even a moderately sized network can easily overfit. It simply doesn’t have enough varied information to learn robust patterns that apply broadly. This is a common bottleneck in deploying machine learning in many specialized ECE domains where data acquisition can be costly or difficult.\nInadequate Network Architecture: This is a crucial point for our current MNIST task. Recall that we flattened our 28x28 images into a 784-element vector. In doing so, we completely discarded all spatial information – the fact that pixels are arranged in a grid and that neighboring pixels are highly correlated. Our dense network treats every pixel as an independent feature. Handwritten digits, however, are fundamentally made of shapes, edges, and patterns that depend on the spatial arrangement of pixels. A dense network has to “reinvent” this spatial understanding from scratch, which is inefficient and often leads to the performance ceiling we’ve hit.\n\nThis inadequacy of our current architecture points us towards the next major advancement for image data: Convolutional Neural Networks (CNNs)."
  },
  {
    "objectID": "google/tfkdl_00.html#introduction-to-convolutional-neural-networks-cnns",
    "href": "google/tfkdl_00.html#introduction-to-convolutional-neural-networks-cnns",
    "title": "Machine-Learning-03",
    "section": "Introduction to Convolutional Neural Networks (CNNs)",
    "text": "Introduction to Convolutional Neural Networks (CNNs)\nOur current model struggles because it treats image pixels as independent features, losing spatial context. Convolutional Neural Networks (CNNs) are designed to leverage this spatial information.\nKey Idea:\n\nInstead of fully-connected layers, CNNs use convolutional filters (kernels).\nThese filters slide across the input image, detecting local features like edges, corners, and textures.\nThey preserve the spatial relationships between pixels.\n\nBenefits for ECE (Image Processing):\n\nFeature Hierarchy: Learn increasingly complex features (edges -&gt; shapes -&gt; object parts).\nParameter Sharing: Detect the same feature anywhere in the image with the same filter.\nTranslation Invariance: Robust to slight shifts in object position.\nCrucial for applications like object detection, medical imaging, and autonomous systems.\n\n\n\n\n\n\n\nImportant\n\n\nWe’ve hit a performance ceiling with our dense network because it fundamentally misunderstands image data. CNNs are the game-changer here!"
  },
  {
    "objectID": "index.html#sources",
    "href": "index.html#sources",
    "title": "Machine Learning",
    "section": "Sources",
    "text": "Sources\n\nGoogle\n\nMachine Learning: Tensor Flow Keras Deep Learning 1\nMachine Learning: Tensor Flow Keras Deep Learning 2\n\n\n\nAMLI\n\nMachine Learning 3.1: Regression\nMachine Learning 3.2: Regression\nMachine Learning 4.1: Classification\nMachine Learning 4.2: Classification\nMachine Learning 5.1: Classification\nMachine Learning 5.2: Classification\n\n\n\nCS231n\n\nCS231n: Classification\nCS231n: Neural Network 1\nCS231n: Neural Network 2\nCS231n: Neural Network 3"
  },
  {
    "objectID": "amli/06_00-03-others.html#what-is-clustering",
    "href": "amli/06_00-03-others.html#what-is-clustering",
    "title": "Machine Learning",
    "section": "What is Clustering?",
    "text": "What is Clustering?\nClustering is an unsupervised machine learning task.\n\nIt groups a set of objects such that objects in the same group are more similar to each other than to those in other groups.\nUnlike classification, there are no predefined labels or target variables.\nThe goal is to discover intrinsic groupings within the data.\n\n\n\n\n\n\n\nTip\n\n\nThink of it like sorting items without being told what categories exist.\n\n\n\n\nLet’s start with clustering. Simply put, clustering is about finding natural groupings in your data. It’s a cornerstone of unsupervised learning because, unlike regression or classification where we have labeled examples, here we don’t have a “correct” answer for each data point. The algorithm itself finds the patterns. This is incredibly useful when you have a lot of data but don’t know what to look for, or when labeling data is too expensive or impossible. Think about anomalies in sensor data, segmenting customers, or grouping similar images."
  },
  {
    "objectID": "amli/06_00-03-others.html#supervised-vs.-unsupervised-learning",
    "href": "amli/06_00-03-others.html#supervised-vs.-unsupervised-learning",
    "title": "Machine Learning",
    "section": "Supervised vs. Unsupervised Learning",
    "text": "Supervised vs. Unsupervised Learning\n\n\nSupervised Learning\n\nLabeled Data: Each data point has a known output (e.g., house price, image label).\nGoal: Predict an output based on given inputs.\nExamples: Regression, Classification.\nQuestions: “Is this a cat or a dog?”, “What is the house price?”\n\n\n\n\n\n\n\nTip\n\n\nAnalogous to learning with a teacher.\n\n\n\n\n\n\nUnsupervised Learning (Clustering)\n\nUnlabeled Data: Data points have no predefined outputs.\nGoal: Discover patterns, structures, or groupings within the data.\nExamples: Clustering, Dimensionality Reduction.\nQuestions: “How can these animals be grouped?”, “What are the natural segments in this market?”\n\n\n\n\n\n\n\nTip\n\n\nAnalogous to self-discovery or exploring on your own.\n\n\n\n\n\nTo solidify our understanding, let’s explicitly compare supervised and unsupervised learning. In supervised learning, our dataset is like a textbook with answers – we have input ‘features’ and corresponding ‘labels’ or ‘targets’. The model learns the mapping from features to labels. In contrast, unsupervised learning is like getting a pile of raw materials and being asked to organize them. There are no given categories; the algorithm must find them. Clustering is a prime example of this, where similarity among data points guides the grouping process."
  },
  {
    "objectID": "amli/06_00-03-others.html#hands-on-analogy-fastener-clustering",
    "href": "amli/06_00-03-others.html#hands-on-analogy-fastener-clustering",
    "title": "Machine Learning",
    "section": "Hands-On Analogy: Fastener Clustering",
    "text": "Hands-On Analogy: Fastener Clustering\n\n\nImagine a pile of assorted fasteners (screws, nuts, bolts, washers).\n\n\nTask 1: Divide them into six distinct groups.\nTask 2: Now, divide them into four distinct groups.\nTask 3: Finally, divide them into just two distinct groups.\n\n\n\n\n\n\n\nNote\n\n\nWhat criteria did you use? How much did the grouping change with different numbers of groups?\n\n\n\n\nLet’s make this tangible. Imagine you have a large assortment of fasteners – screws, nuts, bolts, and washers, of various sizes and materials. If I asked you to group them, you’d intuitively start looking for similarities. Perhaps by head type, size, material, or purpose.\nThis simple exercise perfectly illustrates the core idea behind clustering. You were implicitly deciding on a “distance metric” – what makes two fasteners similar or different – and then grouping them based on that. And importantly, you experienced how changing the target number of groups, typically denoted as ‘k’ in machine learning, significantly affects the way you perceive and form these clusters. Your choices minimized “differences” within each group, much like clustering algorithms do."
  },
  {
    "objectID": "amli/06_00-03-others.html#k-means-the-most-common-algorithm",
    "href": "amli/06_00-03-others.html#k-means-the-most-common-algorithm",
    "title": "Machine Learning",
    "section": "k-means: The Most Common Algorithm",
    "text": "k-means: The Most Common Algorithm\nk-means is an iterative clustering algorithm that aims to partition n data points into k clusters.\nCore Idea: Minimize the within-cluster sum of squares (WCSS). This means making each cluster as compact and homogeneous as possible.\n\\[\n\\arg \\min_{S} \\sum_{i=1}^{k} \\sum_{x \\in S_i} \\| x - \\mu_i \\|^2\n\\]\n\n\\(S\\): The set of all clusters \\(\\{S_1, S_2, \\dots, S_k\\}\\).\n\\(S_i\\): All data points within cluster i.\n\\(x\\): A data point in cluster i.\n\\(\\mu_i\\): The mean (centroid) of the data points in cluster i.\n\n\nAmong the many clustering algorithms, k-means stands out as the most widely used. Its popularity stems from its simplicity and efficiency. The “k” in k-means refers to the number of clusters you want to find. The algorithm determines the ‘mean’ or ‘centroid’ of each cluster.\nMathematically, k-means tries to minimize the sum of squared distances between each data point and the centroid of its assigned cluster. This objective function, called the Within-Cluster Sum of Squares or WCSS, effectively pushes the algorithm to create clusters where points within the same cluster are very close to each other."
  },
  {
    "objectID": "amli/06_00-03-others.html#k-means-algorithm-step-by-step",
    "href": "amli/06_00-03-others.html#k-means-algorithm-step-by-step",
    "title": "Machine Learning",
    "section": "k-means Algorithm: Step-by-Step",
    "text": "k-means Algorithm: Step-by-Step\n\n\n1. Initialization\n\nSelect k number of clusters.\nRandomly generate k initial centroids.\n\n\n\n\n\n2. Assignment\n\nAssign each data point to its closest centroid.\n\n\n\n\nLet’s walk through the k-means algorithm step-by-step. It’s an iterative process.\nFirst, you decide on k, the number of clusters. Then, the algorithm randomly places ‘k’ points, called centroids, in your data space. These are essentially initial guesses for the center of each cluster.\nNext, every single data point in your dataset is assigned to the nearest centroid. “Nearest” is determined by a distance metric, usually Euclidean distance. This creates our initial set of ‘k’ clusters based on these random centroids."
  },
  {
    "objectID": "amli/06_00-03-others.html#k-means-algorithm-iteration",
    "href": "amli/06_00-03-others.html#k-means-algorithm-iteration",
    "title": "Machine Learning",
    "section": "k-means Algorithm: Iteration",
    "text": "k-means Algorithm: Iteration\n\n\n3. Update\n\nRecalculate new centroids as the mean of all points in each cluster.\n\n\n\n\n\n4. Reassignment\n\nReassign data points to their newest closest centroids.\n\n\n\n\nAfter the initial assignment, the algorithm refines the clusters. The third step involves updating the centroids. For each cluster, the new centroid is calculated as the average position (arithmetic mean) of all the data points currently assigned to that cluster. This moves the centroids towards the true ‘center of gravity’ of their respective clusters.\nWith the new centroid positions, all data points are then reassigned. Again, each point goes to the centroid it is now closest to. This might cause some points to switch clusters if a new centroid is closer than their previous one."
  },
  {
    "objectID": "amli/06_00-03-others.html#k-means-algorithm-convergence",
    "href": "amli/06_00-03-others.html#k-means-algorithm-convergence",
    "title": "Machine Learning",
    "section": "k-means Algorithm: Convergence",
    "text": "k-means Algorithm: Convergence\n\n\n5. Repeat & Converge\n\nRepeat steps 3 and 4 until:\n\nCentroids no longer move significantly.\nCluster assignments no longer change.\nA maximum number of iterations is reached.\n\n\n\n\n\n\nK-means Flowchart\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis process of updating centroids and reassigning points continues iteratively. The algorithm eventually converges when the centroids stop moving much between iterations or when the cluster assignments stabilize. At this point, the clusters and their centroids are considered optimally positioned according to the k-means objective. The flowchart on the right summarizes this iterative process, offering a clear visual guide to the algorithm’s flow."
  },
  {
    "objectID": "amli/06_00-03-others.html#interactive-k-means-visualization",
    "href": "amli/06_00-03-others.html#interactive-k-means-visualization",
    "title": "Machine Learning",
    "section": "Interactive K-means Visualization",
    "text": "Interactive K-means Visualization\nAdjust the number of clusters (k) and see how the data points are grouped.\n\n\n\nimport { slider } from \"@jashkenas/inputs\";\nviewof k_clusters = slider({\n  min: 2,\n  max: 10,\n  step: 1,\n  value: 4,\n  label: \"Number of Clusters (k)\"\n});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nThis visualization simulates the outcome of k-means clustering. Observe how varying k changes the resulting clusters.\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive demo allows you to experiment with the crucial hyperparameter k – the number of clusters. On the left, you’ll find a slider to adjust k. As you change it, the Python code on the right will rerun k-means on a synthetic dataset and update the Plotly visualization. Pay attention to how the data points are grouped and how the centroids shift based on your k selection. This highlights the subjective nature of choosing k and how different k values can lead to different interpretations of the underlying data structure."
  },
  {
    "objectID": "amli/06_00-03-others.html#other-clustering-algorithms",
    "href": "amli/06_00-03-others.html#other-clustering-algorithms",
    "title": "Machine Learning",
    "section": "Other Clustering Algorithms",
    "text": "Other Clustering Algorithms\nK-means is just one of many! Here are a few others:\n\nAffinity Propagation: Finds ‘exemplars’ which are representative samples of clusters.\nMean-shift: Locates the centers of dense data regions in a continuous space.\nDBSCAN: Density-Based Spatial Clustering of Applications with Noise; identifies clusters based on density and can find arbitrarily shaped clusters.\nGaussian Mixtures (GMM): Models data as a mixture of Gaussian distributions, providing probabilistic cluster assignments.\nAgglomerative Hierarchical Clustering: Builds a hierarchy of clusters, starting with individual points as clusters and merging them.\n\n\nWhile k-means is popular, it’s essential to know that a rich landscape of clustering algorithms exists, each with its strengths and weaknesses. For example, k-means assumes spherical clusters of similar size, which isn’t always true. DBSCAN, for instance, can discover clusters of arbitrary shapes and identify outliers. Gaussian Mixture Models provide a probabilistic approach, giving you a likelihood that a point belongs to a cluster rather than a hard assignment. The choice of algorithm highly depends on the nature of your data and the specific problem you’re trying to solve."
  },
  {
    "objectID": "amli/06_00-03-others.html#evaluating-cluster-quality",
    "href": "amli/06_00-03-others.html#evaluating-cluster-quality",
    "title": "Machine Learning",
    "section": "Evaluating Cluster Quality",
    "text": "Evaluating Cluster Quality\nHow do we measure the “goodness” of clusters when we don’t have true labels?\n\nInternal Metrics: Evaluate clustering based on the data itself (e.g., compactness, separation).\nExternal Metrics: If we do have ground truth labels (e.g., for comparison or research), we can use these.\n\nToday, we’ll focus on some external metrics that are useful when you happen to have ground truth labels for comparison, even though clustering is unsupervised. These are similar to metrics you’ve seen but adapted for clustering.\n\nA critical question in clustering is: how do we know if our clusters are any good? Unlike supervised learning where we have clear accuracy metrics, clustering lacks a direct ground truth. So, evaluating cluster quality is a nuanced topic.\nWe categorize evaluation metrics into “internal” and “external.” Internal metrics assess how well the clusters are formed using only the data itself, looking at things like how tightly packed points are within a cluster (compactness) or how far apart different clusters are (separation).\nHowever, sometimes, perhaps in a research setting or when comparing to a known categorization, you might have access to ‘ground truth’ labels. In those specific cases, we can use external metrics to compare our clustering results against these known labels. We’ll briefly touch on three such external metrics now."
  },
  {
    "objectID": "amli/06_00-03-others.html#homogeneity",
    "href": "amli/06_00-03-others.html#homogeneity",
    "title": "Machine Learning",
    "section": "Homogeneity",
    "text": "Homogeneity\n\\[ \\text{Homogeneity} = 1 - \\frac{H(C|K)}{H(C)} \\]\n\nDefinition: A clustering result satisfies homogeneity if each cluster contains only data points belonging to a single class.\nRange: 0.0 to 1.0.\nPerfect Homogeneity (1.0): Every cluster is pure; it has items of only one type.\n\n\n\n\n\n\n\nCaution\n\n\nLimitations: Can be artificially high by having many small clusters (e.g., each point is its own cluster!).\n\n\n\n\nHomogeneity measures whether each cluster is “pure” – meaning it ideally contains only members from a single ground-truth class. Think back to our fastener example: if one of your groups contained only screws and no bolts or washers, it would be highly homogeneous. A score of 1.0 means every cluster consists of items from just one class. However, watch out for the pitfall: you can get a perfect homogeneity score by making every single data point its own cluster. This isn’t useful, so we need other metrics."
  },
  {
    "objectID": "amli/06_00-03-others.html#completeness",
    "href": "amli/06_00-03-others.html#completeness",
    "title": "Machine Learning",
    "section": "Completeness",
    "text": "Completeness\n\\[ \\text{Completeness} = 1 - \\frac{H(K|C)}{H(K)} \\]\n\nDefinition: A clustering result satisfies completeness if all data points belonging to a given class are assigned to the same cluster.\nRange: 0.0 to 1.0.\nPerfect Completeness (1.0): All members of a class are in one cluster.\n\n\n\n\n\n\n\nCaution\n\n\nLimitations: Can be artificially high by having very few large clusters (e.g., all points in one big cluster!).\n\n\n\n\nCompleteness is the flip side of homogeneity. It measures whether all data points that should be together (i.e., belong to the same ground-truth class) are indeed grouped into the same cluster. In our fastener example, if all the screws were in a single cluster, that cluster would be complete with respect to screws. A score of 1.0 means that for any given ground-truth class, all its members are found within a single cluster. The hack for completeness is to put all your data into one giant cluster, which would score 1.0 but again, is not useful."
  },
  {
    "objectID": "amli/06_00-03-others.html#v-measure",
    "href": "amli/06_00-03-others.html#v-measure",
    "title": "Machine Learning",
    "section": "V-measure",
    "text": "V-measure\n\\[ V = 2 \\frac{\\text{homogeneity} \\cdot \\text{completeness}}{\\text{homogeneity} + \\text{completeness}} \\]\n\nThe V-measure is the harmonic mean of homogeneity and completeness.\nIt provides a balanced score between the two metrics.\nA high V-measure indicates both:\n\nClusters are homogeneous (pure).\nClusters are complete (all members of a class are together).\n\n\n\n\n\n\n\n\nImportant\n\n\nAim for a high V-measure as it penalizes clustering solutions that are not balanced.\n\n\n\n\nSince homogeneity and completeness can be trivially maximized by extreme clustering solutions, we need a metric that balances them. The V-measure comes to the rescue! Much like the F1 score we saw in classification, V-measure is the harmonic mean of homogeneity and completeness. This means it only gives a high score if both homogeneity and completeness are high. It’s a robust way to assess cluster quality when ground truth labels are available for comparison, offering a single, balanced value."
  },
  {
    "objectID": "amli/06_00-03-others.html#clustering-lab-preview-mushroom-classification",
    "href": "amli/06_00-03-others.html#clustering-lab-preview-mushroom-classification",
    "title": "Machine Learning",
    "section": "Clustering Lab Preview: Mushroom Classification",
    "text": "Clustering Lab Preview: Mushroom Classification\n\n\nCan we cluster mushrooms to determine if they are edible or poisonous? Using attributes like cap shape, odor, and gill size.\n\n# Basic k-means usage in scikit-learn\nfrom sklearn.cluster import KMeans\n\n# Initialize KMeans model with 'k' clusters\n# n_init='auto' ensures best of multiple centroid initializations\nmodel = KMeans(n_clusters=10, random_state=42, n_init='auto')\n\n# Assuming 'X' is your preprocessed feature data\n# model.fit(X)\n# cluster_assignments = model.labels_\n\nNow, let’s look at a practical lab example. We’ll be working with a dataset of mushrooms, where each mushroom has various attributes like cap shape, odor, and gill color. The goal is to see if k-means clustering can naturally group mushrooms into categories that align with their edibility – whether they are poisonous or edible. This is a classic example of using unsupervised learning to derive insights without explicit prior labels for edibility within the clustering process itself. The snippet shows how simple it is to initialize a KMeans model in scikit-learn, setting the number of clusters and a random state for reproducibility."
  },
  {
    "objectID": "amli/06_00-03-others.html#your-turn-k-means-lab",
    "href": "amli/06_00-03-others.html#your-turn-k-means-lab",
    "title": "Machine Learning",
    "section": "Your Turn: K-Means Lab",
    "text": "Your Turn: K-Means Lab\nLet’s dive into the practical application of k-means.\n\nOpen the K-Means Lab in your environment.\nExplore:\n\nLoad and preprocess the mushroom dataset.\nApply KMeans with different k values.\nEvaluate the clusters using metrics like Homogeneity, Completeness, and V-measure against the actual edibility labels (as a proxy for ground truth).\n\nDiscuss: What do the results tell you about distinguishing edible from poisonous mushrooms using clustering?\n\n\nNow it’s your turn to apply what you’ve learned. Head to the K-Means lab. You’ll be working with the mushroom dataset. Your tasks will involve loading and preprocessing this dataset, applying the KMeans algorithm, and then, crucially, evaluating the clusters. Even though clustering is unsupervised, the mushroom dataset does come with an “edibility” label, which we can use as a “ground truth” to evaluate the quality of our clusters using metrics like Homogeneity, Completeness, and V-measure. This is a great way to understand the performance of clustering in a real-world scenario."
  },
  {
    "objectID": "amli/06_00-03-others.html#the-challenge-of-one-hot-encodings",
    "href": "amli/06_00-03-others.html#the-challenge-of-one-hot-encodings",
    "title": "Machine Learning",
    "section": "The Challenge of One-Hot Encodings",
    "text": "The Challenge of One-Hot Encodings\n\n\n\nOften used for categorical data (e.g., words, user IDs, product types).\nEach category represented by a vector with a 1 at its unique index and 0s elsewhere.\n\n\n\n\n\n\n\nCaution\n\n\nProblems:\n\nSparsity & High Dimensionality: Many zeros, very large vectors for large vocabularies.\nNo Semantic Similarity: “Hotel” and “Motel” are represented as equally distant as “Hotel” and “Banana”.\n\n\n\n\n\n\n\n\nLet’s first understand the problem that embeddings solve. Often, our data involves categorical variables like words, user IDs, or types of devices. A common way to convert these into a numerical format that machine learning models can process is “one-hot encoding.” As you can see in the diagram, each unique word gets its own position in a vector, where a ‘1’ indicates the presence of that word and ’0’s elsewhere.\nWhile simple, one-hot encoding suffers from two major drawbacks. First, for large vocabularies or many categories, the vectors become incredibly long and sparse, mostly filled with zeros, leading to inefficient computations. Second, and more critically for machine learning, one-hot encodings fail to capture any semantic relationships. “Hotel” and “Motel” are conceptually very similar, but their one-hot vectors would be orthogonal, implying no relation at all. We need a richer representation."
  },
  {
    "objectID": "amli/06_00-03-others.html#embeddings-efficient-dense-and-semantic",
    "href": "amli/06_00-03-others.html#embeddings-efficient-dense-and-semantic",
    "title": "Machine Learning",
    "section": "Embeddings: Efficient, Dense, and Semantic",
    "text": "Embeddings: Efficient, Dense, and Semantic\n\n\n\nEfficient, Dense Representation:\n\nRepresent discrete items (words, categories) as low-dimensional, dense vectors of real numbers.\nReduces dimensionality and memory footprint.\n\nSemantic Similarity:\n\nCrucially, similar items are represented by similar vectors (close in vector space).\nCaptures relationships beyond simple identity.\n\n\n\n\n\n\n\n\nTip\n\n\nWord embeddings capture meaning! Example: Vector(“King”) - Vector(“Man”) + Vector(“Woman”) \\(\\approx\\) Vector(“Queen”)\n\n\n\n\n\n\n\nThis is where embeddings come in. Instead of sparse, high-dimensional one-hot vectors, embeddings represent each item as a short, dense vector of real numbers. This provides a much more efficient way to store and process categorical data.\nBut the real magic of embeddings lies in their ability to capture semantic meaning and relationships. Items that are conceptually similar, like “hotel” and “motel”, will have embedding vectors that are geometrically close to each other in this lower-dimensional space. This allows models to generalize better and understand nuanced relationships. The famous example is how vector arithmetic on word embeddings can reveal analogies, like “King minus Man plus Woman equals Queen.”"
  },
  {
    "objectID": "amli/06_00-03-others.html#using-embeddings-in-tensorflowkeras",
    "href": "amli/06_00-03-others.html#using-embeddings-in-tensorflowkeras",
    "title": "Machine Learning",
    "section": "Using Embeddings in TensorFlow/Keras",
    "text": "Using Embeddings in TensorFlow/Keras\nIn deep learning models, embeddings are usually learned weights.\n\nThe Embedding layer acts as a lookup table.\nIt maps integer indices to dense vectors.\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Define an Embedding layer\n# Args: (total_vocabulary_size, embedding_dimension)\nembedding_layer = keras.layers.Embedding(\n    input_dim=15,  # e.g., 15 unique words\n    output_dim=4   # e.g., representing each word with a 4-dim vector\n)\n\n# Example Usage:\n# input_data = tf.constant([0, 1, 10]) # Indices for 3 words\n# embedded_output = embedding_layer(input_data)\n# print(embedded_output.shape) # Expected: (3, 4)\n\n\n\n\n\n\nNote\n\n\nThe embedding values are learned parameters during model training, just like other weights.\n\n\n\n\nIn practice, especially within deep learning frameworks like TensorFlow and Keras, embeddings are not hand-engineered but rather learned. Keras provides a convenient Embedding layer for this purpose. You specify two main parameters: input_dim, which is the total number of unique items (your vocabulary size), and output_dim, which is the desired dimensionality of your embedding vectors.\nThis Embedding layer essentially acts as a dynamically updated lookup table. When you feed an integer index (representing a specific word or item) to this layer, it returns the corresponding dense embedding vector. Crucially, the values within these embedding vectors are initialized randomly and then iteratively adjusted and optimized during the model’s training process, allowing them to capture the semantic and contextual relationships present in your data."
  },
  {
    "objectID": "amli/06_00-03-others.html#embedding-layer-a-look-up-table",
    "href": "amli/06_00-03-others.html#embedding-layer-a-look-up-table",
    "title": "Machine Learning",
    "section": "Embedding Layer: A Look-Up Table",
    "text": "Embedding Layer: A Look-Up Table\n\n\nEach row corresponds to a unique item (e.g., word) in your vocabulary.\nEach column is a dimension of the embedding vector.\nThe values (weights) in this table are learned by the model.\n\n\nThis diagram visually illustrates how the Keras Embedding layer functions as a lookup table. Think of it as a matrix where each row corresponds to a unique word or item in your vocabulary, identified by its integer index. The number of rows is your input_dim (vocabulary size). Each column represents one dimension of the embedding vector, and the number of columns is your output_dim.\nWhen your model encounters a word (represented by its integer index), the embedding layer simply ‘looks up’ the corresponding row in this table and returns that dense vector. The magic is that the values within this table are not static; they are the parameters that the neural network learns during training, which allows them to capture meaningful relationships between words."
  },
  {
    "objectID": "amli/06_00-03-others.html#visualizing-embeddings-closeness-implies-similarity",
    "href": "amli/06_00-03-others.html#visualizing-embeddings-closeness-implies-similarity",
    "title": "Machine Learning",
    "section": "Visualizing Embeddings: Closeness Implies Similarity",
    "text": "Visualizing Embeddings: Closeness Implies Similarity\n\n\nWords with similar meanings or contexts cluster together in the embedding space.\nThis geometric closeness is a powerful feature for downstream tasks.\n\n\n\n\n\n\n\nTip\n\n\nECE Application: In natural language processing for voice assistants or anomaly detection in network logs, similar commands or event types should have close embeddings.\n\n\n\n\nOne of the most compelling aspects of embeddings is their visual interpretability. When we project these high-dimensional embedding vectors into 2D or 3D space (using techniques like t-SNE or PCA), we can actually “see” the relationships. As shown in the diagram, words like “hotel” and “resort” are typically found very close to each other, indicating their semantic similarity. Conversely, words with vastly different meanings would be far apart. This visual clustering not only confirms that the embeddings are capturing meaning, but it also provides a powerful intuition for how they enable machine learning models to reason about complex relationships in data. This has direct ECE applications in areas like understanding user commands for a voice interface or detecting anomalies in system logs where similar events should be grouped."
  },
  {
    "objectID": "amli/06_00-03-others.html#interactive-embedding-comparison",
    "href": "amli/06_00-03-others.html#interactive-embedding-comparison",
    "title": "Machine Learning",
    "section": "Interactive Embedding Comparison",
    "text": "Interactive Embedding Comparison\nCompare One-Hot vs. Conceptual Embedding for chosen words.\n\n\n\nvocab = [\"hotel\", \"motel\", \"cat\", \"dog\", \"apple\", \"banana\"]\nviewof selected_word = Inputs.select(vocab, {label: \"Select a Word\"});\n\nviewof embed_dim = Inputs.range([2, 8], {step: 1, value: 4, label: \"Embedding Dimension (max 8)\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nObserve how the one-hot encoding changes vs. the dense, semantic embedding for different words and dimensions.\n\n\n\n\n\n\n\n\n\n\n\n\nThis interactive section allows you to directly compare one-hot encoding with a conceptual embedding. On the left, you can select a word from a dropdown list and adjust the desired dimensionality for the embedding. The Python code on the right then generates and visualizes: first, the sparse one-hot vector for the selected word against the entire vocabulary, and second, a dense conceptual embedding vector truncated to your chosen dimension.\nNotice how the one-hot encoding is always sparse and has the same length as the vocabulary, while the embedding is dense and its length depends on the embedding dimension. Also, try selecting hotel and then motel and observe how their conceptual embeddings show similar patterns compared to cat or dog, which are quite different. This visually demonstrates the efficiency and semantic power of embeddings."
  },
  {
    "objectID": "amli/06_00-03-others.html#embeddings-in-practice-for-ece",
    "href": "amli/06_00-03-others.html#embeddings-in-practice-for-ece",
    "title": "Machine Learning",
    "section": "Embeddings in Practice for ECE",
    "text": "Embeddings in Practice for ECE\nEmbeddings are not just for words!\n\nImage Recognition: Learning embeddings for regions/objects in images.\nSensor Data: Representing different sensor types or states.\nAnomaly Detection: Embedding normal vs. anomalous system behavior for network security.\nRobotics: Learning compact representations of robot states or environments.\nRecommender Systems: Embedding users and items to capture preferences.\nDevice Characterization: Compactly representing device features for classification or clustering.\n\n\nThe power of embeddings extends far beyond natural language. In Electrical and Computer Engineering, embeddings find applications in diverse fields:\n\nIn computer vision, embeddings can represent features extracted from images, allowing for tasks like image retrieval or object recognition where similar objects have close embeddings.\nFor sensor data, you can embed different types of sensor readings or states into a common space, enabling better fusion or comparison of heterogeneous data.\nIn network security, embeddings can be used for anomaly detection by training models to embed normal network traffic or system logs into a dense representation. Deviations would show up as distant points in the embedding space.\nRobotics can use embeddings to learn efficient representations of complex robot states or environmental maps.\nRecommender systems, crucial for many online ECE products, use embeddings to represent users and items, allowing the system to find similar items or predict user preferences efficiently.\nFinally, in device characterization, embeddings can represent complex device characteristics from test data, enabling better classification of devices or clustering for quality control."
  },
  {
    "objectID": "amli/06_00-03-others.html#your-turn-embeddings-lab",
    "href": "amli/06_00-03-others.html#your-turn-embeddings-lab",
    "title": "Machine Learning",
    "section": "Your Turn: Embeddings Lab",
    "text": "Your Turn: Embeddings Lab\nLet’s apply your understanding of embeddings!\n\nOpen the Embeddings Lab in your environment.\nExplore:\n\nWork with an embedding layer in TensorFlow/Keras.\nTrain your own embeddings for a dataset of categorical features.\nVisualize the learned embeddings, perhaps reduced to 2D or 3D.\n\nDiscuss:\n\nHow do the learned embeddings differ from one-hot encodings?\nWhat insights can you gain from visualizing these embeddings?\n\n\n\nAlright, it’s time to put what you’ve learned about embeddings into practice. Proceed to the Embeddings Lab. In this lab, you’ll get hands-on experience by defining and training your own embedding layers within a TensorFlow/Keras model. You’ll work with a dataset that has categorical features, observe how the model learns these dense representations, and then visualize these learned embeddings, potentially projecting them down to 2 or 3 dimensions to better understand the relationships they capture. This lab will deepen your appreciation for how embeddings transform abstract categories into meaningful, actionable data points for your machine learning models."
  },
  {
    "objectID": "others/cnn_zoo.html#what-makes-cnn-architectures-advanced",
    "href": "others/cnn_zoo.html#what-makes-cnn-architectures-advanced",
    "title": "Machine Learning",
    "section": "What makes CNN architectures “Advanced”?",
    "text": "What makes CNN architectures “Advanced”?\nAdvanced CNNs are designed to overcome limitations of simpler networks:\n\nDeeper Networks: Learn more complex features.\n\nChallenge: Vanishing/exploding gradients, computational cost.\n\nEfficiency: Achieve high accuracy with fewer parameters/computations.\n\nCrucial for: Embedded systems, mobile devices (common in ECE).\n\nBetter Generalization: Perform well on unseen data.\n\nImportant for: Robust real-world ECE applications.\n\n\n\n\n\n\n\n\nTip\n\n\nThese architectures often introduce innovative layers or connections to manage depth and efficiency.\n\n\n\n\nSpeaker Notes:\n\nExplain that “advanced” isn’t just about more layers, but smarter ways to design those layers and connections.\nConnect the challenges (vanishing gradients, computational cost) back to practical ECE constraints, like power consumption or real-time processing.\nEmphasize efficiency and generalization as key performance indicators for deploying ML models in ECE systems."
  },
  {
    "objectID": "others/cnn_zoo.html#key-concepts-revisited-convolutions-and-pooling",
    "href": "others/cnn_zoo.html#key-concepts-revisited-convolutions-and-pooling",
    "title": "Machine Learning",
    "section": "Key Concepts Revisited: Convolutions and Pooling",
    "text": "Key Concepts Revisited: Convolutions and Pooling\nBefore diving into complex models, let’s quickly recap:\nConvolutional Layer\nExtracts features by sliding a filter (kernel) over the input.\n\\[ (I * K)(i, j) = \\sum_m \\sum_n I(i-m, j-n) K(m, n) \\]\n\nParameters: Filter size, number of filters, stride, padding.\nOutput: Feature maps revealing specific patterns.\n\nPooling Layer\nReduces spatial dimensions, making the representation smaller and more manageable.\n\nMax Pooling: Selects the maximum value from a region.\nAverage Pooling: Computes the average value from a region.\nBenefits: Reduces parameters, controls overfitting, makes the network invariant to small shifts.\n\n\nSpeaker Notes:\n\nQuickly go over the core operations. Assume students have some basic understanding.\nHighlight how these basic building blocks are combined in intricate ways in advanced architectures.\nMention the role of activation functions (ReLU, etc.) even if not explicitly on the slide."
  },
  {
    "objectID": "others/cnn_zoo.html#interactive-convolution-visualization",
    "href": "others/cnn_zoo.html#interactive-convolution-visualization",
    "title": "Machine Learning",
    "section": "Interactive Convolution Visualization",
    "text": "Interactive Convolution Visualization\nLet’s visualize how a filter slides over an input!\n\nviewof input_matrix = Inputs.textarea({\n  label: \"Input Matrix (3x3 or 4x4, space-separated)\",\n  value: \"1 0 1\\n0 1 0\\n1 0 1\",\n  rows: 4\n});\n\nviewof kernel_matrix = Inputs.textarea({\n  label: \"Kernel Matrix (2x2 or 3x3, space-separated)\",\n  value: \"1 0\\n0 1\",\n  rows: 3\n});\n\nviewof stride_val = Inputs.range([1, 2], {value: 1, step: 1, label: \"Stride\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpeaker Notes:\n\nEncourage students to experiment with different input images (simple matrices), kernels, and strides.\nPoint out how the kernel “filters” different features and how stride affects the output size.\nMention that padding (not implemented here for simplicity) would keep the output size similar to the input."
  },
  {
    "objectID": "others/cnn_zoo.html#model-zoo-advanced-cnn-architectures",
    "href": "others/cnn_zoo.html#model-zoo-advanced-cnn-architectures",
    "title": "Machine Learning",
    "section": "Model Zoo: Advanced CNN Architectures",
    "text": "Model Zoo: Advanced CNN Architectures\nNow, let’s explore some of the most influential advanced CNN architectures available in TensorFlow.\nEach model introduces unique strategies to build deeper, more efficient, and more accurate networks.\n\nVGG (Visual Geometry Group)\nResNet (Residual Network)\nInception (GoogLeNet)\nXception (Extreme Inception)\nMobileNet (Mobile-first design)\nEfficientNet (Compound Scaling)\n\n\n\n\n\n\n\nNote\n\n\nThese models are often pre-trained on large datasets like ImageNet, providing a powerful starting point for various ECE applications via transfer learning.\n\n\n\n\nSpeaker Notes:\n\nIntroduce the “model zoo” concept – a collection of battle-tested architectures.\nEmphasize transfer learning as a huge benefit for ECE students, allowing them to leverage state-of-the-art models without training from scratch. Connect this to practical engineering efficiency."
  },
  {
    "objectID": "others/cnn_zoo.html#vgg16-vgg19",
    "href": "others/cnn_zoo.html#vgg16-vgg19",
    "title": "Machine Learning",
    "section": "VGG16 / VGG19",
    "text": "VGG16 / VGG19\n\n\nDeveloped by the Visual Geometry Group at Oxford, known for its simplicity and uniformity.\n\nArchitecture: Stacks 3x3 convolutional layers with 2x2 max-pooling layers.\n\nVGG16 has 16 layers, VGG19 has 19 layers (counted as weight layers).\n\nKey Idea: Proved that very deep networks with small filters (3x3) could achieve state-of-the-art performance.\nParameters: Very high (VGG16 ~138M, VGG19 ~143M).\n\nDownside: Computationally expensive and memory-intensive.\n\n\n\n\n\n\nVGG-16 and VGG-19\n\n\n\n\nSpeaker Notes:\n\nExplain the “simplicity” of VGG – just 3x3 convs and max pooling, but many of them.\nHighlight the impact of VGG in demonstrating the power of depth.\nDiscuss the major drawback: a huge number of parameters, making it less suitable for resource-constrained ECE systems directly. However, it’s still a great feature extractor."
  },
  {
    "objectID": "others/cnn_zoo.html#resnet-residual-network",
    "href": "others/cnn_zoo.html#resnet-residual-network",
    "title": "Machine Learning",
    "section": "ResNet (Residual Network)",
    "text": "ResNet (Residual Network)\n\n\nIntroduced by Microsoft Research, solving the vanishing gradient problem in very deep networks.\n\nArchitecture: Features “skip connections” or “residual blocks”.\n\nAllows gradients to flow directly through the network.\nEnables training networks with hundreds or even thousands of layers (e.g., ResNet-50, ResNet-101, ResNet-152).\n\nKey Idea: Instead of learning direct mappings, layers learn residual mappings.\n\n\\(H(x) = F(x) + x\\), where \\(F(x)\\) is the residual function.\n\nParameters: ResNet-50 ~25M parameters. Much more efficient than VGG.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpeaker Notes:\n\nEmphasize the problem ResNet solved: the degradation problem (accuracy saturating and then degrading with increasing depth).\nExplain skip connections intuitively: “If a layer doesn’t need to learn anything, it can just pass the input through.”\nResNet’s efficiency and depth make it a go-to for complex image recognition tasks in ECE."
  },
  {
    "objectID": "others/cnn_zoo.html#inception-googlenet",
    "href": "others/cnn_zoo.html#inception-googlenet",
    "title": "Machine Learning",
    "section": "Inception (GoogLeNet)",
    "text": "Inception (GoogLeNet)\n\n\nDeveloped by Google, emphasizing efficiency and “computational budget.”\n\nArchitecture: Uses Inception Modules (or blocks) that perform multiple parallel convolutions with different kernel sizes (1x1, 3x3, 5x5) and pooling.\n\n1x1 convolutions (bottleneck layers) are used to reduce dimensionality before larger convolutions, saving computation.\n\nKey Idea: Allow the network to learn multiple feature representations at once, then concatenate them. Optimizes “width” and “depth” simultaneously.\nParameters: Very low for its accuracy (GoogLeNet ~5M parameters).\n\nHighly efficient for deployment in real-time ECE systems.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpeaker Notes:\n\nFocus on the “parallel processing” and “multi-scale feature extraction” aspects of the Inception module.\nExplain the 1x1 convolution trick (bottleneck layers) for reducing computational cost. This is a very ECE-relevant concept (resource optimization).\nHighlight its low parameter count for high accuracy."
  },
  {
    "objectID": "others/cnn_zoo.html#xception-extreme-inception",
    "href": "others/cnn_zoo.html#xception-extreme-inception",
    "title": "Machine Learning",
    "section": "Xception (Extreme Inception)",
    "text": "Xception (Extreme Inception)\n\n\nProposed by Google, building on the Inception idea by replacing standard convolutions with depthwise separable convolutions.\n\nArchitecture: Inception modules are replaced with depthwise separable convolutions.\n\nDepthwise Conv: Applies a single filter to each input channel independently. For example, if an image has three color channels (red, green, and blue), a separate filter is applied to each color channel.\nPointwise Conv: A 1x1 convolution projects the output of the depthwise operation into a new channel space. This is a 1×1 filter that combines the output of the depthwise convolution into a single feature map.\n\nKey Idea: Separating spatial and channel-wise correlations.\n\nMore efficient parameter usage and computation than traditional convolutions.\n\nParameters: Xception ~22.9M parameters.\n\nAchieves comparable or better accuracy than Inception with fewer parameters and FLOPs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpeaker Notes:\n\nThis is a crucial concept for modern efficient CNNs. Explain depthwise separable convolutions thoroughly.\nAnalogy: imagine filtering each color channel (R, G, B) separately first (depthwise), then mixing the filtered channels (pointwise).\nConnect this efficiency to mobile and embedded device applications in ECE."
  },
  {
    "objectID": "others/cnn_zoo.html#mobilenet-v1-v2-v3",
    "href": "others/cnn_zoo.html#mobilenet-v1-v2-v3",
    "title": "Machine Learning",
    "section": "MobileNet (V1, V2, V3)",
    "text": "MobileNet (V1, V2, V3)\n\n\nDesigned by Google specifically for mobile and embedded vision applications.\n\nArchitecture: Primarily uses depthwise separable convolutions, similar to Xception.\n\nMobileNetV2 introduces “Inverted Residuals” and linear bottlenecks to improve efficiency and avoid information loss.\nMobileNetV3 further optimizes through NAS (Neural Architecture Search) and new activation functions.\n\nKey Idea: Achieve high accuracy with extremely low latency and small model size.\nParameters: MobileNetV1 ~4.2M, MobileNetV2 ~3.5M.\n\nCrucial for: Real-time processing on edge devices, a core ECE application area.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpeaker Notes:\n\nEmphasize the purpose of MobileNet: “mobile-first” design. This is directly relevant to many ECE projects.\nReiterate the role of depthwise separable convolutions and briefly mention inverted residuals for V2/V3 optimizations.\nDiscuss the trade-off between accuracy and model size/speed, and why this trade-off is often acceptable or necessary in ECE."
  },
  {
    "objectID": "others/cnn_zoo.html#efficientnet-b0-to-b7",
    "href": "others/cnn_zoo.html#efficientnet-b0-to-b7",
    "title": "Machine Learning",
    "section": "EfficientNet (B0 to B7)",
    "text": "EfficientNet (B0 to B7)\n\n\nDeveloped by Google, achieving state-of-the-art accuracy with significantly fewer parameters and FLOPs than previous models.\n\nArchitecture: Uses a compound scaling method to uniformly scale width, depth, and resolution of the network.\n\nScales up from a baseline model (EfficientNet-B0) to larger versions (B1-B7).\n\nKey Idea: It found a “recipe” for scaling CNNs more efficiently than arbitrary scaling, leading to better accuracy and efficiency trade-offs.\nParameters: EfficientNet-B0 has ~5.3M parameters, B7 ~66M.\n\nOutperforms ResNets and Inception variants with orders of magnitude fewer parameters and FLOPs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpeaker Notes:\n\n“Compound scaling” is the unique aspect here. Explain that instead of just making it deeper or wider, EfficientNet finds the optimal way to scale all three dimensions simultaneously.\nHighlight its position as the current “state-of-the-art” in terms of efficiency-accuracy trade-off. This is the model to beat in computer vision.\nEmphasize its relevance for competitive ECE projects that require both high performance and computational awareness."
  },
  {
    "objectID": "others/cnn_zoo.html#conclusion-choosing-the-right-cnn-for-ece",
    "href": "others/cnn_zoo.html#conclusion-choosing-the-right-cnn-for-ece",
    "title": "Machine Learning",
    "section": "Conclusion: Choosing the Right CNN for ECE",
    "text": "Conclusion: Choosing the Right CNN for ECE\nSelecting a CNN architecture largely depends on your specific ECE application requirements:\n\nVGG: Good for understanding basic depth, but often too heavy for deployment.\nResNet: Excellent for very deep networks, good accuracy. A strong general-purpose choice.\nInception / Xception: Great for balancing accuracy and efficiency, especially with depthwise separable convolutions.\nMobileNet: Your go-to for edge devices and real-time mobile applications.\nEfficientNet: Achieves state-of-the-art results with remarkable efficiency, often the best choice when pushing performance limits.\n\n\n\n\n\n\n\nImportant\n\n\nAlways consider the trade-off between accuracy, inference speed, model size, and computational power available on your target hardware.\n\n\n\n\nSpeaker Notes:\n\nSummarize the key takeaways for each model regarding its strengths and weaknesses in an ECE context.\nReiterate the importance of the “trade-off” concept, as this is fundamental to engineering design.\nEncourage students to experiment with TensorFlow’s tf.keras.applications module to easily load and use these models."
  },
  {
    "objectID": "others/cnn_zoo.html#further-exploration-discussion",
    "href": "others/cnn_zoo.html#further-exploration-discussion",
    "title": "Machine Learning",
    "section": "Further Exploration & Discussion",
    "text": "Further Exploration & Discussion\n\nHow might these different architectures perform on custom datasets specific to ECE applications (e.g., medical images, SAR data, sensor readings)?\nWhat are the challenges of deploying these models on FPGAs or custom ASICs in ECE systems?\nBeyond classification, how are these models adapted for tasks like object detection, segmentation, or robotics in your field?\n\n\n\n\n\n\n\nTip\n\n\nTensorFlow Keras Applications Documentation: tf.keras.applications This is your starting point for loading pre-trained models.\n\n\n\n\nSpeaker Notes:\n\nOpen the floor for questions and discussion.\nEncourage students to think critically about applying these models beyond simple image classification.\nMention the tf.keras.applications module as a practical tool for them to start experimenting.\nReinforce that these models are tools, and engineers need to understand their characteristics to choose the right tool for the job."
  },
  {
    "objectID": "others/cnn_zoo.html#resnet",
    "href": "others/cnn_zoo.html#resnet",
    "title": "Machine Learning",
    "section": "Resnet",
    "text": "Resnet"
  },
  {
    "objectID": "others/cnn_zoo.html#inception-googlenet-1",
    "href": "others/cnn_zoo.html#inception-googlenet-1",
    "title": "Machine Learning",
    "section": "Inception (GoogLeNet)",
    "text": "Inception (GoogLeNet)"
  },
  {
    "objectID": "others/cnn_zoo.html#resnet-residual-network-1",
    "href": "others/cnn_zoo.html#resnet-residual-network-1",
    "title": "Machine Learning",
    "section": "ResNet (Residual Network)",
    "text": "ResNet (Residual Network)"
  },
  {
    "objectID": "others/cnn_zoo.html#resnet-residual-network-2",
    "href": "others/cnn_zoo.html#resnet-residual-network-2",
    "title": "Machine Learning",
    "section": "ResNet (Residual Network)",
    "text": "ResNet (Residual Network)"
  },
  {
    "objectID": "others/cnn_zoo.html#inception-googlenet-2",
    "href": "others/cnn_zoo.html#inception-googlenet-2",
    "title": "Machine Learning",
    "section": "Inception (GoogLeNet)",
    "text": "Inception (GoogLeNet)"
  },
  {
    "objectID": "others/cnn_zoo.html#xception-extreme-inception-1",
    "href": "others/cnn_zoo.html#xception-extreme-inception-1",
    "title": "Machine Learning",
    "section": "Xception (Extreme Inception)",
    "text": "Xception (Extreme Inception)\n\nXception"
  },
  {
    "objectID": "others/cnn_zoo.html#mobilenet-v1-v2-v3-1",
    "href": "others/cnn_zoo.html#mobilenet-v1-v2-v3-1",
    "title": "Machine Learning",
    "section": "MobileNet (V1, V2, V3)",
    "text": "MobileNet (V1, V2, V3)\n\nMobileNet"
  },
  {
    "objectID": "others/cnn_zoo.html#efficientnet-b0-to-b7-1",
    "href": "others/cnn_zoo.html#efficientnet-b0-to-b7-1",
    "title": "Machine Learning",
    "section": "EfficientNet (B0 to B7)",
    "text": "EfficientNet (B0 to B7)\n\nEfficientNet"
  },
  {
    "objectID": "index.html#week-12",
    "href": "index.html#week-12",
    "title": "Machine Learning",
    "section": "Week 12",
    "text": "Week 12\n\nGoogle-2: Tensor Flow Keras Deep Learning 2\nAMLI 5.1: Deep Learning"
  },
  {
    "objectID": "index.html#week-13",
    "href": "index.html#week-13",
    "title": "Machine Learning",
    "section": "Week 13",
    "text": "Week 13\n\nAMLI 5.2: Deep Learning\nCNN\nCNN Model Zoo"
  },
  {
    "objectID": "others/cnn.html#vgg-16-cnn",
    "href": "others/cnn.html#vgg-16-cnn",
    "title": "Machine Learning",
    "section": "VGG-16 CNN",
    "text": "VGG-16 CNN"
  },
  {
    "objectID": "others/cnn.html#what-well-cover",
    "href": "others/cnn.html#what-well-cover",
    "title": "Machine Learning",
    "section": "What We’ll Cover",
    "text": "What We’ll Cover\n\n\n1. Motivation: Why CNNs? - Limitations of traditional MLPs for images - Translation invariance & parameter efficiency\n2. CNN Architecture Components - Feature Extractors & Classifiers - VGG-16 example\n\n3. The Convolutional Block - Convolutional Layers: Filters, Kernels, Operations - Pooling Layers: Downsampling & regularization\n4. The Fully Connected Classifier - Transforming features to probabilities\n5. Advanced Topics & Summary - Dropout, Batch Normalization, 3D Convolutions\n\n\nOur journey today will start by understanding the “why” behind CNNs, looking at the challenges traditional Multi-Layer Perceptrons face with image data. Then, we’ll dive into the fundamental building blocks of CNNs, specifically the convolutional and pooling layers, which form the feature extraction part. We’ll dissect the convolution operation itself, understanding concepts like filters, kernels, stride, and padding. After features are extracted, we’ll see how fully connected layers classify these features. Finally, we’ll touch upon some advanced techniques and summarize the key takeaways, ensuring we connect theory to practical application in ECE."
  },
  {
    "objectID": "others/cnn.html#motivation-limitations-of-mlps-for-image-data",
    "href": "others/cnn.html#motivation-limitations-of-mlps-for-image-data",
    "title": "Machine Learning",
    "section": "Motivation: Limitations of MLPs for Image Data",
    "text": "Motivation: Limitations of MLPs for Image Data\nTraditional Fully Connected Networks (MLPs) struggle with image data.\n\n\n1. Not Translation Invariant\n\nMain content shifted = different network output.\nRequires training on all possible shifts, which is inefficient.\n\n\n\n\n\n\n\nCaution\n\n\nMLPs treat each pixel as an independent feature. Spatial relationships are lost if features move.\n\n\n\n\n2. Prone to Overfitting (Parameter Explosion)\n\nEach input pixel connects to every neuron in the next layer.\nExample: 224x224x3 color image \\(\\rightarrow\\) 150,528 input neurons.\nWith just three modest hidden layers, parameters can exceed 300 Billion!\n\n\n\n\n\n\n\nTip\n\n\nLarge number of parameters makes training difficult and increases overfitting risk.\n\n\n\n\n\n\n\nNot Translation Invariant\n\n\n\n\n\nProne to Overfitting\n\n\n\nWhen we talk about image data, we’re dealing with spatial information. Think about a handwritten digit ‘7’. If it’s in the center of an image or slightly shifted to the left, it’s still a ‘7’. However, for a traditional MLP, if the ‘7’ shifts, the pixel values hitting the input neurons change dramatically, leading to a different internal representation, and potentially, a wrong classification without extensive training data for all possible shifts. This is what we mean by “not translation invariant”.\nSecondly, images are high-dimensional. A seemingly small image like 224x224 pixels with 3 color channels has over 150,000 pixel values. If we link each of these to even a modest number of neurons in the next layer, the number of trainable weights explodes rapidly. This makes the network incredibly complex, slow to train, and highly susceptible to overfitting, where it memorizes the training data rather than learning generalizable features. This is where CNNs come in."
  },
  {
    "objectID": "others/cnn.html#convolutional-neural-networks-cnns-to-the-rescue",
    "href": "others/cnn.html#convolutional-neural-networks-cnns-to-the-rescue",
    "title": "Machine Learning",
    "section": "Convolutional Neural Networks (CNNs) to the Rescue!",
    "text": "Convolutional Neural Networks (CNNs) to the Rescue!\nCNNs are designed to efficiently process image data.\n\n\nKey Features\n\nConvolution Operations: Extract features effectively.\nParameter Sharing: Same weights process different input parts.\n\nGreatly reduces total trainable parameters.\n\nTranslation Invariance: Detect features regardless of position.\n\nKernel slides, detecting patterns like edges or textures.\n\n\n\n\n\n\n\n\nImportant\n\n\nCNNs leverage local spatial coherence in images.\n\n\n\n\nHow CNNs Solve MLP Issues\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVGG-16\n\n\n\nFortunately, CNNs provide a much more effective solution for image data. The core idea is the “convolution operation.” Instead of individual neurons looking at single pixels, small learnable filters, also called kernels, slide across the image. This parameter sharing means the same filter can detect a specific feature, like a vertical edge, anywhere in the image. This dramatically reduces the number of learnable parameters compared to an MLP. Because the filter slides, it inherently gives CNNs translation invariance. If an object shifts, the same filter will still activate when it encounters that object, just at a different location on its activation map. This leads to much more efficient and robust image processing models, a cornerstone of ECE applications in robotics, autonomous systems, and medical imaging."
  },
  {
    "objectID": "others/cnn.html#cnn-architecture-feature-extractor-classifier",
    "href": "others/cnn.html#cnn-architecture-feature-extractor-classifier",
    "title": "Machine Learning",
    "section": "CNN Architecture: Feature Extractor & Classifier",
    "text": "CNN Architecture: Feature Extractor & Classifier\nMost CNNs follow a two-part structure:\n\n\n1. Feature Extractor (Backbone)\n\nPurpose: Extract meaningful features from raw input.\nComprises Convolutional Blocks (Conv + Activation + Pooling).\nSpatial dimensions are reduced, depth (channels) increased.\n\nExample: Input (224x224x3) \\(\\rightarrow\\) Features (7x7x512).\n\n\n2. Classifier (Head)\n\nPurpose: Transform extracted features into class predictions.\nTypically uses Fully Connected (Dense) Layers.\nFinal layer outputs probabilities (e.g., Softmax).\n\n\nVGG-16 High-Level Architecture\n\n\n\n\n\n\n\nG\n\n\ncluster_FE\n\nFeature Extractor (Backbone)\n\n\ncluster_Classifier\n\nClassifier (Head)\n\n\n\nInput\n\nInput Image\n(224x224x3)\n\n\n\nConvBlock1\n\nConv-Block 1\n(e.g., 2xConv(64), MaxPool)\n\n\n\nInput-&gt;ConvBlock1\n\n\n\n\n\nConvBlock2\n\nConv-Block 2\n(e.g., 2xConv(128), MaxPool)\n\n\n\nConvBlock1-&gt;ConvBlock2\n\n\n\n\n\nConvBlock3\n\nConv-Block 3\n(e.g., 3xConv(256), MaxPool)\n\n\n\nConvBlock2-&gt;ConvBlock3\n\n\n\n\n\nConvBlock4\n\nConv-Block 4\n(e.g., 3xConv(512), MaxPool)\n\n\n\nConvBlock3-&gt;ConvBlock4\n\n\n\n\n\nConvBlock5\n\nConv-Block 5\n(e.g., 3xConv(512), MaxPool)\nOutput: (7x7x512)\n\n\n\nConvBlock4-&gt;ConvBlock5\n\n\n\n\n\nFlatten\n\nFlatten\n(25088 features)\n\n\n\nConvBlock5-&gt;Flatten\n\n\n\n\n\nFC6\n\nFC-6 (4096)\n\n\n\nFlatten-&gt;FC6\n\n\n\n\n\nFC7\n\nFC-7 (4096)\n\n\n\nFC6-&gt;FC7\n\n\n\n\n\nOutput\n\nFC-8 (1000 classes)\n+ Softmax\n\n\n\nFC7-&gt;Output\n\n\n\n\n\n\n\n\n\n\n\n\nFeature Extractor and Classifier\nA key concept in CNN architecture is the separation into a feature extractor and a classifier. The feature extractor, often called the “backbone,” is responsible for taking the raw input image and transforming it into a high-level representation of its features. This is done through a series of “convolutional blocks,” where each block typically involves convolutional layers followed by a pooling layer. As data flows through these blocks, the spatial dimensions of the data are typically reduced, while the ‘depth’ — or the number of channels — increases, representing more complex and abstract features. You can see this visually in the VGG-16 diagram, where the input starts as 224x224x3 and by ConvBlock5, it’s 7x7x512.\nOnce these features are extracted, the classifier, or “head,” takes over. It’s usually composed of one or more fully connected layers that take these abstract features and map them to probabilities for each possible output class. For VGG-16, trained on ImageNet, the output layer has 1000 neurons, each corresponding to one of the 1000 classes.\nThe Graphviz diagram on the right illustrates this high-level structure using the VGG-16 example. You can see the flow from raw image input, through the convolutional blocks, flattening the tensor, and finally through the fully connected layers to the final classification."
  },
  {
    "objectID": "others/cnn.html#convolutional-layers-the-eyes-of-a-cnn",
    "href": "others/cnn.html#convolutional-layers-the-eyes-of-a-cnn",
    "title": "Machine Learning",
    "section": "Convolutional Layers: The “Eyes” of a CNN",
    "text": "Convolutional Layers: The “Eyes” of a CNN\nExtracting features using learnable filters.\n\n\nHow it Works\n\nInput: A 2D array (image or feature map).\nFilter/Kernel: Small (e.g., 3x3) matrix of weights.\nConvolution Operation:\n\nFilter slides across input data.\nAt each position, element-wise multiplication & sum of filter and receptive field.\nResult is a single number.\n\nActivation Map: Output of the convolution, passed through an activation function.\n\nSummarizes features from the input.\n\n\n\nConceptual Diagram\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nFilter weights are learned during training. Unlike fixed filters (e.g., Sobel), CNN filters adapt for optimal feature detection.\n\n\n\n\n\n\n\nConvolution Operation\n\n\n\nThe convolutional layer is the heart of a CNN. It’s often called the “eyes” because it’s where the network actually “looks” for patterns. Imagine you have an input image. A small window, called a filter or kernel, slides over this image. This filter contains a set of weights, which are initially random but become precisely tuned during the training process. At each position the filter lands, it performs an element-wise multiplication of its weights with the corresponding pixel values in the “receptive field” of the input. All these products are then summed up to produce a single number. This process is repeated as the filter slides, creating a new matrix called an activation map. This map essentially highlights where the feature that the filter is looking for is present in the input. Finally, this activation map usually undergoes a non-linear activation function, like ReLU, before being passed to the next layer. This non-linearity is crucial for the network to learn complex patterns."
  },
  {
    "objectID": "others/cnn.html#convolution-operation-stride-and-padding",
    "href": "others/cnn.html#convolution-operation-stride-and-padding",
    "title": "Machine Learning",
    "section": "Convolution Operation: Stride and Padding",
    "text": "Convolution Operation: Stride and Padding\nControlling the output size and feature detection.\n\n\nStride\n\nDefinition: Number of pixels the filter shifts at a time.\nStride 1: Filter moves one pixel at a time (most common).\nStride &gt; 1: Downsamples the output, reducing feature map size.\n\nLearn fewer features, smaller output.\n\n\n\n\n\n\n\n\nTip\n\n\nAdjusting stride allows control over spatial dimension reduction. A stride of 2 halves the spatial dimensions.\n\n\n\n\nPadding\n\nDefinition: Adding extra pixels (usually zeros) around input borders.\nPurpose:\n\nPreserve spatial dimensions (e.g., input 32x32, output 32x32).\nEnsure features at edges are fully processed.\n\nTypes:\n\nValid: No padding (output smaller than input).\nSame: Adds padding to make output size equal to input size.\nZero Padding: Most common, adds zeros."
  },
  {
    "objectID": "others/cnn.html#convolution-operation",
    "href": "others/cnn.html#convolution-operation",
    "title": "Machine Learning",
    "section": "Convolution Operation",
    "text": "Convolution Operation\nSlide\n\n\n\nSlide\n\n\nPadding"
  },
  {
    "objectID": "others/cnn.html#padding-and-stride",
    "href": "others/cnn.html#padding-and-stride",
    "title": "Machine Learning",
    "section": "Padding and Stride",
    "text": "Padding and Stride\n(https://learnopencv.com/wp-content/uploads/2024/06/padding-stride-2.png)\n](https://learnopencv.com/wp-content/uploads/2024/06/no_padding_no_strides.gif)\n](https://learnopencv.com/wp-content/uploads/2024/06/same_padding_no_strides.gif)"
  },
  {
    "objectID": "others/cnn.html#convolution-output-spatial-size",
    "href": "others/cnn.html#convolution-output-spatial-size",
    "title": "Machine Learning",
    "section": "Convolution Output Spatial Size",
    "text": "Convolution Output Spatial Size\nThe output size (O) of a 2D convolution is calculated by:\n\\[ O = \\left\\lfloor \\frac{n - f + 2p}{s} \\right\\rfloor + 1 \\]\nWhere: - n: Input size (height or width) - f: Kernel size - p: Padding - s: Stride\n\n\nCalculate Your Own!\nTry different values to see how they affect output size.\n\nviewof input_n = Inputs.range([16, 256], {value: 32, step: 1, label: \"Input Size (n)\"});\nviewof kernel_f = Inputs.range([1, 7], {value: 3, step: 1, label: \"Kernel Size (f)\"});\nviewof padding_p = Inputs.range([0, 3], {value: 1, step: 1, label: \"Padding (p)\"});\nviewof stride_s = Inputs.range([1, 4], {value: 1, step: 1, label: \"Stride (s)\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive Convolution Illustration\nA basic visual representation of a 2D convolution.\n\n\n\n\n\n\n\n\n\n\n\n\n\nStride and padding are critical parameters that ECE engineers need to tune when designing CNN architectures, as they directly impact the size of feature maps and the computational load. Stride defines how many pixels the filter moves across the input. A stride of 1 is common, meaning the filter moves one pixel at a time. A larger stride, like 2, causes the filter to skip pixels, effectively downsampling the spatial dimension of the output. This reduces computation but might discard some finer-grained features.\nPadding is the practice of adding pixels, usually zeros, around the borders of the input image. This is often done to prevent the spatial dimensions from shrinking too quickly as you apply multiple convolutional layers, and it also ensures that pixels at the edges of the image contribute equally to the features extracted, as they are not “under-sampled.” “Same” padding is popular because it tries to maintain the output dimensions identical to the input.\nOn this slide, you see the formula for calculating output spatial size. This is a fundamental equation for CNN design. Let’s use the interactive calculator on the left. You can adjust the input size, kernel size, padding, and stride to instantly see the calculated output size. Take the example mentioned in the text: N=32, F=3, P=1, S=1. If you enter these values, you’ll see the output is 32, meaning the input spatial dimension is preserved.\nOn the right, we have a static diagram of how a convolution operation works, visualizing the sliding window effect. While not interactive, it helps to reinforce the concept for a small input matrix."
  },
  {
    "objectID": "others/cnn.html#convolutional-operation",
    "href": "others/cnn.html#convolutional-operation",
    "title": "Machine Learning",
    "section": "Convolutional Operation",
    "text": "Convolutional Operation\n\nConvolutional Operation"
  },
  {
    "objectID": "others/cnn.html#sobel-kernel-detecting-vertical-edges",
    "href": "others/cnn.html#sobel-kernel-detecting-vertical-edges",
    "title": "Machine Learning",
    "section": "Sobel Kernel: Detecting Vertical Edges",
    "text": "Sobel Kernel: Detecting Vertical Edges\nA concrete example of a fixed filter’s operation.\n\n\nFixed (Hand-Crafted) Kernel\n\nSobel kernel designed to detect vertical edges.\nComprises positive values on one side, negatives on the other, zeros in middle.\nActs as a numerical approximation of a derivative in the horizontal direction.\nOutput emphasizes sudden intensity changes in the vertical direction.\n\n\n\n\n\n\n\nNote\n\n\nIn CNNs, kernel weights are learned, allowing detection of diverse, complex features, not just predefined edges.\n\n\n\n\nHow it Works\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSobel Kernel\n\n\n\nTo make the convolution concept more tangible, let’s look at a classic example: the Sobel kernel. This is a predefined filter, not a learned one, used to detect vertical edges in an image. As you can see, the kernel has negative values on the left, zeros in the middle, and positive values on the right. When this kernel slides over an image, suppose it encounters a region where pixel values abruptly change from low (dark) to high (bright) from left to right. The negative values in the kernel will amplify the dark pixels, the positive values will amplify the bright pixels, and when summed, will produce a large output value, indicating a strong vertical edge. If there’s no change, the output will be close to zero.\nThe important distinction here is that while Sobel is fixed, the kernels in CNNs are learned. This means CNNs can discover much more complex and subtle features than simple edges, making them incredibly powerful for diverse vision tasks."
  },
  {
    "objectID": "others/cnn.html#convolutional-layer-properties",
    "href": "others/cnn.html#convolutional-layer-properties",
    "title": "Machine Learning",
    "section": "Convolutional Layer Properties",
    "text": "Convolutional Layer Properties\nUnderstanding channels, filters, and trainable parameters.\n\n\nFilters, Kernels, and Channels\n\nFilter Depth: Must match input data depth (number of channels).\n\nExample: Input (HxWx3) requires filter (fxfrx3).\n\nSpatial Size (fxfr): Typically 3x3 or 5x5.\nNumber of Filters: A design choice, dictates output depth.\n\nMultiple filters \\(\\rightarrow\\) Multiple activation maps.\nEach filter learns a different feature.\n\n\n\n\n\n\n\n\nImportant\n\n\nA filter is a container for kernels. If input depth is C, a filter has C kernels.\n\n\n\n\nTrainable Parameters\n\nNumber of trainable parameters in a convolutional layer: (kernel_width * kernel_height * input_channels + 1) * num_filters (The +1 is for the bias term per filter.)\n\nLet’s illustrate:\n\nInput: 224x224x3 (RGB Image)\nFilter: 3x3 spatial size\nNumber of Filters: 32\n\nParameters = (3 * 3 * 3 + 1) * 32 Parameters = (27 + 1) * 32 Parameters = 28 * 32 = 896\n\nMuch fewer than MLPs for large images!\n\n\n\nLet’s clarify some terminology around convolutional layers. First, the depth of a filter must always match the depth of the input data it’s convolving with. If your input is a color image with 3 RGB channels, your filter must also have 3 channels or kernels. The spatial size of the filter, like 3x3 or 5x5, is a design decision. Smaller filters are common as they capture local features efficiently. The number of filters in a convolutional layer is another crucial design choice. Each filter specializes in detecting a particular feature. So, if you have 32 filters, that layer will produce 32 activation maps, each highlighting a different learned feature from the input.\nA single filter in a convolutional layer is actually a collection of kernels, one for each input channel. So, a filter for a 3-channel input consists of three 2D kernels.\nCrucially, let’s look at the trainable parameters. This is where CNNs gain their efficiency. For our example with a 224x224x3 input image, a 3x3 filter, and 32 such filters: Each filter has 3x3x3 = 27 weights (for the 3 input channels) plus 1 bias term. That’s 28 parameters per filter. Multiply this by 32 filters, and you get only 896 trainable parameters for this layer! Compare this to the billions for an MLP. This reduction is a massive advantage from an ECE perspective, making models feasible for embedded systems or real-time applications."
  },
  {
    "objectID": "others/cnn.html#filters",
    "href": "others/cnn.html#filters",
    "title": "Machine Learning",
    "section": "Filters",
    "text": "Filters\n\n\n\nConvolutional Layer with a Single Filter\n\n\n\n\n\nConvolutional Layer with Two Filters\n\n\n\n\n\nFilters Learn to Detect Structure"
  },
  {
    "objectID": "others/cnn.html#cnns-learn-hierarchical-features",
    "href": "others/cnn.html#cnns-learn-hierarchical-features",
    "title": "Machine Learning",
    "section": "CNNs Learn Hierarchical Features",
    "text": "CNNs Learn Hierarchical Features\nFrom simple edges to complex object parts.\n\n\nEarly Layers: Basic Elements\n\nFilters in the first layers learn simple, fundamental features.\nExamples: Edges (vertical, horizontal, diagonal), color blobs, textures.\nThese are general-purpose features.\n\nDeeper Layers: Complex Structures\n\nFilters in deeper layers combine features from previous layers.\nLearn to detect more abstract, composite patterns.\nExamples: Eyes, noses, wheels, ears, specific parts of objects.\n\n\nVisualizing Feature Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nThis hierarchical learning is why CNNs are so powerful. They build up complex understanding from simple visual primitives.\n\n\n\n\n\n\n\nCNNs Learn Hierarchical Features\n\n\n\nOne of the most fascinating aspects of CNNs, and a testament to their deep learning capabilities, is their ability to learn features hierarchically. In the initial layers, the filters tend to detect very basic, low-level features – things like simple edges, lines at different orientations, or blobs of color. These are universal visual primitives.\nAs the data progresses through subsequent convolutional layers, the filters in these deeper layers don’t look for individual pixels or simple edges anymore. Instead, they combine the basic features detected by the earlier layers to build representations of more complex, abstract patterns. For instance, a filter in a middle layer might learn to detect the pattern of an eye, or a wheel, or a specific texture.\nFurther still, in very deep layers, these filters can respond to even more complex, semantic parts of an object, like an entire cat’s face or the body of a car. This “part-to-whole” learning or hierarchical feature extraction is what allows CNNs to achieve such incredible performance in tasks like object recognition. The diagram on the right illustrates this progression conceptually."
  },
  {
    "objectID": "others/cnn.html#pooling-layers-spatial-dimension-reduction",
    "href": "others/cnn.html#pooling-layers-spatial-dimension-reduction",
    "title": "Machine Learning",
    "section": "Pooling Layers: Spatial Dimension Reduction",
    "text": "Pooling Layers: Spatial Dimension Reduction\nSummarizing features and reducing computations.\n\n\nPurpose\n\nDownsampling: Reduce spatial size of activation maps.\nReduced Parameters: Decreases input size to subsequent layers.\nComputation Reduction: Faster inference.\nOverfitting Mitigation: Fewer parameters, less memorization.\nTranslation Invariance Boost: Small shifts in input yield less change in pooled output.\n\nMax Pooling (Most Common)\n\nA 2D sliding filter (e.g., 2x2).\nMoves across input with a defined stride.\nOutputs the maximum value within the receptive field.\nNo trainable parameters in pooling layer itself.\n\n\nMax Pooling Example\nInput 4x4 Activation Map, 2x2 Filter, Stride 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nPooling layers summarily represent features in a smaller space. Think of it as feature aggregation.\n\n\n\n\n\n\n\nMax Pooling Layers\n\n\n\nAfter extracting features with convolutional layers, we often use pooling layers to reduce the spatial size of the activation maps. This is an important step for several reasons. Primarily, it reduces the number of parameters and computations in the network, making it faster to train and less prone to overfitting. From an ECE perspective, this means more efficient hardware utilization and potentially faster inference times on edge devices. Pooling also provides a form of translation invariance. Even if a feature shifts slightly within its receptive field, the max (or average) value will still likely be captured, leading to a more robust representation.\nThe most common type is Max Pooling. With a 2x2 filter and a stride of 2, it slides over the input and, for each window, simply picks the maximum value. Look at the example: from a 4x4 input, a 2x2 max pooling with stride 2 yields a 2x2 output. Notice that pooling layers do not have any trainable parameters themselves; they are deterministic operations. This makes them computationally inexpensive."
  },
  {
    "objectID": "others/cnn.html#the-convolutional-block-building-blocks-of-feature-extraction",
    "href": "others/cnn.html#the-convolutional-block-building-blocks-of-feature-extraction",
    "title": "Machine Learning",
    "section": "The Convolutional Block: Building Blocks of Feature Extraction",
    "text": "The Convolutional Block: Building Blocks of Feature Extraction\nCombining convolution and pooling.\n\n\nTypical Structure\n\nOne or more 2D Convolutional Layers:\n\nFeature extraction.\nFollowed by activation function (e.g., ReLU).\n\nFollowed by a Pooling Layer:\n\nSpatial dimension reduction.\nDownsize activation maps.\n\n\n\n\n\n\n\n\nTip\n\n\nVGG-16 uses 2-3 convolutional layers before each max pooling layer. Number of filters typically doubles with depth (e.g., 64 \\(\\rightarrow\\) 128 \\(\\rightarrow\\) 256).\n\n\n\n\nExample Block\n\n\n\n\n\n\n\nConvBlock\n\n\n\nInput\n\nInput (HxWxCin)\n\n\n\nConv1\n\nConv2D (FxFCout)\n(e.g., 3x3, 64 Filters)\n\n\n\nInput-&gt;Conv1\n\n\n\n\n\nReLU1\n\nReLU Activation\n\n\n\nConv1-&gt;ReLU1\n\n\n\n\n\nConv2\n\nConv2D (FxFCout)\n(e.g., 3x3, 64 Filters)\n\n\n\nReLU1-&gt;Conv2\n\n\n\n\n\nReLU2\n\nReLU Activation\n\n\n\nConv2-&gt;ReLU2\n\n\n\n\n\nMaxPool\n\nMaxPool (2x2, Stride 2)\n\n\n\nReLU2-&gt;MaxPool\n\n\n\n\n\nOutput\n\nOutput (H/2 x W/2 x Cout)\n\n\n\nMaxPool-&gt;Output\n\n\n\n\n\n\n\n\n\n\n\n\nConvolutional Block Detail\nA convolutional block is the fundamental repeating unit within the feature extractor of a CNN. It’s where the magic of feature learning and spatial downsampling happens. A typical block involves a sequence of one or more Convolutional layers, each almost always followed by a non-linear activation function like ReLU. After these convolutional layers, a pooling layer, most commonly max pooling, is applied.\nThe purpose of stacking multiple convolutional layers before pooling is to allow the network to learn increasingly complex features at the same spatial scale before reducing the resolution. Architectures like VGG-16 demonstrate this, using two or three convolutional layers before a max-pooling operation. Also, observe how the number of filters, or output channels, often increases as you go deeper into the network, capturing a richer set of features. This progressive deepening and widening allows for a powerful hierarchical feature representation."
  },
  {
    "objectID": "others/cnn.html#fully-connected-classifier",
    "href": "others/cnn.html#fully-connected-classifier",
    "title": "Machine Learning",
    "section": "Fully Connected Classifier",
    "text": "Fully Connected Classifier\nMapping extracted features to class probabilities.\n\n\nBridging Features to Decisions\n\nConnects the high-level features from the feature extractor to dense layers.\nFlattening: Output from the last convolutional block (e.g., 7x7x512) is reshaped into a 1D vector (e.g., 25088 features).\n\nRequired because dense layers expect 1D input.\n\nHidden Dense Layers: Learn complex non-linear combinations of features.\nOutput Layer:\n\nNumber of neurons = Number of classes.\nOften uses Softmax activation for multi-class probability output ([0,1] range, sums to 1).\n\n\n\nClassifier Structure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nFlattening doesn’t lose spatial information inherently; it just reorganizes it for the dense layer’s input.\n\n\n\n\n\n\n\nFully Connected Classifier\n\n\n\n\n\nFlatenning\n\n\n\nOnce the feature extractor has done its job of deriving high-level features, the fully connected classifier takes over to make sense of these features and classify the input. The output of the last convolutional block is a 3D tensor, like 7x7x512 in our VGG-16 example. However, traditional fully connected layers expect a 1D vector as input. This is where the flattening step comes in. We simply reshapes this 3D tensor into a long 1D vector. It’s important to note this is just a reorganization of data; no information is lost, and the spatial relationships embedded in the features are still there, implicitly informing the dense layers.\nAfter flattening, the data passes through one or more hidden fully connected layers, which learn intricate non-linear relationships between the extracted features. Finally, the output layer has a number of neurons equal to the number of classes. For multi-class problems, like ImageNet with 1000 classes, a Softmax activation function is typically used. Softmax converts the raw outputs into a probability distribution, where each value is between 0 and 1, and all values sum to 1, indicating the likelihood of the input belonging to each class."
  },
  {
    "objectID": "others/cnn.html#intuition-how-cnns-map-features-to-class-probabilities",
    "href": "others/cnn.html#intuition-how-cnns-map-features-to-class-probabilities",
    "title": "Machine Learning",
    "section": "Intuition: How CNNs Map Features to Class Probabilities",
    "text": "Intuition: How CNNs Map Features to Class Probabilities\nConnecting learned features to actionable predictions.\n\n\nHolistic Understanding of Image Content\n\nThe final activation maps (e.g., 7x7x512) contain rich, meaningful information.\nEach spatial location in these maps retains a relationship to the original input.\nFully connected layers can process this entire content from the image.\n\nLearned Association\n\nDuring training, the weights in the FC layers learn to associate specific feature patterns (from the activation maps) with particular output classes.\nThis mapping allows the network to “activate” the correct output neuron based on the combination of features present in the input.\n\n\n\n\n\n\n\nTip\n\n\nMinimizing the loss function tunes the weights to effectively map features to class probabilities.\n\n\n\n\nFlow from Features to Prediction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntuition\n\n\n\nIt’s easy to get lost in the mathematical details, but the core intuition for why CNNs work in classification is elegant. The feature extractor compresses the vast information of an image into a much smaller, but highly informative, set of high-level features. These features are not just random numbers; they represent semantic components like “has whiskers,” “has round eyes,” or “has four legs” for a cat, for example.\nCrucially, the fully connected classifier then takes this rich feature set and learns to associate specific combinations of these features with particular output classes. Through the training process, the billions of connections and weights are adjusted such that if the network “sees” features indicative of a cat, the neuron corresponding to “cat” in the output layer will strongly activate.\nThe process of minimizing a loss function during training guides this learning, forcing the network to adjust its weights so that it correctly maps input images to their true labels. This entire pipeline, from hierarchical feature extraction to learned classification, is why CNNs are so effective. For ECE this translates to robust object detection in autonomous vehicles, or reliable anomaly detection in medical images."
  },
  {
    "objectID": "others/cnn.html#additional-topics-in-cnns",
    "href": "others/cnn.html#additional-topics-in-cnns",
    "title": "Machine Learning",
    "section": "Additional Topics in CNNs",
    "text": "Additional Topics in CNNs\nEnhancing training and performance.\n\n\n1. Dropout\n\nPurpose: Regularization technique to prevent overfitting.\nMechanism: Randomly sets a fraction of neurons’ activations to zero during training.\nBenefit: Forces network to learn more robust features, less reliant on specific neurons.\n\n\n2. Batch Normalization\n\nPurpose: Stabilize and accelerate network training.\nMechanism: Normalizes layer inputs by subtracting batch mean and dividing by batch standard deviation.\nBenefit: Reduces “internal covariate shift,” improves gradient flow, allows higher learning rates."
  },
  {
    "objectID": "others/cnn.html#d-convolution",
    "href": "others/cnn.html#d-convolution",
    "title": "Machine Learning",
    "section": "3. 3D Convolution",
    "text": "3. 3D Convolution\n\n\n\nExtension of 2D Convolution: Kernel shifts across three axes (height, width, AND depth/time).\nApplications:\n\nMedical Imaging: Analyzing volumetric data (e.g., MRI, CT scans).\nVideo Processing: Capturing spatio-temporal features across frames.\n\nBenefit: Captures spatial relationships and temporal/depth relationships.\n\n\nSpatio-temporal Feature Extraction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nCrucial for dynamic signals and volumetric data in ECE applications.\n\n\n\n\n\nBeyond the core architecture, ECE engineers often employ several techniques to improve CNN training and performance. Dropout is a powerful regularization technique. During training, it randomly “drops out” or deactivates a percentage of neurons in a layer. This prevents complex co-adaptations between neurons and forces the network to learn more redundant and robust feature representations, mitigating overfitting.\nBatch Normalization is another critical technique. It normalizes the inputs to layers by adjusting them to have zero mean and unit variance for each mini-batch during training. This stabilizes the learning process, prevents exploding or vanishing gradients, and allows for faster training with potentially higher learning rates.\nFinally, 3D Convolution extends the concept of 2D convolution to handle volumetric data or sequential data like videos. Instead of a 2D filter, we use a 3D filter that slides not just across height and width, but also across depth (for volumetric data) or time (for video frames). This allows the network to learn spatio-temporal features, which is incredibly useful in ECE domains like medical image analysis for 3D scans, or video surveillance and action recognition."
  },
  {
    "objectID": "others/cnn.html#summary-of-key-points",
    "href": "others/cnn.html#summary-of-key-points",
    "title": "Machine Learning",
    "section": "Summary of Key Points",
    "text": "Summary of Key Points\nConsolidating our understanding of CNNs.\n\n\n\nCNNs vs. MLPs: CNNs overcome MLP limitations for images (translation invariance, parameter efficiency).\nArchitecture: Comprise a Feature Extractor (Convolutional Blocks) and a Classifier (Fully Connected Layers).\nConvolutional Layers:\n\nUse learned filters/kernels to extract features.\nParameter sharing crucial for efficiency.\n\\[ O = \\left\\lfloor \\frac{n - f + 2p}{s} \\right\\rfloor + 1 \\] governs output size.\n\nPooling Layers:\n\nDownsample activation maps (e.g., Max Pooling).\nReduce parameters, computation, and mitigate overfitting.\n\n\n\n\nHierarchical Features: CNNs learn from simple edges to complex object parts.\nFully Connected Classifier:\n\nFlattens features and maps them to class probabilities via dense layers + Softmax.\n\nEnhancements:\n\nDropout: Regularization to prevent overfitting.\nBatch Normalization: Stabilizes and accelerates training.\n3D Convolution: For spatio-temporal or volumetric data.\n\n\n\n\n\n\n\n\nImportant\n\n\nCNNs are the backbone of modern computer vision, driving innovation in diverse ECE applications.\n\n\n\n\n\nTo wrap up, let’s quickly review the main concepts we covered today. We started by understanding why CNNs were developed – to address the limitations of traditional MLPs when dealing with high-dimensional image data, particularly concerning translation invariance and the explosion of trainable parameters. We then explored the two main components of a CNN: the feature extractor, built from convolutional blocks, and the fully connected classifier. We deep-dived into convolutional layers, understanding how learned filters extract features through operations influenced by stride and padding, and how the output feature map size is calculated. Remember the formula! We looked at pooling layers, especially max pooling, and their role in downsampling, reducing complexity, and helping to prevent overfitting. A core strength of CNNs is their ability to learn hierarchical features, starting with basic edges and building up to complex patterns. The fully connected classifier then takes these rich features, flattens them, and maps them to final class probabilities using dense layers and Softmax. Finally, we touched upon practical techniques like Dropout and Batch Normalization for improving training, and the concept of 3D convolution for advanced applications involving volumetric or video data. All these elements together make CNNs indispensable tools for ECE professionals working in areas like autonomous systems, medical diagnostics, robotics, and more."
  }
]