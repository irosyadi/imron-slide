<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/quarto-contrib/live-runtime/live-runtime.js" type="module"></script>
<link href="../site_libs/quarto-contrib/live-runtime/live-runtime.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.26">

  <meta name="author" content="Imron Rosyadi">
  <title>Machine Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-b0356e9119c1bdfd0db189d130feb51c.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script type="module" src="../site_libs/quarto-ojs/quarto-ojs-runtime.js"></script>
  <link href="../site_libs/quarto-ojs/quarto-ojs.css" rel="stylesheet">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Machine Learning</h1>
  <p class="subtitle">CNN</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Imron Rosyadi 
</div>
</div>
</div>

</section>
<section>
<section id="convolutional-neural-networks-cnns" class="title-slide slide level1 center">
<h1>Convolutional Neural Networks (CNNs)</h1>
<aside class="notes">
<p>Welcome to today’s lecture on Convolutional Neural Networks. We’ll explore how these powerful models have revolutionized computer vision, focusing on their architecture and underlying operations. As ECE students, understanding the engineering principles behind CNNs is crucial for designing and deploying advanced ML systems.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="vgg-16-cnn" class="slide level2">
<h2>VGG-16 CNN</h2>

<img data-src="https://learnopencv.com/wp-content/uploads/2023/01/Convolutional-Neural-Networks-1024x611.png" class="r-stretch"></section>
<section id="what-well-cover" class="slide level2">
<h2>What We’ll Cover</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>1. Motivation: Why CNNs?</strong> - Limitations of traditional MLPs for images - Translation invariance &amp; parameter efficiency</p>
<p><strong>2. CNN Architecture Components</strong> - Feature Extractors &amp; Classifiers - VGG-16 example</p>
</div><div class="column" style="width:50%;">
<p><strong>3. The Convolutional Block</strong> - Convolutional Layers: Filters, Kernels, Operations - Pooling Layers: Downsampling &amp; regularization</p>
<p><strong>4. The Fully Connected Classifier</strong> - Transforming features to probabilities</p>
<p><strong>5. Advanced Topics &amp; Summary</strong> - Dropout, Batch Normalization, 3D Convolutions</p>
</div></div>
<aside class="notes">
<p>Our journey today will start by understanding the “why” behind CNNs, looking at the challenges traditional Multi-Layer Perceptrons face with image data. Then, we’ll dive into the fundamental building blocks of CNNs, specifically the convolutional and pooling layers, which form the feature extraction part. We’ll dissect the convolution operation itself, understanding concepts like filters, kernels, stride, and padding. After features are extracted, we’ll see how fully connected layers classify these features. Finally, we’ll touch upon some advanced techniques and summarize the key takeaways, ensuring we connect theory to practical application in ECE.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="motivation-limitations-of-mlps-for-image-data" class="slide level2">
<h2>Motivation: Limitations of MLPs for Image Data</h2>
<p>Traditional Fully Connected Networks (MLPs) struggle with image data.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="not-translation-invariant">1. Not Translation Invariant</h3>
<ul>
<li>Main content shifted = different network output.</li>
<li>Requires training on all possible shifts, which is inefficient.</li>
</ul>
<div class="callout callout-caution callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Caution</strong></p>
</div>
<div class="callout-content">
<p>MLPs treat each pixel as an independent feature. Spatial relationships are lost if features move.</p>
</div>
</div>
</div>
</div><div class="column" style="width:50%;">
<h3 id="prone-to-overfitting-parameter-explosion">2. Prone to Overfitting (Parameter Explosion)</h3>
<ul>
<li>Each input pixel connects to every neuron in the next layer.</li>
<li>Example: 224x224x3 color image <span class="math inline">\(\rightarrow\)</span> 150,528 input neurons.</li>
<li>With just three modest hidden layers, parameters can exceed <strong>300 Billion!</strong></li>
</ul>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p>Large number of parameters makes training difficult and increases overfitting risk.</p>
</div>
</div>
</div>
</div></div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://learnopencv.com/wp-content/uploads/2023/01/tensorflow-keras-mlp-image-translation-1024x341.png"></p>
<figcaption>Not Translation Invariant</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://learnopencv.com/wp-content/uploads/2023/01/tensorflow-keras-mlp-parameters-1024x414.png"></p>
<figcaption>Prone to Overfitting</figcaption>
</figure>
</div>
<aside class="notes">
<p>When we talk about image data, we’re dealing with spatial information. Think about a handwritten digit ‘7’. If it’s in the center of an image or slightly shifted to the left, it’s still a ‘7’. However, for a traditional MLP, if the ‘7’ shifts, the pixel values hitting the input neurons change dramatically, leading to a different internal representation, and potentially, a wrong classification without extensive training data for all possible shifts. This is what we mean by “not translation invariant”.</p>
<p>Secondly, images are high-dimensional. A seemingly small image like 224x224 pixels with 3 color channels has over 150,000 pixel values. If we link each of these to even a modest number of neurons in the next layer, the number of trainable weights explodes rapidly. This makes the network incredibly complex, slow to train, and highly susceptible to overfitting, where it memorizes the training data rather than learning generalizable features. This is where CNNs come in.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="convolutional-neural-networks-cnns-to-the-rescue" class="slide level2">
<h2>Convolutional Neural Networks (CNNs) to the Rescue!</h2>
<p>CNNs are designed to efficiently process image data.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="key-features">Key Features</h3>
<ul>
<li><strong>Convolution Operations:</strong> Extract features effectively.</li>
<li><strong>Parameter Sharing:</strong> Same weights process different input parts.
<ul>
<li>Greatly reduces total trainable parameters.</li>
</ul></li>
<li><strong>Translation Invariance:</strong> Detect features regardless of position.
<ul>
<li>Kernel slides, detecting patterns like edges or textures.</li>
</ul></li>
</ul>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>CNNs leverage local spatial coherence in images.</p>
</div>
</div>
</div>
</div><div class="column" style="width:50%;">
<h3 id="how-cnns-solve-mlp-issues">How CNNs Solve MLP Issues</h3>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<p><img data-src="cnn_files\figure-revealjs\mermaid-figure-1.png" style="width:5.55in;height:8.51in"></p>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div></div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://learnopencv.com/wp-content/uploads/2023/10/Convolutional-Neural-Network.png"></p>
<figcaption>VGG-16</figcaption>
</figure>
</div>
<aside class="notes">
<p>Fortunately, CNNs provide a much more effective solution for image data. The core idea is the “convolution operation.” Instead of individual neurons looking at single pixels, small learnable filters, also called kernels, slide across the image. This <strong>parameter sharing</strong> means the same filter can detect a specific feature, like a vertical edge, anywhere in the image. This dramatically reduces the number of learnable parameters compared to an MLP. Because the filter slides, it inherently gives CNNs <strong>translation invariance</strong>. If an object shifts, the same filter will still activate when it encounters that object, just at a different location on its activation map. This leads to much more efficient and robust image processing models, a cornerstone of ECE applications in robotics, autonomous systems, and medical imaging.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="cnn-architecture-feature-extractor-classifier" class="slide level2">
<h2>CNN Architecture: Feature Extractor &amp; Classifier</h2>
<p>Most CNNs follow a two-part structure:</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="feature-extractor-backbone">1. Feature Extractor (Backbone)</h3>
<ul>
<li><strong>Purpose:</strong> Extract meaningful features from raw input.</li>
<li>Comprises <strong>Convolutional Blocks</strong> (Conv + Activation + Pooling).</li>
<li>Spatial dimensions are reduced, depth (channels) increased.
<ul>
<li>Example: Input (224x224x3) <span class="math inline">\(\rightarrow\)</span> Features (7x7x512).</li>
</ul></li>
</ul>
<h3 id="classifier-head">2. Classifier (Head)</h3>
<ul>
<li><strong>Purpose:</strong> Transform extracted features into class predictions.</li>
<li>Typically uses <strong>Fully Connected (Dense) Layers</strong>.</li>
<li>Final layer outputs probabilities (e.g., Softmax).</li>
</ul>
</div><div class="column" style="width:50%;">
<h3 id="vgg-16-high-level-architecture">VGG-16 High-Level Architecture</h3>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<svg width="960" height="480" viewbox="0.00 0.00 1764.35 123.00" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style="; max-width: none; max-height: none; display: block; margin: auto auto auto auto">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 119)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-119 1760.35,-119 1760.35,4 -4,4"></polygon>
<g id="clust1" class="cluster">
<title>cluster_FE</title>
<polygon fill="none" stroke="gray" stroke-dasharray="5,2" points="116.32,-8 116.32,-107 1197.01,-107 1197.01,-8 116.32,-8"></polygon>
<text text-anchor="middle" x="656.67" y="-90.4" font-family="Times,serif" font-size="14.00">Feature Extractor (Backbone)</text>
</g>
<g id="clust2" class="cluster">
<title>cluster_Classifier</title>
<polygon fill="none" stroke="gray" stroke-dasharray="5,2" points="1217.01,-16 1217.01,-99 1748.35,-99 1748.35,-16 1217.01,-16"></polygon>
<text text-anchor="middle" x="1482.68" y="-82.4" font-family="Times,serif" font-size="14.00">Classifier (Head)</text>
</g>
<!-- Input -->
<g id="node1" class="node">
<title>Input</title>
<polygon fill="lightgreen" stroke="black" points="88.49,-65.6 -0.16,-65.6 -0.16,-24.4 88.49,-24.4 88.49,-65.6"></polygon>
<text text-anchor="middle" x="44.16" y="-49.2" font-family="Times,serif" font-size="14.00">Input Image</text>
<text text-anchor="middle" x="44.16" y="-32.4" font-family="Times,serif" font-size="14.00">(224x224x3)</text>
</g>
<!-- ConvBlock1 -->
<g id="node2" class="node">
<title>ConvBlock1</title>
<polygon fill="lightblue" stroke="black" points="302.63,-65.6 124.55,-65.6 124.55,-24.4 302.63,-24.4 302.63,-65.6"></polygon>
<text text-anchor="middle" x="213.59" y="-49.2" font-family="Times,serif" font-size="14.00">Conv-Block 1</text>
<text text-anchor="middle" x="213.59" y="-32.4" font-family="Times,serif" font-size="14.00">(e.g., 2xConv(64), MaxPool)</text>
</g>
<!-- Input&#45;&gt;ConvBlock1 -->
<g id="edge1" class="edge">
<title>Input-&gt;ConvBlock1</title>
<path fill="none" stroke="black" d="M88.72,-45C96.74,-45 105.41,-45 114.31,-45"></path>
<polygon fill="black" stroke="black" points="114.43,-48.5 124.43,-45 114.43,-41.5 114.43,-48.5"></polygon>
</g>
<!-- ConvBlock2 -->
<g id="node3" class="node">
<title>ConvBlock2</title>
<polygon fill="lightblue" stroke="black" points="524.17,-65.6 339.09,-65.6 339.09,-24.4 524.17,-24.4 524.17,-65.6"></polygon>
<text text-anchor="middle" x="431.63" y="-49.2" font-family="Times,serif" font-size="14.00">Conv-Block 2</text>
<text text-anchor="middle" x="431.63" y="-32.4" font-family="Times,serif" font-size="14.00">(e.g., 2xConv(128), MaxPool)</text>
</g>
<!-- ConvBlock1&#45;&gt;ConvBlock2 -->
<g id="edge2" class="edge">
<title>ConvBlock1-&gt;ConvBlock2</title>
<path fill="none" stroke="black" d="M302.78,-45C311.33,-45 320.05,-45 328.73,-45"></path>
<polygon fill="black" stroke="black" points="328.87,-48.5 338.87,-45 328.87,-41.5 328.87,-48.5"></polygon>
</g>
<!-- ConvBlock3 -->
<g id="node4" class="node">
<title>ConvBlock3</title>
<polygon fill="lightblue" stroke="black" points="745.71,-65.6 560.63,-65.6 560.63,-24.4 745.71,-24.4 745.71,-65.6"></polygon>
<text text-anchor="middle" x="653.17" y="-49.2" font-family="Times,serif" font-size="14.00">Conv-Block 3</text>
<text text-anchor="middle" x="653.17" y="-32.4" font-family="Times,serif" font-size="14.00">(e.g., 3xConv(256), MaxPool)</text>
</g>
<!-- ConvBlock2&#45;&gt;ConvBlock3 -->
<g id="edge3" class="edge">
<title>ConvBlock2-&gt;ConvBlock3</title>
<path fill="none" stroke="black" d="M524.46,-45C532.99,-45 541.67,-45 550.28,-45"></path>
<polygon fill="black" stroke="black" points="550.35,-48.5 560.35,-45 550.35,-41.5 550.35,-48.5"></polygon>
</g>
<!-- ConvBlock4 -->
<g id="node5" class="node">
<title>ConvBlock4</title>
<polygon fill="lightblue" stroke="black" points="967.25,-65.6 782.17,-65.6 782.17,-24.4 967.25,-24.4 967.25,-65.6"></polygon>
<text text-anchor="middle" x="874.71" y="-49.2" font-family="Times,serif" font-size="14.00">Conv-Block 4</text>
<text text-anchor="middle" x="874.71" y="-32.4" font-family="Times,serif" font-size="14.00">(e.g., 3xConv(512), MaxPool)</text>
</g>
<!-- ConvBlock3&#45;&gt;ConvBlock4 -->
<g id="edge4" class="edge">
<title>ConvBlock3-&gt;ConvBlock4</title>
<path fill="none" stroke="black" d="M746,-45C754.53,-45 763.21,-45 771.82,-45"></path>
<polygon fill="black" stroke="black" points="771.89,-48.5 781.89,-45 771.89,-41.5 771.89,-48.5"></polygon>
</g>
<!-- ConvBlock5 -->
<g id="node6" class="node">
<title>ConvBlock5</title>
<polygon fill="lightblue" stroke="black" points="1188.78,-74.4 1003.71,-74.4 1003.71,-15.6 1188.78,-15.6 1188.78,-74.4"></polygon>
<text text-anchor="middle" x="1096.25" y="-57.6" font-family="Times,serif" font-size="14.00">Conv-Block 5</text>
<text text-anchor="middle" x="1096.25" y="-40.8" font-family="Times,serif" font-size="14.00">(e.g., 3xConv(512), MaxPool)</text>
<text text-anchor="middle" x="1096.25" y="-24" font-family="Times,serif" font-size="14.00">Output: (7x7x512)</text>
</g>
<!-- ConvBlock4&#45;&gt;ConvBlock5 -->
<g id="edge5" class="edge">
<title>ConvBlock4-&gt;ConvBlock5</title>
<path fill="none" stroke="black" d="M967.54,-45C976.07,-45 984.75,-45 993.36,-45"></path>
<polygon fill="black" stroke="black" points="993.43,-48.5 1003.43,-45 993.43,-41.5 993.43,-48.5"></polygon>
</g>
<!-- Flatten -->
<g id="node7" class="node">
<title>Flatten</title>
<polygon fill="lightblue" stroke="black" points="1333.21,-65.6 1224.95,-65.6 1224.95,-24.4 1333.21,-24.4 1333.21,-65.6"></polygon>
<text text-anchor="middle" x="1279.08" y="-49.2" font-family="Times,serif" font-size="14.00">Flatten</text>
<text text-anchor="middle" x="1279.08" y="-32.4" font-family="Times,serif" font-size="14.00">(25088 features)</text>
</g>
<!-- ConvBlock5&#45;&gt;Flatten -->
<g id="edge6" class="edge">
<title>ConvBlock5-&gt;Flatten</title>
<path fill="none" stroke="black" d="M1188.99,-45C1197.59,-45 1206.18,-45 1214.44,-45"></path>
<polygon fill="black" stroke="black" points="1214.71,-48.5 1224.71,-45 1214.71,-41.5 1214.71,-48.5"></polygon>
</g>
<!-- FC6 -->
<g id="node8" class="node">
<title>FC6</title>
<polygon fill="lightblue" stroke="black" points="1454.56,-63 1369.34,-63 1369.34,-27 1454.56,-27 1454.56,-63"></polygon>
<text text-anchor="middle" x="1411.95" y="-40.8" font-family="Times,serif" font-size="14.00">FC-6 (4096)</text>
</g>
<!-- Flatten&#45;&gt;FC6 -->
<g id="edge7" class="edge">
<title>Flatten-&gt;FC6</title>
<path fill="none" stroke="black" d="M1333.31,-45C1341.76,-45 1350.49,-45 1358.92,-45"></path>
<polygon fill="black" stroke="black" points="1359,-48.5 1369,-45 1359,-41.5 1359,-48.5"></polygon>
</g>
<!-- FC7 -->
<g id="node9" class="node">
<title>FC7</title>
<polygon fill="lightblue" stroke="black" points="1576.17,-63 1490.95,-63 1490.95,-27 1576.17,-27 1576.17,-63"></polygon>
<text text-anchor="middle" x="1533.56" y="-40.8" font-family="Times,serif" font-size="14.00">FC-7 (4096)</text>
</g>
<!-- FC6&#45;&gt;FC7 -->
<g id="edge8" class="edge">
<title>FC6-&gt;FC7</title>
<path fill="none" stroke="black" d="M1454.78,-45C1463.02,-45 1471.76,-45 1480.29,-45"></path>
<polygon fill="black" stroke="black" points="1480.55,-48.5 1490.55,-45 1480.55,-41.5 1480.55,-48.5"></polygon>
</g>
<!-- Output -->
<g id="node10" class="node">
<title>Output</title>
<polygon fill="#eedd82" stroke="black" points="1740.34,-65.6 1612.37,-65.6 1612.37,-24.4 1740.34,-24.4 1740.34,-65.6"></polygon>
<text text-anchor="middle" x="1676.36" y="-49.2" font-family="Times,serif" font-size="14.00">FC-8 (1000 classes)</text>
<text text-anchor="middle" x="1676.36" y="-32.4" font-family="Times,serif" font-size="14.00">+ Softmax</text>
</g>
<!-- FC7&#45;&gt;Output -->
<g id="edge9" class="edge">
<title>FC7-&gt;Output</title>
<path fill="none" stroke="black" d="M1576.42,-45C1584.61,-45 1593.41,-45 1602.28,-45"></path>
<polygon fill="black" stroke="black" points="1602.28,-48.5 1612.28,-45 1602.28,-41.5 1602.28,-48.5"></polygon>
</g>
</g>
</svg>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div></div>

<img data-src="https://learnopencv.com/wp-content/uploads/2023/01/tensorflow-keras-convolutional-blocks-summary-1024x299.png" class="r-stretch quarto-figure-center"><p class="caption">Feature Extractor and Classifier</p><aside class="notes">
<p>A key concept in CNN architecture is the separation into a <strong>feature extractor</strong> and a <strong>classifier</strong>. The feature extractor, often called the “backbone,” is responsible for taking the raw input image and transforming it into a high-level representation of its features. This is done through a series of “convolutional blocks,” where each block typically involves convolutional layers followed by a pooling layer. As data flows through these blocks, the spatial dimensions of the data are typically reduced, while the ‘depth’ — or the number of channels — increases, representing more complex and abstract features. You can see this visually in the VGG-16 diagram, where the input starts as 224x224x3 and by ConvBlock5, it’s 7x7x512.</p>
<p>Once these features are extracted, the <strong>classifier</strong>, or “head,” takes over. It’s usually composed of one or more fully connected layers that take these abstract features and map them to probabilities for each possible output class. For VGG-16, trained on ImageNet, the output layer has 1000 neurons, each corresponding to one of the 1000 classes.</p>
<p>The Graphviz diagram on the right illustrates this high-level structure using the VGG-16 example. You can see the flow from raw image input, through the convolutional blocks, flattening the tensor, and finally through the fully connected layers to the final classification.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="convolutional-layers-the-eyes-of-a-cnn" class="slide level2 scrollable">
<h2>Convolutional Layers: The “Eyes” of a CNN</h2>
<p>Extracting features using learnable filters.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="how-it-works">How it Works</h3>
<ol type="1">
<li><strong>Input:</strong> A 2D array (image or feature map).</li>
<li><strong>Filter/Kernel:</strong> Small <code>(e.g., 3x3)</code> matrix of weights.</li>
<li><strong>Convolution Operation:</strong>
<ul>
<li>Filter slides across input data.</li>
<li>At each position, element-wise multiplication &amp; sum of filter and receptive field.</li>
<li>Result is a single number.</li>
</ul></li>
<li><strong>Activation Map:</strong> Output of the convolution, passed through an activation function.
<ul>
<li>Summarizes features from the input.</li>
</ul></li>
</ol>
</div><div class="column" style="width:50%;">
<h3 id="conceptual-diagram">Conceptual Diagram</h3>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<p><img data-src="cnn_files\figure-revealjs\mermaid-figure-9.png" style="width:25.13in;height:5.33in"></p>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Filter weights are <strong>learned</strong> during training. Unlike fixed filters (e.g., Sobel), CNN filters adapt for optimal feature detection.</p>
</div>
</div>
</div>
</div></div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://learnopencv.com/wp-content/uploads/2023/01/tensorflow-keras-convolutional-layer-example-1024x704.png"></p>
<figcaption>Convolution Operation</figcaption>
</figure>
</div>
<aside class="notes">
<p>The convolutional layer is the heart of a CNN. It’s often called the “eyes” because it’s where the network actually “looks” for patterns. Imagine you have an input image. A small window, called a filter or kernel, slides over this image. This filter contains a set of weights, which are initially random but become precisely tuned during the training process. At each position the filter lands, it performs an element-wise multiplication of its weights with the corresponding pixel values in the “receptive field” of the input. All these products are then summed up to produce a single number. This process is repeated as the filter slides, creating a new matrix called an <strong>activation map</strong>. This map essentially highlights where the feature that the filter is looking for is present in the input. Finally, this activation map usually undergoes a non-linear activation function, like ReLU, before being passed to the next layer. This non-linearity is crucial for the network to learn complex patterns.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="convolution-operation-stride-and-padding" class="slide level2">
<h2>Convolution Operation: Stride and Padding</h2>
<p>Controlling the output size and feature detection.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="stride">Stride</h3>
<ul>
<li><strong>Definition:</strong> Number of pixels the filter shifts at a time.</li>
<li><strong>Stride 1:</strong> Filter moves one pixel at a time (most common).</li>
<li><strong>Stride &gt; 1:</strong> Downsamples the output, reducing feature map size.
<ul>
<li>Learn fewer features, smaller output.</li>
</ul></li>
</ul>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p>Adjusting stride allows control over spatial dimension reduction. A stride of 2 halves the spatial dimensions.</p>
</div>
</div>
</div>
</div><div class="column" style="width:50%;">
<h3 id="padding">Padding</h3>
<ul>
<li><strong>Definition:</strong> Adding extra pixels (usually zeros) around input borders.</li>
<li><strong>Purpose:</strong>
<ul>
<li>Preserve spatial dimensions (e.g., input 32x32, output 32x32).</li>
<li>Ensure features at edges are fully processed.</li>
</ul></li>
<li><strong>Types:</strong>
<ul>
<li><strong>Valid:</strong> No padding (output smaller than input).</li>
<li><strong>Same:</strong> Adds padding to make output size equal to input size.</li>
<li><strong>Zero Padding:</strong> Most common, adds zeros.</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="convolution-operation" class="slide level2">
<h2>Convolution Operation</h2>
<h3 id="slide">Slide</h3>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://learnopencv.com/wp-content/uploads/2023/01/tensorflow-keras-convolution-example-1-1024x506.png"></p>
<figcaption>Slide</figcaption>
</figure>
</div>
<h3 id="padding-1">Padding</h3>
<p><img data-src="https://learnopencv.com/wp-content/uploads/2023/01/tensorflow-keras-convolution-example-1-1024x506.png"></p>
</section>
<section id="padding-and-stride" class="slide level2">
<h2>Padding and Stride</h2>
<p><img data-src="https://learnopencv.com/wp-content/uploads/2024/06/padding-stride-2.png" alt="With Padding, Stride = 2">(https://learnopencv.com/wp-content/uploads/2024/06/padding-stride-2.png)</p>
<p><img data-src="https://learnopencv.com/wp-content/uploads/2024/06/no_padding_no_strides.gif" alt="No Padding, Stride = 1">](https://learnopencv.com/wp-content/uploads/2024/06/no_padding_no_strides.gif)</p>
<p><img data-src="https://learnopencv.com/wp-content/uploads/2024/06/same_padding_no_strides.gif" alt="Same Padding, Stride =1">](https://learnopencv.com/wp-content/uploads/2024/06/same_padding_no_strides.gif)</p>
</section>
<section id="convolution-output-spatial-size" class="slide level2">
<h2>Convolution Output Spatial Size</h2>
<p>The output size <code>(O)</code> of a 2D convolution is calculated by:</p>
<p><span class="math display">\[ O = \left\lfloor \frac{n - f + 2p}{s} \right\rfloor + 1 \]</span></p>
<p>Where: - <code>n</code>: Input size (height or width) - <code>f</code>: Kernel size - <code>p</code>: Padding - <code>s</code>: Stride</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="calculate-your-own">Calculate Your Own!</h3>
<p>Try different values to see how they affect output size.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code hidden" id="cb1" data-startfrom="376" data-source-offset="-0"><pre class="sourceCode numberSource js number-lines code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 375;"><span id="cb1-376"><a></a>viewof input_n <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">16</span><span class="op">,</span> <span class="dv">256</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="dv">32</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="dv">1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Input Size (n)"</span>})<span class="op">;</span></span>
<span id="cb1-377"><a></a>viewof kernel_f <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">1</span><span class="op">,</span> <span class="dv">7</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="dv">3</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="dv">1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Kernel Size (f)"</span>})<span class="op">;</span></span>
<span id="cb1-378"><a></a>viewof padding_p <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">0</span><span class="op">,</span> <span class="dv">3</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="dv">1</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="dv">1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Padding (p)"</span>})<span class="op">;</span></span>
<span id="cb1-379"><a></a>viewof stride_s <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">1</span><span class="op">,</span> <span class="dv">4</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="dv">1</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="dv">1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Stride (s)"</span>})<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-2" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-3" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-4" data-nodetype="declaration">

</div>
</div>
</div>
</div>
<div>
<div id="pyodide-1" class="exercise-cell">

</div>
<script type="pyodide-1-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWUsImlucHV0IjpbImlucHV0X24iLCJrZXJuZWxfZiIsInBhZGRpbmdfcCIsInN0cmlkZV9zIl19LCJjb2RlIjoiaW1wb3J0IG1hdGhcblxubiA9IGlucHV0X25cbmYgPSBrZXJuZWxfZlxucCA9IHBhZGRpbmdfcFxucyA9IHN0cmlkZV9zXG5cbnRyeTpcbiAgICBvdXRwdXRfc2l6ZSA9IG1hdGguZmxvb3IoKG4gLSBmICsgMiAqIHApIC8gcykgKyAxXG4gICAgaWYgb3V0cHV0X3NpemUgPD0gMDpcbiAgICAgICAgcHJpbnQoZlwiT3V0cHV0IFNpemU6IHtvdXRwdXRfc2l6ZX0gKEludmFsaWQgY29uZmlndXJhdGlvbiwgcmVzdWx0cyBpbiB6ZXJvIG9yIG5lZ2F0aXZlIGRpbWVuc2lvbilcIilcbiAgICBlbHNlOlxuICAgICAgICBwcmludChmXCJJbnB1dCBTaXplIChuKToge259XCIpXG4gICAgICAgIHByaW50KGZcIktlcm5lbCBTaXplIChmKToge2Z9XCIpXG4gICAgICAgIHByaW50KGZcIlBhZGRpbmcgKHApOiB7cH1cIilcbiAgICAgICAgcHJpbnQoZlwiU3RyaWRlIChzKToge3N9XCIpXG4gICAgICAgIHByaW50KGZcIkNhbGN1bGF0ZWQgT3V0cHV0IFNpemUgKE8pOiB7b3V0cHV0X3NpemV9XCIpXG5leGNlcHQgRXhjZXB0aW9uIGFzIGU6XG4gICAgcHJpbnQoZlwiRXJyb3IgY2FsY3VsYXRpbmcgb3V0cHV0IHNpemU6IHtlfVwiKSJ9
</script>
</div>
</div><div class="column" style="width:50%;">
<h3 id="interactive-convolution-illustration">Interactive Convolution Illustration</h3>
<p>A basic visual representation of a 2D convolution.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<p><img data-src="cnn_files\figure-revealjs\mermaid-figure-8.png" style="width:25.13in;height:5.33in"></p>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div></div>
<aside class="notes">
<p>Stride and padding are critical parameters that ECE engineers need to tune when designing CNN architectures, as they directly impact the size of feature maps and the computational load. <strong>Stride</strong> defines how many pixels the filter moves across the input. A stride of 1 is common, meaning the filter moves one pixel at a time. A larger stride, like 2, causes the filter to skip pixels, effectively downsampling the spatial dimension of the output. This reduces computation but might discard some finer-grained features.</p>
<p><strong>Padding</strong> is the practice of adding pixels, usually zeros, around the borders of the input image. This is often done to prevent the spatial dimensions from shrinking too quickly as you apply multiple convolutional layers, and it also ensures that pixels at the edges of the image contribute equally to the features extracted, as they are not “under-sampled.” “Same” padding is popular because it tries to maintain the output dimensions identical to the input.</p>
<p>On this slide, you see the formula for calculating output spatial size. This is a fundamental equation for CNN design. Let’s use the interactive calculator on the left. You can adjust the input size, kernel size, padding, and stride to instantly see the calculated output size. Take the example mentioned in the text: N=32, F=3, P=1, S=1. If you enter these values, you’ll see the output is 32, meaning the input spatial dimension is preserved.</p>
<p>On the right, we have a static diagram of how a convolution operation works, visualizing the sliding window effect. While not interactive, it helps to reinforce the concept for a small input matrix.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="convolutional-operation" class="slide level2">
<h2>Convolutional Operation</h2>

<img data-src="https://learnopencv.com/wp-content/uploads/2024/06/Convolution-Operation-1-1.gif" class="r-stretch quarto-figure-center"><p class="caption">Convolutional Operation</p></section>
<section id="sobel-kernel-detecting-vertical-edges" class="slide level2">
<h2>Sobel Kernel: Detecting Vertical Edges</h2>
<p>A concrete example of a fixed filter’s operation.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="fixed-hand-crafted-kernel">Fixed (Hand-Crafted) Kernel</h3>
<ul>
<li>Sobel kernel designed to detect vertical edges.</li>
<li>Comprises positive values on one side, negatives on the other, zeros in middle.</li>
<li>Acts as a numerical approximation of a derivative in the horizontal direction.</li>
<li>Output emphasizes sudden intensity changes in the vertical direction.</li>
</ul>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>In CNNs, kernel weights are <strong>learned</strong>, allowing detection of diverse, complex features, not just predefined edges.</p>
</div>
</div>
</div>
</div><div class="column" style="width:50%;">
<h3 id="how-it-works-1">How it Works</h3>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<p><img data-src="cnn_files\figure-revealjs\mermaid-figure-7.png" style="width:25.13in;height:5.33in"></p>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div></div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://learnopencv.com/wp-content/uploads/2023/01/tensorflow-keras-cnn-sobel-filter-example.png"></p>
<figcaption>Sobel Kernel</figcaption>
</figure>
</div>
<aside class="notes">
<p>To make the convolution concept more tangible, let’s look at a classic example: the Sobel kernel. This is a predefined filter, not a learned one, used to detect vertical edges in an image. As you can see, the kernel has negative values on the left, zeros in the middle, and positive values on the right. When this kernel slides over an image, suppose it encounters a region where pixel values abruptly change from low (dark) to high (bright) from left to right. The negative values in the kernel will amplify the dark pixels, the positive values will amplify the bright pixels, and when summed, will produce a large output value, indicating a strong vertical edge. If there’s no change, the output will be close to zero.</p>
<p>The important distinction here is that while Sobel is fixed, the kernels in CNNs are <em>learned</em>. This means CNNs can discover much more complex and subtle features than simple edges, making them incredibly powerful for diverse vision tasks.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="convolutional-layer-properties" class="slide level2">
<h2>Convolutional Layer Properties</h2>
<p>Understanding channels, filters, and trainable parameters.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="filters-kernels-and-channels">Filters, Kernels, and Channels</h3>
<ul>
<li><strong>Filter Depth:</strong> Must match input data depth (number of channels).
<ul>
<li>Example: Input (HxWx<strong>3</strong>) requires filter (fxfrx<strong>3</strong>).</li>
</ul></li>
<li><strong>Spatial Size (<code>fxfr</code>):</strong> Typically <code>3x3</code> or <code>5x5</code>.</li>
<li><strong>Number of Filters:</strong> A design choice, dictates output depth.
<ul>
<li>Multiple filters <span class="math inline">\(\rightarrow\)</span> Multiple activation maps.</li>
<li>Each filter learns a different feature.</li>
</ul></li>
</ul>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>A filter is a container for kernels. If input depth is <code>C</code>, a filter has <code>C</code> kernels.</p>
</div>
</div>
</div>
</div><div class="column" style="width:50%;">
<h3 id="trainable-parameters">Trainable Parameters</h3>
<ul>
<li>Number of trainable parameters in a convolutional layer: <code>(kernel_width * kernel_height * input_channels + 1) * num_filters</code> (The <code>+1</code> is for the bias term per filter.)</li>
</ul>
<p>Let’s illustrate:</p>
<ul>
<li><strong>Input:</strong> <code>224x224x3</code> (RGB Image)</li>
<li><strong>Filter:</strong> <code>3x3</code> spatial size</li>
<li><strong>Number of Filters:</strong> <code>32</code></li>
</ul>
<p>Parameters = <code>(3 * 3 * 3 + 1) * 32</code> Parameters = <code>(27 + 1) * 32</code> Parameters = <code>28 * 32 = 896</code></p>
<ul>
<li>Much fewer than MLPs for large images!</li>
</ul>
</div></div>
<aside class="notes">
<p>Let’s clarify some terminology around convolutional layers. First, the <strong>depth of a filter</strong> must always match the depth of the input data it’s convolving with. If your input is a color image with 3 RGB channels, your filter must also have 3 channels or kernels. The <strong>spatial size</strong> of the filter, like 3x3 or 5x5, is a design decision. Smaller filters are common as they capture local features efficiently. The <strong>number of filters</strong> in a convolutional layer is another crucial design choice. Each filter specializes in detecting a particular feature. So, if you have 32 filters, that layer will produce 32 activation maps, each highlighting a different learned feature from the input.</p>
<p>A single filter in a convolutional layer is actually a collection of kernels, one for each input channel. So, a filter for a 3-channel input consists of three 2D kernels.</p>
<p>Crucially, let’s look at the <strong>trainable parameters</strong>. This is where CNNs gain their efficiency. For our example with a 224x224x3 input image, a 3x3 filter, and 32 such filters: Each filter has 3x3x3 = 27 weights (for the 3 input channels) plus 1 bias term. That’s 28 parameters per filter. Multiply this by 32 filters, and you get only 896 trainable parameters for this layer! Compare this to the billions for an MLP. This reduction is a massive advantage from an ECE perspective, making models feasible for embedded systems or real-time applications.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="filters" class="slide level2">
<h2>Filters</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://learnopencv.com/wp-content/uploads/2023/01/tensorflow-keras-convolution-one-filter-example-1024x533.png"></p>
<figcaption>Convolutional Layer with a Single Filter</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://learnopencv.com/wp-content/uploads/2023/01/tensorflow-keras-convolution-two-filter-example-1024x534.png"></p>
<figcaption>Convolutional Layer with Two Filters</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://learnopencv.com/wp-content/uploads/2023/01/tensorflow-keras-cnn-filters-learn-structure-1024x424.png"></p>
<figcaption>Filters Learn to Detect Structure</figcaption>
</figure>
</div>
</section>
<section id="cnns-learn-hierarchical-features" class="slide level2">
<h2>CNNs Learn Hierarchical Features</h2>
<p>From simple edges to complex object parts.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="early-layers-basic-elements">Early Layers: Basic Elements</h3>
<ul>
<li>Filters in the first layers learn simple, fundamental features.</li>
<li>Examples: Edges (vertical, horizontal, diagonal), color blobs, textures.</li>
<li>These are general-purpose features.</li>
</ul>
<h3 id="deeper-layers-complex-structures">Deeper Layers: Complex Structures</h3>
<ul>
<li>Filters in deeper layers combine features from previous layers.</li>
<li>Learn to detect more abstract, composite patterns.</li>
<li>Examples: Eyes, noses, wheels, ears, specific parts of objects.</li>
</ul>
</div><div class="column" style="width:50%;">
<h3 id="visualizing-feature-learning">Visualizing Feature Learning</h3>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<p><img data-src="cnn_files\figure-revealjs\mermaid-figure-6.png" style="width:25.13in;height:5.33in"></p>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p>This hierarchical learning is why CNNs are so powerful. They build up complex understanding from simple visual primitives.</p>
</div>
</div>
</div>
</div></div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://learnopencv.com/wp-content/uploads/2023/01/tensorflow-keras-cnn-hierarchical-structure-1024x590.png"></p>
<figcaption>CNNs Learn Hierarchical Features</figcaption>
</figure>
</div>
<aside class="notes">
<p>One of the most fascinating aspects of CNNs, and a testament to their deep learning capabilities, is their ability to learn features hierarchically. In the initial layers, the filters tend to detect very basic, low-level features – things like simple edges, lines at different orientations, or blobs of color. These are universal visual primitives.</p>
<p>As the data progresses through subsequent convolutional layers, the filters in these deeper layers don’t look for individual pixels or simple edges anymore. Instead, they combine the basic features detected by the earlier layers to build representations of more complex, abstract patterns. For instance, a filter in a middle layer might learn to detect the pattern of an eye, or a wheel, or a specific texture.</p>
<p>Further still, in very deep layers, these filters can respond to even more complex, semantic parts of an object, like an entire cat’s face or the body of a car. This “part-to-whole” learning or hierarchical feature extraction is what allows CNNs to achieve such incredible performance in tasks like object recognition. The diagram on the right illustrates this progression conceptually.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="pooling-layers-spatial-dimension-reduction" class="slide level2">
<h2>Pooling Layers: Spatial Dimension Reduction</h2>
<p>Summarizing features and reducing computations.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="purpose">Purpose</h3>
<ul>
<li><strong>Downsampling:</strong> Reduce spatial size of activation maps.</li>
<li><strong>Reduced Parameters:</strong> Decreases input size to subsequent layers.</li>
<li><strong>Computation Reduction:</strong> Faster inference.</li>
<li><strong>Overfitting Mitigation:</strong> Fewer parameters, less memorization.</li>
<li><strong>Translation Invariance Boost:</strong> Small shifts in input yield less change in pooled output.</li>
</ul>
<h3 id="max-pooling-most-common">Max Pooling (Most Common)</h3>
<ul>
<li>A 2D sliding filter (e.g., <code>2x2</code>).</li>
<li>Moves across input with a defined stride.</li>
<li>Outputs the <strong>maximum value</strong> within the receptive field.</li>
<li><strong>No trainable parameters</strong> in pooling layer itself.</li>
</ul>
</div><div class="column" style="width:50%;">
<h3 id="max-pooling-example">Max Pooling Example</h3>
<p>Input <code>4x4</code> Activation Map, <code>2x2</code> Filter, Stride <code>2</code></p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<p><img data-src="cnn_files\figure-revealjs\mermaid-figure-5.png" style="width:12.38in;height:8.69in"></p>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Pooling layers summarily represent features in a smaller space. Think of it as feature aggregation.</p>
</div>
</div>
</div>
</div></div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://learnopencv.com/wp-content/uploads/2023/01/tensorflow-keras-max-pooling-example-1024x470.png"></p>
<figcaption>Max Pooling Layers</figcaption>
</figure>
</div>
<aside class="notes">
<p>After extracting features with convolutional layers, we often use <strong>pooling layers</strong> to reduce the spatial size of the activation maps. This is an important step for several reasons. Primarily, it reduces the number of parameters and computations in the network, making it faster to train and less prone to overfitting. From an ECE perspective, this means more efficient hardware utilization and potentially faster inference times on edge devices. Pooling also provides a form of translation invariance. Even if a feature shifts slightly within its receptive field, the max (or average) value will still likely be captured, leading to a more robust representation.</p>
<p>The most common type is <strong>Max Pooling</strong>. With a 2x2 filter and a stride of 2, it slides over the input and, for each window, simply picks the maximum value. Look at the example: from a 4x4 input, a 2x2 max pooling with stride 2 yields a 2x2 output. Notice that pooling layers do not have any trainable parameters themselves; they are deterministic operations. This makes them computationally inexpensive.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="the-convolutional-block-building-blocks-of-feature-extraction" class="slide level2">
<h2>The Convolutional Block: Building Blocks of Feature Extraction</h2>
<p>Combining convolution and pooling.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="typical-structure">Typical Structure</h3>
<ul>
<li>One or more <strong>2D Convolutional Layers</strong>:
<ul>
<li>Feature extraction.</li>
<li>Followed by activation function (e.g., ReLU).</li>
</ul></li>
<li>Followed by a <strong>Pooling Layer</strong>:
<ul>
<li>Spatial dimension reduction.</li>
<li>Downsize activation maps.</li>
</ul></li>
</ul>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p>VGG-16 uses 2-3 convolutional layers before each max pooling layer. Number of filters typically doubles with depth (e.g., 64 <span class="math inline">\(\rightarrow\)</span> 128 <span class="math inline">\(\rightarrow\)</span> 256).</p>
</div>
</div>
</div>
</div><div class="column" style="width:50%;">
<h3 id="example-block">Example Block</h3>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<svg width="960" height="480" viewbox="0.00 0.00 176.04 487.20" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style="; max-width: none; max-height: none; display: block; margin: auto auto auto auto">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 483.2)">
<title>ConvBlock</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-483.2 172.04,-483.2 172.04,4 -4,4"></polygon>
<!-- Input -->
<g id="node1" class="node">
<title>Input</title>
<polygon fill="lightcyan" stroke="black" points="141.95,-479.2 26.09,-479.2 26.09,-443.2 141.95,-443.2 141.95,-479.2"></polygon>
<text text-anchor="middle" x="84.02" y="-457" font-family="Times,serif" font-size="14.00">Input (HxWxCin)</text>
</g>
<!-- Conv1 -->
<g id="node2" class="node">
<title>Conv1</title>
<polygon fill="lightcyan" stroke="black" points="150.84,-407 17.2,-407 17.2,-365.8 150.84,-365.8 150.84,-407"></polygon>
<text text-anchor="middle" x="84.02" y="-390.6" font-family="Times,serif" font-size="14.00">Conv2D (FxFCout)</text>
<text text-anchor="middle" x="84.02" y="-373.8" font-family="Times,serif" font-size="14.00">(e.g., 3x3, 64 Filters)</text>
</g>
<!-- Input&#45;&gt;Conv1 -->
<g id="edge1" class="edge">
<title>Input-&gt;Conv1</title>
<path fill="none" stroke="black" d="M84.02,-442.95C84.02,-435.29 84.02,-426.06 84.02,-417.34"></path>
<polygon fill="black" stroke="black" points="87.52,-417.1 84.02,-407.1 80.52,-417.1 87.52,-417.1"></polygon>
</g>
<!-- ReLU1 -->
<g id="node3" class="node">
<title>ReLU1</title>
<polygon fill="lightcyan" stroke="black" points="140.33,-329.6 27.71,-329.6 27.71,-293.6 140.33,-293.6 140.33,-329.6"></polygon>
<text text-anchor="middle" x="84.02" y="-307.4" font-family="Times,serif" font-size="14.00">ReLU Activation</text>
</g>
<!-- Conv1&#45;&gt;ReLU1 -->
<g id="edge2" class="edge">
<title>Conv1-&gt;ReLU1</title>
<path fill="none" stroke="black" d="M84.02,-365.51C84.02,-357.69 84.02,-348.59 84.02,-340.16"></path>
<polygon fill="black" stroke="black" points="87.52,-339.99 84.02,-329.99 80.52,-339.99 87.52,-339.99"></polygon>
</g>
<!-- Conv2 -->
<g id="node4" class="node">
<title>Conv2</title>
<polygon fill="lightcyan" stroke="black" points="150.84,-257.4 17.2,-257.4 17.2,-216.2 150.84,-216.2 150.84,-257.4"></polygon>
<text text-anchor="middle" x="84.02" y="-241" font-family="Times,serif" font-size="14.00">Conv2D (FxFCout)</text>
<text text-anchor="middle" x="84.02" y="-224.2" font-family="Times,serif" font-size="14.00">(e.g., 3x3, 64 Filters)</text>
</g>
<!-- ReLU1&#45;&gt;Conv2 -->
<g id="edge3" class="edge">
<title>ReLU1-&gt;Conv2</title>
<path fill="none" stroke="black" d="M84.02,-293.35C84.02,-285.69 84.02,-276.46 84.02,-267.74"></path>
<polygon fill="black" stroke="black" points="87.52,-267.5 84.02,-257.5 80.52,-267.5 87.52,-267.5"></polygon>
</g>
<!-- ReLU2 -->
<g id="node5" class="node">
<title>ReLU2</title>
<polygon fill="lightcyan" stroke="black" points="140.33,-180 27.71,-180 27.71,-144 140.33,-144 140.33,-180"></polygon>
<text text-anchor="middle" x="84.02" y="-157.8" font-family="Times,serif" font-size="14.00">ReLU Activation</text>
</g>
<!-- Conv2&#45;&gt;ReLU2 -->
<g id="edge4" class="edge">
<title>Conv2-&gt;ReLU2</title>
<path fill="none" stroke="black" d="M84.02,-215.91C84.02,-208.09 84.02,-198.99 84.02,-190.56"></path>
<polygon fill="black" stroke="black" points="87.52,-190.39 84.02,-180.39 80.52,-190.39 87.52,-190.39"></polygon>
</g>
<!-- MaxPool -->
<g id="node6" class="node">
<title>MaxPool</title>
<polygon fill="lightcyan" stroke="black" points="160.13,-108 7.92,-108 7.92,-72 160.13,-72 160.13,-108"></polygon>
<text text-anchor="middle" x="84.02" y="-85.8" font-family="Times,serif" font-size="14.00">MaxPool (2x2, Stride 2)</text>
</g>
<!-- ReLU2&#45;&gt;MaxPool -->
<g id="edge5" class="edge">
<title>ReLU2-&gt;MaxPool</title>
<path fill="none" stroke="black" d="M84.02,-143.7C84.02,-135.98 84.02,-126.71 84.02,-118.11"></path>
<polygon fill="black" stroke="black" points="87.52,-118.1 84.02,-108.1 80.52,-118.1 87.52,-118.1"></polygon>
</g>
<!-- Output -->
<g id="node7" class="node">
<title>Output</title>
<polygon fill="lightgreen" stroke="black" points="168.07,-36 -0.02,-36 -0.02,0 168.07,0 168.07,-36"></polygon>
<text text-anchor="middle" x="84.02" y="-13.8" font-family="Times,serif" font-size="14.00">Output (H/2 x W/2 x Cout)</text>
</g>
<!-- MaxPool&#45;&gt;Output -->
<g id="edge6" class="edge">
<title>MaxPool-&gt;Output</title>
<path fill="none" stroke="black" d="M84.02,-71.7C84.02,-63.98 84.02,-54.71 84.02,-46.11"></path>
<polygon fill="black" stroke="black" points="87.52,-46.1 84.02,-36.1 80.52,-46.1 87.52,-46.1"></polygon>
</g>
</g>
</svg>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div></div>

<img data-src="https://learnopencv.com/wp-content/uploads/2023/01/tensorflow-keras-convolution-block-detail-1024x602.png" class="r-stretch quarto-figure-center"><p class="caption">Convolutional Block Detail</p><aside class="notes">
<p>A <strong>convolutional block</strong> is the fundamental repeating unit within the feature extractor of a CNN. It’s where the magic of feature learning and spatial downsampling happens. A typical block involves a sequence of one or more Convolutional layers, each almost always followed by a non-linear activation function like ReLU. After these convolutional layers, a pooling layer, most commonly max pooling, is applied.</p>
<p>The purpose of stacking multiple convolutional layers before pooling is to allow the network to learn increasingly complex features at the same spatial scale before reducing the resolution. Architectures like VGG-16 demonstrate this, using two or three convolutional layers before a max-pooling operation. Also, observe how the number of filters, or output channels, often increases as you go deeper into the network, capturing a richer set of features. This progressive deepening and widening allows for a powerful hierarchical feature representation.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="fully-connected-classifier" class="slide level2">
<h2>Fully Connected Classifier</h2>
<p>Mapping extracted features to class probabilities.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="bridging-features-to-decisions">Bridging Features to Decisions</h3>
<ul>
<li>Connects the high-level features from the feature extractor to dense layers.</li>
<li><strong>Flattening:</strong> Output from the last convolutional block (e.g., <code>7x7x512</code>) is reshaped into a 1D vector (e.g., <code>25088</code> features).
<ul>
<li>Required because dense layers expect 1D input.</li>
</ul></li>
<li><strong>Hidden Dense Layers:</strong> Learn complex non-linear combinations of features.</li>
<li><strong>Output Layer:</strong>
<ul>
<li>Number of neurons = Number of classes.</li>
<li>Often uses <strong>Softmax</strong> activation for multi-class probability output (<code>[0,1]</code> range, sums to 1).</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<h3 id="classifier-structure">Classifier Structure</h3>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<p><img data-src="cnn_files\figure-revealjs\mermaid-figure-4.png" style="width:25.13in;height:5.33in"></p>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Flattening doesn’t lose spatial information inherently; it just reorganizes it for the dense layer’s input.</p>
</div>
</div>
</div>
</div></div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://learnopencv.com/wp-content/uploads/2023/01/tensorflow-keras-convolutional-blocks-summary-channels-1024x365.png"></p>
<figcaption>Fully Connected Classifier</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://learnopencv.com/wp-content/uploads/2023/01/tensorflow-keras-vgg16-three-fully-connected-layers-1024x583.png"></p>
<figcaption>Flatenning</figcaption>
</figure>
</div>
<aside class="notes">
<p>Once the feature extractor has done its job of deriving high-level features, the <strong>fully connected classifier</strong> takes over to make sense of these features and classify the input. The output of the last convolutional block is a 3D tensor, like 7x7x512 in our VGG-16 example. However, traditional fully connected layers expect a 1D vector as input. This is where the <strong>flattening</strong> step comes in. We simply reshapes this 3D tensor into a long 1D vector. It’s important to note this is just a reorganization of data; no information is lost, and the spatial relationships embedded in the features are still there, implicitly informing the dense layers.</p>
<p>After flattening, the data passes through one or more hidden fully connected layers, which learn intricate non-linear relationships between the extracted features. Finally, the output layer has a number of neurons equal to the number of classes. For multi-class problems, like ImageNet with 1000 classes, a <strong>Softmax activation function</strong> is typically used. Softmax converts the raw outputs into a probability distribution, where each value is between 0 and 1, and all values sum to 1, indicating the likelihood of the input belonging to each class.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="intuition-how-cnns-map-features-to-class-probabilities" class="slide level2">
<h2>Intuition: How CNNs Map Features to Class Probabilities</h2>
<p>Connecting learned features to actionable predictions.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="holistic-understanding-of-image-content">Holistic Understanding of Image Content</h3>
<ul>
<li>The final activation maps (e.g., 7x7x512) contain rich, meaningful information.</li>
<li>Each spatial location in these maps retains a relationship to the original input.</li>
<li>Fully connected layers can process this <strong>entire content</strong> from the image.</li>
</ul>
<h3 id="learned-association">Learned Association</h3>
<ul>
<li>During training, the weights in the FC layers learn to associate specific feature patterns (from the activation maps) with particular output classes.</li>
<li>This mapping allows the network to “activate” the correct output neuron based on the combination of features present in the input.</li>
</ul>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p>Minimizing the loss function tunes the weights to effectively map features to class probabilities.</p>
</div>
</div>
</div>
</div><div class="column" style="width:50%;">
<h3 id="flow-from-features-to-prediction">Flow from Features to Prediction</h3>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<p><img data-src="cnn_files\figure-revealjs\mermaid-figure-3.png" style="width:25.13in;height:5.33in"></p>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div></div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://learnopencv.com/wp-content/uploads/2023/01/tensorflow-keras-cnn-activation-maps-classifier-intuition-1024x648.jpg"></p>
<figcaption>Intuition</figcaption>
</figure>
</div>
<aside class="notes">
<p>It’s easy to get lost in the mathematical details, but the core intuition for why CNNs work in classification is elegant. The feature extractor compresses the vast information of an image into a much smaller, but highly informative, set of high-level features. These features are not just random numbers; they represent semantic components like “has whiskers,” “has round eyes,” or “has four legs” for a cat, for example.</p>
<p>Crucially, the fully connected classifier then takes this rich feature set and learns to associate specific combinations of these features with particular output classes. Through the training process, the billions of connections and weights are adjusted such that if the network “sees” features indicative of a cat, the neuron corresponding to “cat” in the output layer will strongly activate.</p>
<p>The process of minimizing a loss function during training guides this learning, forcing the network to adjust its weights so that it correctly maps input images to their true labels. This entire pipeline, from hierarchical feature extraction to learned classification, is why CNNs are so effective. For ECE this translates to robust object detection in autonomous vehicles, or reliable anomaly detection in medical images.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="additional-topics-in-cnns" class="slide level2">
<h2>Additional Topics in CNNs</h2>
<p>Enhancing training and performance.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="dropout">1. Dropout</h3>
<ul>
<li><strong>Purpose:</strong> Regularization technique to prevent overfitting.</li>
<li><strong>Mechanism:</strong> Randomly sets a fraction of neurons’ activations to zero during training.</li>
<li><strong>Benefit:</strong> Forces network to learn more robust features, less reliant on specific neurons.</li>
</ul>
</div><div class="column" style="width:50%;">
<h3 id="batch-normalization">2. Batch Normalization</h3>
<ul>
<li><strong>Purpose:</strong> Stabilize and accelerate network training.</li>
<li><strong>Mechanism:</strong> Normalizes layer inputs by subtracting batch mean and dividing by batch standard deviation.</li>
<li><strong>Benefit:</strong> Reduces “internal covariate shift,” improves gradient flow, allows higher learning rates.</li>
</ul>
</div></div>
</section>
<section id="d-convolution" class="slide level2">
<h2>3. 3D Convolution</h2>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><strong>Extension of 2D Convolution:</strong> Kernel shifts across three axes (height, width, AND depth/time).</li>
<li><strong>Applications:</strong>
<ul>
<li><strong>Medical Imaging:</strong> Analyzing volumetric data (e.g., MRI, CT scans).</li>
<li><strong>Video Processing:</strong> Capturing spatio-temporal features across frames.</li>
</ul></li>
<li><strong>Benefit:</strong> Captures spatial relationships <em>and</em> temporal/depth relationships.</li>
</ul>
</div><div class="column" style="width:50%;">
<h3 id="spatio-temporal-feature-extraction">Spatio-temporal Feature Extraction</h3>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<p><img data-src="cnn_files\figure-revealjs\mermaid-figure-2.png" style="width:2.88in;height:4.48in"></p>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Crucial for dynamic signals and volumetric data in ECE applications.</p>
</div>
</div>
</div>
</div></div>
<aside class="notes">
<p>Beyond the core architecture, ECE engineers often employ several techniques to improve CNN training and performance. <strong>Dropout</strong> is a powerful regularization technique. During training, it randomly “drops out” or deactivates a percentage of neurons in a layer. This prevents complex co-adaptations between neurons and forces the network to learn more redundant and robust feature representations, mitigating overfitting.</p>
<p><strong>Batch Normalization</strong> is another critical technique. It normalizes the inputs to layers by adjusting them to have zero mean and unit variance for each mini-batch during training. This stabilizes the learning process, prevents exploding or vanishing gradients, and allows for faster training with potentially higher learning rates.</p>
<p>Finally, <strong>3D Convolution</strong> extends the concept of 2D convolution to handle volumetric data or sequential data like videos. Instead of a 2D filter, we use a 3D filter that slides not just across height and width, but also across depth (for volumetric data) or time (for video frames). This allows the network to learn spatio-temporal features, which is incredibly useful in ECE domains like medical image analysis for 3D scans, or video surveillance and action recognition.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="summary-of-key-points" class="slide level2">
<h2>Summary of Key Points</h2>
<p>Consolidating our understanding of CNNs.</p>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><strong>CNNs vs.&nbsp;MLPs:</strong> CNNs overcome MLP limitations for images (translation invariance, parameter efficiency).</li>
<li><strong>Architecture:</strong> Comprise a Feature Extractor (Convolutional Blocks) and a Classifier (Fully Connected Layers).</li>
<li><strong>Convolutional Layers:</strong>
<ul>
<li>Use learned filters/kernels to extract features.</li>
<li>Parameter sharing crucial for efficiency.</li>
<li><strong><span class="math display">\[ O = \left\lfloor \frac{n - f + 2p}{s} \right\rfloor + 1 \]</span></strong> governs output size.</li>
</ul></li>
<li><strong>Pooling Layers:</strong>
<ul>
<li>Downsample activation maps (e.g., Max Pooling).</li>
<li>Reduce parameters, computation, and mitigate overfitting.</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<ul>
<li><strong>Hierarchical Features:</strong> CNNs learn from simple edges to complex object parts.</li>
<li><strong>Fully Connected Classifier:</strong>
<ul>
<li>Flattens features and maps them to class probabilities via dense layers + Softmax.</li>
</ul></li>
<li><strong>Enhancements:</strong>
<ul>
<li><strong>Dropout:</strong> Regularization to prevent overfitting.</li>
<li><strong>Batch Normalization:</strong> Stabilizes and accelerates training.</li>
<li><strong>3D Convolution:</strong> For spatio-temporal or volumetric data.</li>
</ul></li>
</ul>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>CNNs are the backbone of modern computer vision, driving innovation in diverse ECE applications.</p>
</div>
</div>
</div>
</div></div>
<aside class="notes">
<p>To wrap up, let’s quickly review the main concepts we covered today. We started by understanding why CNNs were developed – to address the limitations of traditional MLPs when dealing with high-dimensional image data, particularly concerning translation invariance and the explosion of trainable parameters. We then explored the two main components of a CNN: the feature extractor, built from convolutional blocks, and the fully connected classifier. We deep-dived into convolutional layers, understanding how learned filters extract features through operations influenced by stride and padding, and how the output feature map size is calculated. Remember the formula! We looked at pooling layers, especially max pooling, and their role in downsampling, reducing complexity, and helping to prevent overfitting. A core strength of CNNs is their ability to learn hierarchical features, starting with basic edges and building up to complex patterns. The fully connected classifier then takes these rich features, flattens them, and maps them to final class probabilities using dense layers and Softmax. Finally, we touched upon practical techniques like Dropout and Batch Normalization for improving training, and the concept of 3D convolution for advanced applications involving volumetric or video data. All these elements together make CNNs indispensable tools for ECE professionals working in areas like autonomous systems, medical diagnostics, robotics, and more.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>


<script type="pyodide-data">
eyJvcHRpb25zIjp7ImVudiI6eyJQTE9UTFlfUkVOREVSRVIiOiJwbG90bHlfbWltZXR5cGUifSwiaW5kZXhVUkwiOiJodHRwczovL2Nkbi5qc2RlbGl2ci5uZXQvcHlvZGlkZS92MC4yNy4wL2Z1bGwvIn0sInBhY2thZ2VzIjp7InBrZ3MiOlsicHlvZGlkZV9odHRwIiwibWljcm9waXAiLCJpcHl0aG9uIiwibnVtcHkiLCJwbG90bHkiLCJuYmZvcm1hdCJdfX0=
</script>
<script type="ojs-module-contents">
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
</script>
<div id="exercise-loading-indicator" class="exercise-loading-indicator d-none d-flex align-items-center gap-2">
<div id="exercise-loading-status" class="d-flex gap-2">

</div>
<div class="spinner-grow spinner-grow-sm">

</div>
</div>
<script type="vfs-file">
W10=
</script>
</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="../qrjs_pics/unsoed_logo.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://imron-slide.vercel.app">irosyadi-2025</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script type="ojs-module-contents">
    eyJjb250ZW50cyI6W3sibWV0aG9kTmFtZSI6ImludGVycHJldCIsImNlbGxOYW1lIjoib2pzLWNlbGwtMSIsImlubGluZSI6ZmFsc2UsInNvdXJjZSI6InZpZXdvZiBpbnB1dF9uID0gSW5wdXRzLnJhbmdlKFsxNiwgMjU2XSwge3ZhbHVlOiAzMiwgc3RlcDogMSwgbGFiZWw6IFwiSW5wdXQgU2l6ZSAobilcIn0pO1xudmlld29mIGtlcm5lbF9mID0gSW5wdXRzLnJhbmdlKFsxLCA3XSwge3ZhbHVlOiAzLCBzdGVwOiAxLCBsYWJlbDogXCJLZXJuZWwgU2l6ZSAoZilcIn0pO1xudmlld29mIHBhZGRpbmdfcCA9IElucHV0cy5yYW5nZShbMCwgM10sIHt2YWx1ZTogMSwgc3RlcDogMSwgbGFiZWw6IFwiUGFkZGluZyAocClcIn0pO1xudmlld29mIHN0cmlkZV9zID0gSW5wdXRzLnJhbmdlKFsxLCA0XSwge3ZhbHVlOiAxLCBzdGVwOiAxLCBsYWJlbDogXCJTdHJpZGUgKHMpXCJ9KTtcbiJ9LHsibWV0aG9kTmFtZSI6ImludGVycHJldFF1aWV0Iiwic291cmNlIjoic2hpbnlJbnB1dCgnaW5wdXRfbicpIn0seyJtZXRob2ROYW1lIjoiaW50ZXJwcmV0UXVpZXQiLCJzb3VyY2UiOiJzaGlueUlucHV0KCdrZXJuZWxfZicpIn0seyJtZXRob2ROYW1lIjoiaW50ZXJwcmV0UXVpZXQiLCJzb3VyY2UiOiJzaGlueUlucHV0KCdwYWRkaW5nX3AnKSJ9LHsibWV0aG9kTmFtZSI6ImludGVycHJldFF1aWV0Iiwic291cmNlIjoic2hpbnlJbnB1dCgnc3RyaWRlX3MnKSJ9XX0=
    </script>
    <script type="module">
    if (window.location.protocol === "file:") { alert("The OJS runtime does not work with file:// URLs. Please use a web server to view this document."); }
    window._ojs.paths.runtimeToDoc = "../../others";
    window._ojs.paths.runtimeToRoot = "../..";
    window._ojs.paths.docToRoot = "..";
    window._ojs.selfContained = false;
    window._ojs.runtime.interpretFromScriptTags();
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>