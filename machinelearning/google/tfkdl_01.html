<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/quarto-contrib/live-runtime/live-runtime.js" type="module"></script>
<link href="../site_libs/quarto-contrib/live-runtime/live-runtime.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.25">

  <meta name="author" content="Imron Rosyadi">
  <title>Machine Learning – Machine-Learning-03</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-b0356e9119c1bdfd0db189d130feb51c.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script type="module" src="../site_libs/quarto-ojs/quarto-ojs-runtime.js"></script>
  <link href="../site_libs/quarto-ojs/quarto-ojs.css" rel="stylesheet">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Machine-Learning-03</h1>
  <p class="subtitle">Tensorflow, Keras, Deep Learning</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Imron Rosyadi 
</div>
</div>
</div>

</section>
<section id="introduction-to-cnns" class="slide level2">
<h2>Introduction to CNNs</h2>
<p>Bridging Theory and ECE Applications</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>What is a CNN?</strong></p>
<ul>
<li>A type of <strong>neural network</strong> specialized in processing data with a grid-like topology.</li>
<li>Primarily used for <strong>image recognition</strong>, computer vision, and signal processing.</li>
<li>Inspired by the <strong>human visual cortex</strong>.</li>
</ul>
<p><strong>Why CNNs for ECE?</strong></p>
<ul>
<li><strong>Image/Video Processing:</strong> Autonomous vehicles, surveillance, medical imaging.</li>
<li><strong>Signal Processing:</strong> Audio analysis, sensor data interpretation.</li>
<li><strong>Embedded Systems:</strong> Efficient deployment on edge devices.</li>
</ul>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p>In ECE, understanding CNNs is crucial for designing intelligent systems that interact with multimodal data.</p>
</div>
</div>
</div>
</div><div class="column" style="width:50%;">
<p><img data-src="https://codelabs.developers.google.com/static/codelabs/cloud-tensorflow-mnist/img/ca8f34f3d28ea528.gif"></p>
<p><em>Illustration: Filtering an image with two successive filters, each made of 4x4x3=48 learnable weights.</em></p>
<aside class="notes">
<p>CNNs are a cornerstone of modern AI, especially for tasks involving spatial data. For ECE students, it’s not just about theoretical understanding but also about their practical implementation in hardware and software for real-world applications. This introductory slide sets the stage for diving into the core mechanisms.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="core-concept-the-convolution-operation" class="slide level2">
<h2>Core Concept: The Convolution Operation</h2>
<p>A Localized Filtering Approach</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Localized Receptive Fields</strong></p>
<ul>
<li>Unlike dense layers where each neuron sees the entire input, a CNN neuron processes only a <strong>small, local region</strong> of the input (its <strong>receptive field</strong>).</li>
<li>This significantly reduces the number of learnable parameters.</li>
</ul>
<p><strong>Weight Sharing (Filtered Operations)</strong></p>
<ul>
<li>The <strong>same set of weights</strong> (a <strong>filter</strong> or <strong>kernel</strong>) is applied across the entire image.</li>
<li>This operation is known as <strong>convolution</strong>.
<ul>
<li>It acts like a digital filter, detecting specific features (edges, textures, patterns).</li>
</ul></li>
</ul>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>This operation is analogous to applying FIR/IIR filters in digital signal processing, but here the filter coefficients are learned.</p>
</div>
</div>
</div>
</div><div class="column" style="width:50%;">
<p><img data-src="https://codelabs.developers.google.com/static/codelabs/cloud-tensorflow-mnist/img/bd9f83d6cb62b5fb.png"></p>
<p><em>Analogy: Each filter creates a “feature map” or “channel” highlighting specific patterns.</em></p>
<p><strong>Why Multiple Filters/Channels?</strong></p>
<ul>
<li>A single filter might detect one type of feature (e.g., horizontal edges).</li>
<li>To detect a variety of features, we use <strong>multiple filters</strong>.</li>
<li>Each filter produces a separate <strong>output channel</strong> or <strong>feature map</strong>.</li>
<li>These multiple channels are stacked to form a new “data cube”.</li>
</ul>
</div></div>
</section>
<section id="visualizing-the-convolution-operation" class="slide level2">
<h2>Visualizing the Convolution Operation</h2>
<p>An Interactive Example</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Input:</strong> A simple 2D grid representing an image or signal.</p>
<p><strong>Kernel:</strong> A smaller 2D filter that slides over the input.</p>
<p><strong>Operation:</strong> At each position, the kernel’s values are multiplied element-wise by the corresponding input values within its receptive field, and the products are summed to form a single output pixel.</p>
<p><strong>Key Parameters:</strong> - <strong>Kernel Size:</strong> Dimensions of the filter (e.g., 3x3, 5x5). - <strong>Stride:</strong> Number of pixels the filter shifts at each step. - <strong>Padding:</strong> Adding zeros to the input edges to preserve spatial dimensions.</p>
</div><div class="column" style="width:50%;">
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code hidden" id="cb1" data-startfrom="114" data-source-offset="-0"><pre class="sourceCode numberSource js number-lines code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 113;"><span id="cb1-114"><a></a>viewof input_size <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">3</span><span class="op">,</span> <span class="dv">10</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="dv">5</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="dv">1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Input Size (N x N)"</span>})<span class="op">;</span></span>
<span id="cb1-115"><a></a>viewof kernel_size <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">1</span><span class="op">,</span> <span class="dv">3</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="dv">3</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="dv">2</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Kernel Size (K x K)"</span>})<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-2" data-nodetype="declaration">

</div>
</div>
</div>
</div>
<div>
<div id="pyodide-1" class="exercise-cell">

</div>
<script type="pyodide-1-contents">
eyJhdHRyIjp7ImVjaG8iOmZhbHNlLCJlZGl0IjpmYWxzZSwiZXZhbCI6dHJ1ZSwiaW5wdXQiOlsiaW5wdXRfc2l6ZSIsImtlcm5lbF9zaXplIl19LCJjb2RlIjoiXG5pbXBvcnQgbnVtcHkgYXMgbnBcbmltcG9ydCBwbG90bHkuZ3JhcGhfb2JqZWN0cyBhcyBnb1xuXG4jIEVuc3VyZSBrZXJuZWxfc2l6ZSBpcyBhbHdheXMgb2RkIGZvciBzaW1wbGljaXR5IGluIGNlbnRlcmluZ1xua2VybmVsX3NpemVfdmFsID0gaW50KGtlcm5lbF9zaXplKSBpZiBpbnQoa2VybmVsX3NpemUpICUgMiAhPSAwIGVsc2UgaW50KGtlcm5lbF9zaXplKSAtIDFcbmlmIGtlcm5lbF9zaXplX3ZhbCA8IDE6IGtlcm5lbF9zaXplX3ZhbCA9IDEgIyBNaW5pbXVtIGtlcm5lbCBzaXplIGlzIDFcblxuaW5wdXRfZGF0YSA9IG5wLnJhbmRvbS5yYW5kaW50KDAsIDEwLCBzaXplPShpbnB1dF9zaXplLCBpbnB1dF9zaXplKSlcbmtlcm5lbCA9IG5wLnJhbmRvbS5yYW5kKGtlcm5lbF9zaXplX3ZhbCwga2VybmVsX3NpemVfdmFsKSAqIDIgLSAxICMgUmFuZG9tIHZhbHVlcyBiZXR3ZWVuIC0xIGFuZCAxXG5rZXJuZWwgPSBucC5yb3VuZChrZXJuZWwsIDEpICMgUm91bmQgZm9yIGJldHRlciBkaXNwbGF5XG5cbiMgSW5pdGlhbGl6ZSBvdXRwdXQgd2l0aCBwYWRkaW5nIHRvIHNpbXBsaWZ5IHZpc3VhbGl6YXRpb24gKGZ1bGwgb3V0cHV0KVxub3V0cHV0X3NpemUgPSBpbnB1dF9zaXplIC0ga2VybmVsX3NpemVfdmFsICsgMVxuaWYgb3V0cHV0X3NpemUgPCAxOiBvdXRwdXRfc2l6ZSA9IDEgIyBFbnN1cmUgb3V0cHV0IHNpemUgaXMgYXQgbGVhc3QgMVxub3V0cHV0ID0gbnAuemVyb3MoKG91dHB1dF9zaXplLCBvdXRwdXRfc2l6ZSkpXG5cbiMgQ3JlYXRlIHRyYWNlcyBmb3IgaW5wdXQgYW5kIGtlcm5lbFxuZmlnID0gZ28uRmlndXJlKClcblxuIyBIZWxwZXIgdG8gYWRkIGhlYXRtYXAgd2l0aCB0ZXh0XG5kZWYgYWRkX2hlYXRtYXBfd2l0aF90ZXh0KGZpZywgZGF0YSwgbmFtZSwgeF9vZmZzZXQ9MCwgeV9vZmZzZXQ9MCwgdGV4dF9jb2xvcj1cImJsYWNrXCIpOlxuICAgIHJvd3MsIGNvbHMgPSBkYXRhLnNoYXBlXG4gICAgZmlnLmFkZF90cmFjZShnby5IZWF0bWFwKFxuICAgICAgICB6PWRhdGEsXG4gICAgICAgIHg9bnAuYXJhbmdlKGNvbHMpICsgeF9vZmZzZXQsXG4gICAgICAgIHk9bnAuYXJhbmdlKHJvd3MpICsgeV9vZmZzZXQsXG4gICAgICAgIGNvbG9yc2NhbGU9J1ZpcmlkaXMnLFxuICAgICAgICBzaG93c2NhbGU9RmFsc2UsXG4gICAgICAgIG5hbWU9bmFtZVxuICAgICkpXG4gICAgZm9yIHIgaW4gcmFuZ2Uocm93cyk6XG4gICAgICAgIGZvciBjIGluIHJhbmdlKGNvbHMpOlxuICAgICAgICAgICAgZmlnLmFkZF9hbm5vdGF0aW9uKFxuICAgICAgICAgICAgICAgIHg9YyArIHhfb2Zmc2V0LCB5PXIgKyB5X29mZnNldCxcbiAgICAgICAgICAgICAgICB0ZXh0PXN0cihkYXRhW3IsIGNdKSxcbiAgICAgICAgICAgICAgICBzaG93YXJyb3c9RmFsc2UsXG4gICAgICAgICAgICAgICAgZm9udD1kaWN0KGNvbG9yPXRleHRfY29sb3IsIHNpemU9MTIpXG4gICAgICAgICAgICApXG5cbmFkZF9oZWF0bWFwX3dpdGhfdGV4dChmaWcsIGlucHV0X2RhdGEsIFwiSW5wdXRcIiwgdGV4dF9jb2xvcj1cIndoaXRlXCIpXG5cbiMgQ2FsY3VsYXRlIGNvbnZvbHV0aW9uIG1hbnVhbGx5IGZvciB2aXN1YWxpemF0aW9uXG5mb3Igcl9vdXQgaW4gcmFuZ2Uob3V0cHV0X3NpemUpOlxuICAgIGZvciBjX291dCBpbiByYW5nZShvdXRwdXRfc2l6ZSk6XG4gICAgICAgICMgRXh0cmFjdCB0aGUgcmVjZXB0aXZlIGZpZWxkXG4gICAgICAgIHJlY2VwdGl2ZV9maWVsZCA9IGlucHV0X2RhdGFbcl9vdXQ6cl9vdXQra2VybmVsX3NpemVfdmFsLCBjX291dDpjX291dCtrZXJuZWxfc2l6ZV92YWxdXG4gICAgICBcbiAgICAgICAgIyBQZXJmb3JtIGVsZW1lbnQtd2lzZSBtdWx0aXBsaWNhdGlvbiBhbmQgc3VtXG4gICAgICAgIGNvbnZvbHZlZF92YWx1ZSA9IG5wLnN1bShyZWNlcHRpdmVfZmllbGQgKiBrZXJuZWwpXG4gICAgICAgIG91dHB1dFtyX291dCwgY19vdXRdID0gY29udm9sdmVkX3ZhbHVlXG4gICAgICBcbiAgICAgICAgIyBBZGQgYSB0ZXh0IGFubm90YXRpb24gZm9yIHdoZXJlIHRoZSBrZXJuZWwgd291bGQgYmUgYXBwbGllZFxuICAgICAgICAjIEp1c3Qgc2hvdyB0aGUga2VybmVsIGluaXRpYWxseSwgbm8gbmVlZCB0byBzaG93IGl0IHNsaWRpbmcgd2l0aCB0aGUgb3V0cHV0XG4gICAgICAgIGlmIHJfb3V0ID09IDAgYW5kIGNfb3V0ID09IDA6XG4gICAgICAgICAgICBhZGRfaGVhdG1hcF93aXRoX3RleHQoZmlnLCBrZXJuZWwsIFwiS2VybmVsXCIsIHhfb2Zmc2V0PWlucHV0X3NpemUgKyAyLCB0ZXh0X2NvbG9yPVwiYmxhY2tcIilcbiAgICAgICAgICAgIGZpZy5hZGRfYW5ub3RhdGlvbih4PWlucHV0X3NpemUgKyAyICsga2VybmVsX3NpemVfdmFsIC8gMiAtIDAuNSwgeT0tMSwgdGV4dD1cIktlcm5lbFwiLCBzaG93YXJyb3c9RmFsc2UpXG5cbiMgQWRkIG91dHB1dCBoZWF0bWFwXG5hZGRfaGVhdG1hcF93aXRoX3RleHQoZmlnLCBucC5yb3VuZChvdXRwdXQsIDEpLCBcIk91dHB1dFwiLCB4X29mZnNldD1pbnB1dF9zaXplICsga2VybmVsX3NpemVfdmFsICsgNCwgdGV4dF9jb2xvcj1cIndoaXRlXCIpXG5cblxuIyBVcGRhdGUgbGF5b3V0XG5maWcudXBkYXRlX2xheW91dChcbiAgICB0aXRsZT1mXCJDb252b2x1dGlvbiB3aXRoIElucHV0IFNpemU6IHtpbnB1dF9zaXplfXh7aW5wdXRfc2l6ZX0sIEtlcm5lbCBTaXplOiB7a2VybmVsX3NpemVfdmFsfXh7a2VybmVsX3NpemVfdmFsfVwiLFxuICAgIHhheGlzPWRpY3QoXG4gICAgICAgIHRpY2ttb2RlPSdhcnJheScsXG4gICAgICAgIHRpY2t2YWxzPW5wLmFyYW5nZShpbnB1dF9zaXplICsga2VybmVsX3NpemVfdmFsICsgNCArIG91dHB1dF9zaXplKSxcbiAgICAgICAgc2hvd3RpY2tsYWJlbHM9RmFsc2UsXG4gICAgICAgIHplcm9saW5lPUZhbHNlXG4gICAgKSxcbiAgICB5YXhpcz1kaWN0KFxuICAgICAgICB0aWNrbW9kZT0nYXJyYXknLFxuICAgICAgICB0aWNrdmFscz1ucC5hcmFuZ2UoaW5wdXRfc2l6ZSksXG4gICAgICAgIHNob3d0aWNrbGFiZWxzPUZhbHNlLFxuICAgICAgICB6ZXJvbGluZT1GYWxzZSxcbiAgICAgICAgYXV0b3JhbmdlPVwicmV2ZXJzZWRcIiAjIFRvIGRpc3BsYXkgKDAsMCkgYXQgdG9wLWxlZnRcbiAgICApLFxuICAgIHdpZHRoPTkwMCxcbiAgICBoZWlnaHQ9NDUwLFxuICAgIG1hcmdpbj1kaWN0KGw9MCwgcj0wLCBiPTAsIHQ9NTApLFxuICAgIHNob3dsZWdlbmQ9RmFsc2VcbilcblxuZmlnLmFkZF9hbm5vdGF0aW9uKHg9aW5wdXRfc2l6ZSAvIDIgLSAwLjUsIHk9LTEsIHRleHQ9XCJJbnB1dFwiLCBzaG93YXJyb3c9RmFsc2UpXG5maWcuYWRkX2Fubm90YXRpb24oeD1pbnB1dF9zaXplICsga2VybmVsX3NpemVfdmFsICsgNCArIG91dHB1dF9zaXplIC8gMiAtIDAuNSwgeT0tMSwgdGV4dD1cIk91dHB1dFwiLCBzaG93YXJyb3c9RmFsc2UpXG5cbmZpZyJ9
</script>
</div>
<aside class="notes">
<p>This interactive slide allows students to dynamically adjust input and kernel sizes to see how the convolution operation changes. It visually demonstrates the concept of a sliding window. Emphasize that in actual CNNs, the kernel values are learned through backpropagation, not randomly assigned. This interactivity helps bridge the abstract concept with a concrete visual.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="building-blocks-of-a-cnn" class="slide level2">
<h2>Building Blocks of a CNN</h2>
<p>Layer by Layer</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Convolutional Layer (<code>Conv2D</code>)</strong></p>
<ul>
<li>Applies a set of learnable filters to the input image / feature maps.</li>
<li>Generates new feature maps that highlight specific patterns.</li>
<li>Parameters: <code>filters</code>, <code>kernel_size</code>, <code>strides</code>, <code>padding</code>, <code>activation</code>.</li>
</ul>
<p><strong>Activation Function (<code>ReLU</code>)</strong></p>
<ul>
<li>Introduces non-linearity to the model.</li>
<li><code>ReLU</code> (Rectified Linear Unit) is popular due to its computational efficiency.
<ul>
<li><span class="math inline">\(f(x) = \max(0, x)\)</span></li>
</ul></li>
</ul>
<p><strong>Pooling Layer (<code>Max Pooling</code>, <code>GlobalAveragePooling2D</code>)</strong></p>
<ul>
<li>Reduces the spatial dimensions of the feature maps.</li>
<li>Decreases computation and helps control overfitting.</li>
<li><code>Max Pooling</code>: Takes the maximum value from each patch.</li>
<li><code>GlobalAveragePooling2D</code>: Averages entire feature maps, often used before the final classification.</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Example Keras Model Snippet:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a></a>model <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb2-2"><a></a>    tf.keras.layers.Reshape(input_shape<span class="op">=</span>(<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>,), target_shape<span class="op">=</span>(<span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>)),</span>
<span id="cb2-3"><a></a>    tf.keras.layers.Conv2D(kernel_size<span class="op">=</span><span class="dv">3</span>, filters<span class="op">=</span><span class="dv">12</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb2-4"><a></a>    tf.keras.layers.Conv2D(kernel_size<span class="op">=</span><span class="dv">6</span>, filters<span class="op">=</span><span class="dv">24</span>, strides<span class="op">=</span><span class="dv">2</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb2-5"><a></a>    tf.keras.layers.Conv2D(kernel_size<span class="op">=</span><span class="dv">6</span>, filters<span class="op">=</span><span class="dv">32</span>, strides<span class="op">=</span><span class="dv">2</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb2-6"><a></a>    tf.keras.layers.Flatten(), <span class="co"># or GlobalAveragePooling2D()</span></span>
<span id="cb2-7"><a></a>    tf.keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)</span>
<span id="cb2-8"><a></a>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Dense Layer (<code>Dense</code>)</strong></p>
<ul>
<li>A fully connected layer, typically at the end of the network.</li>
<li>Used for classification, after the spatial features have been extracted and flattened.</li>
</ul>
<p><strong>Flatten Layer (<code>Flatten</code>)</strong></p>
<ul>
<li>Converts the multi-dimensional output of convolutional/pooling layers into a 1D vector.</li>
<li>Prepares the data for input to a dense layer.</li>
</ul>
</div></div>
</section>
<section id="architectural-view-of-a-cnn" class="slide level2">
<h2>Architectural View of a CNN</h2>
<p>Data Cubes in Motion</p>
<p><strong>Data Transformation:</strong></p>
<ul>
<li>A CNN can be visualized as transforming “cubes” of data.</li>
<li>Each operation (convolution, pooling) manipulates the dimensions of these cubes.</li>
</ul>
<p><strong>Input Layer:</strong> - Starts with an input image (e.g., 28x28x1 for grayscale MNIST, 224x224x3 for RGB images).</p>
<p><strong>Intermediate Layers:</strong> - Convolutional layers increase the number of channels (filters) while possibly reducing spatial dimensions (if <code>strides &gt; 1</code>). - Pooling layers reduce spatial dimensions, preserving the number of channels.</p>
<p><strong>Output Layer:</strong> - Typically, a <code>Flatten</code> or <code>GlobalAveragePooling2D</code> layer converts the cube into a vector. - Followed by a <code>Dense</code> layer with a <code>softmax</code> activation for classification probabilities.</p>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>The precise management of spatial dimensions (height, width) and channels is a key ECE consideration for memory and computational efficiency.</p>
</div>
</div>
</div>
<aside class="notes">
<p>This diagram visually reinforces the concept of data flowing as “cubes” through different layers, changing their dimensions. Explain how each layer type manipulates the height, width, and channel dimensions. For ECE, also consider how these dimension changes impact memory bandwidth and computational load on custom hardware accelerators.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="strided-convolutions-max-pooling" class="slide level2">
<h2>Strided Convolutions &amp; Max Pooling</h2>
<p>Downsampling Strategies</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Purpose of Downsampling:</strong></p>
<ul>
<li><strong>Reduce computational load:</strong> Fewer pixels to process in subsequent layers.</li>
<li><strong>Extract more abstract features:</strong> Larger receptive fields for higher-level features.</li>
<li><strong>Increase robustness:</strong> Make the network less sensitive to small shifts or distortions in the input.</li>
</ul>
<p><strong>1. Strided Convolution:</strong></p>
<ul>
<li>The convolution filter moves by <code>stride</code> pixels at each step (e.g., <code>stride=2</code>).</li>
<li>Directly reduces the output feature map size.</li>
<li>Example: <code>tf.keras.layers.Conv2D(..., strides=2, ...)</code></li>
</ul>
<p><strong>2. Max Pooling:</strong></p>
<ul>
<li>A non-parametric operation.</li>
<li>Slides a window (e.g., 2x2) across the feature map.</li>
<li>Outputs the <strong>maximum value</strong> within that window.</li>
<li>Commonly used with <code>pool_size</code> = <code>strides</code> (e.g., 2x2 window with stride 2).</li>
<li>Example: <code>tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))</code></li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="https://codelabs.developers.google.com/static/codelabs/cloud-tensorflow-mnist/img/2b2d4263bb8470b.gif"></p>
<p><em>Illustration: Strided convolutions or max pooling reduce spatial dimensions.</em></p>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>In ECE, choice between strided convolution and pooling impacts hardware architecture. Strided convo uses multiply-accumulate units. Max pooling requires comparators and a maximum selector.</p>
</div>
</div>
</div>
<p><strong>Comparison:</strong></p>
<ul>
<li><strong>Strided Conv:</strong> Learns how to downsample, can adapt filters for optimal feature reduction.</li>
<li><strong>Max Pooling:</strong> Simpler, fixed operation; preserves only the most salient features. Reduces overfitting by taking only the maximum.</li>
</ul>
</div></div>
</section>
<section id="the-final-layer-classification-head" class="slide level2 scrollable">
<h2>The Final Layer: Classification Head</h2>
<p>Connecting Features to Decisions</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Goal:</strong> Transform the extracted features into class probabilities.</p>
<p><strong>Option 1: Flatten and Dense Layers</strong></p>
<ol type="1">
<li><strong>Flatten:</strong> Converts the 3D feature cube into a 1D vector.
<ul>
<li><code>tf.keras.layers.Flatten()</code></li>
</ul></li>
<li><strong>Dense Layer(s):</strong> One or more fully connected layers.
<ul>
<li>Can be computationally expensive.</li>
<li>High number of weights, especially for large feature maps.</li>
</ul></li>
<li><strong>Softmax Activation:</strong> Last dense layer outputs probabilities for each class.</li>
</ol>
<div class="callout callout-warning callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Warning</strong></p>
</div>
<div class="callout-content">
<p>A large <code>Flatten</code> output connected to a <code>Dense</code> layer can lead to a <em>parameter explosion</em>, a major concern for memory and computation in embedded ECE systems.</p>
</div>
</div>
</div>
</div><div class="column" style="width:50%;">
<p><img data-src="https://codelabs.developers.google.com/static/codelabs/cloud-tensorflow-mnist/img/a44aa392c7b0e32a.png"></p>
<p><em>Illustration showing two options for the final layer.</em></p>
<p><strong>Option 2: Global Average Pooling (GAP) and Dense/Softmax</strong></p>
<ol type="1">
<li><strong>GlobalAveragePooling2D:</strong>
<ul>
<li>Averages each feature map across its spatial dimensions (height x width).</li>
<li>Converts a <code>[H x W x C]</code> cube into a <code>[C]</code> vector.</li>
<li>Example: <code>tf.keras.layers.GlobalAveragePooling2D()</code></li>
</ul></li>
<li><strong>Dense Layer (optional) / Softmax:</strong>
<ul>
<li>Often, this is fed directly into a final <code>Dense</code> layer with <code>softmax</code>.</li>
<li><strong>0 weights</strong> if directly connected to output softmax layer (no intermediate Dense layer).</li>
<li>Significantly reduces parameters and helps prevent overfitting.</li>
</ul></li>
</ol>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p>For ECE applications, especially on resource-constrained devices, <code>GlobalAveragePooling2D</code> is often preferred for its parameter efficiency.</p>
</div>
</div>
</div>
</div></div>
</section>
<section id="conclusion" class="slide level2">
<h2>Conclusion</h2>
<p>Key Takeaways for ECE</p>
<p><strong>Recap of CNN Fundamentals:</strong></p>
<ul>
<li><strong>Localized Receptive Fields:</strong> Neurons respond to small regions.</li>
<li><strong>Weight Sharing:</strong> Filters slide across the input, making CNNs efficient.</li>
<li><strong>Feature Hierarchy:</strong> Layers progressively learn more complex features.</li>
<li><strong>Downsampling:</strong> Strided convolutions and pooling reduce data dimensions, manage computation.</li>
<li><strong>Classification Head:</strong> <code>Flatten</code> + <code>Dense</code> or <code>GlobalAveragePooling2D</code> for final prediction.</li>
</ul>
<p><strong>Relevance for Electrical and Computer Engineers:</strong></p>
<ul>
<li><strong>Hardware Acceleration:</strong> Designing custom chips (ASICs, FPGAs) for efficient CNN inference.</li>
<li><strong>Embedded AI:</strong> Deploying CNNs on low-power, edge devices (e.g., IoT, drones, automotive ECUs).</li>
<li><strong>Sensor Fusion:</strong> Processing data from multiple ECE sensors (cameras, LiDAR, radar) using CNNs.</li>
<li><strong>Real-time Processing:</strong> Optimizing CNN architectures and implementations for latency-critical applications.</li>
<li><strong>Resource Optimization:</strong> Balancing accuracy with computational cost, power consumption, and memory footprint.</li>
</ul>
<aside class="notes">
<p>Reinforce the practical implications of CNN knowledge. Emphasize that ECE professionals don’t just use these models, they build the underlying hardware and software tools that make them possible, and they face unique challenges related to resource constraints and real-time performance.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="regularization-dropout" class="slide level2">
<h2>Regularization: Dropout</h2>
<p>Mitigating Overfitting in Machine Learning Models</p>
</section>
<section id="understanding-overfitting" class="slide level2 scrollable">
<h2>Understanding Overfitting</h2>
<p>A Critical Challenge in ML</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>What is Overfitting?</strong></p>
<ul>
<li>A model that performs exceptionally well on the <strong>training data</strong> but poorly on <strong>unseen data</strong> (validation/test data).</li>
<li>The model essentially “memorizes” the training examples, including noise and specific patterns, rather than learning generalizable features.</li>
</ul>
<p><strong>Signs of Overfitting:</strong></p>
<ol type="1">
<li><strong>Training Loss Decreases:</strong> Continues to go down during training.</li>
<li><strong>Validation Loss Increases:</strong> Starts to rise after an initial decrease, indicating the model is losing generalization ability.</li>
<li><strong>High Training Accuracy, Low Validation Accuracy:</strong> A significant gap between performance on seen and unseen data.</li>
</ol>
</div><div class="column" style="width:50%;">
<p><img data-src="https://codelabs.developers.google.com/static/codelabs/cloud-tensorflow-mnist/img/63e0cc982cee2030.png"></p>
<p><em>Graph: Illustrates validation loss flattening and training loss decreasing, a positive sign with dropout.</em></p>
<p><strong>Causes of Overfitting:</strong></p>
<ul>
<li><strong>Model Complexity:</strong> Too many parameters or layers relative to data.</li>
<li><strong>Insufficient Data:</strong> Not enough training examples to learn general patterns.</li>
<li><strong>Noisy Data:</strong> The model learns to fit the noise in the training set.</li>
</ul>
<div class="callout callout-warning callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Warning</strong></p>
</div>
<div class="callout-content">
<p>For ECE applications, overfitting can lead to unreliable system performance in real-world scenarios, which is unacceptable for safety-critical systems like autonomous vehicles or medical devices.</p>
</div>
</div>
</div>
</div></div>
</section>
<section id="introduction-to-dropout" class="slide level2">
<h2>Introduction to Dropout</h2>
<p>A Simple Yet Powerful Regularization Technique</p>
<p><strong>What is Dropout?</strong></p>
<ul>
<li>A regularization technique introduced by Hinton et al.&nbsp;in 2012.</li>
<li>Randomly sets a fraction of neuron outputs to zero at each training step.</li>
<li><strong>“Dropping out”</strong> neurons means they temporarily do not contribute to the forward pass and do not participate in backpropagation.</li>
</ul>
<p><strong>Analogy:</strong></p>
<ul>
<li>Imagine a team where, for each task, a random subset of members is absent. The remaining members must learn to perform their roles more robustly without relying on any single individual.</li>
</ul>
<p><strong>How it Works (Training Phase):</strong></p>
<ul>
<li>A <code>dropout rate</code> (e.g., 0.2 to 0.5) specifies the probability of a neuron being dropped.</li>
<li>Each neuron has its state (active/inactive) sampled independently.</li>
<li>The weights of the remaining active neurons are scaled up by <code>1 / (1 - dropout_rate)</code> to maintain the same expected sum of outputs.</li>
</ul>
<p><strong>Inference/Testing Phase:</strong></p>
<ul>
<li>All neurons are active and contribute to the output.</li>
<li>Their weights are not scaled (or equivalently, they are pre-scaled by the dropout rate during training).</li>
</ul>
<aside class="notes">
<p>Explain that dropout is like training an ensemble of neural networks, where each epoch a different “thinned” network is trained. This forces the network to learn more robust features that are not reliant on specific co-adaptations of neurons. Emphasize the difference between training (random drops and scaling) and inference (all neurons active, no scaling).</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="dropout-in-keras" class="slide level2">
<h2>Dropout in Keras</h2>
<p>Implementation and Placement</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Adding Dropout Layers:</strong></p>
<ul>
<li>In Keras, dropout is implemented as a layer: <code>tf.keras.layers.Dropout(rate)</code>.</li>
<li>The <code>rate</code> is the fraction of inputs to drop (e.g., 0.2 for 20%).</li>
</ul>
<p><strong>Typical Placement:</strong></p>
<ul>
<li>Often applied after activation functions (or after convolutional and pooling layers, before <code>Flatten</code> or <code>Dense</code>).</li>
<li>Commonly used after convolutional blocks and before fully connected (dense) layers to prevent overfitting on extracted features.</li>
</ul>
<p><strong>Example Keras Model with Dropout:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a></a>model <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb3-2"><a></a>    tf.keras.layers.Reshape(input_shape<span class="op">=</span>(<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>,), target_shape<span class="op">=</span>(<span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>)),</span>
<span id="cb3-3"><a></a>    tf.keras.layers.Conv2D(kernel_size<span class="op">=</span><span class="dv">3</span>, filters<span class="op">=</span><span class="dv">12</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb3-4"><a></a>    tf.keras.layers.MaxPooling2D(pool_size<span class="op">=</span>(<span class="dv">2</span>,<span class="dv">2</span>), strides<span class="op">=</span>(<span class="dv">2</span>,<span class="dv">2</span>)), <span class="co"># Added pooling</span></span>
<span id="cb3-5"><a></a>    tf.keras.layers.Dropout(<span class="fl">0.25</span>), <span class="co"># Dropout after pooling</span></span>
<span id="cb3-6"><a></a>    tf.keras.layers.Conv2D(kernel_size<span class="op">=</span><span class="dv">6</span>, filters<span class="op">=</span><span class="dv">24</span>, strides<span class="op">=</span><span class="dv">1</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb3-7"><a></a>    tf.keras.layers.MaxPooling2D(pool_size<span class="op">=</span>(<span class="dv">2</span>,<span class="dv">2</span>), strides<span class="op">=</span>(<span class="dv">2</span>,<span class="dv">2</span>)), <span class="co"># Another pooling</span></span>
<span id="cb3-8"><a></a>    tf.keras.layers.Dropout(<span class="fl">0.25</span>), <span class="co"># Dropout after second pooling</span></span>
<span id="cb3-9"><a></a>    tf.keras.layers.Flatten(),</span>
<span id="cb3-10"><a></a>    tf.keras.layers.Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">'relu'</span>), <span class="co"># Intermediate Dense layer</span></span>
<span id="cb3-11"><a></a>    tf.keras.layers.Dropout(<span class="fl">0.5</span>), <span class="co"># High dropout for dense layers</span></span>
<span id="cb3-12"><a></a>    tf.keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)</span>
<span id="cb3-13"><a></a>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div><div class="column" style="width:50%;">
<p><strong>Practical Considerations:</strong></p>
<ul>
<li><strong>Dropout Rate:</strong>
<ul>
<li>Usually between 0.2 and 0.5. Needs tuning.</li>
<li>Higher rates for larger models or smaller datasets.</li>
</ul></li>
<li><strong>Placement:</strong> Experiment with placement.
<ul>
<li>Often effective before dense layers due to their high parameter count.</li>
<li>Can be used after convolutional layers too.</li>
</ul></li>
<li><strong>Training Time:</strong> Dropout can sometimes increase training time as the network needs more iterations to converge.</li>
</ul>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p>In ECE, understanding dropout’s impact on model robustness is critical. A robust model performs reliably even with minor input variations, which is vital for real-world environmental noise or sensor inaccuracies.</p>
</div>
</div>
</div>
</div></div>
</section>
<section id="dropout-in-keras-1" class="slide level2">
<h2>Dropout in Keras</h2>
<p><strong>Dropout Effect on Training</strong> This Python code block does not execute a full training loop to demonstrate dropout, but illustrates how the dropout layer works by modifying input tensors.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a></a><span class="co">#| max-lines: 10</span></span>
<span id="cb4-2"><a></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb4-3"><a></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-4"><a></a></span>
<span id="cb4-5"><a></a><span class="co"># Create a sample input tensor (e.g., output from a previous layer)</span></span>
<span id="cb4-6"><a></a>input_tensor <span class="op">=</span> tf.constant(np.arange(<span class="dv">1</span>, <span class="dv">11</span>, dtype<span class="op">=</span>np.float32).reshape(<span class="dv">1</span>, <span class="dv">10</span>))</span>
<span id="cb4-7"><a></a><span class="bu">print</span>(<span class="st">"Original Input Tensor:"</span>)</span>
<span id="cb4-8"><a></a><span class="bu">print</span>(input_tensor.numpy())</span>
<span id="cb4-9"><a></a></span>
<span id="cb4-10"><a></a><span class="co"># Create a Dropout layer with a rate of 0.5</span></span>
<span id="cb4-11"><a></a>dropout_layer <span class="op">=</span> tf.keras.layers.Dropout(rate<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb4-12"><a></a></span>
<span id="cb4-13"><a></a><span class="co"># Simulate training mode (where dropout is active)</span></span>
<span id="cb4-14"><a></a><span class="co"># The is_training=True argument is crucial for Dropout to be active</span></span>
<span id="cb4-15"><a></a>dropped_output <span class="op">=</span> dropout_layer(input_tensor, training<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-16"><a></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Output with Dropout (training=True):"</span>)</span>
<span id="cb4-17"><a></a><span class="bu">print</span>(dropped_output.numpy())</span>
<span id="cb4-18"><a></a></span>
<span id="cb4-19"><a></a><span class="co"># Simulate inference mode (where dropout is inactive, and scaling is applied - though handled internally)</span></span>
<span id="cb4-20"><a></a>regular_output <span class="op">=</span> dropout_layer(input_tensor, training<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-21"><a></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Output without Dropout (training=False - all values present, no explicit scaling shown here as it's for weights):"</span>)</span>
<span id="cb4-22"><a></a><span class="bu">print</span>(regular_output.numpy())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<aside class="notes">
<p>The interactive example shows how <code>Dropout</code> literally sets some values to zero in training. Emphasize that in inference, no values are dropped, and all neurons contribute. The scaling factor applied during training (or to weights during inference, depending on implementation) ensures the expected output magnitude remains consistent.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="when-dropout-helps-and-when-it-doesnt" class="slide level2">
<h2>When Dropout Helps (and When It Doesn’t)</h2>
<p>Diagnosing Overfitting</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>When Dropout is Effective:</strong></p>
<ul>
<li><strong>Overfitting is the primary problem:</strong> When validation loss starts increasing while training loss continues to decrease.</li>
<li><strong>Complex Models:</strong> Helps prevent co-adaptation of neurons in deep architectures.</li>
<li><strong>Sufficiently Large Datasets:</strong> Dropout works best when the model has enough data to learn diverse features across different “sub-networks.”</li>
</ul>
<p><strong>When Dropout Might Not Help (or Even Hurt):</strong></p>
<ul>
<li><strong>Underfitting:</strong> If the model is too simple or data is too scarce, dropout can make it even harder for the model to learn.</li>
<li><strong>Wrong Architecture:</strong> If the model itself is fundamentally unsuited for the task (e.g., fully connected layers for image data without convolutions).
<ul>
<li>In such cases, dropout cannot compensate for a poor architectural design.</li>
</ul></li>
<li><strong>Too High Dropout Rate:</strong> Can lead to underfitting if too many neurons are dropped, effectively simplifying the network too much.</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Analyzing Loss Curves:</strong></p>
<ul>
<li><strong>Training loss decreasing, Validation loss increasing:</strong> <em>Classic sign of overfitting</em> <span class="math inline">\(\rightarrow\)</span> <strong>Use Dropout</strong>.</li>
<li><strong>Both training and validation loss decreasing but high:</strong> <em>Underfitting or insufficient capacity</em> <span class="math inline">\(\rightarrow\)</span> <strong>Adjust architecture, add layers, remove dropout</strong>.</li>
<li><strong>Both training and validation loss erratic:</strong> <em>Learning rate too high, batch size too small, or bad initialization</em> <span class="math inline">\(\rightarrow\)</span> <strong>Adjust hyperparameters</strong>.</li>
</ul>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>For ECE systems, debugging model performance requires a systematic approach. Distinguishing between architectural issues, data limitations, and overfitting is key to efficient resource allocation in hardware design and algorithm tuning.</p>
</div>
</div>
</div>
<p><strong>Beyond Dropout:</strong> Other regularization techniques include L1/L2 regularization, data augmentation, batch normalization, and early stopping.</p>
</div></div>
</section>
<section id="conclusion-1" class="slide level2">
<h2>Conclusion</h2>
<p>Dropout Summary &amp; ECE Relevance</p>
<p><strong>Key Learnings about Dropout:</strong></p>
<ul>
<li><strong>Mechanism:</strong> Randomly deactivates neurons during training to prevent co-adaptation.</li>
<li><strong>Goal:</strong> Mitigates overfitting, leading to better generalization on unseen data.</li>
<li><strong>Implementation:</strong> Simple Keras <code>Dropout</code> layer.</li>
<li><strong>Diagnosis:</strong> Effective when validation loss deviates upwards from training loss.</li>
</ul>
<p><strong>ECE Relevance:</strong></p>
<ul>
<li><strong>Robustness:</strong> Dropout improves model robustness, crucial for deployment in noisy or varying real-world environments (e.g., sensor data processing).</li>
<li><strong>Deployment Reliability:</strong> Reduces the chance of models failing on edge cases not perfectly represented in the training data, enhancing system reliability.</li>
<li><strong>Model Selection:</strong> ECE engineers need to select appropriate regularization techniques to optimize the trade-off between model accuracy, complexity, and deployment constraints (memory, power).</li>
<li><strong>Debugging:</strong> Understanding loss curves and the role of regularization is vital for efficient troubleshooting and performance tuning of hardware-accelerated ML systems.</li>
</ul>
<aside class="notes">
<p>Conclude by reiterating the direct benefits of dropout for ECE students: not just better theoretical understanding, but practical tools for building more reliable and robust intelligent systems. Emphasize that regularization is a critical skill for any ECE professional working with AI/ML.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="batch-normalization" class="slide level2">
<h2>Batch Normalization</h2>
<p>Stabilizing and Accelerating Neural Network Training</p>
</section>
<section id="the-challenge-of-internal-covariate-shift" class="slide level2">
<h2>The Challenge of Internal Covariate Shift</h2>
<p>Why Normalization is Needed in Deep Networks</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Internal Covariate Shift (ICS):</strong></p>
<ul>
<li>During neural network training, the distribution of inputs to hidden layers changes as the parameters of the preceding layers change.</li>
<li>This continuous shift means that a layer constantly has to adapt to a new input distribution.</li>
<li>Analogy: Imagine trying to learn to ride a bike where the ground underneath is constantly shifting.</li>
</ul>
<p><strong>Problems Caused by ICS:</strong></p>
<ul>
<li><strong>Slower Training:</strong> Requires lower learning rates and careful initialization.</li>
<li><strong>Vanishing/Exploding Gradients:</strong> Can make activations too small or too large.</li>
<li><strong>Difficulty in Optimization:</strong> Makes it harder for the optimizer to find good weights.</li>
<li><strong>Reduced Generalization:</strong> Can negatively impact model performance.</li>
</ul>
<div class="callout callout-warning callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Warning</strong></p>
</div>
<div class="callout-content">
<p>Understanding and mitigating ICS is crucial for ECE, especially in designing custom hardware for deep learning, as it impacts convergence speed and resource utilization.</p>
</div>
</div>
</div>
</div><div class="column" style="width:50%;">
<p><img data-src="https://lh6.googleusercontent.com/oggEbikl2I6_sOo7FlaX2KLdNeaYhJnVSS8GyG8FHXid75PVJX73CRiOynwpMZpLZq6_xAy69wgyez5T-ZlpuC2XSlcmjk7oVcOzefKKTFhTEoLO3kljz2RDyKcaFtHvtTey-I4VpQ.png"></p>
<p><em>Illustration: Conceptual representation of the impact of Batch Normalization, allowing for smoother training curves.</em></p>
<p><strong>How Batch Normalization Addresses ICS:</strong></p>
<ul>
<li>Standardizes the inputs to hidden layers.</li>
<li>It normalizes the activations of each previous layer at each mini-batch, making the distribution more stable.</li>
<li>For each feature/channel across the mini-batch, it computes the mean and variance, then normalizes the data to have zero mean and unit variance.</li>
</ul>
</div></div>
</section>
<section id="what-is-batch-normalization" class="slide level2 scrollable">
<h2>What is Batch Normalization?</h2>
<p>The Mechanism</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Per-Batch, Per-Feature Normalization:</strong></p>
<p>For each mini-batch during training, and for each feature (or channel in CNNs) independently:</p>
<ol type="1">
<li><strong>Calculate Mean (<span class="math inline">\(\mu_B\)</span>):</strong> Compute the mean of the activations for that feature across all samples in the mini-batch.</li>
<li><strong>Calculate Variance (<span class="math inline">\(\sigma_B^2\)</span>):</strong> Compute the variance of the activations for that feature across all samples in the mini-batch.</li>
<li><strong>Normalize:</strong> Scale the activations to have zero mean and unit variance. <span class="math display">\[ \hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} \]</span> (<span class="math inline">\(\epsilon\)</span> is a small constant for numerical stability)</li>
<li><strong>Scale and Shift:</strong> Apply learnable parameters <span class="math inline">\(\gamma\)</span> (scale) and <span class="math inline">\(\beta\)</span> (shift) to restore representation power. <span class="math display">\[ y_i = \gamma \hat{x}_i + \beta \]</span>
<ul>
<li><span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\beta\)</span> allow the network to learn optimal mean/variance for each layer, not just forcing it to be 0 and 1.</li>
<li>These are trained parameters, just like weights and biases.</li>
</ul></li>
</ol>
</div><div class="column" style="width:50%;">
<p><strong>Training vs.&nbsp;Inference:</strong></p>
<ul>
<li><strong>Training:</strong> Mean and variance are calculated per mini-batch.</li>
<li><strong>Inference:</strong> Moving averages of mean and variance (collected during training) are used.
<ul>
<li>Ensures deterministic output for a single input, as batch statistics are unavailable.</li>
</ul></li>
</ul>
<aside class="notes">
<p>Detail the four steps of Batch Norm. Emphasize that <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\beta\)</span> are learnable parameters, making Batch Norm a flexible transformation, not just a rigid standardization. Clarify the different behavior during training (batch statistics) and inference (running averages). This distinction is critical for ECE in deployment.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="benefits-of-batch-normalization" class="slide level2 scrollable">
<h2>Benefits of Batch Normalization</h2>
<p>Why it’s a Game Changer</p>
<div class="columns">
<div class="column" style="width:50%;">
<ol type="1">
<li><strong>Faster Training (Higher Learning Rates):</strong>
<ul>
<li>Stabilizes gradients, allowing for much higher learning rates without risk of divergence.</li>
<li>This significantly reduces training time.</li>
</ul></li>
<li><strong>Reduces Impact of Initialization:</strong>
<ul>
<li>Less sensitive to initial weights.</li>
<li>The normalization process acts as a buffer against poor initial conditions.</li>
</ul></li>
<li><strong>Regularization Effect:</strong>
<ul>
<li>Adds a slight amount of noise to the network’s activations, much like dropout.</li>
<li>Can sometimes reduce or negate the need for dropout, as seen in the text.</li>
</ul></li>
<li><strong>Improved Gradient Flow:</strong>
<ul>
<li>Prevents vanishing/exploding gradients in very deep networks by keeping activation distributions stable.</li>
</ul></li>
</ol>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p>For ECE, faster training means shorter design cycles and more iterations for experimentation, while improved stability leads to more robust and deployable ML models.</p>
</div>
</div>
</div>
</div><div class="column" style="width:50%;">
<p><strong>Impact on Training Curves:</strong></p>
<p><img data-src="https://codelabs.developers.google.com/static/codelabs/cloud-tensorflow-mnist/img/ea48193334c565a1.png"></p>
<p><em>Graph: Batch Norm often leads to smoother, faster convergence to higher accuracy.</em></p>
<p><strong>Practical Rules of Thumb from the Reference:</strong></p>
<ul>
<li>Batch Norm helps neural networks converge and usually allows you to train faster.</li>
<li>Batch Norm is a regularizer. You can usually decrease the amount of dropout you use, or even not use dropout at all.</li>
</ul>
</div></div>
</section>
<section id="implementing-batch-normalization-in-keras" class="slide level2">
<h2>Implementing Batch Normalization in Keras</h2>
<p>Practical Integration</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Keras Layer:</strong></p>
<ul>
<li>Implemented as <code>tf.keras.layers.BatchNormalization()</code>.</li>
</ul>
<p><strong>Placement:</strong></p>
<ul>
<li>Typically placed <strong>after a convolutional or dense layer</strong> and <strong>before its activation function</strong>.</li>
<li><strong>Why <code>use_bias=False</code>?</strong> The <code>beta</code> parameter in Batch Norm effectively acts as a bias term, so the biases from the preceding <code>Conv2D</code> or <code>Dense</code> layer are redundant.</li>
<li><strong>Why <code>scale=False</code> for ReLU?</strong> For ReLU, <code>scale=False</code> can sometimes be used because ReLU sets negative values to zero, and the learned <span class="math inline">\(\gamma\)</span> parameter might not be strictly necessary for scaling. However, <code>scale=True</code> (default) is generally fine and allowing <span class="math inline">\(\gamma\)</span> to be learned provides more flexibility.</li>
</ul>
<p><strong>Example Keras Snippet:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a></a><span class="co"># Original approach with activation in the layer</span></span>
<span id="cb5-2"><a></a><span class="co"># tf.keras.layers.Conv2D(kernel_size=3, filters=12, activation='relu')</span></span>
<span id="cb5-3"><a></a></span>
<span id="cb5-4"><a></a><span class="co"># With Batch Normalization:</span></span>
<span id="cb5-5"><a></a>model <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb5-6"><a></a>    tf.keras.layers.Conv2D(kernel_size<span class="op">=</span><span class="dv">3</span>, filters<span class="op">=</span><span class="dv">12</span>, use_bias<span class="op">=</span><span class="va">False</span>), <span class="co"># No bias</span></span>
<span id="cb5-7"><a></a>    tf.keras.layers.BatchNormalization(scale<span class="op">=</span><span class="va">True</span>), <span class="co"># Default scale=True</span></span>
<span id="cb5-8"><a></a>    tf.keras.layers.Activation(<span class="st">'relu'</span>), <span class="co"># Activation after Batch Norm</span></span>
<span id="cb5-9"><a></a>  </span>
<span id="cb5-10"><a></a>    tf.keras.layers.Conv2D(kernel_size<span class="op">=</span><span class="dv">6</span>, filters<span class="op">=</span><span class="dv">24</span>, strides<span class="op">=</span><span class="dv">2</span>, use_bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb5-11"><a></a>    tf.keras.layers.BatchNormalization(scale<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb5-12"><a></a>    tf.keras.layers.Activation(<span class="st">'relu'</span>),</span>
<span id="cb5-13"><a></a>  </span>
<span id="cb5-14"><a></a>    tf.keras.layers.Flatten(),</span>
<span id="cb5-15"><a></a>    tf.keras.layers.Dense(<span class="dv">128</span>, use_bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb5-16"><a></a>    tf.keras.layers.BatchNormalization(scale<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb5-17"><a></a>    tf.keras.layers.Activation(<span class="st">'relu'</span>),</span>
<span id="cb5-18"><a></a>    tf.keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span>) <span class="co"># No Batch Norm here</span></span>
<span id="cb5-19"><a></a>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div><div class="column" style="width:50%;">
<p><strong>Interactive Example: Batch Norm Effect on Activation Distribution</strong></p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code hidden" id="cb6" data-startfrom="793" data-source-offset="-0"><pre class="sourceCode numberSource js number-lines code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 792;"><span id="cb6-793"><a></a>viewof mean_val <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="op">-</span><span class="dv">2</span><span class="op">,</span> <span class="dv">2</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="fl">0.5</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Input Mean"</span>})<span class="op">;</span></span>
<span id="cb6-794"><a></a>viewof std_val <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="fl">0.1</span><span class="op">,</span> <span class="dv">3</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="fl">1.5</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Input Std Dev"</span>})<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-2-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-2-2" data-nodetype="declaration">

</div>
</div>
</div>
</div>
<div>
<div id="pyodide-2" class="exercise-cell">

</div>
<script type="pyodide-2-contents">
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
</script>
</div>
<aside class="notes">
<p>The interactive demo clearly shows how Batch Normalization transforms an arbitrary input distribution to one with approximately zero mean and unit variance. This visual “stabilization” helps students intuitively grasp the core idea of reducing internal covariate shift. Explain that the <code>gamma</code> and <code>beta</code> parameters (set to 1 and 0 here for simplicity) allow the network to ‘undo’ this if needed, but the normalization acts as a helpful processing step.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="conclusion-2" class="slide level2">
<h2>Conclusion</h2>
<p>Batch Normalization’s Impact on Modern ML</p>
<p><strong>Key Takeaways on Batch Normalization:</strong></p>
<ul>
<li><strong>Addresses Internal Covariate Shift:</strong> Stabilizes the input distribution to layers.</li>
<li><strong>Accelerates Training:</strong> Allows higher learning rates and faster convergence.</li>
<li><strong>Acts as a Regularizer:</strong> Reduces the need for dropout.</li>
<li><strong>Improves Gradient Flow:</strong> Aids optimization in deep networks.</li>
<li><strong>Implementation:</strong> <code>tf.keras.layers.BatchNormalization()</code> typically placed between a layer and its activation, with <code>use_bias=False</code> in the preceding layer.</li>
</ul>
<p><strong>Significance for ECE Professionals:</strong></p>
<ul>
<li><strong>Hardware Efficiency:</strong> Designing efficient hardware accelerators for Batch Normalization (e.g., dedicated arithmetic units for mean/variance, fixed-point implementations).</li>
<li><strong>Embedded Systems:</strong> Crucial for deploying deep models on resource-constrained devices, as faster training means quicker model updates and fewer training epochs.</li>
<li><strong>Power Optimization:</strong> Faster convergence contributes to less power consumption during training.</li>
<li><strong>Robust Model Design:</strong> Leads to more stable and robust models, essential for mission-critical ECE applications in real-world deployments.</li>
<li><strong>State-of-the-Art Models:</strong> Batch Normalization is a fundamental component of almost all modern, high-performing deep learning architectures.</li>
</ul>
<aside class="notes">
<p>Conclude with a strong emphasis on why Batch Normalization is indispensable for ECE students. It’s not just an algorithmic trick but a critical component influencing hardware design, deployment strategies, and overall system performance in AI-driven ECE applications. Highlight its role in enabling the development of deeper, more complex, and more effective neural networks.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>


<script type="pyodide-data">
eyJvcHRpb25zIjp7ImVudiI6eyJQTE9UTFlfUkVOREVSRVIiOiJwbG90bHlfbWltZXR5cGUifSwiaW5kZXhVUkwiOiJodHRwczovL2Nkbi5qc2RlbGl2ci5uZXQvcHlvZGlkZS92MC4yNy4wL2Z1bGwvIn0sInBhY2thZ2VzIjp7InBrZ3MiOlsicHlvZGlkZV9odHRwIiwibWljcm9waXAiLCJpcHl0aG9uIiwibnVtcHkiLCJwbG90bHkiLCJuYmZvcm1hdCIsInNjaXB5Il19fQ==
</script>
<script type="ojs-module-contents">
{"contents":[{"cellName":"pyodide-2","inline":false,"methodName":"interpret","source":"_pyodide_value_2 = {\n  const { highlightPython, b64Decode} = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-2-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  // Default evaluation configuration\n  const options = Object.assign({\n    id: \"pyodide-2-contents\",\n    echo: true,\n    output: true\n  }, block.attr);\n\n  // Evaluate the provided Python code\n  const result = pyodideOjs.process({code: block.code, options}, {mean_val, std_val});\n\n  // Early yield while we wait for the first evaluation and render\n  if (options.output && !(\"2\" in pyodideOjs.renderedOjs)) {\n    const container = document.createElement(\"div\");\n    const spinner = document.createElement(\"div\");\n\n    if (options.echo) {\n      // Show output as highlighted source\n      const preElem = document.createElement(\"pre\");\n      container.className = \"sourceCode\";\n      preElem.className = \"sourceCode python\";\n      preElem.appendChild(highlightPython(block.code));\n      spinner.className = \"spinner-grow spinner-grow-sm m-2 position-absolute top-0 end-0\";\n      preElem.appendChild(spinner);\n      container.appendChild(preElem);\n    } else {\n      spinner.className = \"spinner-border spinner-border-sm\";\n      container.appendChild(spinner);\n    }\n\n    yield container;\n  }\n\n  pyodideOjs.renderedOjs[\"2\"] = true;\n  yield await result;\n}\n"},{"cellName":"pyodide-1","inline":false,"methodName":"interpret","source":"_pyodide_value_1 = {\n  const { highlightPython, b64Decode} = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-1-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  // Default evaluation configuration\n  const options = Object.assign({\n    id: \"pyodide-1-contents\",\n    echo: true,\n    output: true\n  }, block.attr);\n\n  // Evaluate the provided Python code\n  const result = pyodideOjs.process({code: block.code, options}, {input_size, kernel_size});\n\n  // Early yield while we wait for the first evaluation and render\n  if (options.output && !(\"1\" in pyodideOjs.renderedOjs)) {\n    const container = document.createElement(\"div\");\n    const spinner = document.createElement(\"div\");\n\n    if (options.echo) {\n      // Show output as highlighted source\n      const preElem = document.createElement(\"pre\");\n      container.className = \"sourceCode\";\n      preElem.className = \"sourceCode python\";\n      preElem.appendChild(highlightPython(block.code));\n      spinner.className = \"spinner-grow spinner-grow-sm m-2 position-absolute top-0 end-0\";\n      preElem.appendChild(spinner);\n      container.appendChild(preElem);\n    } else {\n      spinner.className = \"spinner-border spinner-border-sm\";\n      container.appendChild(spinner);\n    }\n\n    yield container;\n  }\n\n  pyodideOjs.renderedOjs[\"1\"] = true;\n  yield await result;\n}\n"},{"cellName":"pyodide-prelude","inline":false,"methodName":"interpretQuiet","source":"pyodideOjs = {\n  const {\n    PyodideEvaluator,\n    PyodideEnvironmentManager,\n    setupPython,\n    startPyodideWorker,\n    b64Decode,\n    collapsePath,\n  } = window._exercise_ojs_runtime;\n\n  const statusContainer = document.getElementById(\"exercise-loading-status\");\n  const indicatorContainer = document.getElementById(\"exercise-loading-indicator\");\n  indicatorContainer.classList.remove(\"d-none\");\n\n  let statusText = document.createElement(\"div\")\n  statusText.classList = \"exercise-loading-details\";\n  statusText = statusContainer.appendChild(statusText);\n  statusText.textContent = `Initialise`;\n\n  // Hoist indicator out from final slide when running under reveal\n  const revealStatus = document.querySelector(\".reveal .exercise-loading-indicator\");\n  if (revealStatus) {\n    revealStatus.remove();\n    document.querySelector(\".reveal > .slides\").appendChild(revealStatus);\n  }\n\n  // Make any reveal slides with live cells scrollable\n  document.querySelectorAll(\".reveal .exercise-cell\").forEach((el) => {\n    el.closest('section.slide').classList.add(\"scrollable\");\n  })\n\n  // Pyodide supplemental data and options\n  const dataContent = document.querySelector(`script[type=\\\"pyodide-data\\\"]`).textContent;\n  const data = JSON.parse(b64Decode(dataContent));\n\n  // Grab list of resources to be downloaded\n  const filesContent = document.querySelector(`script[type=\\\"vfs-file\\\"]`).textContent;\n  const files = JSON.parse(b64Decode(filesContent));\n\n  let pyodidePromise = (async () => {\n    statusText.textContent = `Downloading Pyodide`;\n    const pyodide = await startPyodideWorker(data.options);\n\n    statusText.textContent = `Downloading package: micropip`;\n    await pyodide.loadPackage(\"micropip\");\n    const micropip = await pyodide.pyimport(\"micropip\");\n    await data.packages.pkgs.map((pkg) => () => {\n      statusText.textContent = `Downloading package: ${pkg}`;\n      return micropip.install(pkg);\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n    await micropip.destroy();\n\n    // Download and install resources\n    await files.map((file) => async () => {\n      const name = file.substring(file.lastIndexOf('/') + 1);\n      statusText.textContent = `Downloading resource: ${name}`;\n      const response = await fetch(file);\n      if (!response.ok) {\n        throw new Error(`Can't download \\`${file}\\`. Error ${response.status}: \"${response.statusText}\".`);\n      }\n      const data = await response.arrayBuffer();\n\n      // Store URLs in the cwd without any subdirectory structure\n      if (file.includes(\"://\")) {\n        file = name;\n      }\n\n      // Collapse higher directory structure\n      file = collapsePath(file);\n\n      // Create directory tree, ignoring \"directory exists\" VFS errors\n      const parts = file.split('/').slice(0, -1);\n      let path = '';\n      while (parts.length > 0) {\n        path += parts.shift() + '/';\n        try {\n          await pyodide.FS.mkdir(path);\n        } catch (e) {\n          if (e.name !== \"ErrnoError\") throw e;\n          if (e.errno !== 20) {\n            const errorTextPtr = await pyodide._module._strerror(e.errno);\n            const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n            throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n          }\n        }\n      }\n\n      // Write this file to the VFS\n      try {\n        return await pyodide.FS.writeFile(file, new Uint8Array(data));\n      } catch (e) {\n        if (e.name !== \"ErrnoError\") throw e;\n        const errorTextPtr = await pyodide._module._strerror(e.errno);\n        const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n        throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n      }\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n\n    statusText.textContent = `Pyodide environment setup`;\n    await setupPython(pyodide);\n\n    statusText.remove();\n    if (statusContainer.children.length == 0) {\n      statusContainer.parentNode.remove();\n    }\n    return pyodide;\n  })().catch((err) => {\n    statusText.style.color = \"var(--exercise-editor-hl-er, #AD0000)\";\n    statusText.textContent = err.message;\n    //indicatorContainer.querySelector(\".spinner-grow\").classList.add(\"d-none\");\n    throw err;\n  });\n\n  // Keep track of initial OJS block render\n  const renderedOjs = {};\n\n  const process = async (context, inputs) => {\n    const pyodide = await pyodidePromise;\n    const evaluator = new PyodideEvaluator(pyodide, context);\n    await evaluator.process(inputs);\n    return evaluator.container;\n  }\n\n  return {\n    pyodidePromise,\n    renderedOjs,\n    process,\n  };\n}\n"}]}
</script>
<div id="exercise-loading-indicator" class="exercise-loading-indicator d-none d-flex align-items-center gap-2">
<div id="exercise-loading-status" class="d-flex gap-2">

</div>
<div class="spinner-grow spinner-grow-sm">

</div>
</div>
<script type="vfs-file">
W10=
</script>
</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="../qrjs_pics/unsoed_logo.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://imron-slide.vercel.app">irosyadi-2025</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script type="ojs-module-contents">
    eyJjb250ZW50cyI6W3sibWV0aG9kTmFtZSI6ImludGVycHJldCIsImNlbGxOYW1lIjoib2pzLWNlbGwtMSIsImlubGluZSI6ZmFsc2UsInNvdXJjZSI6InZpZXdvZiBpbnB1dF9zaXplID0gSW5wdXRzLnJhbmdlKFszLCAxMF0sIHt2YWx1ZTogNSwgc3RlcDogMSwgbGFiZWw6IFwiSW5wdXQgU2l6ZSAoTiB4IE4pXCJ9KTtcbnZpZXdvZiBrZXJuZWxfc2l6ZSA9IElucHV0cy5yYW5nZShbMSwgM10sIHt2YWx1ZTogMywgc3RlcDogMiwgbGFiZWw6IFwiS2VybmVsIFNpemUgKEsgeCBLKVwifSk7XG4ifSx7Im1ldGhvZE5hbWUiOiJpbnRlcnByZXQiLCJjZWxsTmFtZSI6Im9qcy1jZWxsLTIiLCJpbmxpbmUiOmZhbHNlLCJzb3VyY2UiOiJ2aWV3b2YgbWVhbl92YWwgPSBJbnB1dHMucmFuZ2UoWy0yLCAyXSwge3ZhbHVlOiAwLjUsIHN0ZXA6IDAuMSwgbGFiZWw6IFwiSW5wdXQgTWVhblwifSk7XG52aWV3b2Ygc3RkX3ZhbCA9IElucHV0cy5yYW5nZShbMC4xLCAzXSwge3ZhbHVlOiAxLjUsIHN0ZXA6IDAuMSwgbGFiZWw6IFwiSW5wdXQgU3RkIERldlwifSk7XG4ifSx7Im1ldGhvZE5hbWUiOiJpbnRlcnByZXRRdWlldCIsInNvdXJjZSI6InNoaW55SW5wdXQoJ2lucHV0X3NpemUnKSJ9LHsibWV0aG9kTmFtZSI6ImludGVycHJldFF1aWV0Iiwic291cmNlIjoic2hpbnlJbnB1dCgna2VybmVsX3NpemUnKSJ9LHsibWV0aG9kTmFtZSI6ImludGVycHJldFF1aWV0Iiwic291cmNlIjoic2hpbnlJbnB1dCgnbWVhbl92YWwnKSJ9LHsibWV0aG9kTmFtZSI6ImludGVycHJldFF1aWV0Iiwic291cmNlIjoic2hpbnlJbnB1dCgnc3RkX3ZhbCcpIn1dfQ==
    </script>
    <script type="module">
    if (window.location.protocol === "file:") { alert("The OJS runtime does not work with file:// URLs. Please use a web server to view this document."); }
    window._ojs.paths.runtimeToDoc = "../../google";
    window._ojs.paths.runtimeToRoot = "../..";
    window._ojs.paths.docToRoot = "..";
    window._ojs.selfContained = false;
    window._ojs.runtime.interpretFromScriptTags();
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>