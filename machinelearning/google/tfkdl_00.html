<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/quarto-contrib/live-runtime/live-runtime.js" type="module"></script>
<link href="../site_libs/quarto-contrib/live-runtime/live-runtime.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.25">

  <meta name="author" content="Imron Rosyadi">
  <title>Machine Learning – Machine-Learning-03</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-b0356e9119c1bdfd0db189d130feb51c.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script type="module" src="../site_libs/quarto-ojs/quarto-ojs-runtime.js"></script>
  <link href="../site_libs/quarto-ojs/quarto-ojs.css" rel="stylesheet">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Machine-Learning-03</h1>
  <p class="subtitle">Tensorflow, Keras, Deep Learning</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Imron Rosyadi 
</div>
</div>
</div>

</section>
<section id="overview-recognizing-handwritten-digits" class="slide level2">
<h2><strong>Overview: Recognizing Handwritten Digits</strong></h2>
<p>Today, we’ll dive into building a neural network to recognize handwritten digits.</p>
<p>We’ll achieve ~99% accuracy using fewer than 100 lines of Python/Keras code.</p>
<p>This is a classic problem in Machine Learning, often tackled with the <strong>MNIST dataset</strong>.</p>
<aside class="notes">
<p>Welcome to this lecture on Deep Learning for ECE! Today, we’re going to tackle a fascinating problem: teaching a computer to recognize handwritten digits. This isn’t just a theoretical exercise; it has real-world applications in areas like postal code recognition, check processing, and even automated data entry.</p>
<p>We’ll be using one of the most famous datasets in machine learning, MNIST, which consists of tens of thousands of handwritten digits. Our goal is ambitious: to achieve around 99% accuracy. And the best part? We’ll do it with remarkably concise code using Python’s TensorFlow and Keras libraries. This means focusing on the concepts and building blocks rather than getting bogged down in low-level details.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="what-youll-learn" class="slide level2">
<h2><strong>What You’ll Learn</strong></h2>
<p>This session will cover key concepts and practical techniques:</p>
<ul>
<li>What a neural network is and how it learns.</li>
<li>Building basic 1-layer neural networks with <code>tf.keras</code>.</li>
<li>Adding more layers for improved performance.</li>
<li>Implementing learning rate schedules.</li>
<li>Introduction to Convolutional Neural Networks (CNNs).</li>
<li>Regularization techniques: Dropout and Batch Normalization.</li>
<li>Understanding and mitigating overfitting.</li>
</ul>
<aside class="notes">
<p>Here’s a roadmap of what we’ll cover. We’ll start with the fundamentals of neural networks, then progressively build more complex models. We’ll explore techniques to optimize training, such as learning rate schedules, and introduce you to Convolutional Neural Networks, which are particularly powerful for image data like the MNIST digits. We’ll also discuss critical techniques like dropout and batch normalization to ensure our models are robust, and we’ll address the common pitfall of overfitting.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="understanding-the-training-data-mnist" class="slide level2">
<h2><strong>Understanding the Training Data: MNIST</strong></h2>
<p>The MNIST dataset contains 60,000 labeled images of handwritten digits (0-9).</p>
<div class="columns">
<div class="column" style="width:50%;">
<p>Each image is associated with its correct numerical label.</p>
<p>This “labeled dataset” is crucial for training.</p>
<p>Our neural network learns to classify these images into 10 classes (0 through 9).</p>
</div><div class="column" style="width:50%;">
<p><strong>Example MNIST Digits:</strong></p>
<p><img data-src="https://codelabs.developers.google.com/static/codelabs/cloud-tensorflow-mnist/img/ad83f98e56054737.png"></p>
</div></div>
<aside class="notes">
<p>The core of any supervised machine learning task is the data, and for image classification, especially with neural networks, we need <strong>labeled data</strong>. The MNIST dataset is a classic example. It comprises 60,000 images, each a 28x28 pixel grayscale representation of a handwritten digit, and crucially, each image comes with a label indicating the actual digit it represents. For instance, an image of a handwritten ‘5’ is accompanied by the label ‘5’. This labeled data is what allows our neural network to <em>learn</em> by example.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="training-vs.-validation-datasets" class="slide level2">
<h2><strong>Training vs.&nbsp;Validation Datasets</strong></h2>
<p>How do we assess our model’s “real-world” performance?</p>
<ul>
<li><strong>Training Dataset:</strong> Used to update the model’s internal parameters. The model <em>sees</em> this data multiple times.</li>
<li><strong>Validation Dataset:</strong> A separate, unseen labeled dataset to evaluate performance and prevent cheating. It reflects how well the model generalizes to new data.</li>
</ul>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>Using “unseen” data for validation is fundamental for robust model evaluation.</p>
</div>
</div>
</div>
<aside class="notes">
<p>It’s vital to know if our trained neural network can generalize its knowledge to new, unseen examples. If we only tested it on the data it was trained on, it would likely perform exceptionally well, but that wouldn’t tell us if it’s truly learned the underlying patterns or just memorized the training examples. This is where the distinction between training and validation datasets comes in.</p>
<p>The <strong>training dataset</strong> is like the practice problems you do to learn a subject. The model sees these examples, adjusts its internal parameters, and tries to get better.</p>
<p>The <strong>validation dataset</strong> is like a surprise pop quiz. The model has never seen these examples before. Its performance on this dataset gives us an unbiased estimate of how well it will perform on new, real-world data. A good validation score indicates the model is truly learning generalized features, not just memorizing.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="monitoring-training-progress" class="slide level2">
<h2><strong>Monitoring Training Progress</strong></h2>
<p>During training, we track two key metrics: <strong>accuracy</strong> and <strong>loss</strong>.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Accuracy</strong> (Right Plot):</p>
<ul>
<li>Percentage of correctly recognized digits.</li>
<li><em>Should increase</em> as training progresses.</li>
</ul>
<p><strong>Loss</strong> (Left Plot):</p>
<ul>
<li>Measures how “badly” the model performs.</li>
<li>The goal is to <em>minimize this value</em>.</li>
<li><em>Should decrease</em> on both training and validation data.</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="https://codelabs.developers.google.com/static/codelabs/cloud-tensorflow-mnist/img/3f7b405649301ea.png"></p>
<p><em>X-axis: Epochs (iterations over entire dataset)</em></p>
</div></div>
<aside class="notes">
<p>As our neural network trains, we want to monitor its progress. Neural network training is an iterative process where the model continually adjusts its internal weights and biases to improve its performance. The two primary metrics we observe during this process are <strong>accuracy</strong> and <strong>loss</strong>.</p>
<p><strong>Accuracy</strong> is straightforward: it’s the proportion of correctly classified examples. Ideally, this should increase over time, indicating the model is getting better at its task.</p>
<p><strong>Loss</strong>, on the other hand, is a numerical value that quantifies how “wrong” the model’s predictions are. Our training objective is to minimize this loss function. A decreasing loss on both the training and validation datasets is a strong indicator that the model is learning effectively and generalizing well. If the training loss continues to decrease but the validation loss starts to increase, that’s a sign of <em>overfitting</em>, which we’ll discuss later.</p>
<p>The X-axis in these plots represents <strong>epochs</strong>, where one epoch signifies one complete pass through the entire training dataset.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="making-predictions" class="slide level2">
<h2><strong>Making Predictions</strong></h2>
<p>After training, the model can predict digits it hasn’t seen.</p>

<img data-src="https://codelabs.developers.google.com/static/codelabs/cloud-tensorflow-mnist/img/c0699216ba0effdb.png" class="r-stretch"><p>This initial model reaches ~90% validation accuracy, meaning it still misclassifies 1000 out of 10,000 validation digits.</p>
<div class="callout callout-caution callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Caution</strong></p>
</div>
<div class="callout-content">
<p>Even 90% accuracy leaves room for improvement, especially in critical ECE applications like medical imaging or autonomous systems.</p>
</div>
</div>
</div>
<aside class="notes">
<p>Once training is complete, the true test of our model is its ability to make accurate predictions on new, unseen data. The visualization shows how our initial model performs on sample digits. Correct predictions are shown in black, and incorrect ones in red.</p>
<p>At 90% validation accuracy, the model is certainly respectable for a basic setup. However, in Many ECE applications, 90% might not be enough. For instance, if this were an autonomous driving system recognizing stop signs, a 10% error rate would be catastrophic. Even for digit recognition, if you’re processing millions of checks, a 10% error rate means a lot of manual corrections. This highlights why we constantly strive for higher accuracy and robustness in deep learning models.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="understanding-tensors-the-language-of-data" class="slide level2">
<h2><strong>Understanding Tensors: The Language of Data</strong></h2>
<p>In deep learning, data is represented as <strong>tensors</strong>. Tensors are multi-dimensional arrays, analogous to vectors and matrices.</p>
<ul>
<li><p><strong>Grayscale Image (28x28 pixels):</strong> A 2D tensor (matrix) with shape <code>[28, 28]</code>.</p></li>
<li><p><strong>Color Image (28x28 pixels, RGB):</strong> A 3D tensor with shape <code>[28, 28, 3]</code>. (Height, Width, Color Channels)</p></li>
<li><p><strong>Batch of Color Images (e.g., 128 images):</strong> A 4D tensor with shape <code>[128, 28, 28, 3]</code>. (Batch Size, Height, Width, Color Channels)</p></li>
</ul>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>The list of dimensions is called the <strong>“shape”</strong> of the tensor.</p>
<p>Understanding tensor shapes is crucial for building and debugging neural networks.</p>
</div>
</div>
</div>
<aside class="notes">
<p>Before we dive into building models, it’s critical to understand how data is represented in deep learning. The fundamental data structure is the <strong>tensor</strong>. Think of a tensor as a generalization of a scalar (a single number), a vector (a 1D array), and a matrix (a 2D array) to an arbitrary number of dimensions.</p>
<p>For example, a simple grayscale handwritten digit image, which is 28x28 pixels, can be represented as a 2D tensor, or a matrix, with a shape of <code>[28, 28]</code>. If we had a color image, with red, green, and blue channels, it would need a third dimension for the color channels, giving it a shape like <code>[28, 28, 3]</code>. And often, we process images in batches to make training more efficient. So, a batch of 128 color images would become a 4D tensor with a shape of <code>[128, 28, 28, 3]</code>.</p>
<p>The <strong>shape</strong> of a tensor tells you its dimensions, and understanding these shapes is absolutely crucial when designing neural network architectures and debugging issues. Incorrect tensor shapes are a very common source of errors.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="interactive-example-image-compression-analogy" class="slide level2">
<h2><strong>Interactive Example: Image Compression Analogy</strong></h2>
<p>Let’s visualize how much information we retain when we reduce the “dimensions” of an image. This is analogous to how neural networks extract features.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code hidden" id="cb1" data-startfrom="207" data-source-offset="-0"><pre class="sourceCode numberSource js number-lines code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 206;"><span id="cb1-207"><a></a>viewof originalWidth <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">100</span><span class="op">,</span> <span class="dv">500</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="dv">300</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="dv">10</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Original Width"</span>})<span class="op">;</span></span>
<span id="cb1-208"><a></a>viewof compressedWidth <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">10</span><span class="op">,</span> <span class="dv">100</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="dv">50</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="dv">5</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Compressed Width"</span>})<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-2" data-nodetype="declaration">

</div>
</div>
</div>
</div>
<div>
<div id="pyodide-1" class="exercise-cell">

</div>
<script type="pyodide-1-contents">
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
</script>
</div>
<aside class="notes">
<p>This interactive example uses an image compression analogy to illustrate how much information is retained as we reduce an image’s dimensions. In neural networks, particularly convolutional neural networks, we often downsample images (e.g., through pooling layers) to extract higher-level features and reduce computational load. By adjusting the “Compressed Width” slider, you can observe how a simplified representation of the image still captures its essence, much like how a neural network learns to represent complex patterns in a condensed form. This helps us understand the trade-offs between detail and efficiency in feature extraction.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="introduction-to-neural-networks" class="slide level2">
<h2>Introduction to Neural Networks</h2>
<p>Neural Networks are powerful computational models inspired by the human brain. They are used to learn complex patterns from data.</p>
<p>For ECE, neural networks are crucial in:</p>
<ul>
<li><strong>Signal Processing:</strong> Noise reduction, feature extraction.</li>
<li><strong>Image Recognition:</strong> Object detection, medical imaging analysis.</li>
<li><strong>Control Systems:</strong> Adaptive control, robotics.</li>
</ul>
<aside class="notes">
<p>Welcome to this lecture on Neural Networks! In ECE, understanding how these networks work is becoming increasingly vital. We’ll explore their fundamental concepts and see how they apply directly to engineering problems you’ll encounter. Think of them as sophisticated tools for processing and understanding complex data, much like how you design circuits to process signals.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="the-keras-sequential-api" class="slide level2">
<h2>The Keras Sequential API</h2>
<p>When building neural networks with TensorFlow and Keras, the <code>Sequential</code> API is a straightforward way to stack layers. This is ideal for models where layers have exactly one input tensor and one output tensor.</p>
<p><strong>Example: Image Classifier using Dense Layers</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a></a>model <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb2-2"><a></a>    tf.keras.layers.Flatten(input_shape<span class="op">=</span>[<span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>]), <span class="co"># Flattens input images</span></span>
<span id="cb2-3"><a></a>    tf.keras.layers.Dense(<span class="dv">200</span>, activation<span class="op">=</span><span class="st">"relu"</span>),    <span class="co"># Hidden layer with ReLU</span></span>
<span id="cb2-4"><a></a>    tf.keras.layers.Dense(<span class="dv">60</span>, activation<span class="op">=</span><span class="st">"relu"</span>),     <span class="co"># Another hidden layer</span></span>
<span id="cb2-5"><a></a>    tf.keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)   <span class="co"># Output layer for 10 classes</span></span>
<span id="cb2-6"><a></a>])</span>
<span id="cb2-7"><a></a></span>
<span id="cb2-8"><a></a>model.<span class="bu">compile</span>(</span>
<span id="cb2-9"><a></a>    optimizer<span class="op">=</span><span class="st">'adam'</span>,</span>
<span id="cb2-10"><a></a>    loss<span class="op">=</span><span class="st">'categorical_crossentropy'</span>,</span>
<span id="cb2-11"><a></a>    metrics<span class="op">=</span>[<span class="st">'accuracy'</span>]</span>
<span id="cb2-12"><a></a>)</span>
<span id="cb2-13"><a></a><span class="co"># Train the model</span></span>
<span id="cb2-14"><a></a><span class="co"># model.fit(dataset, ...)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<aside class="notes">
<p>The Keras Sequential API is your go-to for building simple, layer-by-layer models. It’s like stacking different circuit components one after another. In ECE, you might think of each layer as a stage in a signal processing pipeline. The <code>Flatten</code> layer converts your 2D image data into a 1D vector, which is necessary for the <code>Dense</code> layers. <code>Dense</code> layers are fully connected, meaning every neuron in one layer connects to every neuron in the next. We’ll discuss activation functions like “relu” and “softmax” shortly.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="what-is-a-neuron-the-basic-building-block" class="slide level2 scrollable">
<h2>What is a Neuron? The Basic Building Block</h2>
<p>The fundamental unit of a neural network is the <strong>neuron</strong>, a concept analogous to a processing unit in digital circuits.</p>
<div class="columns">
<div class="column" style="width:60%;">
<p>Each neuron performs three main operations:</p>
<ol type="1">
<li><strong>Weighted Sum:</strong> Multiplies each input by a corresponding <strong>weight</strong> and sums them up.</li>
<li><strong>Bias Addition:</strong> Adds a <strong>bias</strong> constant to the weighted sum.</li>
<li><strong>Activation:</strong> Passes the result through a non-linear <strong>activation function</strong>.</li>
</ol>
<p>The <strong>weights</strong> (<code>W</code>) and <strong>biases</strong> (<code>b</code>) are the <em>parameters</em> learned during training. Initially, they are random and get adjusted to minimize error.</p>
</div><div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://codelabs.developers.google.com/static/codelabs/cloud-tensorflow-mnist/img/e218e6eee9da4e.png"></p>
<figcaption>neuron.png</figcaption>
</figure>
</div>
</div></div>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p>Think of weights as variable resistors and biases as constant voltage offsets in an analog circuit. The activation function is like a threshold detector or a non-linear amplifier.</p>
</div>
</div>
</div>
<aside class="notes">
<p>The neuron is the basic computational element. It’s not unlike an operational amplifier with multiple inputs, where each input has a gain (weight) and an offset (bias), and the output is then shaped by a non-linear transfer function. These weights and biases are the “knobs” we turn during training to make the network learn.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="single-dense-layer-mnist-example" class="slide level2">
<h2>Single Dense Layer: MNIST Example</h2>
<p>Let’s consider classifying handwritten digits from the MNIST dataset. Each image is 28x28 pixels grayscale.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p>The simplest neural network for this task uses 784 pixels (28x28) as inputs to a <strong>single dense layer</strong>.</p>
<p>This layer has 10 output neurons, one for each digit class (0-9).</p>
<p>Each of these 10 output neurons takes all 784 pixel values as input, performs a weighted sum, adds a bias, and applies an activation.</p>
</div><div class="column" style="width:50%;">
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<svg width="960" height="480" viewbox="0.00 0.00 160.00 258.00" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style="; max-width: none; max-height: none; display: block; margin: auto auto auto auto">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 254)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-254 156,-254 156,4 -4,4"></polygon>
<!-- _implicit_input_layer -->
<!-- pixel_1 -->
<g id="node2" class="node">
<title>pixel_1</title>
<polygon fill="#add8e6" stroke="black" points="58,-196 0,-196 0,-174 58,-174 58,-196"></polygon>
<text text-anchor="middle" x="29" y="-182" font-family="Times,serif" font-size="10.00">Pixel 1</text>
</g>
<!-- pixel_2 -->
<g id="node3" class="node">
<title>pixel_2</title>
<polygon fill="#add8e6" stroke="black" points="58,-156 0,-156 0,-134 58,-134 58,-156"></polygon>
<text text-anchor="middle" x="29" y="-142" font-family="Times,serif" font-size="10.00">Pixel 2</text>
</g>
<!-- pixel_1&#45;&gt;pixel_2 -->
<!-- digit_0 -->
<g id="node6" class="node">
<title>digit_0</title>
<ellipse fill="#ffd700" stroke="black" cx="123" cy="-221" rx="29" ry="29"></ellipse>
<text text-anchor="middle" x="123" y="-218" font-family="Times,serif" font-size="10.00">Digit 0</text>
</g>
<!-- pixel_1&#45;&gt;digit_0 -->
<g id="edge1" class="edge">
<title>pixel_1-&gt;digit_0</title>
<path fill="none" stroke="black" d="M58.1,-196C70.05,-200.67 83.89,-206.09 95.64,-210.69"></path>
</g>
<!-- digit_9 -->
<g id="node9" class="node">
<title>digit_9</title>
<ellipse fill="#ffd700" stroke="black" cx="123" cy="-29" rx="29" ry="29"></ellipse>
<text text-anchor="middle" x="123" y="-26" font-family="Times,serif" font-size="10.00">Digit 9</text>
</g>
<!-- pixel_1&#45;&gt;digit_9 -->
<g id="edge2" class="edge">
<title>pixel_1-&gt;digit_9</title>
<path fill="none" stroke="black" d="M48.85,-173.94C52.22,-171.33 55.46,-168.33 58,-165 86.13,-128.1 72.02,-107.86 94,-67 96.87,-61.66 100.42,-56.23 104.02,-51.21"></path>
</g>
<!-- pixel_3 -->
<g id="node4" class="node">
<title>pixel_3</title>
<polygon fill="#add8e6" stroke="transparent" points="58,-116 0,-116 0,-94 58,-94 58,-116"></polygon>
<text text-anchor="middle" x="29" y="-102" font-family="Times,serif" font-size="10.00">...</text>
</g>
<!-- pixel_2&#45;&gt;pixel_3 -->
<!-- pixel_784 -->
<g id="node5" class="node">
<title>pixel_784</title>
<polygon fill="#add8e6" stroke="black" points="58,-76 0,-76 0,-54 58,-54 58,-76"></polygon>
<text text-anchor="middle" x="29" y="-62" font-family="Times,serif" font-size="10.00">Pixel 784</text>
</g>
<!-- pixel_3&#45;&gt;pixel_784 -->
<!-- pixel_784&#45;&gt;digit_0 -->
<g id="edge3" class="edge">
<title>pixel_784-&gt;digit_0</title>
<path fill="none" stroke="black" d="M48.85,-76.06C52.22,-78.67 55.46,-81.67 58,-85 86.13,-121.9 72.02,-142.14 94,-183 96.87,-188.34 100.42,-193.77 104.02,-198.79"></path>
</g>
<!-- pixel_784&#45;&gt;digit_9 -->
<g id="edge4" class="edge">
<title>pixel_784-&gt;digit_9</title>
<path fill="none" stroke="black" d="M58.1,-54C70.05,-49.33 83.89,-43.91 95.64,-39.31"></path>
</g>
<!-- digit_1 -->
<g id="node7" class="node">
<title>digit_1</title>
<ellipse fill="#ffd700" stroke="black" cx="123" cy="-145" rx="29" ry="29"></ellipse>
<text text-anchor="middle" x="123" y="-142" font-family="Times,serif" font-size="10.00">Digit 1</text>
</g>
<!-- digit_0&#45;&gt;digit_1 -->
<!-- digit_2 -->
<g id="node8" class="node">
<title>digit_2</title>
<polygon fill="#ffd700" stroke="transparent" points="152,-98 94,-98 94,-76 152,-76 152,-98"></polygon>
<text text-anchor="middle" x="123" y="-84" font-family="Times,serif" font-size="10.00">...</text>
</g>
<!-- digit_1&#45;&gt;digit_2 -->
<!-- digit_2&#45;&gt;digit_9 -->
</g>
</svg>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div></div>
<aside class="notes">
<p>For MNIST, each 28x28 pixel image contains 784 individual pixel values. These 784 values become the direct inputs to the first layer of our neural network. Since we want to classify digits from 0 to 9, we need 10 output neurons, each corresponding to a specific digit. This diagram illustrates the “dense” or fully connected nature of this layer, where every input pixel influences every output neuron.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="matrix-multiplication-for-a-single-layer" class="slide level2">
<h2>Matrix Multiplication for a Single Layer</h2>
<p>A dense layer’s operations can be efficiently represented using <strong>matrix multiplication</strong>.</p>
<p>If <code>X</code> is a matrix of 100 images (each flattened to 784 pixels), and <code>W</code> is the weight matrix (784 inputs x 10 outputs), then:</p>
<p><span class="math display">\[ \text{Weighted Sums} = X \cdot W \]</span></p>
<p><span class="math display">\[ \text{Output} = \text{Activation}(X \cdot W + b) \]</span></p>
<p>Where <code>b</code> is the bias vector (10 elements), broadcasted across the 100 images.</p>
</section>
<section id="matrix-multiplication-for-a-single-layer-1" class="slide level2">
<h2>Matrix Multiplication for a Single Layer</h2>

<img data-src="https://codelabs.developers.google.com/static/codelabs/cloud-tensorflow-mnist/img/706248ddefae07f0.gif" class="column-page r-stretch quarto-figure-center"><p class="caption">matmul.gif</p><p>In Keras, this is simplified:<code>tf.keras.layers.Dense(10, activation='softmax')</code></p>
<aside class="notes">
<p>The magic of linear algebra allows us to express thousands, or even millions, of <code>neuron</code> calculations as simple matrix multiplications. This is incredibly efficient for modern hardware like GPUs, which are optimized for these operations. <code>X</code> would be a 100x784 matrix, and <code>W</code> would be a 784x10 matrix. The product <code>X \cdot W</code> would then be a 100x10 matrix, where each row represents the 10 neuron outputs for one of the 100 images.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="going-deep-chaining-layers" class="slide level2">
<h2>Going Deep: Chaining Layers</h2>
<p>“Deep learning” refers to using multiple hidden layers. Each layer computes weighted sums of the outputs of the previous layer.</p>
<p>This architecture allows the network to learn progressively more complex and abstract features from the raw input data.</p>
<p>For example, early layers might detect edges or simple shapes, while later layers combine these to recognize parts of objects or entire objects.</p>
<p>The choice of <strong>activation function</strong> is critical and typically changes only for the very last layer in a classifier.</p>
</section>
<section id="going-deep-chaining-layers-1" class="slide level2">
<h2>Going Deep: Chaining Layers</h2>

<img data-src="https://codelabs.developers.google.com/static/codelabs/cloud-tensorflow-mnist/img/fba0638cc213a29.png" class="r-stretch quarto-figure-center"><p class="caption">fba0638cc213a29.png</p><aside class="notes">
<p>“Going deep” is like adding more stages to a complex signal processing pipeline, where each stage refines the information from the previous one. In image processing, the first layer might learn to identify basic lines or corners, while the second layer might combine these to detect ears or eyes, and final layers assemble these into a full face recognition. This hierarchical learning is a key advantage of deep networks.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="activation-functions-relu-and-softmax" class="slide level2">
<h2>Activation Functions: ReLU and Softmax</h2>
<p>Activation functions introduce non-linearity, allowing neural networks to learn complex, non-linear relationships in data.</p>
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="sigmoid">Sigmoid</h3>
<ul>
<li>The most classical</li>
<li>Used on intermediate layers</li>
</ul>
<h3 id="rectified-linear-unit-relu">Rectified Linear Unit (ReLU)</h3>
<p><span class="math inline">\(f(x) = \max(0, x)\)</span></p>
<ul>
<li>Most popular activation for hidden layers.</li>
<li>Simple and computationally efficient.</li>
<li>Helps prevent vanishing gradients.</li>
</ul>
<h3 id="softmax">Softmax</h3>
<p><span class="math display">\[ \sigma(z_i) = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}} \]</span></p>
<ul>
<li>Used in the output layer of multi-class classifiers.</li>
<li>Converts logits into probabilities that sum to 1.</li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://codelabs.developers.google.com/static/codelabs/cloud-tensorflow-mnist/img/41fc82288c4aff5d.png"></p>
<figcaption>sigmoid.png</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://codelabs.developers.google.com/static/codelabs/cloud-tensorflow-mnist/img/644f4213a4ee70e5.png"></p>
<figcaption>relu.png</figcaption>
</figure>
</div>
</div></div>
<aside class="notes">
<p>Non-linearity is crucial. Without it, stacking layers would just result in another linear transformation, essentially a single layer. ReLU is like a simple switch: if the input is positive, it passes it; otherwise, it stops it. Softmax is a smart way to get probabilities out of our network for classification. For instance, if you have ten possible digit classes, Softmax ensures that the network’s output is a set of ten probabilities that add up to one, indicating the likelihood of the input belonging to each class.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="softmax-in-action-interactive-example" class="slide level2">
<h2>Softmax in Action: Interactive Example</h2>
<p>Adjust the <code>Logit Value</code> for a single class and observe how Softmax normalizes probabilities. Here, we simulate 10 classes, with one <code>Logit Value</code> adjusted at a time.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code hidden" id="cb3" data-startfrom="498" data-source-offset="0"><pre class="sourceCode numberSource js number-lines code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 497;"><span id="cb3-498"><a></a>viewof logit_val <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">0</span><span class="op">,</span> <span class="dv">10</span>]<span class="op">,</span> {<span class="dt">step</span><span class="op">:</span> <span class="fl">0.1</span><span class="op">,</span> <span class="dt">value</span><span class="op">:</span> <span class="dv">5</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Logit Value for Class 0 (others fixed at 1.0)"</span>})<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div id="ojs-cell-2" data-nodetype="declaration">

</div>
</div>
</div>
<div>
<div id="pyodide-2" class="exercise-cell">

</div>
<script type="pyodide-2-contents">
eyJhdHRyIjp7ImVjaG8iOmZhbHNlLCJlZGl0IjpmYWxzZSwiZXZhbCI6dHJ1ZSwiaW5wdXQiOlsibG9naXRfdmFsIl19LCJjb2RlIjoiXG5pbXBvcnQgbnVtcHkgYXMgbnBcbmltcG9ydCBwbG90bHkuZ3JhcGhfb2JqZWN0cyBhcyBnb1xuXG4jIFNpbXVsYXRlIGxvZ2l0cyBmb3IgMTAgY2xhc3Nlc1xubG9naXRzID0gbnAub25lcygxMClcbmxvZ2l0c1swXSA9IGxvZ2l0X3ZhbCAgIyBBcHBseSB1c2VyLWRlZmluZWQgbG9naXQgdmFsdWUgdG8gY2xhc3MgMFxuXG4jIEFwcGx5IHNvZnRtYXhcbnNvZnRtYXhfb3V0cHV0ID0gbnAuZXhwKGxvZ2l0cykgLyBucC5zdW0obnAuZXhwKGxvZ2l0cykpXG5cbiMgQ3JlYXRlIFBsb3RseSBiYXIgY2hhcnRcbmZpZyA9IGdvLkZpZ3VyZShkYXRhPVtnby5CYXIoeD1bZlwiQ2xhc3Mge2l9XCIgZm9yIGkgaW4gcmFuZ2UoMTApXSwgeT1zb2Z0bWF4X291dHB1dCldKVxuZmlnLnVwZGF0ZV9sYXlvdXQoXG4gICAgdGl0bGU9ZlwiU29mdG1heCBPdXRwdXQgZm9yIExvZ2l0cyAoTG9naXRfMD17bG9naXRfdmFsOi4xZn0pXCIsXG4gICAgeGF4aXNfdGl0bGU9XCJDbGFzc1wiLFxuICAgIHlheGlzX3RpdGxlPVwiUHJvYmFiaWxpdHlcIixcbiAgICB5YXhpc19yYW5nZT1bMCwgMV0sXG4gICAgaGVpZ2h0PTQwMCxcbiAgICBtYXJnaW49ZGljdChsPTAsIHI9MCwgYj0wLCB0PTQwKVxuKVxuZmlnIn0=
</script>
</div>
<aside class="notes">
<p>This interactive example demonstrates the power of Softmax. Notice how even a small increase in one logit value can significantly boost its corresponding probability, while simultaneously reducing the probabilities of other classes to ensure their sum remains 1. This is critical for making clear, probabilistic predictions in classification tasks.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="loss-function-cross-entropy" class="slide level2">
<h2>Loss Function: Cross-Entropy</h2>
<p>To train a neural network, we need to measure how “wrong” its predictions are compared to the true labels. This measure is called the <strong>loss function</strong>.</p>
<p>For multi-class classification, <strong>Cross-Entropy Loss</strong> is the standard.</p>
<p><span class="math display">\[ H(p, q) = - \sum_{i=1}^{K} p_i \log(q_i) \]</span></p>
<p>Where: * <span class="math inline">\(p_i\)</span> is the true probability for class <span class="math inline">\(i\)</span> (1 for the correct class, 0 otherwise). * <span class="math inline">\(q_i\)</span> is the predicted probability for class <span class="math inline">\(i\)</span> (output of softmax). * <span class="math inline">\(K\)</span> is the number of classes.</p>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Cross-entropy loss heavily penalizes incorrect high-confidence predictions, guiding the network to both be correct and confident.</p>
</div>
</div>
</div>
</section>
<section id="loss-function-cross-entropy-1" class="slide level2">
<h2>Loss Function: Cross-Entropy</h2>

<img data-src="https://codelabs.developers.google.com/static/codelabs/cloud-tensorflow-mnist/img/6dbba1bce3cadc36.png" class="r-stretch quarto-figure-center"><p class="caption">cross_entropy.png</p><aside class="notes">
<p>The loss function is our feedback mechanism. It tells us how far off our network’s predictions are from the ground truth. Cross-entropy is particularly effective because it’s not just about getting the right answer; it’s also about <em>how confident</em> the network is when it’s right or wrong. Misclassifying an image with high confidence leads to a very large penalty, forcing the network to learn better.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="training-gradient-descent" class="slide level2 scrollable">
<h2>Training: Gradient Descent</h2>
<p>“Training” a neural network means iteratively adjusting its weights and biases to minimize the loss function. This is achieved using an optimization algorithm called <strong>Gradient Descent</strong>.</p>
<ol type="1">
<li><strong>Compute Gradient:</strong> Calculate the partial derivatives of the loss function with respect to every weight and bias. This “gradient” vector points in the direction of the steepest increase of the loss.</li>
<li><strong>Update Parameters:</strong> Adjust weights and biases in the <em>opposite</em> direction of the gradient, typically by a small step size called the <strong>learning rate</strong>.</li>
</ol>
<p><span class="math display">\[ W_{new} = W_{old} - \alpha \frac{\partial L}{\partial W} \\ b_{new} = b_{old} - \alpha \frac{\partial L}{\partial b} \]</span></p>
<p>This process is repeated over many <strong>epochs</strong> (passes through the entire dataset).</p>
</section>
<section id="training-gradient-descent-1" class="slide level2">
<h2>Training: Gradient Descent</h2>

<img data-src="https://codelabs.developers.google.com/static/codelabs/cloud-tensorflow-mnist/img/9d3ab44f06bb0f7a.png" class="r-stretch quarto-figure-center"><p class="caption">gradient_descent.png</p><aside class="notes">
<p>Gradient descent is like a guided search. Imagine you’re blindfolded on a mountainous landscape (the loss function surface) and you want to find the lowest point. You feel the slope around you (compute the gradient) and take a small step downhill. You repeat this until you reach a valley. The ‘learning rate’ determines how big a step you take. Too large, and you might overshoot the minimum; too small, and it takes too long to converge.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="mini-batching-and-momentum" class="slide level2">
<h2>Mini-Batching and Momentum</h2>
<p>To improve training efficiency and stability:</p>
<h3 id="mini-batching">Mini-Batching</h3>
<ul>
<li>Instead of computing the gradient for one image at a time, we use a <strong>batch</strong> (e.g., 32, 64, or 128 images).</li>
<li>Provides a more stable and representative gradient estimate.</li>
<li>Leverages highly optimized matrix operations on GPUs/TPUs.</li>
</ul>
<h3 id="momentum">Momentum</h3>
<ul>
<li>Adds a fraction of the previous update vector to the current update.</li>
<li>Helps overcome local minima, saddle points, and speeds up convergence in relevant directions.</li>
</ul>
</section>
<section id="mini-batching-and-momentum-1" class="slide level2">
<h2>Mini-Batching and Momentum</h2>

<img data-src="https://codelabs.developers.google.com/static/codelabs/cloud-tensorflow-mnist/img/cc544924671fa208.png" class="r-stretch quarto-figure-center"><p class="caption">saddle_point.png</p><p><em>Illustration: A saddle point, where the gradient is zero but not a true minimum in all directions.</em></p>
<aside class="notes">
<p>Mini-batching is a practical optimization. Imagine getting directions from one person versus getting an averaged direction from a group of people; the latter is usually more reliable. Momentum, on the other hand, gives the optimization process inertia. It prevents the optimizer from getting stuck in “saddle points”—flat regions in the loss landscape that are not true minima. This is crucial for navigating high-dimensional spaces effectively.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="glossary-of-key-terms" class="slide level2">
<h2>Glossary of Key Terms</h2>
<ul>
<li><strong>Batch/Mini-batch:</strong> A subset of the training data used in one iteration of gradient descent.</li>
<li><strong>Cross-Entropy Loss:</strong> A common loss function for classification tasks, measuring dissimilarity between predicted and true probability distributions.</li>
<li><strong>Dense Layer:</strong> A layer where each neuron is connected to every neuron in the preceding layer.</li>
<li><strong>Features:</strong> The input attributes or data points fed into a neural network.</li>
<li><strong>Labels:</strong> The correct outputs or target values in supervised learning.</li>
<li><strong>Learning Rate:</strong> A hyperparameter controlling the step size during gradient descent.</li>
<li><strong>Logits:</strong> The raw, unnormalized outputs of a neural network layer before being passed through an activation function like softmax.</li>
<li><strong>Loss Function:</strong> A function that quantifies the error between predicted outputs and true labels.</li>
<li><strong>Neuron:</strong> The fundamental computational unit of a neural network.</li>
<li><strong>One-Hot Encoding:</strong> A categorical variable representation where each category is a binary vector (e.g., [0,0,1,0] for class 3 of 4).</li>
<li><strong>ReLU (Rectified Linear Unit):</strong> A popular activation function, <span class="math inline">\(f(x) = \max(0, x)\)</span>.</li>
<li><strong>Sigmoid:</strong> An S-shaped activation function, <span class="math inline">\(f(x) = 1 / (1 + e^{-x})\)</span>.</li>
<li><strong>Softmax:</strong> An activation function that converts a vector of numbers into a probability distribution.</li>
<li><strong>Tensor:</strong> A generalization of vectors and matrices to an arbitrary number of dimensions.</li>
</ul>
<aside class="notes">
<p>This glossary provides a quick reference for the key terms we’ve covered. Understanding these terms is foundational for further study and application of machine learning in ECE.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="diving-into-the-code-part-1" class="slide level2">
<h2><strong>Diving into the Code (Part 1)</strong></h2>
<p>Now, let’s dissect the code from the <code>keras_01_mnist.ipynb</code> notebook. Understanding each section is key to building and modifying models.</p>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Follow along in the <a href="https://colab.research.google.com/github/GoogleCloudPlatform/tensorflow-without-a-phd/blob/master/tensorflow-mnist-tutorial/keras_01_mnist.ipynb">Colab notebook</a> if you can!</p>
</div>
</div>
</div>
<p>We’ll cover core components: - Model Parameters and Imports - Data Preparation with <code>tf.data.Dataset</code> - Building a Keras Sequential Model - Training and Validation - Visualizing Predictions</p>
</section>
<section id="model-parameters-and-imports" class="slide level2">
<h2><strong>Model Parameters and Imports</strong></h2>
<p>These initial cells set up the environment and define global constants.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Parameters Cell:</strong></p>
<p>Sets values for:</p>
<ul>
<li><code>BATCH_SIZE</code>: Number of samples processed per gradient update.</li>
<li><code>EPOCHS</code>: Number of complete passes through the training dataset.</li>
<li><code>GCS_PATTERN</code>: Location of MNIST data files on Google Cloud Storage.</li>
</ul>
<p><strong>Imports Cell:</strong></p>
<p>Imports necessary libraries:</p>
<ul>
<li><code>tensorflow</code> (<code>tf</code>): Core Deep Learning framework.</li>
<li><code>numpy</code> (<code>np</code>): For numerical operations (especially tensor manipulation).</li>
<li><code>matplotlib.pyplot</code> (<code>plt</code>): For plotting and visualization.</li>
</ul>
</div><div class="column" style="width:50%;">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a></a><span class="co"># Example of Parameters</span></span>
<span id="cb4-2"><a></a>BATCH_SIZE <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb4-3"><a></a>EPOCHS <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb4-4"><a></a>GCS_PATTERN <span class="op">=</span> <span class="st">"gs://cloud-tpu-datasets/mnist/mnist_</span><span class="sc">{}</span><span class="st">.tfrec"</span></span>
<span id="cb4-5"><a></a></span>
<span id="cb4-6"><a></a><span class="bu">print</span>(<span class="ss">f"Batch Size: </span><span class="sc">{</span>BATCH_SIZE<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-7"><a></a><span class="bu">print</span>(<span class="ss">f"Epochs: </span><span class="sc">{</span>EPOCHS<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a></a><span class="co"># Example of Imports</span></span>
<span id="cb5-2"><a></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb5-3"><a></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-4"><a></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-5"><a></a></span>
<span id="cb5-6"><a></a><span class="bu">print</span>(<span class="st">"TensorFlow version:"</span>, tf.__version__)</span>
<span id="cb5-7"><a></a><span class="bu">print</span>(<span class="st">"NumPy version:"</span>, np.__version__)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div></div>
<aside class="notes">
<p>The first crucial step in any machine learning project is to set up our environment and define key hyper-parameters.</p>
<p>The <strong>“Parameters”</strong> cell defines values like <code>BATCH_SIZE</code>, which controls how many training examples are processed together before the model’s weights are updated. <code>EPOCHS</code> determines how many full iterations the model makes over the entire training dataset. The <code>GCS_PATTERN</code> indicates where our MNIST dataset is stored, in this case, on Google Cloud Storage.</p>
<p>The <strong>“Imports”</strong> cell brings in all the necessary Python libraries. <code>tensorflow</code> is our backbone for deep learning. <code>numpy</code> is essential for numerical operations and efficient array manipulation, which is fundamental to working with tensors. Lastly, <code>matplotlib.pyplot</code> is indispensable for visualizing our data, training progress, and results.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="data-preparation-with-tf.data.dataset" class="slide level2 scrollable">
<h2><strong>Data Preparation with <code>tf.data.Dataset</code></strong></h2>
<p>The <code>tf.data.Dataset</code> API is powerful for building efficient data pipelines. It handles loading, parsing, and preprocessing data, especially at scale.</p>
<p><strong>Key Steps:</strong></p>
<ol type="1">
<li><p><strong>Load Fixed-Length Records:</strong> Images and labels are stored in <code>tfrec</code> files. We decode raw byte strings into images (float32, normalized 0-1) and flatten them.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a></a>imagedataset <span class="op">=</span> tf.data.FixedLengthRecordDataset(image_filename, <span class="dv">28</span><span class="op">*</span><span class="dv">28</span>, header_bytes<span class="op">=</span><span class="dv">16</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong><code>read_image</code> Function:</strong> Parses byte strings into float32, normalizes pixels (0-1), and reshapes to <code>[28*28]</code> (flattened for initial dense layer).</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a></a><span class="kw">def</span> read_image(tf_bytestring):</span>
<span id="cb7-2"><a></a>    image <span class="op">=</span> tf.io.decode_raw(tf_bytestring, tf.uint8)</span>
<span id="cb7-3"><a></a>    image <span class="op">=</span> tf.cast(image, tf.float32)<span class="op">/</span><span class="fl">256.0</span></span>
<span id="cb7-4"><a></a>    image <span class="op">=</span> tf.reshape(image, [<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>])</span>
<span id="cb7-5"><a></a>    <span class="cf">return</span> image</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
</ol>
<aside class="notes">
<p>Preparing data efficiently is as important as designing the neural network itself. TensorFlow’s <code>tf.data.Dataset</code> API is specifically designed for this, allowing us to build highly optimized input pipelines.</p>
<p>The MNIST dataset images and labels are stored in a specialized format known as <code>tfrec</code> (TFRecord) files. The <code>FixedLengthRecordDataset</code> is used to read these files, assuming each record (image) has a fixed length. We also skip a header of 16 bytes.</p>
<p>The <code>read_image</code> function is crucial. Raw image data comes as bytes (<code>tf.uint8</code>). We cast this to <code>tf.float32</code> because neural networks generally prefer floating-point inputs. We then normalize the pixel values from the <code>[0, 255]</code> range to <code>[0, 1]</code> by dividing by 256. Finally, we reshape the 2D 28x28 image into a 1D vector of <code>28*28 = 784</code> pixels because our initial single-layer model expects a flattened input.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="data-preparation-cont.-pipeline-operations" class="slide level2">
<h2><strong>Data Preparation (Cont.): Pipeline Operations</strong></h2>
<p>After parsing, we apply various transformations to optimize the dataset for training:</p>
<p><strong>1. Mapping &amp; Zipping:</strong></p>
<ul>
<li>Apply <code>read_image</code> to all images using <code>.map()</code>.</li>
<li>Do similar steps for labels.</li>
<li>Combine images and labels using <code>.zip()</code>: <code>dataset = tf.data.Dataset.zip((imagedataset, labelsdataset))</code></li>
</ul>
<p><strong>2. Optimizations:</strong></p>
<ul>
<li><code>.cache()</code>: Store dataset in RAM for faster epoch transitions (for small datasets).</li>
<li><code>.shuffle(buffer_size)</code>: Randomize training order to prevent batch order biases.</li>
<li><code>.repeat()</code>: Loop the dataset indefinitely for multiple epochs.</li>
<li><code>.batch(batch_size)</code>: Group samples into mini-batches for efficient processing.</li>
<li><code>.prefetch(tf.data.experimental.AUTOTUNE)</code>: Overlap data preprocessing and model execution to keep the GPU busy.</li>
</ul>
<aside class="notes">
<p>Once individual images and labels are read and decoded, we build the full data pipeline using a sequence of <code>tf.data.Dataset</code> operations.</p>
<p>First, we use <code>.map()</code> to apply our <code>read_image</code> function across the entire image dataset. We perform analogous steps for the labels, then use <code>.zip()</code> to pair each image with its corresponding label, forming a dataset of <code>(image, label)</code> pairs.</p>
<p>Next come the crucial optimizations for training:</p>
<ul>
<li><code>.cache()</code> stores the prepared dataset in RAM after the first epoch, eliminating redundant processing in subsequent epochs, which is excellent for smaller datasets like MNIST.</li>
<li><code>.shuffle()</code> randomizes the order of examples within a specified buffer. This prevents the model from learning spurious patterns related to the order of appearance in the dataset.</li>
<li><code>.repeat()</code> ensures that the dataset loops indefinitely, so we can train for multiple epochs without explicitly re-initializing the data pipeline.</li>
<li><code>.batch()</code> groups individual examples into mini-batches. This is fundamental for efficient training, as gradient updates are typically computed over batches, not individual examples.</li>
<li><code>.prefetch(tf.data.experimental.AUTOTUNE)</code> is a performance booster. It allows the data pipeline to prepare the next batch of data on the CPU <em>while</em> the current batch is being processed by the GPU, maximizing hardware utilization.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="building-a-keras-sequential-model-the-1-layer-network" class="slide level2 scrollable">
<h2><strong>Building a Keras Sequential Model: The 1-Layer Network</strong></h2>
<p>Our first model is a simple, single-layer dense neural network.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Model Definition:</strong></p>
<ul>
<li>We use <code>tf.keras.Sequential</code> for a linear stack of layers.</li>
<li><code>tf.keras.layers.Input(shape=(28*28,))</code>: Defines the input shape (784-element flattened vector).</li>
<li><code>tf.keras.layers.Dense(10, activation='softmax')</code>:
<ul>
<li><code>10</code> neurons: One for each digit class (0-9).</li>
<li><code>'softmax'</code> activation: Outputs a probability distribution over the 10 classes, summing to 1. (The highest probability indicates the predicted class.)</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<svg width="960" height="480" viewbox="0.00 0.00 422.33 49.60" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style="; max-width: none; max-height: none; display: block; margin: auto auto auto auto">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 45.6)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-45.6 418.33,-45.6 418.33,4 -4,4"></polygon>
<!-- Input -->
<g id="node1" class="node">
<title>Input</title>
<polygon fill="none" stroke="black" points="87.89,-41.4 0.04,-41.4 0.04,-0.2 87.89,-0.2 87.89,-41.4"></polygon>
<text text-anchor="middle" x="43.96" y="-25" font-family="Times,serif" font-size="14.00">Input Layer</text>
<text text-anchor="middle" x="43.96" y="-8.2" font-family="Times,serif" font-size="14.00">(Shape: 784)</text>
</g>
<!-- Dense -->
<g id="node2" class="node">
<title>Dense</title>
<polygon fill="none" stroke="black" points="265.31,-41.4 124.13,-41.4 124.13,-0.2 265.31,-0.2 265.31,-41.4"></polygon>
<text text-anchor="middle" x="194.72" y="-25" font-family="Times,serif" font-size="14.00">Dense Layer</text>
<text text-anchor="middle" x="194.72" y="-8.2" font-family="Times,serif" font-size="14.00">(10 neurons, Softmax)</text>
</g>
<!-- Input&#45;&gt;Dense -->
<g id="edge1" class="edge">
<title>Input-&gt;Dense</title>
<path fill="none" stroke="black" d="M87.99,-20.8C96.16,-20.8 104.94,-20.8 113.83,-20.8"></path>
<polygon fill="black" stroke="black" points="113.89,-24.3 123.89,-20.8 113.89,-17.3 113.89,-24.3"></polygon>
</g>
<!-- Output -->
<g id="node3" class="node">
<title>Output</title>
<polygon fill="none" stroke="black" points="414.23,-41.4 301.61,-41.4 301.61,-0.2 414.23,-0.2 414.23,-41.4"></polygon>
<text text-anchor="middle" x="357.92" y="-25" font-family="Times,serif" font-size="14.00">Output</text>
<text text-anchor="middle" x="357.92" y="-8.2" font-family="Times,serif" font-size="14.00">(10 probabilities)</text>
</g>
<!-- Dense&#45;&gt;Output -->
<g id="edge2" class="edge">
<title>Dense-&gt;Output</title>
<path fill="none" stroke="black" d="M265.49,-20.8C273.96,-20.8 282.59,-20.8 291,-20.8"></path>
<polygon fill="black" stroke="black" points="291.13,-24.3 301.13,-20.8 291.13,-17.3 291.13,-24.3"></polygon>
</g>
</g>
</svg>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div></div>
<aside class="notes">
<p>Now, let’s get to the heart of the neural network: the model itself. Keras provides a very intuitive API for building models. For simple, layer-by-layer architectures, <code>tf.keras.Sequential</code> is perfect.</p>
<p>Our first model is remarkably simple:</p>
<ol type="1">
<li><strong>Input Layer</strong>: We explicitly define the input shape using <code>tf.keras.layers.Input</code>. Since we flattened our 28x28 images, the input to this layer is a 1D vector of 784 pixels.</li>
<li><strong>Dense Layer</strong>: This is the core of our model. A “Dense” layer (also known as a fully connected layer) means every neuron in this layer is connected to every neuron in the previous layer.
<ul>
<li>It has <code>10</code> neurons, one for each output class (digits 0 through 9).</li>
<li>The <code>activation='softmax'</code> is critical for classification tasks. Softmax converts the raw output of the neurons into a probability distribution, ensuring that all 10 output values are positive and sum up to 1. The class with the highest probability is our model’s prediction.</li>
</ul></li>
</ol>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="compiling-the-model" class="slide level2 scrollable">
<h2><strong>Compiling the Model</strong></h2>
<p>Before training, the model needs to be compiled. Compilation configures key aspects of the training process.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'sgd'</span>,</span>
<span id="cb8-2"><a></a>              loss<span class="op">=</span><span class="st">'categorical_crossentropy'</span>,</span>
<span id="cb8-3"><a></a>              metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><strong><code>optimizer='sgd'</code> (Stochastic Gradient Descent):</strong> The algorithm used to update the model’s weights based on the loss. <code>SGD</code> is a foundational optimizer for neural networks.</li>
<li><strong><code>loss='categorical_crossentropy'</code>:</strong> The loss function measures the discrepancy between predicted and true class probabilities. <code>Categorical crossentropy</code> is standard for multi-class classification when labels are one-hot encoded.</li>
<li><strong><code>metrics=['accuracy']</code>:</strong> Additional metrics to monitor during training and evaluation. <code>'accuracy'</code> measures the percentage of correct predictions.</li>
</ul>
<aside class="notes">
<p>Compiling the model is like setting up the engine and navigation system before going on a trip. It’s where we define how the model will learn.</p>
<ol type="1">
<li><p><strong>Optimizer:</strong> The optimizer is the algorithm that adjusts the weights of the neural network during training to minimize the loss function. ‘SGD’, or Stochastic Gradient Descent, is the simplest and a very common optimizer. It makes small, iterative adjustments to the weights based on the gradient of the loss function with respect to the weights.</p></li>
<li><p><strong>Loss Function:</strong> The loss function quantifies how ‘bad’ our model’s predictions are compared to the true labels. For multi-class classification problems like MNIST where each input belongs to exactly one class out of many, <code>categorical_crossentropy</code> is the standard choice. It penalizes incorrect predictions more heavily when the model was very confident in its wrong answer.</p></li>
<li><p><strong>Metrics:</strong> Metrics are used to monitor the training and validation process. While the loss function guides the optimization, metrics provide a more human-interpretable measure of performance. ‘Accuracy’ is intuitive: it’s the proportion of correctly classified examples.</p></li>
</ol>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="model-summary-training-utility" class="slide level2">
<h2><strong>Model Summary &amp; Training Utility</strong></h2>
<p>After compilation, we can inspect the model’s architecture.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong><code>model.summary()</code>:</strong></p>
<p>Prints a detailed overview of the model:</p>
<ul>
<li>Layers (type, output shape).</li>
<li>Number of trainable parameters in each layer.</li>
<li>Total parameters in the model.</li>
</ul>
<p>This is invaluable for debugging and understanding model complexity.</p>
<p><strong><code>PlotTraining</code> Callback:</strong></p>
<p>A custom utility (from the notebook) to visualize training curves dynamically. It shows loss and accuracy for both training and validation sets in real-time.</p>
</div><div class="column" style="width:50%;">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb9-2"><a></a><span class="co"># Define a simple model for demonstration</span></span>
<span id="cb9-3"><a></a>model_summary_demo <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb9-4"><a></a>    tf.keras.layers.Input(shape<span class="op">=</span>(<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>,)), <span class="co"># For MNIST, inputs are 784-element vectors</span></span>
<span id="cb9-5"><a></a>    tf.keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span>) <span class="co"># 10 output classes</span></span>
<span id="cb9-6"><a></a>])</span>
<span id="cb9-7"><a></a></span>
<span id="cb9-8"><a></a><span class="co"># Simulate compile for summary to show expected parameters</span></span>
<span id="cb9-9"><a></a>model_summary_demo.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'sgd'</span>, loss<span class="op">=</span><span class="st">'categorical_crossentropy'</span>, metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb9-10"><a></a></span>
<span id="cb9-11"><a></a><span class="co"># Print the model summary</span></span>
<span id="cb9-12"><a></a>model_summary_demo.summary()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div></div>
<aside class="notes">
<p>Before we start training, it’s good practice to inspect the model’s structure.</p>
<p><code>model.summary()</code> is an extremely useful Keras utility. It outputs a table showing each layer, its output shape, and the number of parameters it has.</p>
<ul>
<li><strong>Output Shape:</strong> This tells us the shape of the tensor outputted by each layer. For our Dense layer with 10 neurons, the output shape is <code>(None, 10)</code>, where <code>None</code> indicates the batch size, which can vary.</li>
<li><strong>Trainable parameters:</strong> For the Dense layer, this is <code>(input_features + 1) * output_neurons</code>. In our case <code>(784 + 1) * 10 = 7850</code>. The <code>+1</code> accounts for the bias term for each neuron.</li>
</ul>
<p>The <code>PlotTraining</code> callback, though custom, is typical of utilities used to provide visual feedback during training. It helps identify issues like overfitting early on.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="training-and-validation" class="slide level2">
<h2><strong>Training and Validation</strong></h2>
<p>The <code>model.fit()</code> function is where the actual learning takes place.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a></a>model.fit(training_dataset,</span>
<span id="cb10-2"><a></a>          steps_per_epoch<span class="op">=</span>steps_per_epoch,</span>
<span id="cb10-3"><a></a>          epochs<span class="op">=</span>EPOCHS,</span>
<span id="cb10-4"><a></a>          validation_data<span class="op">=</span>validation_dataset,</span>
<span id="cb10-5"><a></a>          validation_steps<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb10-6"><a></a>          callbacks<span class="op">=</span>[plot_training])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><strong><code>training_dataset</code></strong>: The preprocessed dataset used for learning.</li>
<li><strong><code>steps_per_epoch</code></strong>: Number of batches per epoch (total training samples / batch size).</li>
<li><strong><code>epochs</code></strong>: Total number of times the model iterates over the entire training data.</li>
<li><strong><code>validation_data</code></strong>: The unseen dataset used to evaluate performance after each epoch.</li>
<li><strong><code>validation_steps</code></strong>: Number of batches from <code>validation_data</code> to run per validation round.</li>
<li><strong><code>callbacks</code></strong>: List of custom functions executed at various stages of training (e.g., <code>plot_training</code>).</li>
</ul>
<aside class="notes">
<p>With the model defined and compiled, we’re ready for the most exciting part: training! This is done using the <code>model.fit()</code> method.</p>
<ul>
<li><code>training_dataset</code>: This is our <code>tf.data.Dataset</code> pipeline for the training data.</li>
<li><code>steps_per_epoch</code>: Since we used <code>dataset.repeat()</code>, the data pipeline could theoretically run forever. <code>steps_per_epoch</code> tells Keras how many <em>batches</em> constitute one “epoch” or one full pass through our conceptual dataset. It is typically calculated as <code>total_training_samples / BATCH_SIZE</code>.</li>
<li><code>epochs</code>: As defined in our parameters, this is the total number of epochs the model will train for.</li>
<li><code>validation_data</code>: Here, we pass our <code>tf.data.Dataset</code> pipeline for the validation data. Keras automatically evaluates the model on this data at the end of each epoch.</li>
<li><code>validation_steps</code>: Similar to <code>steps_per_epoch</code>, this specifies how many validation batches to process during each validation run.</li>
<li><code>callbacks</code>: This is a list where we can include custom functions that execute during training. Our <code>plot_training</code> callback is a great example, providing real-time visualization of metrics.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="visualizing-predictions" class="slide level2">
<h2><strong>Visualizing Predictions</strong></h2>
<p>After training, we use <code>model.predict()</code> to evaluate the model on new inputs.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a></a>probabilities <span class="op">=</span> model.predict(font_digits, steps<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb11-2"><a></a>predicted_labels <span class="op">=</span> np.argmax(probabilities, axis<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><strong><code>model.predict(input_data)</code>:</strong> Generates output predictions for the input samples. For a classification model with <code>softmax</code> activation, it returns a 2D array where each row is a probability distribution over the classes for one input. (e.g., <code>[[0.01, 0.05, ..., 0.90, ..., 0.02], ...]</code>)</li>
<li><strong><code>np.argmax(probabilities, axis=1)</code>:</strong> Converts the probability distributions into a single predicted class label.
<ul>
<li><code>np.argmax()</code>: Returns the index of the maximum value.</li>
<li><code>axis=1</code>: Specifies to find the maximum along the “class” dimension (i.e., for each image, find the class with the highest probability).</li>
</ul></li>
</ul>
</section>
<section id="visualizing-predictions-1" class="slide level2">
<h2><strong>Visualizing Predictions</strong></h2>

<img data-src="https://codelabs.developers.google.com/static/codelabs/cloud-tensorflow-mnist/img/396c54ef66fad27f_1440.png" class="r-stretch"><div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>This simple 1-layer model already achieves ~90% accuracy! But we can do much better.</p>
</div>
</div>
</div>
</section>
<section id="adding-layers-going-deeper" class="slide level2">
<h2><strong>Adding Layers: Going Deeper</strong></h2>
<p>To improve our model’s accuracy beyond 90%, we need to <strong>add more layers</strong>. This allows the network to learn more complex, hierarchical features.</p>
</section>
<section id="the-concept-of-depth" class="slide level2">
<h2><strong>The Concept of Depth</strong></h2>
<ul>
<li>A deeper network can model non-linear relationships more effectively.</li>
<li>Each hidden layer learns increasingly abstract representations of the input data.</li>
</ul>
<aside class="notes">
<p>Our single-layer neural network achieved about 90% accuracy, which is a good baseline, but not yet sufficient for many real-world ECE applications. To push this accuracy higher, the most intuitive next step is to make our neural network <code>deeper</code> by adding more layers.</p>
<p>Why go deeper? Deep networks are powerful because they can learn hierarchies of features. For image recognition, the first layer might learn simple edges or lines. A second layer might combine these to recognize shapes like circles or corners. A third layer might combine shapes to recognize parts of digits, and so on. This hierarchical learning allows deep networks to model very complex, non-linear relationships in the data, far beyond what a single layer can achieve.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="activation-functions-revisited" class="slide level2">
<h2><strong>Activation Functions Revisited</strong></h2>
<p>While <code>softmax</code> is for the output layer of a classifier, <strong>hidden layers</strong> need different activation functions.</p>
</section>
<section id="sigmoid-activation-function" class="slide level2">
<h2><strong>Sigmoid Activation Function</strong></h2>
<p>For intermediate (hidden) layers, the <code>sigmoid</code> function is a classical choice:</p>
<p><span class="math display">\[ \sigma(x) = \frac{1}{1 + e^{-x}} \]</span></p>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><strong>Output Range:</strong> Maps any input to a value between 0 and 1.</li>
<li><strong>Interpretation:</strong> Can be seen as a “soft” switch, where values close to 0 or 1 indicate strong decisions.</li>
<li><strong>Historical Significance:</strong> Widely used in early neural networks.</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="https://codelabs.developers.google.com/static/codelabs/cloud-tensorflow-mnist/img/41fc82288c4aff5d.png"></p>
<p><em>Plot of the Sigmoid Function</em></p>
</div></div>
<aside class="notes">
<p>When we add more layers, we introduce <code>hidden layers</code> between the input and output. Each neuron in these hidden layers needs an activation function to introduce non-linearity into the network. Without non-linearity, no matter how many layers you add, the neural network would only be able to learn linear relationships, limiting its power.</p>
<p>While <code>softmax</code> is perfect for the final classification layer, where we need probabilities for multiple classes, <code>sigmoid</code> is a classic choice for hidden layers. The sigmoid function squashes any input value into a range between 0 and 1. This can be interpreted as the “activation” strength of a neuron. Historically, sigmoid was one of the most popular activation functions, but we will soon see more effective alternatives for deeper networks.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="designing-a-deeper-model" class="slide level2">
<h2><strong>Designing a Deeper Model</strong></h2>
<p>Let’s expand our simple model by adding two hidden <code>Dense</code> layers with <code>sigmoid</code> activation.</p>
<div class="columns">
<div class="column" style="width:50%;">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a></a>model <span class="op">=</span> tf.keras.Sequential(</span>
<span id="cb12-2"><a></a>    [</span>
<span id="cb12-3"><a></a>        tf.keras.layers.Input(shape<span class="op">=</span>(<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>,)),        <span class="co"># Input Layer</span></span>
<span id="cb12-4"><a></a>        tf.keras.layers.Dense(<span class="dv">200</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>), <span class="co"># Hidden Layer 1</span></span>
<span id="cb12-5"><a></a>        tf.keras.layers.Dense(<span class="dv">60</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>),  <span class="co"># Hidden Layer 2</span></span>
<span id="cb12-6"><a></a>        tf.keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)   <span class="co"># Output Layer</span></span>
<span id="cb12-7"><a></a>    ]</span>
<span id="cb12-8"><a></a>)</span>
<span id="cb12-9"><a></a><span class="co"># (Compilation and training code follows)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><strong>Hidden Layer 1:</strong> 200 neurons with <code>sigmoid</code> activation.</li>
<li><strong>Hidden Layer 2:</strong> 60 neurons with <code>sigmoid</code> activation.</li>
<li><strong>Output Layer:</strong> Remains 10 neurons with <code>softmax</code> for classification.</li>
</ul>
</div><div class="column" style="width:50%;">
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<svg width="960" height="480" viewbox="0.00 0.00 635.72 49.60" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style="; max-width: none; max-height: none; display: block; margin: auto auto auto auto">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 45.6)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-45.6 631.72,-45.6 631.72,4 -4,4"></polygon>
<!-- Input -->
<g id="node1" class="node">
<title>Input</title>
<polygon fill="none" stroke="black" points="87.89,-41.4 0.04,-41.4 0.04,-0.2 87.89,-0.2 87.89,-41.4"></polygon>
<text text-anchor="middle" x="43.96" y="-25" font-family="Times,serif" font-size="14.00">Input Layer</text>
<text text-anchor="middle" x="43.96" y="-8.2" font-family="Times,serif" font-size="14.00">(Shape: 784)</text>
</g>
<!-- Dense1 -->
<g id="node2" class="node">
<title>Dense1</title>
<polygon fill="none" stroke="black" points="272.33,-41.4 124.13,-41.4 124.13,-0.2 272.33,-0.2 272.33,-41.4"></polygon>
<text text-anchor="middle" x="198.23" y="-25" font-family="Times,serif" font-size="14.00">Dense Layer 1</text>
<text text-anchor="middle" x="198.23" y="-8.2" font-family="Times,serif" font-size="14.00">(200 neurons, Sigmoid)</text>
</g>
<!-- Input&#45;&gt;Dense1 -->
<g id="edge1" class="edge">
<title>Input-&gt;Dense1</title>
<path fill="none" stroke="black" d="M88.19,-20.8C96.3,-20.8 105.03,-20.8 113.89,-20.8"></path>
<polygon fill="black" stroke="black" points="113.93,-24.3 123.93,-20.8 113.93,-17.3 113.93,-24.3"></polygon>
</g>
<!-- Dense2 -->
<g id="node3" class="node">
<title>Dense2</title>
<polygon fill="none" stroke="black" points="449.94,-41.4 308.73,-41.4 308.73,-0.2 449.94,-0.2 449.94,-41.4"></polygon>
<text text-anchor="middle" x="379.33" y="-25" font-family="Times,serif" font-size="14.00">Dense Layer 2</text>
<text text-anchor="middle" x="379.33" y="-8.2" font-family="Times,serif" font-size="14.00">(60 neurons, Sigmoid)</text>
</g>
<!-- Dense1&#45;&gt;Dense2 -->
<g id="edge2" class="edge">
<title>Dense1-&gt;Dense2</title>
<path fill="none" stroke="black" d="M272.34,-20.8C280.96,-20.8 289.79,-20.8 298.5,-20.8"></path>
<polygon fill="black" stroke="black" points="298.64,-24.3 308.64,-20.8 298.64,-17.3 298.64,-24.3"></polygon>
</g>
<!-- Output -->
<g id="node4" class="node">
<title>Output</title>
<polygon fill="none" stroke="black" points="627.52,-41.4 486.34,-41.4 486.34,-0.2 627.52,-0.2 627.52,-41.4"></polygon>
<text text-anchor="middle" x="556.93" y="-25" font-family="Times,serif" font-size="14.00">Output Layer</text>
<text text-anchor="middle" x="556.93" y="-8.2" font-family="Times,serif" font-size="14.00">(10 neurons, Softmax)</text>
</g>
<!-- Dense2&#45;&gt;Output -->
<g id="edge3" class="edge">
<title>Dense2-&gt;Output</title>
<path fill="none" stroke="black" d="M449.99,-20.8C458.52,-20.8 467.29,-20.8 475.96,-20.8"></path>
<polygon fill="black" stroke="black" points="476.07,-24.3 486.07,-20.8 476.07,-17.3 476.07,-24.3"></polygon>
</g>
</g>
</svg>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div></div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Notice the increase in the number of parameters with multiple layers. Run this model in the Colab notebook.</p>
</div>
</div>
</div>
<aside class="notes">
<p>Here’s how we’d implement a deeper network in Keras. We simply add more <code>tf.keras.layers.Dense</code> components to our <code>Sequential</code> model.</p>
<ul>
<li>We start with our <code>Input</code> layer, still expecting the flattened 784-pixel vector.</li>
<li>Then we add <code>tf.keras.layers.Dense(200, activation='sigmoid')</code>. This layer has 200 neurons, processing the 784 inputs and producing 200 outputs. The sigmoid activation squashes these outputs between 0 and 1.</li>
<li>Next, another hidden layer: <code>tf.keras.layers.Dense(60, activation='sigmoid')</code>. This layer takes the 200 outputs from the previous layer as its input, processes them with 60 neurons, and outputs 60 values.</li>
<li>Finally, our output layer remains the same: <code>tf.keras.layers.Dense(10, activation='softmax')</code>, producing the 10 probabilities for digit classification.</li>
</ul>
<p>By running <code>model.summary()</code> again, you’ll see a significant increase in the total number of trainable parameters. This increased capacity should, in theory, allow the a model to learn more intricate patterns.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="unexpected-behavior-what-happened" class="slide level2">
<h2><strong>Unexpected Behavior: What Happened?</strong></h2>
<p>Despite adding layers and parameters, the model doesn’t always improve as expected.</p>

<img data-src="https://codelabs.developers.google.com/static/codelabs/cloud-tensorflow-mnist/img/5236f91ba6e07d85.png" class="r-stretch"><ul>
<li><strong>High Loss:</strong> The training loss and validation loss are extremely high.</li>
<li><strong>Low Accuracy:</strong> Accuracy barely increases above random guessing (around 10%).</li>
</ul>
<div class="callout callout-warning callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Warning</strong></p>
</div>
<div class="callout-content">
<p>More parameters don’t automatically mean better performance. Deeper networks introduce new challenges!</p>
</div>
</div>
</div>
<aside class="notes">
<p>After running the deeper model in the Colab notebook, you might observe a surprising and discouraging result: the model performs <em>worse</em> than our single-layer network. The training and validation loss remain prohibitively high, and the accuracy barely moves beyond 10%, which is what you’d expect from random guessing among 10 classes.</p>
<p>This outcome clearly illustrates an important point in deep learning: simply adding more layers and parameters isn’t a silver bullet. Deeper networks come with their own set of challenges. This particular issue with sigmoid activations in deep networks is a classic problem known as the <strong>vanishing gradient problem</strong>, which we will discuss next. The sigmoid function, especially for inputs far from zero, has very small gradients. In a deep network, these small gradients are multiplied together as they propagate backward through the layers, causing them to “vanish” by the time they reach earlier layers. This prevents the weights in those earlier layers from being updated effectively, stalling learning.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="why-did-the-deeper-model-fail-the-vanishing-gradient-problem" class="slide level2">
<h2><strong>Why Did the Deeper Model Fail? The Vanishing Gradient Problem</strong></h2>
<p>The <code>sigmoid</code> activation function can hinder learning in deep networks.</p>
<p><strong>The Problem:</strong></p>
<ul>
<li>The gradient (derivative) of the sigmoid function is very small for inputs far from 0.</li>
<li>In a deep network, these small gradients are multiplied together during backpropagation.</li>
<li>This causes gradients to <strong>“vanish”</strong> as they propagate back to earlier layers.</li>
</ul>
<p><strong>Consequence for ECE:</strong></p>
<ul>
<li>Early layers’ weights are hardly updated.</li>
<li>The network struggles to learn useful features from the input.</li>
<li>Training stalls, leading to poor performance.</li>
</ul>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>This is a common issue with traditional activation functions like sigmoid and tanh in deep architectures.</p>
</div>
</div>
</div>
</section>
<section id="special-care-for-deep-networks" class="slide level2">
<h2><strong>Special Care for Deep Networks</strong></h2>
<p>The “AI winter” of the 80s and 90s was partly due to the challenges of training deep networks. Modern deep learning thrives due to “dirty tricks” that ensure convergence.</p>
</section>
<section id="overcoming-deep-network-challenges" class="slide level2">
<h2><strong>Overcoming Deep Network Challenges</strong></h2>
<ul>
<li><strong>RELU Activation:</strong> A simple yet powerful non-linearity.</li>
<li><strong>Better Optimizers:</strong> Algorithms that navigate complex loss landscapes.</li>
<li><strong>Careful Initialization:</strong> Setting initial weights to facilitate learning.</li>
<li><strong>Numerical Stability:</strong> Ensuring calculations don’t crash.</li>
</ul>
<aside class="notes">
<p>As we saw, simply adding more layers to a network with sigmoid activations often leads to poor performance, a problem that significantly contributed to the “AI winter.” However, thanks to a few crucial mathematical and algorithmic advancements, deep neural networks, even with 20, 50, or hundreds of layers, can now be trained effectively. These “dirty tricks,” as the source calls them, address the fundamental challenges of deep network convergence and have been instrumental in the recent resurgence of deep learning. We’ll explore these techniques now.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="relu-activation-the-modern-choice" class="slide level2">
<h2><strong>RELU Activation: The Modern Choice</strong></h2>
<p>The <code>sigmoid</code> function’s vanishing gradients made it problematic for deep networks. The <strong>Rectified Linear Unit (RELU)</strong> is the de-facto standard activation today.</p>
<p><span class="math display">\[ \text{ReLU}(x) = \max(0, x) \]</span></p>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><strong>Simplicity:</strong> Returns <code>x</code> for positive inputs, <code>0</code> for negative inputs.</li>
<li><strong>Gradient:</strong> Has a constant gradient of <code>1</code> for positive inputs.</li>
<li><strong>Benefits for ECE:</strong>
<ul>
<li>Mitigates vanishing gradient problem.</li>
<li>Speeds up convergence.</li>
<li>Computationally much cheaper than sigmoid/tanh.</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="https://codelabs.developers.google.com/static/codelabs/cloud-tensorflow-mnist/img/1abce89f7143a69c.png"></p>
<p><em>Plot of the ReLU Function</em></p>
</div></div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Replace <code>activation='sigmoid'</code> with <code>activation='relu'</code> in hidden layers. The output layer retains <code>softmax</code> for classification.</p>
</div>
</div>
</div>
<aside class="notes">
<p>The <code>sigmoid</code> activation function was historically significant, but its propensity to squash values between 0 and 1 resulted in very small gradients, leading to the vanishing gradient problem in deep networks. The solution that revolutionized deep learning is surprisingly simple: the <code>Rectified Linear Unit</code>, or <code>ReLU</code>.</p>
<p><code>ReLU(x)</code> simply outputs <code>x</code> if <code>x</code> is positive, and <code>0</code> if <code>x</code> is negative.</p>
<ul>
<li><strong>Simplicity:</strong> This piece-wise linear function is incredibly simple to compute.</li>
<li><strong>Gradient:</strong> Crucially, its gradient is <code>1</code> for positive inputs and <code>0</code> for negative inputs. This <code>1</code> gradient for a large portion of its domain means that gradients can flow much more effectively through the network without vanishing, thereby addressing the core problem. This allows deeper networks to learn much faster and more effectively.</li>
<li><strong>Computational Efficiency:</strong> ReLU is also much faster to compute than exponentials (in sigmoid) or hyperbolic tangents, which speeds up training significantly.</li>
</ul>
<p>For our ECE applications where efficiency and convergence are critical, ReLU is the go-to choice for hidden layers. Remember to keep <code>softmax</code> for the final output layer in classification tasks.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="better-optimizers-beyond-sgd" class="slide level2">
<h2><strong>Better Optimizers: Beyond SGD</strong></h2>
<p>Stochastic Gradient Descent (SGD) can get stuck in “saddle points” in high-dimensional spaces.</p>
<p>Modern optimizers are more robust and efficient.</p>
<p><strong>Saddle Points:</strong></p>
<ul>
<li>Points in the loss landscape where the gradient is zero, but it’s not a true minimum.</li>
<li>SGD can get stuck here, preventing further learning.</li>
</ul>
<p><strong>Adaptive Optimizers:</strong></p>
<ul>
<li>Use concepts like “momentum” and “adaptive learning rates” for each parameter.</li>
<li>Help the model “sail past” saddle points and converge faster.</li>
<li>Examples: <code>Adam</code>, <code>RMSprop</code>, <code>Adagrad</code>.</li>
</ul>
<p><strong>Keras Implementation:</strong></p>
<p>Update the <code>optimizer</code> in <code>model.compile</code>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'adam'</span>, <span class="co"># Use Adam optimizer</span></span>
<span id="cb13-2"><a></a>              loss<span class="op">=</span><span class="st">'categorical_crossentropy'</span>,</span>
<span id="cb13-3"><a></a>              metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p><code>Adam</code> is widely considered a good default choice for most deep learning tasks.</p>
</div>
</div>
</div>
<aside class="notes">
<p>While SGD is foundational, its simplicity can be a drawback in complex, high-dimensional loss landscapes, which are typical for deep neural networks. One significant issue is <code>saddle points</code>. Imagine a mountain pass where the terrain flattens out. The gradient at this point is zero, just like at a local minimum. SGD, looking only at the immediate gradient, would mistakenly think it’s reached a minimum and stop, even if there are lower points to explore.</p>
<p>Modern optimizers like <code>Adam</code>, <code>RMSprop</code>, and <code>Adagrad</code> address this by incorporating more sophisticated mechanics. They often include:</p>
<ul>
<li><strong>Momentum:</strong> This helps the optimizer build up speed in a consistent direction and <code>overshoot</code> small local minima or saddle points, like a ball rolling down a hill that doesn’t just stop at the first dip.</li>
<li><strong>Adaptive Learning Rates:</strong> Instead of using a single learning rate for all parameters, these optimizers maintain a separate learning rate for each network weight, adapting it based on the historical gradients. This allows for faster learning in some dimensions and more cautious steps in others.</li>
</ul>
<p>By simply changing our optimizer from <code>'sgd'</code> to <code>'adam'</code> in Keras, we leverage these advanced techniques immediately, leading to faster and more reliable convergence.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="weight-initialization-numerical-stability" class="slide level2 scrollable">
<h2><strong>Weight Initialization &amp; Numerical Stability</strong></h2>
<p>Two critical, often hidden, factors for stable deep network training.</p>
<p><strong>1. Random Initializations:</strong> - How the network’s weights and biases are set before training begins. - Poor initialization can lead to slow convergence or vanishing/exploding gradients. - <strong>Keras Default:</strong> Uses <code>'glorot_uniform'</code> (also known as Xavier uniform). - Designed to keep activation values and gradients roughly in the same scale across layers. - <strong>No action needed:</strong> Keras handles this optimally by default.</p>
<p><strong>2. Numerical Stability (NaNs):</strong> - <code>Categorical crossentropy</code> involves <code>log()</code>. If input to log is <code>0</code>, it’s <code>NaN</code> (Not a Number). - <code>softmax</code> output (probabilities) can be numerically <code>0</code> in <code>float32</code> despite being mathematically non-zero. - <strong>Keras Solution:</strong> <code>tf.keras.losses.CategoricalCrossentropy(from_logits=True)</code> - Computes <code>softmax</code> and <code>crossentropy</code> together in a numerically stable way. - <strong>No action needed:</strong> Keras handles this automatically when <code>softmax</code> is the last activation and <code>categorical_crossentropy</code> is the loss.</p>
<aside class="notes">
<p>Beyond activations and optimizers, two more subtle but profoundly important aspects of deep learning convergence are <strong>weight initialization</strong> and <strong>numerical stability</strong>.</p>
<ol type="1">
<li><p><strong>Weight Initialization:</strong> The starting values of a neural network’s weights and biases are surprisingly critical. If they are too large, activations might saturate; if too small, gradients might vanish. The goal is to initialize weights such that the activations and gradients flowing through the network remain in a reasonable range. Keras, by default, uses clever initialization schemes like <code>'glorot_uniform'</code> (also known as Xavier uniform initialization), which attempts to balance the variance of activations and gradients across layers. The good news for us is that Keras usually “does the right thing” here, so we often don’t need to specify it manually.</p></li>
<li><p><strong>Numerical Stability (NaNs):</strong> Our <code>categorical_crossentropy</code> loss function involves a logarithm. Mathematically, the <code>softmax</code> activation function produces probabilities that are strictly greater than zero (since it uses exponentials). However, in the finite precision world of <code>float32</code> computer arithmetic, a very small positive number can be represented as <code>0</code>. If <code>log(0)</code> occurs, the result is <code>NaN</code> (Not a Number), which crashes the training process. Keras smartly handles this by offering a version of <code>categorical_crossentropy</code> that optimally combines the <code>softmax</code> calculation with the cross-entropy loss in a numerically stable way, often internally using a <code>from_logits=True</code> argument when you specify <code>'softmax'</code> and <code>'categorical_crossentropy'</code>. This prevents those dreaded <code>NaN</code>s. Again, Keras typically manages this under the hood, so usually, no explicit action is required from us.</p></li>
</ol>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="success-so-far-97-accuracy" class="slide level2">
<h2><strong>Success So Far: ~97% Accuracy!</strong></h2>
<p>With ReLU activation and the Adam optimizer, our deeper model should now converge effectively.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p>You should observe: - Training and validation loss decreasing steadily. - Training and validation accuracy climbing to around <strong>97%</strong>.</p>
<p>This marks a significant improvement over the initial 90% and the failed deep <code>sigmoid</code> model.</p>
<p>We’re approaching our goal of “significantly above 99% accuracy!”</p>
</div><div class="column" style="width:50%;">
<p><img data-src="https://codelabs.developers.google.com/static/codelabs/cloud-tensorflow-mnist/img/e1521c9dd936d9bc.png"> <em>(Example of ~97% accuracy training curves)</em></p>
</div></div>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p>If you’re stuck, refer to <code>keras_02_mnist_dense.ipynb</code> in the Colab repo.</p>
</div>
</div>
</div>
<aside class="notes">
<p>After implementing <code>ReLU</code> activations in the hidden layers and switching to the <code>Adam</code> optimizer, you should see a dramatic improvement in your model’s performance. The training and validation curves for both loss and accuracy will now look much healthier, steadily converging. We’re now hitting around 97% accuracy, which is a major leap from our initial 90% and the failed sigmoid model. This demonstrates the critical importance of these “dirty tricks” in making deep networks viable. We’re definitely on the right track towards our ultimate goal!</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="learning-rate-decay-fine-tuning-convergence" class="slide level2">
<h2><strong>Learning Rate Decay: Fine-Tuning Convergence</strong></h2>
<p>Training too fast can lead to noisy convergence or even divergence. A <strong>learning rate decay schedule</strong> starts fast and slows down over time.</p>
<p><strong>The Problem with High Learning Rates:</strong> - Training curves become noisy. - Validation metrics jump erratically (<code>d4fd66346d7c480e.png</code>). - Model might jump over optimal solutions or oscillate.</p>
<p><strong>The Solution:</strong> - Start with a higher learning rate to explore the loss landscape quickly. - Gradually decrease the learning rate as training progresses, allowing for finer adjustments and more stable convergence. - Often exponential decay: <span class="math inline">\(LR = LR_0 \times \text{decay_rate}^{\text{epoch}}\)</span></p>
<aside class="notes">
<p>Even with a powerful optimizer like Adam, simply using a fixed learning rate throughout training isn’t always optimal. Imagine searching for the lowest point in a valley. You might want to take large strides initially to cover ground quickly, but as you approach the bottom, you’d need smaller, more precise steps to find the exact lowest point without overshooting it.</p>
<p>In deep learning, the <code>learning rate</code> controls the step size of our optimizer. A very high learning rate can cause the model to bounce around the loss landscape without converging or even diverge. A very low learning rate can make training painfully slow.</p>
<p>The solution is <code>learning rate decay</code>. We start with a relatively high learning rate to make quick progress early on, and then gradually decrease it over epochs. This allows the model to fine-tune its weights as it gets closer to an optimal solution. Exponential decay is a common and effective schedule, where the learning rate shrinks by a constant factor after each epoch.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="implementing-learning-rate-decay-with-keras" class="slide level2 scrollable">
<h2><strong>Implementing Learning Rate Decay with Keras</strong></h2>
<p>Keras makes it easy to add a learning rate scheduler using a <strong>callback</strong>.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>1. Define the Decay Function:</strong> A Python function that calculates the learning rate for a given epoch.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a></a><span class="im">import</span> math</span>
<span id="cb14-2"><a></a><span class="kw">def</span> lr_decay(epoch):</span>
<span id="cb14-3"><a></a>    <span class="cf">return</span> <span class="fl">0.01</span> <span class="op">*</span> math.<span class="bu">pow</span>(<span class="fl">0.6</span>, epoch) <span class="co"># Exponential decay</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This function starts at <code>0.01</code> and reduces it by <code>0.6</code> (60%) each epoch.</p>
<p><strong>2. Create the Callback:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a></a>lr_decay_callback <span class="op">=</span> tf.keras.callbacks.LearningRateScheduler(lr_decay, verbose<span class="op">=</span><span class="va">True</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><code>verbose=True</code> prints the learning rate at the start of each epoch.</p>
<p><strong>3. Add to <code>model.fit()</code>:</strong></p>
<p>Include the <code>lr_decay_callback</code> in the list of <code>callbacks</code>.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a></a>model.fit(..., callbacks<span class="op">=</span>[plot_training, lr_decay_callback])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div><div class="column" style="width:50%;">
<div>
<div id="pyodide-3" class="exercise-cell">

</div>
<script type="pyodide-3-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWV9LCJjb2RlIjoiIyBDYWxsYmFjayB0byB2aXN1YWxpemUgbGVhcm5pbmcgcmF0ZSBzY2hlZHVsZVxuaW1wb3J0IHBsb3RseS5ncmFwaF9vYmplY3RzIGFzIGdvXG5pbXBvcnQgbWF0aFxuXG5kZWYgcGxvdF9sZWFybmluZ19yYXRlKGxyX3NjaGVkdWxlX2Z1bmMsIGVwb2Nocyk6XG4gICAgbHJzID0gW2xyX3NjaGVkdWxlX2Z1bmMoZSkgZm9yIGUgaW4gcmFuZ2UoZXBvY2hzKV1cbiAgICBmaWcgPSBnby5GaWd1cmUoKVxuICAgIGZpZy5hZGRfdHJhY2UoZ28uU2NhdHRlcih4PWxpc3QocmFuZ2UoZXBvY2hzKSksIHk9bHJzLCBtb2RlPSdsaW5lcycsIG5hbWU9J0xlYXJuaW5nIFJhdGUnKSlcbiAgICBmaWcudXBkYXRlX2xheW91dCh0aXRsZT0nTGVhcm5pbmcgUmF0ZSBTY2hlZHVsZSBPdmVyIEVwb2NocycsXG4gICAgICAgICAgICAgICAgICAgICAgeGF4aXNfdGl0bGU9J0Vwb2NoJyxcbiAgICAgICAgICAgICAgICAgICAgICB5YXhpc190aXRsZT0nTGVhcm5pbmcgUmF0ZScsXG4gICAgICAgICAgICAgICAgICAgICAgd2lkdGg9NDAwLFxuICAgICAgICAgICAgICAgICAgICAgIGhlaWdodD0zMDAsXG4gICAgICAgICAgICAgICAgICAgICAgbWFyZ2luPWRpY3QobD0wLCByPTAsIGI9MCwgdD01MCkpXG4gICAgcmV0dXJuIGZpZ1xuXG4jIERlZmluZSBhIHNhbXBsZSBscl9kZWNheSBmdW5jdGlvblxuZGVmIGxyX2RlY2F5X2V4YW1wbGUoZXBvY2gpOlxuICAgIHJldHVybiAwLjAxICogbWF0aC5wb3coMC42LCBlcG9jaClcblxuIyBQbG90IHRoZSBleGFtcGxlXG5wbG90X2xlYXJuaW5nX3JhdGUobHJfZGVjYXlfZXhhbXBsZSwgMTApIn0=
</script>
</div>
</div></div>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>The <code>lr_decay_callback</code> must be added to the <code>callbacks</code> list for it to take effect.</p>
</div>
</div>
</div>
<aside class="notes">
<p>Keras provides a flexible way to implement custom learning rate schedules using the <code>tf.keras.callbacks.LearningRateScheduler</code>.</p>
<ol type="1">
<li><strong>Define the Decay Function:</strong> First, you define a Python function (like <code>lr_decay</code> here) that takes the current <code>epoch</code> number as input and returns the desired learning rate for that epoch. Our example uses an exponential decay, starting at <code>0.01</code> and multiplying by <code>0.6</code> (a 40% reduction) at each subsequent epoch.</li>
<li><strong>Create the Callback:</strong> You then instantiate <code>tf.keras.callbacks.LearningRateScheduler</code>, passing your decay function to it. Setting <code>verbose=True</code> is helpful for debugging, as it prints the calculated learning rate at the beginning of each epoch.</li>
<li><strong>Add to <code>model.fit()</code>:</strong> The final step is to include this <code>lr_decay_callback</code> in the <code>callbacks</code> list when you call <code>model.fit()</code>. Keras will then automatically call your function at the start of each epoch and update the optimizer’s learning rate accordingly.</li>
</ol>
<p>The interactive plot generated by the <code>plot_learning_rate</code> utility demonstrates how the learning rate will decrease over the epochs according to our defined function. This visual confirms our strategy.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="impact-of-learning-rate-decay-cleaner-convergence" class="slide level2">
<h2><strong>Impact of Learning Rate Decay: Cleaner Convergence</strong></h2>
<p>The effect of learning rate decay is often dramatic.</p>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><strong>Noise Reduction:</strong> Training curves become much smoother.</li>
<li><strong>Stable Validation:</strong> Validation accuracy and loss fluctuate less, showing cleaner convergence.</li>
<li><strong>Improved Accuracy:</strong> Sustained test accuracy can now be observed above <strong>98%</strong>.</li>
</ul>
<p>This technique is crucial for pushing models to higher performance ceilings.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="https://codelabs.developers.google.com/static/codelabs/cloud-tensorflow-mnist/img/8c1ae90976c4a0c1.png"></p>
<p><em>(Training curves with learning rate decay)</em></p>
</div></div>
<aside class="notes">
<p>The impact of implementing learning rate decay is usually quite significant and immediately visible in the training plots. You’ll notice that the training and validation curves, which might have been noisy and erratic before, become much smoother. This indicates that the model is making more stable and directed progress towards minimizing the loss.</p>
<p>More importantly, the validation accuracy, which is our true measure of generalization, becomes <code>more stable</code> and can push higher, often achieving levels above <code>98%</code> in a consistent manner. This clearly shows that learning rate decay is not just a tweak but a fundamental technique for achieving robust and high-performing deep learning models. It helps the optimizer fine-tune the weights effectively as the model approaches optimal solutions.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="overfitting-and-dropout" class="slide level2">
<h2><strong>7. Overfitting and Dropout</strong></h2>
<p>Even with a deeper network, ReLU, Adam, and learning rate decay, we often hit a wall around 98% accuracy. This is frequently due to <strong>overfitting</strong>.</p>
</section>
<section id="the-signs-of-overfitting" class="slide level2">
<h2><strong>The Signs of Overfitting</strong></h2>
<ul>
<li><strong>Validation loss increases:</strong> While training loss continues to decrease.</li>
<li><strong>Validation accuracy plateaus or drops:</strong> While training accuracy keeps improving.</li>
</ul>
<p>This means the model is learning details specific to the training data, but failing to generalize to new, unseen data (like our validation set).</p>

<img data-src="https://codelabs.developers.google.com/static/codelabs/cloud-tensorflow-mnist/img/e36c09a3088104c6.png" class="r-stretch"><p><em>Example: Validation loss rising while training loss drops</em></p>
<aside class="notes">
<p>We’ve made significant progress, reaching around 97-98% accuracy. However, pushing beyond this often reveals a common problem in machine learning: <code>overfitting</code>. Overfitting occurs when a model learns the training data <em>too well</em>, including its noise and specific quirks, but fails to capture the underlying patterns that generalize to new, unseen data.</p>
<p>The tell-tale signs of overfitting, as seen in the provided image, are when the <code>training loss</code> continues to decrease, indicating the model is still learning on the training set, but the <code>validation loss</code> either plateaus or, more critically, starts to increase. Similarly, <code>validation accuracy</code> might stop improving even as <code>training accuracy</code> climbs higher. This divergence signifies that the model is no longer effectively learning features that help it generalize.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="dropout-a-regularization-technique" class="slide level2">
<h2><strong>Dropout: A Regularization Technique</strong></h2>
<p><strong>Dropout</strong> is a powerful and widely used technique to combat overfitting.</p>
<p><strong>How it Works:</strong></p>
<ul>
<li>During <em>each training iteration</em>, a random subset of neurons (and their connections) are temporarily “dropped out” (set to zero).</li>
<li>This means the network <code>cannot rely on any single neuron</code> to be present.</li>
<li>It forces the network to learn more robust and redundant features.</li>
</ul>
<p><strong>Analogy:</strong></p>
<p>Like training <strong>multiple smaller, “thinner” networks</strong> simultaneously, combined into one.</p>
<p><strong>Keras Implementation:</strong></p>
<p>Add <code>tf.keras.layers.Dropout</code> to your model:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a></a>model <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb17-2"><a></a>    <span class="co"># ... previous layers</span></span>
<span id="cb17-3"><a></a>    tf.keras.layers.Dense(<span class="dv">200</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb17-4"><a></a>    tf.keras.layers.Dropout(<span class="fl">0.2</span>), <span class="co"># Dropout layer</span></span>
<span id="cb17-5"><a></a>    tf.keras.layers.Dense(<span class="dv">60</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb17-6"><a></a>    tf.keras.layers.Dropout(<span class="fl">0.2</span>), <span class="co"># Another Dropout layer</span></span>
<span id="cb17-7"><a></a>    tf.keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)</span>
<span id="cb17-8"><a></a>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>The <code>0.2</code> indicates dropping out 20% of neurons.</p>
<aside class="notes">
<p>When confronted with overfitting, one of the first and most effective regularization techniques to try is <code>Dropout</code>.</p>
<p>Here’s how it works: During each training step, for every hidden layer that has a dropout layer, a randomly selected percentage of the neurons (along with their incoming and outgoing connections) are temporarily ignored – effectively “dropped out” – for that specific forward and backward pass. For example, a dropout rate of 0.2 means 20% of the neurons are randomly deactivated.</p>
<p>The critical insight here is that the network can no longer rely on any single neuron or specific configuration of neurons to be active. This forces the network to learn <code>more robust features</code> and <code>less interdependent representations</code>. It’s like having multiple experts (neurons) for a task, but during training, you randomly remove some experts, forcing the remaining ones to learn to pick up the slack, making the entire team more resilient.</p>
<p>In Keras, you simply add a <code>tf.keras.layers.Dropout</code> layer after a dense layer. The argument to <code>Dropout</code> is the fraction of neurons to drop. Common values are between 0.1 and 0.5. Note that dropout is <code>only applied during training</code>; during inference (when making predictions), all neurons are active, but their weights are scaled down appropriately to account for the dropout rate used during training.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="dropouts-initial-impact-a-complex-picture" class="slide level2">
<h2><strong>Dropout’s Initial Impact: A Complex Picture</strong></h2>
<p>Applying dropout can lead to a mixed initial response.</p>

<img data-src="https://codelabs.developers.google.com/static/codelabs/cloud-tensorflow-mnist/img/43fd33801264743f.png" class="r-stretch"><p><em>(Training curves with Dropout)</em></p>
<ul>
<li><strong>Increased Noise:</strong> The training curves show more fluctuation due to the random dropping of neurons.</li>
<li><strong>Higher Overall Loss:</strong> Both training and validation loss might be higher than without dropout.</li>
<li><strong>Slight Accuracy Drop:</strong> Validation accuracy might initially decrease.</li>
</ul>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>This doesn’t mean dropout failed; it indicates the model is being forced to learn differently.</p>
<p>We are pushing it to generalize better, not just memorize.</p>
</div>
</div>
</div>
<aside class="notes">
<p>When you first apply dropout and observe the training curves, the results might seem counterintuitive or even disappointing. You’ll likely see:</p>
<ul>
<li><strong>Increased Noise:</strong> The curves will become noisier because of the inherent randomness introduced by dropping neurons at each step.</li>
<li><strong>Higher Overall Loss:</strong> Both training and validation loss might be higher than before. This is because we are intentionally hindering the network’s ability to perfectly fit the training data, forcing it to generalize.</li>
<li><strong>Slight Accuracy Drop:</strong> Validation accuracy might even dip slightly.</li>
</ul>
<p>It’s important not to conclude that dropout has “failed” at this point. These initial observations are actually consistent with dropout’s intended effect: it’s making the network’s learning process <em>harder</em> in a structured way, forcing it to discover more resilient and generalizable features. The goal with dropout isn’t necessarily to immediately boost accuracy, but to prevent the validation loss from creeping up and ultimately achieve better generalization on truly unseen data, even if it means slightly lower peak training performance. The long-term benefits typically outweigh the short-term perceived drawbacks.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="deeper-roots-of-overfitting-the-nature-of-the-problem" class="slide level2 scrollable">
<h2><strong>Deeper Roots of Overfitting: The Nature of the Problem</strong></h2>
<p>Overfitting isn’t always easily fixed by dropout alone; it stems from fundamental issues.</p>
<p><strong>1. “Too Many Degrees of Freedom”:</strong></p>
<ul>
<li>If a network is too large for the complexity of the data, it can simply “memorize” training examples.</li>
<li>It fails to extract underlying patterns, resulting in poor generalization.</li>
</ul>
<p><strong>Analogy for ECE:</strong></p>
<ul>
<li>Imagine fitting a 10th-order polynomial to only three data points. It will perfectly hit those points but be wild everywhere else.</li>
</ul>
<p><strong>2. Insufficient Training Data:</strong></p>
<ul>
<li>Neural networks are data-hungry.</li>
<li>With too little data, even a reasonably sized network can overfit because there isn’t enough variety to learn robust patterns.</li>
</ul>
<p><strong>3. Inadequate Network Architecture:</strong></p>
<ul>
<li>Sometimes, the chosen network type isn’t suitable for the data’s structure.</li>
<li>Our current <code>Dense</code> (fully-connected) only network struggles with image <strong>spatial relationships</strong>.</li>
</ul>
<aside class="notes">
<p>While dropout is a great tool, it’s essential to understand that overfitting has deeper, more fundamental causes. Simply applying dropout might not fully solve the problem if these underlying issues are present.</p>
<ol type="1">
<li><p><strong>Too Many Degrees of Freedom:</strong> This happens when your neural network is disproportionately complex for the problem it’s trying to solve. If a network has an excessive number of neurons and parameters, it gains the capacity to essentially <code>memorize</code> the training data points rather than learning the generalized rules that govern them. It’s like fitting a very high-degree polynomial to a small number of data points; it will pass through all of them perfectly (low training loss) but will be wildly inaccurate for any new point. A well-designed network needs a kind of constraint that forces it to extract meaningful, generalizable features.</p></li>
<li><p><strong>Insufficient Training Data:</strong> Deep neural networks are notoriously data-hungry. If you don’t have enough diverse training examples, even a moderately sized network can easily overfit. It simply doesn’t have enough varied information to learn robust patterns that apply broadly. This is a common bottleneck in deploying machine learning in many specialized ECE domains where data acquisition can be costly or difficult.</p></li>
<li><p><strong>Inadequate Network Architecture:</strong> This is a crucial point for our current MNIST task. Recall that we flattened our 28x28 images into a 784-element vector. In doing so, we completely discarded all <code>spatial information</code> – the fact that pixels are arranged in a grid and that neighboring pixels are highly correlated. Our dense network treats every pixel as an independent feature. Handwritten digits, however, are fundamentally made of <code>shapes</code>, <code>edges</code>, and <code>patterns</code> that depend on the spatial arrangement of pixels. A dense network has to “reinvent” this spatial understanding from scratch, which is inefficient and often leads to the performance ceiling we’ve hit.</p></li>
</ol>
<p>This inadequacy of our current architecture points us towards the next major advancement for image data: <code>Convolutional Neural Networks (CNNs)</code>.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="introduction-to-convolutional-neural-networks-cnns" class="slide level2">
<h2><strong>Introduction to Convolutional Neural Networks (CNNs)</strong></h2>
<p>Our current model struggles because it treats image pixels as independent features, losing spatial context. <strong>Convolutional Neural Networks (CNNs)</strong> are designed to leverage this spatial information.</p>
<p><strong>Key Idea:</strong></p>
<ul>
<li>Instead of fully-connected layers, CNNs use <strong>convolutional filters (kernels)</strong>.</li>
<li>These filters slide across the input image, detecting local features like edges, corners, and textures.</li>
<li>They preserve the spatial relationships between pixels.</li>
</ul>
<p><strong>Benefits for ECE (Image Processing):</strong></p>
<ul>
<li><strong>Feature Hierarchy:</strong> Learn increasingly complex features (edges -&gt; shapes -&gt; object parts).</li>
<li><strong>Parameter Sharing:</strong> Detect the same feature anywhere in the image with the same filter.</li>
<li><strong>Translation Invariance:</strong> Robust to slight shifts in object position.</li>
<li>Crucial for applications like object detection, medical imaging, and autonomous systems.</li>
</ul>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>We’ve hit a performance ceiling with our dense network because it fundamentally misunderstands image data. CNNs are the game-changer here!</p>
</div>
</div>
</div>


<script type="pyodide-data">
eyJvcHRpb25zIjp7ImVudiI6eyJQTE9UTFlfUkVOREVSRVIiOiJwbG90bHlfbWltZXR5cGUifSwiaW5kZXhVUkwiOiJodHRwczovL2Nkbi5qc2RlbGl2ci5uZXQvcHlvZGlkZS92MC4yNy4wL2Z1bGwvIn0sInBhY2thZ2VzIjp7InBrZ3MiOlsicHlvZGlkZV9odHRwIiwibWljcm9waXAiLCJpcHl0aG9uIiwibnVtcHkiLCJwbG90bHkiLCJuYmZvcm1hdCJdfX0=
</script>
<script type="ojs-module-contents">
{"contents":[{"cellName":"pyodide-3","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_3 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-3-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-3-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_3 = pyodideOjs.process(_pyodide_editor_3, {});\n"},{"cellName":"pyodide-2","inline":false,"methodName":"interpret","source":"_pyodide_value_2 = {\n  const { highlightPython, b64Decode} = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-2-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  // Default evaluation configuration\n  const options = Object.assign({\n    id: \"pyodide-2-contents\",\n    echo: true,\n    output: true\n  }, block.attr);\n\n  // Evaluate the provided Python code\n  const result = pyodideOjs.process({code: block.code, options}, {logit_val});\n\n  // Early yield while we wait for the first evaluation and render\n  if (options.output && !(\"2\" in pyodideOjs.renderedOjs)) {\n    const container = document.createElement(\"div\");\n    const spinner = document.createElement(\"div\");\n\n    if (options.echo) {\n      // Show output as highlighted source\n      const preElem = document.createElement(\"pre\");\n      container.className = \"sourceCode\";\n      preElem.className = \"sourceCode python\";\n      preElem.appendChild(highlightPython(block.code));\n      spinner.className = \"spinner-grow spinner-grow-sm m-2 position-absolute top-0 end-0\";\n      preElem.appendChild(spinner);\n      container.appendChild(preElem);\n    } else {\n      spinner.className = \"spinner-border spinner-border-sm\";\n      container.appendChild(spinner);\n    }\n\n    yield container;\n  }\n\n  pyodideOjs.renderedOjs[\"2\"] = true;\n  yield await result;\n}\n"},{"cellName":"pyodide-1","inline":false,"methodName":"interpret","source":"_pyodide_value_1 = {\n  const { highlightPython, b64Decode} = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-1-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  // Default evaluation configuration\n  const options = Object.assign({\n    id: \"pyodide-1-contents\",\n    echo: true,\n    output: true\n  }, block.attr);\n\n  // Evaluate the provided Python code\n  const result = pyodideOjs.process({code: block.code, options}, {originalWidth, compressedWidth});\n\n  // Early yield while we wait for the first evaluation and render\n  if (options.output && !(\"1\" in pyodideOjs.renderedOjs)) {\n    const container = document.createElement(\"div\");\n    const spinner = document.createElement(\"div\");\n\n    if (options.echo) {\n      // Show output as highlighted source\n      const preElem = document.createElement(\"pre\");\n      container.className = \"sourceCode\";\n      preElem.className = \"sourceCode python\";\n      preElem.appendChild(highlightPython(block.code));\n      spinner.className = \"spinner-grow spinner-grow-sm m-2 position-absolute top-0 end-0\";\n      preElem.appendChild(spinner);\n      container.appendChild(preElem);\n    } else {\n      spinner.className = \"spinner-border spinner-border-sm\";\n      container.appendChild(spinner);\n    }\n\n    yield container;\n  }\n\n  pyodideOjs.renderedOjs[\"1\"] = true;\n  yield await result;\n}\n"},{"cellName":"pyodide-prelude","inline":false,"methodName":"interpretQuiet","source":"pyodideOjs = {\n  const {\n    PyodideEvaluator,\n    PyodideEnvironmentManager,\n    setupPython,\n    startPyodideWorker,\n    b64Decode,\n    collapsePath,\n  } = window._exercise_ojs_runtime;\n\n  const statusContainer = document.getElementById(\"exercise-loading-status\");\n  const indicatorContainer = document.getElementById(\"exercise-loading-indicator\");\n  indicatorContainer.classList.remove(\"d-none\");\n\n  let statusText = document.createElement(\"div\")\n  statusText.classList = \"exercise-loading-details\";\n  statusText = statusContainer.appendChild(statusText);\n  statusText.textContent = `Initialise`;\n\n  // Hoist indicator out from final slide when running under reveal\n  const revealStatus = document.querySelector(\".reveal .exercise-loading-indicator\");\n  if (revealStatus) {\n    revealStatus.remove();\n    document.querySelector(\".reveal > .slides\").appendChild(revealStatus);\n  }\n\n  // Make any reveal slides with live cells scrollable\n  document.querySelectorAll(\".reveal .exercise-cell\").forEach((el) => {\n    el.closest('section.slide').classList.add(\"scrollable\");\n  })\n\n  // Pyodide supplemental data and options\n  const dataContent = document.querySelector(`script[type=\\\"pyodide-data\\\"]`).textContent;\n  const data = JSON.parse(b64Decode(dataContent));\n\n  // Grab list of resources to be downloaded\n  const filesContent = document.querySelector(`script[type=\\\"vfs-file\\\"]`).textContent;\n  const files = JSON.parse(b64Decode(filesContent));\n\n  let pyodidePromise = (async () => {\n    statusText.textContent = `Downloading Pyodide`;\n    const pyodide = await startPyodideWorker(data.options);\n\n    statusText.textContent = `Downloading package: micropip`;\n    await pyodide.loadPackage(\"micropip\");\n    const micropip = await pyodide.pyimport(\"micropip\");\n    await data.packages.pkgs.map((pkg) => () => {\n      statusText.textContent = `Downloading package: ${pkg}`;\n      return micropip.install(pkg);\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n    await micropip.destroy();\n\n    // Download and install resources\n    await files.map((file) => async () => {\n      const name = file.substring(file.lastIndexOf('/') + 1);\n      statusText.textContent = `Downloading resource: ${name}`;\n      const response = await fetch(file);\n      if (!response.ok) {\n        throw new Error(`Can't download \\`${file}\\`. Error ${response.status}: \"${response.statusText}\".`);\n      }\n      const data = await response.arrayBuffer();\n\n      // Store URLs in the cwd without any subdirectory structure\n      if (file.includes(\"://\")) {\n        file = name;\n      }\n\n      // Collapse higher directory structure\n      file = collapsePath(file);\n\n      // Create directory tree, ignoring \"directory exists\" VFS errors\n      const parts = file.split('/').slice(0, -1);\n      let path = '';\n      while (parts.length > 0) {\n        path += parts.shift() + '/';\n        try {\n          await pyodide.FS.mkdir(path);\n        } catch (e) {\n          if (e.name !== \"ErrnoError\") throw e;\n          if (e.errno !== 20) {\n            const errorTextPtr = await pyodide._module._strerror(e.errno);\n            const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n            throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n          }\n        }\n      }\n\n      // Write this file to the VFS\n      try {\n        return await pyodide.FS.writeFile(file, new Uint8Array(data));\n      } catch (e) {\n        if (e.name !== \"ErrnoError\") throw e;\n        const errorTextPtr = await pyodide._module._strerror(e.errno);\n        const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n        throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n      }\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n\n    statusText.textContent = `Pyodide environment setup`;\n    await setupPython(pyodide);\n\n    statusText.remove();\n    if (statusContainer.children.length == 0) {\n      statusContainer.parentNode.remove();\n    }\n    return pyodide;\n  })().catch((err) => {\n    statusText.style.color = \"var(--exercise-editor-hl-er, #AD0000)\";\n    statusText.textContent = err.message;\n    //indicatorContainer.querySelector(\".spinner-grow\").classList.add(\"d-none\");\n    throw err;\n  });\n\n  // Keep track of initial OJS block render\n  const renderedOjs = {};\n\n  const process = async (context, inputs) => {\n    const pyodide = await pyodidePromise;\n    const evaluator = new PyodideEvaluator(pyodide, context);\n    await evaluator.process(inputs);\n    return evaluator.container;\n  }\n\n  return {\n    pyodidePromise,\n    renderedOjs,\n    process,\n  };\n}\n"}]}
</script>
<div id="exercise-loading-indicator" class="exercise-loading-indicator d-none d-flex align-items-center gap-2">
<div id="exercise-loading-status" class="d-flex gap-2">

</div>
<div class="spinner-grow spinner-grow-sm">

</div>
</div>
<script type="vfs-file">
W10=
</script>
</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="../qrjs_pics/unsoed_logo.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://imron-slide.vercel.app">irosyadi-2025</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script type="ojs-module-contents">
    eyJjb250ZW50cyI6W3sibWV0aG9kTmFtZSI6ImludGVycHJldCIsImNlbGxOYW1lIjoib2pzLWNlbGwtMSIsImlubGluZSI6ZmFsc2UsInNvdXJjZSI6InZpZXdvZiBvcmlnaW5hbFdpZHRoID0gSW5wdXRzLnJhbmdlKFsxMDAsIDUwMF0sIHt2YWx1ZTogMzAwLCBzdGVwOiAxMCwgbGFiZWw6IFwiT3JpZ2luYWwgV2lkdGhcIn0pO1xudmlld29mIGNvbXByZXNzZWRXaWR0aCA9IElucHV0cy5yYW5nZShbMTAsIDEwMF0sIHt2YWx1ZTogNTAsIHN0ZXA6IDUsIGxhYmVsOiBcIkNvbXByZXNzZWQgV2lkdGhcIn0pO1xuIn0seyJtZXRob2ROYW1lIjoiaW50ZXJwcmV0IiwiY2VsbE5hbWUiOiJvanMtY2VsbC0yIiwiaW5saW5lIjpmYWxzZSwic291cmNlIjoidmlld29mIGxvZ2l0X3ZhbCA9IElucHV0cy5yYW5nZShbMCwgMTBdLCB7c3RlcDogMC4xLCB2YWx1ZTogNSwgbGFiZWw6IFwiTG9naXQgVmFsdWUgZm9yIENsYXNzIDAgKG90aGVycyBmaXhlZCBhdCAxLjApXCJ9KTtcbiJ9LHsibWV0aG9kTmFtZSI6ImludGVycHJldFF1aWV0Iiwic291cmNlIjoic2hpbnlJbnB1dCgnb3JpZ2luYWxXaWR0aCcpIn0seyJtZXRob2ROYW1lIjoiaW50ZXJwcmV0UXVpZXQiLCJzb3VyY2UiOiJzaGlueUlucHV0KCdjb21wcmVzc2VkV2lkdGgnKSJ9LHsibWV0aG9kTmFtZSI6ImludGVycHJldFF1aWV0Iiwic291cmNlIjoic2hpbnlJbnB1dCgnbG9naXRfdmFsJykifV19
    </script>
    <script type="module">
    if (window.location.protocol === "file:") { alert("The OJS runtime does not work with file:// URLs. Please use a web server to view this document."); }
    window._ojs.paths.runtimeToDoc = "../../google";
    window._ojs.paths.runtimeToRoot = "../..";
    window._ojs.paths.docToRoot = "..";
    window._ojs.selfContained = false;
    window._ojs.runtime.interpretFromScriptTags();
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>