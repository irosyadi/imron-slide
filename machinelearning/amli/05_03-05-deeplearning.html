<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/quarto-contrib/live-runtime/live-runtime.js" type="module"></script>
<link href="../site_libs/quarto-contrib/live-runtime/live-runtime.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.25">

  <meta name="author" content="Imron Rosyadi">
  <title>Machine Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-b0356e9119c1bdfd0db189d130feb51c.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script type="module" src="../site_libs/quarto-ojs/quarto-ojs-runtime.js"></script>
  <link href="../site_libs/quarto-ojs/quarto-ojs.css" rel="stylesheet">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Machine Learning</h1>
  <p class="subtitle">5. Deep Learning: AE, TL</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Imron Rosyadi 
</div>
</div>
</div>

</section>
<section>
<section id="autoencoders" class="title-slide slide level1 center">
<h1>Autoencoders</h1>

</section>
<section id="what-is-an-autoencoder" class="slide level2">
<h2>What Is an Autoencoder?</h2>
<p>A neural network designed to learn an efficient data encoding (compression)<br>
and then reconstruct the input data from that encoding (decompression).</p>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p><strong>Core Idea:</strong> Learn a compressed, “latent” representation of data without supervision.</p>
</div>
</div>
</div>
<aside class="notes">
<p>First things first, what is an autoencoder? It is a model that learns how to encode data and then decode data.</p>
<p>Let’s look at a visualization.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="encoder" class="slide level2">
<h2>Encoder</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>The <strong>encoder</strong> is a neural network that transforms input data into a smaller,<br>
lower-dimensional representation.</p>
<ul>
<li><strong>Goal:</strong> Reduce data dimensionality while retaining essential information.<br>
</li>
<li><strong>Layers:</strong> Can use dense, convolutional, pooling layers, etc.<br>
</li>
<li><strong>Output:</strong> A compressed “latent space” representation.<br>
</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/encoder.png"></p>
</div></div>
<aside class="notes">
<p>Encoding looks a whole lot like the neural networks that we have already seen, and that’s because it is. The encoder is a neural network that starts out with some input data and outputs a smaller form of that data. The encoder can use dense layers, convolutional layers, pooling layers, and more.</p>
<p>The goal of the encoder is to take some form of input data and reduce it down to a smaller representation.</p>
<p>But there has to be some way to know if this smaller representation is useful. We do that with the decoder.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="decoder" class="slide level2">
<h2>Decoder</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>The <strong>decoder</strong> performs the reverse operation of the encoder.</p>
<ul>
<li><strong>Input:</strong> The compressed latent representation from the encoder.<br>
</li>
<li><strong>Goal:</strong> Reconstruct an approximation of the original input data.<br>
</li>
<li><strong>Method:</strong> Expands the data back to its original dimensions.<br>
</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/decoder.png"></p>
</div></div>
<aside class="notes">
<p>As you might expect, the decoder does the opposite of the encoder. The decoder starts with a compressed representation of the data and inflates it back to the original size.</p>
<p>We haven’t really seen this before. The networks we have built tend to get narrower as data flows through them. This widening is less common. Sure, we could add wider and wider dense layers in a deep neural network, but it isn’t common to see outside of this context.</p>
<p>How do we do this?</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="decoder-upsampling" class="slide level2">
<h2>Decoder: Upsampling</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>To expand the data in the decoder, we use <strong>upsampling</strong> techniques.</p>
<ul>
<li><strong>Concept:</strong> Reverse of pooling; expands input dimensions.<br>
</li>
<li><strong><code>UpSampling2D</code>:</strong> Commonly used in TensorFlow Keras for image data.<br>
</li>
<li><strong>Example:</strong> Doubles spatial dimensions (e.g., 4x4 to 8x8).<br>
</li>
</ul>
</div><div class="column" style="width:50%;">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a>tf.keras.layers.UpSampling2D(</span>
<span id="cb1-2"><a></a>    size<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb1-3"><a></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode numberSource text number-lines code-with-copy"><code class="sourceCode"><span id="cb2-1"><a></a>conv2d_3 (Conv2D)            (None, 4, 4, 16)          2320      </span>
<span id="cb2-2"><a></a>_________________________________________________________________</span>
<span id="cb2-3"><a></a>up_sampling2d (UpSampling2D) (None, 8, 8, 16)          0      </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div></div>
<aside class="notes">
<p>We use upsampling to add wider dense layers and create the decoder. You can think of upsampling as the reverse of the pooling layers we used in the convolutional neural networks we created for classification. While a pooling layer shrinks its input, the upsampling layer expands its input.</p>
<p>In TensorFlow Keras we’ll use the <code>UpSampling2D</code> layer to decode our encoded data.</p>
<p>In the example on this slide, you can see a convolutional layer that outputs a 4x4x16 matrix. The upsampling layer doubles the first two dimensions to 8x8x16.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="autoencoder" class="slide level2">
<h2>Autoencoder</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Combining the encoder and decoder forms an <strong>autoencoder</strong>.</p>
<ul>
<li><strong>Encoder:</strong> Learns efficient data representation.<br>
</li>
<li><strong>Decoder:</strong> Reconstructs data from the encoded representation.<br>
</li>
<li><strong>“Lossy” Compression:</strong> Output is an approximation, not exact replica.</li>
</ul>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>The autoencoder aims to reconstruct its own input.</p>
</div>
</div>
</div>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/autoencoder.png"></p>
</div></div>
<aside class="notes">
<p>What do you get when you mix an encoder and a decoder? An autoencoder!</p>
<p>The encoder finds an efficient representation for the data. The decoder is able to revive some approximation of the original data from the encoded data.</p>
<p>This is “lossy” compression. The output of the model is not typically exactly what was put in, but is hopefully a reasonable approximation.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="what-are-autoencoders-good-for" class="slide level2">
<h2>What Are Autoencoders Good For?</h2>
<ul>
<li><strong>Lossy Data Compression:</strong> Efficiently reduce data size.<br>
</li>
<li><strong>Non-linear Principal Component Analysis (PCA):</strong> Discover underlying data structure.<br>
</li>
<li><strong>Data Denoising/Cleaning:</strong> Remove noise or artifacts from data.<br>
</li>
<li><strong>Feature Learning:</strong> Extract meaningful features for other ML tasks.<br>
</li>
<li><strong>Anomaly Detection:</strong> Identify data points that deviate from learned patterns.</li>
</ul>
<aside class="notes">
<p>Obviously autoencoders are good at lossy data compression. Once trained, the encoder part of the model can be used to compress our input data. The decoder can then later be used to expand that data to a version that is close to the original.</p>
<p>Another application is principal component analysis. If you think about what an autoencoder is doing, it is reducing input data down into the minimal amount of information required to then revive that data. It is finding principal components using a neural network. You can train the model and then use the encoder to reduce the dimensionality of your data before feeding it into another model.</p>
<p>Another interesting application is data cleaning. Autoencoders can be used to remove noise from data. In our lab we’ll remove static and watermarks from images. Admittedly, there is some data loss, but it is still an interesting application.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="keras-model-building-the-encoder" class="slide level2">
<h2>Keras Model: Building the Encoder</h2>
<p>Utilize the Keras <code>Model</code> class for flexible architecture.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> Conv2D, MaxPool2D, UpSampling2D, Input</span>
<span id="cb3-2"><a></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> Model</span>
<span id="cb3-3"><a></a></span>
<span id="cb3-4"><a></a><span class="co"># Define input layer</span></span>
<span id="cb3-5"><a></a>input_layer <span class="op">=</span> Input(shape<span class="op">=</span>(<span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>), name<span class="op">=</span><span class="st">'input_image'</span>)</span>
<span id="cb3-6"><a></a></span>
<span id="cb3-7"><a></a><span class="co"># Encoder path</span></span>
<span id="cb3-8"><a></a>conv_layer <span class="op">=</span> Conv2D(<span class="dv">32</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>, padding<span class="op">=</span><span class="st">'same'</span>)(input_layer)</span>
<span id="cb3-9"><a></a>latent_layer <span class="op">=</span> MaxPool2D((<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span><span class="st">'same'</span>)(conv_layer)</span>
<span id="cb3-10"><a></a></span>
<span id="cb3-11"><a></a><span class="co"># Define the encoder model</span></span>
<span id="cb3-12"><a></a>encoder <span class="op">=</span> Model(input_layer, latent_layer, name<span class="op">=</span><span class="st">'encoder'</span>)</span>
<span id="cb3-13"><a></a></span>
<span id="cb3-14"><a></a><span class="bu">print</span>(encoder.summary())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>The <code>latent_layer</code> represents the compressed output of the encoder.</p>
</div>
</div>
</div>
<aside class="notes">
<p>You could build an autoencoder with a standard <code>Sequential</code> model, but often you’ll want to use the encoder and decoder separately. In order to do this, we can use the Keras <code>Model</code> class.</p>
<p>In this example we build an input layer and pass it to a convolutional layer, which is then passed to a pooling layer. The input and output layers are then passed to the <code>Model</code>.</p>
<p>You might also notice that we called the output of the encoder the “latent layer.” This is a common term used to identify the intermediate data representation that is output by the encoder and input to the decoder.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="keras-model-assembling-the-autoencoder" class="slide level2">
<h2>Keras Model: Assembling the Autoencoder</h2>
<p>Connect encoder and decoder using the Keras Functional API.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> Conv2D, MaxPool2D, UpSampling2D, Input</span>
<span id="cb4-2"><a></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> Model</span>
<span id="cb4-3"><a></a></span>
<span id="cb4-4"><a></a><span class="co"># Example placeholder for decoder output (in a real model, this would be built from latent_layer)</span></span>
<span id="cb4-5"><a></a><span class="co"># For this example, let's just make a simple decoder structure</span></span>
<span id="cb4-6"><a></a>latent_input <span class="op">=</span> Input(shape<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">14</span>, <span class="dv">32</span>), name<span class="op">=</span><span class="st">'latent_input'</span>)</span>
<span id="cb4-7"><a></a>conv_decoder <span class="op">=</span> Conv2D(<span class="dv">32</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>, padding<span class="op">=</span><span class="st">'same'</span>)(latent_input)</span>
<span id="cb4-8"><a></a>up_sample <span class="op">=</span> UpSampling2D((<span class="dv">2</span>, <span class="dv">2</span>))(conv_decoder)</span>
<span id="cb4-9"><a></a>output_layer <span class="op">=</span> Conv2D(<span class="dv">1</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'sigmoid'</span>, padding<span class="op">=</span><span class="st">'same'</span>)(up_sample) <span class="co"># Assuming 1-channel output like original input</span></span>
<span id="cb4-10"><a></a></span>
<span id="cb4-11"><a></a><span class="co"># Define the encoder model (as on previous slide)</span></span>
<span id="cb4-12"><a></a>input_autoencoder <span class="op">=</span> Input(shape<span class="op">=</span>(<span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>), name<span class="op">=</span><span class="st">'input_image_autoencoder'</span>)</span>
<span id="cb4-13"><a></a>encoder_conv <span class="op">=</span> Conv2D(<span class="dv">32</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>, padding<span class="op">=</span><span class="st">'same'</span>)(input_autoencoder)</span>
<span id="cb4-14"><a></a>latent_encoded <span class="op">=</span> MaxPool2D((<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span><span class="st">'same'</span>)(encoder_conv)</span>
<span id="cb4-15"><a></a>encoder <span class="op">=</span> Model(input_autoencoder, latent_encoded, name<span class="op">=</span><span class="st">'encoder'</span>)</span>
<span id="cb4-16"><a></a></span>
<span id="cb4-17"><a></a><span class="co"># Define the decoder model</span></span>
<span id="cb4-18"><a></a>decoder <span class="op">=</span> Model(latent_input, output_layer, name<span class="op">=</span><span class="st">'decoder'</span>)</span>
<span id="cb4-19"><a></a></span>
<span id="cb4-20"><a></a><span class="co"># Connect them to form the autoencoder</span></span>
<span id="cb4-21"><a></a>autoencoder <span class="op">=</span> Model(</span>
<span id="cb4-22"><a></a>    input_autoencoder,</span>
<span id="cb4-23"><a></a>    decoder(encoder(input_autoencoder)),</span>
<span id="cb4-24"><a></a>    name<span class="op">=</span><span class="st">"autoencoder"</span></span>
<span id="cb4-25"><a></a>)</span>
<span id="cb4-26"><a></a></span>
<span id="cb4-27"><a></a><span class="bu">print</span>(autoencoder.summary())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p>Training the <code>autoencoder</code> model simultaneously trains both the <code>encoder</code> and <code>decoder</code> sub-models.</p>
</div>
</div>
</div>
<aside class="notes">
<p>To build an autoencoder, you create an encoder and a decoder. The encoder accepts an input layer and outputs a latent layer. The decoder accepts a latent layer and outputs an output layer.</p>
<p>The encoder and decoder are stitched together into a third model, the autoencoder. Notice that the autoencoder accepts the input layer and passes it directly to the encoder. The encoder is the input to the decoder (via the latent layer).</p>
<p>When the autoencoder is trained, the encoder and decoder are also trained and can be used separately.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="interactive-convolution-visualization" class="slide level2">
<h2>Interactive Convolution Visualization</h2>
<p>Explore how a 2D convolution filter processes an input matrix.<br>
Adjust filter size and visualize the output.</p>
<div class="columns">
<div class="column" style="width:40%;">
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code hidden" id="cb5" data-startfrom="270" data-source-offset="-0"><pre class="sourceCode numberSource js number-lines code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 269;"><span id="cb5-270"><a></a>viewof input_val <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">0</span><span class="op">,</span> <span class="dv">10</span>]<span class="op">,</span> {<span class="dt">step</span><span class="op">:</span> <span class="dv">1</span><span class="op">,</span> <span class="dt">value</span><span class="op">:</span> <span class="dv">5</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Input Matrix Base Value"</span>})<span class="op">;</span></span>
<span id="cb5-271"><a></a>viewof filter_size <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">select</span>([<span class="st">"3x3"</span><span class="op">,</span> <span class="st">"5x5"</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="st">"3x3"</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Filter Size"</span>})<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-2" data-nodetype="declaration">

</div>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>This visualization shows a single convolution operation with a simple filter.</p>
</div>
</div>
</div>
</div><div class="column" style="width:60%;">
<div>
<div id="pyodide-1" class="exercise-cell">

</div>
<script type="pyodide-1-contents">
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
</script>
</div>
</div></div>
<aside class="notes">
<p>Here we visualize a simplified 2D convolution. You can adjust the base value of the input matrix and the size of the filter. The input matrix is a simple gradient. The filter matrix is a basic edge detection or blur kernel. The output feature map shows the result of applying the filter across the input.<br>
Convolution involves sliding the filter over the input, performing element-wise multiplication, and summing the results for each position. This process helps detect features like edges or textures in the input data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="your-turn" class="slide level2">
<h2>Your Turn!</h2>
<p>Now it is your turn to apply what you’ve learned about autoencoders.<br>
In the lab, you will work through examples of:</p>
<ul>
<li>Using an autoencoder for <strong>image compression</strong>.<br>
</li>
<li>Applying autoencoders for <strong>denoising</strong> (e.g., removing static from images).<br>
</li>
<li>An exercise to <strong>remove a watermark</strong> from a video or image.</li>
</ul>
<p>What specific ECE applications do you envision for autoencoders, beyond those discussed?</p>
<aside class="notes">
<p>Now it is your turn. In this lab we will walk through examples of using an autoencoder for compression and for removing static. Our exercise will be to remove a watermark from a video.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

</section></section>
<section>
<section id="image-classification-project" class="title-slide slide level1 center">
<h1>Image Classification Project</h1>

</section>
<section id="identifying-pneumonia-from-x-rays" class="slide level2">
<h2>Identifying Pneumonia from X-rays</h2>
<p>This project consolidates your knowledge of image classification.<br>
You will apply various techniques learned throughout the course.</p>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Focus on building, evaluating, and tuning models for a real-world medical imaging task.</p>
</div>
</div>
</div>
<aside class="notes">
<p>We are nearing the end of the classification track. We’ve learned quite a bit. In the last few labs we’ve created binary and multiclass classifiers. We’ve used scikit-learn and TensorFlow to create various models we then evaluated and tuned.</p>
<p>In this final project, you’ll get to show off what you’ve learned in one large project.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="image-classification-project-chest-x-rays" class="slide level2">
<h2>Image Classification Project: Chest X-rays</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>You will work with a dataset of chest X-ray images.</p>
<ul>
<li><strong>Task:</strong> Classify images as either <code>NORMAL</code> or <code>PNEUMONIA</code>.<br>
</li>
<li><strong>Source:</strong> Dataset from Kaggle, pre-divided for training, testing, and validation.<br>
</li>
<li><strong>Relevance:</strong> Demonstrates ML application in medical diagnostics.<br>
</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/x-ray.jpg"></p>
</div></div>
<aside class="notes">
<p>In the lab we’ll download a dataset from Kaggle. The dataset contains images of x-rays of patient lungs. Some of the images are classified as having pneumonia, while others are classified as normal.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="review-what-is-convolution" class="slide level2">
<h2>Review: What is Convolution?</h2>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><strong>Definition:</strong> A mathematical operation involving two functions to produce a third.<br>
</li>
<li><strong>Image Processing:</strong> A “filter” (kernel) slides over an input image.<br>
</li>
<li><strong>Process:</strong> Element-wise multiplication of filter with image region, then sum.<br>
</li>
<li><strong>Goal:</strong> Detect specific features (e.g., edges, textures) in the image.<br>
</li>
<li><strong>Parameters:</strong> Filter size, stride, padding.<br>
</li>
</ul>
</div><div class="column" style="width:50%;">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode numberSource mermaid number-lines code-with-copy"><code class="sourceCode"><span id="cb6-1"><a></a>flowchart LR</span>
<span id="cb6-2"><a></a>    subgraph "Input Image"</span>
<span id="cb6-3"><a></a>        A["Pixel Grid"]</span>
<span id="cb6-4"><a></a>    end</span>
<span id="cb6-5"><a></a></span>
<span id="cb6-6"><a></a>    subgraph "Filter (Kernel)"</span>
<span id="cb6-7"><a></a>        B["Small Matrix"]</span>
<span id="cb6-8"><a></a>    end</span>
<span id="cb6-9"><a></a></span>
<span id="cb6-10"><a></a>    subgraph "Operation"</span>
<span id="cb6-11"><a></a>        C(("Slide &amp; Multiply-Add"))</span>
<span id="cb6-12"><a></a>    end</span>
<span id="cb6-13"><a></a></span>
<span id="cb6-14"><a></a>    subgraph "Output"</span>
<span id="cb6-15"><a></a>        D["Feature Map"]</span>
<span id="cb6-16"><a></a>    end</span>
<span id="cb6-17"><a></a></span>
<span id="cb6-18"><a></a>    A -- "Apply Filter" --&gt; C</span>
<span id="cb6-19"><a></a>    B -- "Over Region" --&gt; C</span>
<span id="cb6-20"><a></a>    C --&gt; D</span>
<span id="cb6-21"><a></a></span>
<span id="cb6-22"><a></a>    style A fill:#cef,stroke:#333,stroke-width:2px</span>
<span id="cb6-23"><a></a>    style B fill:#fec,stroke:#333,stroke-width:2px</span>
<span id="cb6-24"><a></a>    style C fill:#ccf,stroke:#333,stroke-width:2px</span>
<span id="cb6-25"><a></a>    style D fill:#cfc,stroke:#333,stroke-width:2px</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div></div>
<aside class="notes">
<p><span class="citation" data-cites="Exercise">@Exercise</span>(5 minutes) {<br>
Have students discuss convolution. It is a process of passing a filter (kernel) over an image and computing new pixel values. This process involves multiplying the values in the image by those in the filter and adding them up. You need to know the size of your filter and the stride. The goal is to detect features in the image. Remind students that we saw simple kernels that were line detectors.<br>
}</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="review-convolutional-neural-network-cnn-architecture" class="slide level2">
<h2>Review: Convolutional Neural Network (CNN) Architecture</h2>
<ul>
<li><strong>Key Components:</strong>
<ul>
<li><strong>Convolutional Layers:</strong> Feature extraction using filters.<br>
</li>
<li><strong>Pooling Layers:</strong> Downsampling to reduce dimensionality and computational cost.<br>
</li>
<li><strong>Activation Functions:</strong> Introduce non-linearity (e.g., ReLU).<br>
</li>
<li><strong>Fully Connected Layers:</strong> Classification based on extracted features.<br>
</li>
</ul></li>
<li><strong>Tunable “Knobs”:</strong>
<ul>
<li>Number of layers, layer order.<br>
</li>
<li>Filter size, number of filters, stride.<br>
</li>
<li>Pooling size, type of pooling.<br>
</li>
<li>Activation functions (e.g., ReLU, Sigmoid for output).<br>
</li>
<li>Dropout rates, learning rate, batch size.</li>
</ul></li>
</ul>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<svg width="960" height="480" viewbox="0.00 0.00 1635.49 71.68" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style="; max-width: none; max-height: none; display: block; margin: auto auto auto auto">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 67.68)">
<title>CNN_Architecture</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-67.68 1631.49,-67.68 1631.49,4 -4,4"></polygon>
<!-- Input -->
<g id="node1" class="node">
<title>Input</title>
<polygon fill="#ccffcc" stroke="black" points="124.94,-52.44 0.02,-52.44 0.02,-11.24 124.94,-11.24 124.94,-52.44"></polygon>
<text text-anchor="middle" x="62.48" y="-36.04" font-family="Helvetica,sans-Serif" font-size="14.00">Input Image</text>
<text text-anchor="middle" x="62.48" y="-19.24" font-family="Helvetica,sans-Serif" font-size="14.00">(e.g., 256x256x3)</text>
</g>
<!-- Conv1 -->
<g id="node2" class="node">
<title>Conv1</title>
<path fill="#e0e0ff" stroke="black" d="M431.4,-55.32C431.4,-58.2 396.42,-60.54 353.36,-60.54 310.3,-60.54 275.33,-58.2 275.33,-55.32 275.33,-55.32 275.33,-8.36 275.33,-8.36 275.33,-5.48 310.3,-3.14 353.36,-3.14 396.42,-3.14 431.4,-5.48 431.4,-8.36 431.4,-8.36 431.4,-55.32 431.4,-55.32"></path>
<path fill="none" stroke="black" d="M431.4,-55.32C431.4,-52.44 396.42,-50.1 353.36,-50.1 310.3,-50.1 275.33,-52.44 275.33,-55.32"></path>
<text text-anchor="middle" x="353.36" y="-36.04" font-family="Helvetica,sans-Serif" font-size="14.00">Conv Layer 1</text>
<text text-anchor="middle" x="353.36" y="-19.24" font-family="Helvetica,sans-Serif" font-size="14.00">(Filters, Kernel, Stride)</text>
</g>
<!-- Input&#45;&gt;Conv1 -->
<g id="edge1" class="edge">
<title>Input-&gt;Conv1</title>
<path fill="none" stroke="black" d="M125.1,-31.84C165.84,-31.84 219.87,-31.84 265.03,-31.84"></path>
<polygon fill="black" stroke="black" points="265.11,-35.34 275.11,-31.84 265.11,-28.34 265.11,-35.34"></polygon>
<text text-anchor="middle" x="200.15" y="-36.04" font-family="Helvetica,sans-Serif" font-size="14.00">Feature Extraction</text>
</g>
<!-- Pool1 -->
<g id="node3" class="node">
<title>Pool1</title>
<polygon fill="#ffddc1" stroke="black" points="732.82,-18.72 732.82,-44.96 681.91,-63.52 609.9,-63.52 558.99,-44.96 558.99,-18.72 609.9,-0.16 681.91,-0.16 732.82,-18.72"></polygon>
<text text-anchor="middle" x="645.91" y="-36.04" font-family="Helvetica,sans-Serif" font-size="14.00">Pooling Layer 1</text>
<text text-anchor="middle" x="645.91" y="-19.24" font-family="Helvetica,sans-Serif" font-size="14.00">(Max/Avg Pool)</text>
</g>
<!-- Conv1&#45;&gt;Pool1 -->
<g id="edge2" class="edge">
<title>Conv1-&gt;Pool1</title>
<path fill="none" stroke="black" d="M431.46,-31.84C467.49,-31.84 510.77,-31.84 548.9,-31.84"></path>
<polygon fill="black" stroke="black" points="548.99,-35.34 558.99,-31.84 548.99,-28.34 548.99,-35.34"></polygon>
<text text-anchor="middle" x="495.29" y="-36.04" font-family="Helvetica,sans-Serif" font-size="14.00">Downsampling</text>
</g>
<!-- Conv2 -->
<g id="node4" class="node">
<title>Conv2</title>
<polygon fill="#e0e0ff" stroke="black" points="869.02,-49.84 769.48,-49.84 769.48,-13.84 869.02,-13.84 869.02,-49.84"></polygon>
<text text-anchor="middle" x="819.25" y="-27.64" font-family="Helvetica,sans-Serif" font-size="14.00">Conv Layer 2</text>
</g>
<!-- Pool1&#45;&gt;Conv2 -->
<g id="edge3" class="edge">
<title>Pool1-&gt;Conv2</title>
<path fill="none" stroke="black" d="M732.83,-31.84C741.73,-31.84 750.63,-31.84 759.15,-31.84"></path>
<polygon fill="black" stroke="black" points="759.31,-35.34 769.31,-31.84 759.31,-28.34 759.31,-35.34"></polygon>
</g>
<!-- Pool2 -->
<g id="node5" class="node">
<title>Pool2</title>
<polygon fill="#e0e0ff" stroke="black" points="1019.32,-49.84 905.74,-49.84 905.74,-13.84 1019.32,-13.84 1019.32,-49.84"></polygon>
<text text-anchor="middle" x="962.53" y="-27.64" font-family="Helvetica,sans-Serif" font-size="14.00">Pooling Layer 2</text>
</g>
<!-- Conv2&#45;&gt;Pool2 -->
<g id="edge4" class="edge">
<title>Conv2-&gt;Pool2</title>
<path fill="none" stroke="black" d="M869.26,-31.84C877.83,-31.84 886.87,-31.84 895.81,-31.84"></path>
<polygon fill="black" stroke="black" points="895.82,-35.34 905.82,-31.84 895.82,-28.34 895.82,-35.34"></polygon>
</g>
<!-- Flatten -->
<g id="node6" class="node">
<title>Flatten</title>
<polygon fill="#ffffcc" stroke="black" points="1233.67,-49.84 1136.25,-49.84 1136.25,-13.84 1233.67,-13.84 1233.67,-49.84"></polygon>
<text text-anchor="middle" x="1184.96" y="-27.64" font-family="Helvetica,sans-Serif" font-size="14.00">Flatten Layer</text>
</g>
<!-- Pool2&#45;&gt;Flatten -->
<g id="edge5" class="edge">
<title>Pool2-&gt;Flatten</title>
<path fill="none" stroke="black" d="M1019.44,-31.84C1051.89,-31.84 1092.72,-31.84 1125.59,-31.84"></path>
<polygon fill="black" stroke="black" points="1125.96,-35.34 1135.96,-31.84 1125.96,-28.34 1125.96,-35.34"></polygon>
<text text-anchor="middle" x="1077.64" y="-36.04" font-family="Helvetica,sans-Serif" font-size="14.00">Vectorization</text>
</g>
<!-- Dense1 -->
<g id="node7" class="node">
<title>Dense1</title>
<polygon fill="#e0e0ff" stroke="black" points="1459.37,-52.44 1352.26,-52.44 1352.26,-11.24 1459.37,-11.24 1459.37,-52.44"></polygon>
<text text-anchor="middle" x="1405.81" y="-36.04" font-family="Helvetica,sans-Serif" font-size="14.00">Dense Layer 1</text>
<text text-anchor="middle" x="1405.81" y="-19.24" font-family="Helvetica,sans-Serif" font-size="14.00">(ReLU)</text>
</g>
<!-- Flatten&#45;&gt;Dense1 -->
<g id="edge6" class="edge">
<title>Flatten-&gt;Dense1</title>
<path fill="none" stroke="black" d="M1233.72,-31.84C1265.4,-31.84 1307.33,-31.84 1341.75,-31.84"></path>
<polygon fill="black" stroke="black" points="1342.18,-35.34 1352.18,-31.84 1342.18,-28.34 1342.18,-35.34"></polygon>
<text text-anchor="middle" x="1293.05" y="-36.04" font-family="Helvetica,sans-Serif" font-size="14.00">Classification</text>
</g>
<!-- Output -->
<g id="node8" class="node">
<title>Output</title>
<polygon fill="#ffcccc" stroke="black" points="1627.56,-52.44 1496.27,-52.44 1496.27,-11.24 1627.56,-11.24 1627.56,-52.44"></polygon>
<text text-anchor="middle" x="1561.91" y="-36.04" font-family="Helvetica,sans-Serif" font-size="14.00">Output Layer</text>
<text text-anchor="middle" x="1561.91" y="-19.24" font-family="Helvetica,sans-Serif" font-size="14.00">(Softmax/Sigmoid)</text>
</g>
<!-- Dense1&#45;&gt;Output -->
<g id="edge7" class="edge">
<title>Dense1-&gt;Output</title>
<path fill="none" stroke="black" d="M1459.42,-31.84C1468.03,-31.84 1477.1,-31.84 1486.13,-31.84"></path>
<polygon fill="black" stroke="black" points="1486.28,-35.34 1496.28,-31.84 1486.28,-28.34 1486.28,-35.34"></polygon>
</g>
</g>
</svg>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<aside class="notes">
<p>Exercise(5 minutes) {<br>
Have students discuss CNNs. In general, there are convolutional layers and pooling layers. Then the information is fed into a typical fully-connected neural network. Furthermore, an important choice the user needs to make is which activation function to use. Since this is a binary classification task, it is useful to use the sigmoid function on the final output layer. Relu works well on the other layers.<br>
}</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="image-classification-project-dataset-structure" class="slide level2">
<h2>Image Classification Project: Dataset Structure</h2>
<p>The dataset is pre-organized into standard machine learning splits.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode numberSource text number-lines code-with-copy"><code class="sourceCode"><span id="cb7-1"><a></a>chest_xray/</span>
<span id="cb7-2"><a></a>     ├── test/</span>
<span id="cb7-3"><a></a>     │       ├── NORMAL/</span>
<span id="cb7-4"><a></a>     │       └── PNEUMONIA/</span>
<span id="cb7-5"><a></a>     ├── train/</span>
<span id="cb7-6"><a></a>     │       ├── NORMAL/</span>
<span id="cb7-7"><a></a>     │       └── PNEUMONIA/</span>
<span id="cb7-8"><a></a>     └── val/</span>
<span id="cb7-9"><a></a>             ├── NORMAL/</span>
<span id="cb7-10"><a></a>             └── PNEUMONIA/</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><strong><code>train</code> set:</strong> Used for model training.<br>
</li>
<li><strong><code>test</code> set:</strong> For hyperparameter tuning and model selection.<br>
</li>
<li><strong><code>val</code> set:</strong> Final, unbiased evaluation of model generalization.</li>
</ul>
<aside class="notes">
<p>The images in the dataset are already divided into test, train, and validation sets. The training set is, of course, used for training your model. The testing dataset should be used to adjust model hyperparameters, shape, etc. Once you have found a model that tests well, check it against the validation dataset. That will serve as one final test for the ability for your model to generalize.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="image-classification-project-tips-for-success" class="slide level2">
<h2>Image Classification Project: Tips for Success</h2>
<ul>
<li><strong>Enable GPU:</strong> Significantly speeds up training for image models.
<ul>
<li><em>In Google Colab:</em> Runtime &gt; Change runtime type &gt; Hardware accelerator &gt; GPU.<br>
</li>
</ul></li>
<li><strong>Perform Exploratory Data Analysis (EDA):</strong>
<ul>
<li>Verify dataset integrity and structure.<br>
</li>
<li>Check for image dimensions, class balance, and potential anomalies.<br>
</li>
</ul></li>
<li><strong>Data Augmentation:</strong> Consider techniques like rotation, flipping, zooming to expand training data.<br>
</li>
<li><strong>Transfer Learning:</strong> Explore pre-trained models for faster convergence and better performance.</li>
</ul>
<p>What challenges do you anticipate when working with medical image data?</p>
<aside class="notes">
<p>First tip: enable GPU in Google Colab. This dataset tends to train significantly faster if you enable GPU in the runtime.</p>
<p>Also, perform EDA on your dataset. The dataset may have duplication, undocumented folders, etc.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="your-turn-1" class="slide level2">
<h2>Your Turn!</h2>
<p>Now, apply your skills to the Image Classification Project.<br>
Good luck building a robust pneumonia detection model!</p>
<p>What strategies will you prioritize for model evaluation and fine-tuning?</p>
<aside class="notes">
<p>And with that, it is your turn to work on the lab.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

</section></section>
<section>
<section id="transfer-learning" class="title-slide slide level1 center">
<h1>Transfer Learning</h1>

</section>
<section id="leveraging-pre-trained-models" class="slide level2">
<h2>Leveraging Pre-trained Models</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Concept:</strong> Reusing a pre-trained model as a starting point for a new task.</p>
<ul>
<li><strong>Traditional ML:</strong> Models trained from scratch on specific datasets.<br>
</li>
<li><strong>Transfer Learning:</strong> Utilizes knowledge gained from a large, general dataset.<br>
</li>
<li><strong>Benefit:</strong> Accelerates training, improves performance with limited data.<br>
</li>
</ul>
</div><div class="column" style="width:50%;">
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>Why train from scratch when you can stand on the shoulders of giants?</p>
</div>
</div>
</div>
</div></div>
<aside class="notes">
<p>Most of the models we have trained so far have been trained from scratch. We start with a randomly-weighted model and then use large amounts of data and many, many epochs over that data in order to build a reasonable model.</p>
<p>But is that how we learn?</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="transferring-knowledge-human-analogy" class="slide level2">
<h2>Transferring Knowledge: Human Analogy</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Just as humans learn by building upon existing knowledge,<br>
ML models can benefit from “transferred” insights.</p>
<ul>
<li><strong>Direct Learning:</strong> Observing examples directly.<br>
</li>
<li><strong>Knowledge Transfer:</strong> Gaining insights from others’ experiences or related domains.<br>
</li>
<li><strong>Efficiency:</strong> Speeds up the learning process for new, related tasks.<br>
</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/learning.jpg"></p>
</div></div>
<aside class="notes">
<p>Well, yes and no.</p>
<p>We do indeed learn in self-guided ways by looking at examples. But we also learn through the transfer of knowledge. Others can provide insights that can be used to accelerate our learning process.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="transferring-knowledge-the-zebra-example" class="slide level2">
<h2>Transferring Knowledge: The Zebra Example</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Imagine identifying a zebra:</p>
<ul>
<li><strong>Knowns:</strong> Horse shape, tiger stripes, penguin colors.<br>
</li>
<li><strong>Transferred Knowledge:</strong> Combining these known features accelerates zebra identification.<br>
</li>
<li><strong>ML Parallel:</strong> A model good at general image recognition can quickly adapt to new categories.<br>
</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/zebra.jpg"></p>
</div></div>
<aside class="notes">
<p>Let’s say we already know what a horse, tiger, and penguin are. If we wanted to learn how to identify a zebra, we could look at pictures of zebras. Or, someone might tell us that a zebra is the shape of a horse, has the coat patterns of a tiger, and the colors of a penguin. This would greatly accelerate our ability to identify zebras, even if we only had a handful of pictures of zebras to study.</p>
<p>Transfer learning is a similar idea. A model that can already identify some classes of data can be extended to fit a different problem that we are trying to solve. The base model is already good at finding key features. The new model can utilize this ability and perform better and faster than if it was trained from scratch.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="transfer-learning-high-level-overview" class="slide level2">
<h2>Transfer Learning: High-Level Overview</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Typically involves attaching new layers to a pre-trained base model.</p>
<ul>
<li><strong>Pre-trained Model:</strong> Base model with learned weights (e.g., ImageNet classifier).<br>
</li>
<li><strong>Customization Model:</strong> New, untrained layers added on top.<br>
</li>
<li><strong>Data Flow:</strong> Input goes through pre-trained model, then into new layers.<br>
</li>
<li><strong>Output:</strong> The final prediction from the new layers.<br>
</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/high-level.png"></p>
</div></div>
<aside class="notes">
<p>At a very high level, transfer learning can look a lot like adding an extra few layers to the end of a pre-trained model.</p>
<p>In this diagram the pre-trained model is an existing model that has been trained and performs acceptably well. This model has persisted weights that are packaged and loaded with the model.</p>
<p>The customizations model is a new set of untrained layers. They have random, or at least naive, initial weights. These weights still need to be learned through training.</p>
<p>As you can decipher from the data flow arrow, data typically still enters the model through the pre-trained input layer. However, the output layer of the pre-trained model then feeds the new model. The final output is the output layer of the new model.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="do-you-retrain-the-pre-trained-model" class="slide level2">
<h2>Do You Retrain the Pre-Trained Model?</h2>
<p>The decision to retrain (fine-tune) the pre-trained model’s weights depends on several factors:</p>
<ul>
<li><strong>“Freezing” Weights:</strong>
<ul>
<li><strong>Usually “No”:</strong> If new data is small or classes don’t largely overlap.<br>
</li>
<li><strong>Benefit:</strong> Prevents overfitting, faster training of new layers.<br>
</li>
</ul></li>
<li><strong>Fine-tuning Weights:</strong>
<ul>
<li><strong>Sometimes “Yes”:</strong> If new data is large and similar to original training data.<br>
</li>
<li><strong>Method:</strong> Unfreeze some or all layers and train with a very small learning rate.<br>
</li>
</ul></li>
<li><strong>Layer-Specific Freezing:</strong> Often, layers closer to the input are frozen, while later layers are fine-tuned.</li>
</ul>
<aside class="notes">
<p>This begs the question: Do you re-train the pre-trained model?</p>
<p>The answer is usually “no” but not always.</p>
<p>If the data you have to train your new model is similar in size or larger than the data used to train the pre-trained model, and if the classes that they identify largely overlap, then it may be worthwhile. Otherwise it is advisable to “freeze” the pre-trained model and not update the weights.</p>
<p>This freezing can be for the whole model or for only a few specific layers. Typically those layers closer to the input layer are frozen.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="which-output-layer-to-use" class="slide level2">
<h2>Which Output Layer to Use?</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>When using a pre-trained model, we typically don’t use its final classification layer.</p>
<ul>
<li><strong>Problem:</strong> Final layers usually flatten data into class-specific vectors.<br>
</li>
<li><strong>Solution:</strong> Use an <strong>intermediate high-dimensional layer</strong> as the output from the pre-trained model.<br>
</li>
<li><strong>Benefit:</strong> Provides rich feature representations for the new custom layers.<br>
</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/which-output.png"></p>
</div></div>
<aside class="notes">
<p>We also need to think about which layer is actually the output layer from a pre-trained model.</p>
<p>In most classification problems, we have multiple layers of high-dimensional matrices. But then at the very end of the model, we flatten the data down to a vector of class-estimates.</p>
<p>We don’t want to flatten the data before feeding it to our extended model, so instead we need to use an intermediate high-dimensional layer.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="model-terminology-bottom-and-top" class="slide level2">
<h2>Model Terminology: “Bottom” and “Top”</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Understanding these terms helps when configuring pre-trained models.</p>
<ul>
<li><strong>“Bottom”:</strong> Refers to the <strong>input layers</strong> of the model.<br>
</li>
<li><strong>“Top”:</strong> Refers to the <strong>output layers</strong> of the model.</li>
</ul>
<p>This convention often comes from how models are diagrammed, with input at the bottom and output at the top.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/bottom-top.png"></p>
</div></div>
<aside class="notes">
<p>We need to introduce a little modelling terminology at this point. You sometimes hear about the “bottom” or “top” of a model. Which end is which?</p>
<p>Many papers illustrate models with the input layer at the bottom and the output layer at the top of diagrams. For this reason, culture now dictates that the “bottom” of a model is the input, and the “top” of a model is the output.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="include_top-a-key-parameter" class="slide level2">
<h2><code>include_top</code>: A Key Parameter</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Many pre-trained models (e.g., in Keras) offer an <code>include_top</code> parameter.</p>
<ul>
<li><strong><code>include_top=True</code> (default):</strong> Includes the original classification layers.<br>
</li>
<li><strong><code>include_top=False</code>:</strong> Excludes the original classification layers.
<ul>
<li><strong>Benefit:</strong> Provides a high-dimensional feature extractor.<br>
</li>
<li><strong>Use Case:</strong> Ideal for building custom classifiers on top.<br>
</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/new-top.png"></p>
</div></div>
<aside class="notes">
<p>This terminology is important because some models allow you to choose to include the “top” of the model or not. If you leave out the top, then you get a higher-dimensional input for your custom model, which is typically a good thing.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="your-turn-2" class="slide level2">
<h2>Your Turn!</h2>
<p>Now, let’s put transfer learning into practice.</p>
<p>You will use <strong>MobileNetV2</strong>, a powerful pre-trained model,<br>
to build a network capable of classifying cats and dogs.</p>
<p>What advantages do you expect from using MobileNetV2 compared to training from scratch for this task?</p>
<aside class="notes">
<p>Now we’ll attempt a little transfer learning of our own. We’ll use MobileNetV2, which can classify 1,000 classes, to build a network that can reliably classify cats and dogs.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>


<script type="pyodide-data">
eyJvcHRpb25zIjp7ImVudiI6eyJQTE9UTFlfUkVOREVSRVIiOiJwbG90bHlfbWltZXR5cGUifSwiaW5kZXhVUkwiOiJodHRwczovL2Nkbi5qc2RlbGl2ci5uZXQvcHlvZGlkZS92MC4yNy4wL2Z1bGwvIn0sInBhY2thZ2VzIjp7InBrZ3MiOlsicHlvZGlkZV9odHRwIiwibWljcm9waXAiLCJpcHl0aG9uIiwibnVtcHkiLCJwbG90bHkiLCJuYmZvcm1hdCJdfX0=
</script>
<script type="ojs-module-contents">
{"contents":[{"cellName":"pyodide-1","inline":false,"methodName":"interpret","source":"_pyodide_value_1 = {\n  const { highlightPython, b64Decode} = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-1-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  // Default evaluation configuration\n  const options = Object.assign({\n    id: \"pyodide-1-contents\",\n    echo: true,\n    output: true\n  }, block.attr);\n\n  // Evaluate the provided Python code\n  const result = pyodideOjs.process({code: block.code, options}, {input_val, filter_size});\n\n  // Early yield while we wait for the first evaluation and render\n  if (options.output && !(\"1\" in pyodideOjs.renderedOjs)) {\n    const container = document.createElement(\"div\");\n    const spinner = document.createElement(\"div\");\n\n    if (options.echo) {\n      // Show output as highlighted source\n      const preElem = document.createElement(\"pre\");\n      container.className = \"sourceCode\";\n      preElem.className = \"sourceCode python\";\n      preElem.appendChild(highlightPython(block.code));\n      spinner.className = \"spinner-grow spinner-grow-sm m-2 position-absolute top-0 end-0\";\n      preElem.appendChild(spinner);\n      container.appendChild(preElem);\n    } else {\n      spinner.className = \"spinner-border spinner-border-sm\";\n      container.appendChild(spinner);\n    }\n\n    yield container;\n  }\n\n  pyodideOjs.renderedOjs[\"1\"] = true;\n  yield await result;\n}\n"},{"cellName":"pyodide-prelude","inline":false,"methodName":"interpretQuiet","source":"pyodideOjs = {\n  const {\n    PyodideEvaluator,\n    PyodideEnvironmentManager,\n    setupPython,\n    startPyodideWorker,\n    b64Decode,\n    collapsePath,\n  } = window._exercise_ojs_runtime;\n\n  const statusContainer = document.getElementById(\"exercise-loading-status\");\n  const indicatorContainer = document.getElementById(\"exercise-loading-indicator\");\n  indicatorContainer.classList.remove(\"d-none\");\n\n  let statusText = document.createElement(\"div\")\n  statusText.classList = \"exercise-loading-details\";\n  statusText = statusContainer.appendChild(statusText);\n  statusText.textContent = `Initialise`;\n\n  // Hoist indicator out from final slide when running under reveal\n  const revealStatus = document.querySelector(\".reveal .exercise-loading-indicator\");\n  if (revealStatus) {\n    revealStatus.remove();\n    document.querySelector(\".reveal > .slides\").appendChild(revealStatus);\n  }\n\n  // Make any reveal slides with live cells scrollable\n  document.querySelectorAll(\".reveal .exercise-cell\").forEach((el) => {\n    el.closest('section.slide').classList.add(\"scrollable\");\n  })\n\n  // Pyodide supplemental data and options\n  const dataContent = document.querySelector(`script[type=\\\"pyodide-data\\\"]`).textContent;\n  const data = JSON.parse(b64Decode(dataContent));\n\n  // Grab list of resources to be downloaded\n  const filesContent = document.querySelector(`script[type=\\\"vfs-file\\\"]`).textContent;\n  const files = JSON.parse(b64Decode(filesContent));\n\n  let pyodidePromise = (async () => {\n    statusText.textContent = `Downloading Pyodide`;\n    const pyodide = await startPyodideWorker(data.options);\n\n    statusText.textContent = `Downloading package: micropip`;\n    await pyodide.loadPackage(\"micropip\");\n    const micropip = await pyodide.pyimport(\"micropip\");\n    await data.packages.pkgs.map((pkg) => () => {\n      statusText.textContent = `Downloading package: ${pkg}`;\n      return micropip.install(pkg);\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n    await micropip.destroy();\n\n    // Download and install resources\n    await files.map((file) => async () => {\n      const name = file.substring(file.lastIndexOf('/') + 1);\n      statusText.textContent = `Downloading resource: ${name}`;\n      const response = await fetch(file);\n      if (!response.ok) {\n        throw new Error(`Can't download \\`${file}\\`. Error ${response.status}: \"${response.statusText}\".`);\n      }\n      const data = await response.arrayBuffer();\n\n      // Store URLs in the cwd without any subdirectory structure\n      if (file.includes(\"://\")) {\n        file = name;\n      }\n\n      // Collapse higher directory structure\n      file = collapsePath(file);\n\n      // Create directory tree, ignoring \"directory exists\" VFS errors\n      const parts = file.split('/').slice(0, -1);\n      let path = '';\n      while (parts.length > 0) {\n        path += parts.shift() + '/';\n        try {\n          await pyodide.FS.mkdir(path);\n        } catch (e) {\n          if (e.name !== \"ErrnoError\") throw e;\n          if (e.errno !== 20) {\n            const errorTextPtr = await pyodide._module._strerror(e.errno);\n            const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n            throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n          }\n        }\n      }\n\n      // Write this file to the VFS\n      try {\n        return await pyodide.FS.writeFile(file, new Uint8Array(data));\n      } catch (e) {\n        if (e.name !== \"ErrnoError\") throw e;\n        const errorTextPtr = await pyodide._module._strerror(e.errno);\n        const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n        throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n      }\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n\n    statusText.textContent = `Pyodide environment setup`;\n    await setupPython(pyodide);\n\n    statusText.remove();\n    if (statusContainer.children.length == 0) {\n      statusContainer.parentNode.remove();\n    }\n    return pyodide;\n  })().catch((err) => {\n    statusText.style.color = \"var(--exercise-editor-hl-er, #AD0000)\";\n    statusText.textContent = err.message;\n    //indicatorContainer.querySelector(\".spinner-grow\").classList.add(\"d-none\");\n    throw err;\n  });\n\n  // Keep track of initial OJS block render\n  const renderedOjs = {};\n\n  const process = async (context, inputs) => {\n    const pyodide = await pyodidePromise;\n    const evaluator = new PyodideEvaluator(pyodide, context);\n    await evaluator.process(inputs);\n    return evaluator.container;\n  }\n\n  return {\n    pyodidePromise,\n    renderedOjs,\n    process,\n  };\n}\n"}]}
</script>
<div id="exercise-loading-indicator" class="exercise-loading-indicator d-none d-flex align-items-center gap-2">
<div id="exercise-loading-status" class="d-flex gap-2">

</div>
<div class="spinner-grow spinner-grow-sm">

</div>
</div>
<script type="vfs-file">
W10=
</script>
</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="../qrjs_pics/unsoed_logo.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://imron-slide.vercel.app">irosyadi-2025</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script type="ojs-module-contents">
    eyJjb250ZW50cyI6W3sibWV0aG9kTmFtZSI6ImludGVycHJldCIsImNlbGxOYW1lIjoib2pzLWNlbGwtMSIsImlubGluZSI6ZmFsc2UsInNvdXJjZSI6InZpZXdvZiBpbnB1dF92YWwgPSBJbnB1dHMucmFuZ2UoWzAsIDEwXSwge3N0ZXA6IDEsIHZhbHVlOiA1LCBsYWJlbDogXCJJbnB1dCBNYXRyaXggQmFzZSBWYWx1ZVwifSk7XG52aWV3b2YgZmlsdGVyX3NpemUgPSBJbnB1dHMuc2VsZWN0KFtcIjN4M1wiLCBcIjV4NVwiXSwge3ZhbHVlOiBcIjN4M1wiLCBsYWJlbDogXCJGaWx0ZXIgU2l6ZVwifSk7XG4ifSx7Im1ldGhvZE5hbWUiOiJpbnRlcnByZXRRdWlldCIsInNvdXJjZSI6InNoaW55SW5wdXQoJ2lucHV0X3ZhbCcpIn0seyJtZXRob2ROYW1lIjoiaW50ZXJwcmV0UXVpZXQiLCJzb3VyY2UiOiJzaGlueUlucHV0KCdmaWx0ZXJfc2l6ZScpIn1dfQ==
    </script>
    <script type="module">
    if (window.location.protocol === "file:") { alert("The OJS runtime does not work with file:// URLs. Please use a web server to view this document."); }
    window._ojs.paths.runtimeToDoc = "../../amli";
    window._ojs.paths.runtimeToRoot = "../..";
    window._ojs.paths.docToRoot = "..";
    window._ojs.selfContained = false;
    window._ojs.runtime.interpretFromScriptTags();
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>