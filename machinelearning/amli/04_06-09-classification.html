<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/quarto-contrib/live-runtime/live-runtime.js" type="module"></script>
<link href="../site_libs/quarto-contrib/live-runtime/live-runtime.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.25">

  <meta name="author" content="Imron Rosyadi">
  <title>Machine Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-b0356e9119c1bdfd0db189d130feb51c.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script type="module" src="../site_libs/quarto-ojs/quarto-ojs-runtime.js"></script>
  <link href="../site_libs/quarto-ojs/quarto-ojs.css" rel="stylesheet">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Machine Learning</h1>
  <p class="subtitle">Introduction to Image Classification &amp; Processing</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Imron Rosyadi 
</div>
</div>
</div>

</section>
<section id="introduction-to-image-classification" class="slide level2">
<h2>Introduction to Image Classification</h2>
<p>We’ll explore how machine learning tackles visual data, understanding its unique challenges and opportunities for ECE applications.</p>
<aside class="notes">
<p>We have performed binary and multiclass classification on datasets containing string and numeric values. In this unit we’ll perform classification on images. This transition from abstract data to concrete visual information is crucial for many ECE applications.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="the-nature-of-image-features" class="slide level2">
<h2>The Nature of Image Features</h2>
<p>What makes image classification unique? Each pixel is a feature, represented by numerical values.</p>

<img data-src="04_res/pixels.jpg" class="r-stretch quarto-figure-center"><p class="caption">Pixel Grid Representation</p><aside class="notes">
<p>What makes image classification different from other forms of classification? One major difference is the features. When classifying an image, each pixel is a feature. How are these pixels represented? Understanding this representation is fundamental for ECE students to grasp data acquisition and processing.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="rgb-pixel-values" class="slide level2">
<h2>RGB Pixel Values</h2>
<p>Pixels are often encoded using Red, Green, and Blue (RGB) values. Each color component typically ranges from 0 to 255.</p>

<img data-src="04_res/rgb.png" class="r-stretch quarto-figure-center"><p class="caption">RGB Color Model</p><aside class="notes">
<p>Often pixels are represented as RGB values. These are three numbers that indicate the amount of red, green, and blue in an image. These numbers often range from 0 to 255. This multi-channel representation is key for color images, but it significantly increases feature dimensionality.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="feature-dimensions-a-challenge" class="slide level2">
<h2>Feature Dimensions: A Challenge</h2>
<p>Consider a 1920 × 1920 image. How many features if represented by RGB values?</p>
<aside class="notes">
<p>Let’s take a moment to think about the number of features we are dealing with there. Say we have a 1920 by 1920 pixel image. How many features would we have? This calculation highlights the computational burden of raw pixel data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="feature-dimensions-the-calculation" class="slide level2">
<h2>Feature Dimensions: The Calculation</h2>
<p><span class="math display">\[ 1920 \times 1920 \times 3 = 11,059,200 \]</span></p>
<p>That’s over 11 million input features! Data can be viewed as a 3D tensor: Width × Height × Channels.</p>
<aside class="notes">
<p>That’s over 11 million input features! This is an enormous number for a standard machine learning model to handle directly. We can think of the data as a 3-d matrix (or a tensor) with dimensions 1920 x 1920 x 3.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="interactive-feature-calculator" class="slide level2">
<h2>Interactive Feature Calculator</h2>
<p>Explore how image dimensions and color channels impact the total number of features.</p>
<div class="columns">
<div class="column" style="width:60%;">
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code hidden" id="cb1" data-startfrom="92" data-source-offset="-0"><pre class="sourceCode numberSource js number-lines code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 91;"><span id="cb1-92"><a></a>viewof image_width <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">10</span><span class="op">,</span> <span class="dv">2000</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="dv">1920</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="dv">10</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Image Width (pixels):"</span>})<span class="op">;</span></span>
<span id="cb1-93"><a></a>viewof image_height <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">10</span><span class="op">,</span> <span class="dv">2000</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="dv">1920</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="dv">10</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Image Height (pixels):"</span>})<span class="op">;</span></span>
<span id="cb1-94"><a></a>viewof color_channels <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">select</span>([<span class="st">"1 (Grayscale)"</span><span class="op">,</span> <span class="st">"3 (RGB/BGR)"</span><span class="op">,</span> <span class="st">"4 (RGBA/BGRA)"</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="st">"3 (RGB/BGR)"</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Color Channels:"</span>})<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-2" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-3" data-nodetype="declaration">

</div>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Adjust the sliders and dropdown to see the feature count change dynamically!</p>
</div>
</div>
</div>
</div><div class="column" style="width:40%;">
<div>
<div id="pyodide-1" class="exercise-cell">

</div>
<script type="pyodide-1-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWUsImlucHV0IjpbImltYWdlX3dpZHRoIiwiaW1hZ2VfaGVpZ2h0IiwiY29sb3JfY2hhbm5lbHMiXX0sImNvZGUiOiJcbiMgRXh0cmFjdCBudW1lcmljYWwgY2hhbm5lbCBjb3VudFxuY2hhbm5lbHNfbWFwID0ge1xuICAgIFwiMSAoR3JheXNjYWxlKVwiOiAxLFxuICAgIFwiMyAoUkdCL0JHUilcIjogMyxcbiAgICBcIjQgKFJHQkEvQkdSQSlcIjogNFxufVxubnVtX2NoYW5uZWxzID0gY2hhbm5lbHNfbWFwW2NvbG9yX2NoYW5uZWxzXVxuXG50b3RhbF9mZWF0dXJlcyA9IGltYWdlX3dpZHRoICogaW1hZ2VfaGVpZ2h0ICogbnVtX2NoYW5uZWxzXG5wcmludChmXCJUb3RhbCBGZWF0dXJlczoge3RvdGFsX2ZlYXR1cmVzOix9XCIpIn0=
</script>
</div>
</div></div>
<aside class="notes">
<p>This interactive calculator helps visualize the scale of image data. Students can play with different resolutions and channel counts to directly observe the explosion of features. This ties into computational complexity and hardware considerations in ECE.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="even-more-features" class="slide level2">
<h2>Even More Features</h2>
<p>How many features would a 12 megapixel image (common for phone cameras) have if stored in RGB?</p>
<aside class="notes">
<p>Let’s try another one. How many features would we have for a a 12 megapixel image stored in RGB? This resolution (or greater) is common for mobile phones these days. This is a practical example of real-world image sizes.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="megapixel-feature-count" class="slide level2">
<h2>Megapixel Feature Count</h2>
<p><span class="math display">\[ 12,000,000 \times 3 = 36,000,000 \]</span></p>
<p>An extremely large number of features! This makes training traditional models difficult. Lower resolution images are often used for initial training.</p>
<aside class="notes">
<p>This is an insanely huge number of features. It is extremely difficult for a model to perform well with such a huge number of features directly. That is why you’ll notice that the images we use in this lab are very low resolution. This also motivates the need for specialized architectures like Convolutional Neural Networks (CNNs) that can handle such high dimensionality efficiently.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="grayscale-reducing-complexity" class="slide level2">
<h2>Grayscale: Reducing Complexity</h2>
<p>Grayscale images use a single number (e.g., 0-255 or 0.0-1.0) to represent pixel intensity. This effectively reduces the feature count by a factor of three.</p>

<img data-src="04_res/gray.jpg" class="r-stretch quarto-figure-center"><p class="caption">Grayscale Image</p><aside class="notes">
<p>Another way to reduce the number of features is to convert them to grayscale. Grayscale uses a single number to represent the intensity of color in a pixel, but it doesn’t specify the color. The range of values that you’ll find vary. In this lab we work with one dataset that has a grayscale range of 0 through 255 and another that goes from 0 through 16. Grayscale values might even be in the range from 0.0 through 1.0. For neural networks this smaller range is easier to train on and aids in faster convergence.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="other-image-formats" class="slide level2">
<h2>Other Image Formats</h2>
<p>Beyond RGB and Grayscale, various formats exist, each with specific applications:</p>
<ul>
<li><strong>HSV:</strong> Hue, Saturation, Value (perceptually organized)</li>
<li><strong>HSL:</strong> Hue, Saturation, Light (perceptually organized)</li>
<li><strong>CMYK:</strong> Cyan, Magenta, Yellow, Black (for printing)</li>
<li><strong>BGR:</strong> Blue, Green, Red (common in some computer vision libraries)</li>
</ul>
<aside class="notes">
<p>There are more color models than RGB and grayscale. A few alternatives are listed in this slide. You’ll notice that some, like CMYK, have more values than RGB. BGR, on the other hand, is just RGB in a different order. Understanding these alternatives is important for ECE students who might work with cameras, displays, or different image processing pipelines.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="real-world-image-challenges" class="slide level2">
<h2>Real-World Image Challenges</h2>
<p>Images encountered in real-world scenarios are rarely simple. They often contain multiple objects and background clutter.</p>

<img data-src="04_res/street.jpg" class="r-stretch quarto-figure-center"><p class="caption">Busy Street Scene</p><aside class="notes">
<p>Another interesting aspect of image classification is that images rarely contain just a single item. Take this image, for instance. It contains buildings, cars, people, and more. With an image like this, it can be hard for the model to identify the important features. Sometimes this requires the researcher to pre-process and clean the images. Sometimes it requires additional model tuning. This is where ECE expertise in signal processing and intelligent systems becomes critical.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="lab-exercise-fashion-mnist" class="slide level2">
<h2>Lab Exercise: Fashion-MNIST</h2>
<p>We’ll start with controlled datasets, like Fashion-MNIST: 70,000 grayscale, 28x28 pixel images of single clothing items.</p>

<img data-src="04_res/heel.png" class="r-stretch quarto-figure-center"><p class="caption">Ankle Boot Example</p><aside class="notes">
<p>In the lab for this unit, we’ll work with some very curated datasets. The first dataset we’ll work with is the Fashion-MNIST dataset. The dataset contains 70,000 images of different clothing items. Each image is a grayscale image, only contains one item, and is only 28x28 pixels. This low-resolution and curated nature allows us to focus on the classification task without overwhelming computational resources.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="fashion-mnist-class-labels" class="slide level2">
<h2>Fashion-MNIST Class Labels</h2>
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: left;">Label</th>
<th style="text-align: left;">Class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">0</td>
<td style="text-align: left;">T-shirt/top</td>
</tr>
<tr class="even">
<td style="text-align: left;">1</td>
<td style="text-align: left;">Trouser</td>
</tr>
<tr class="odd">
<td style="text-align: left;">2</td>
<td style="text-align: left;">Pullover</td>
</tr>
<tr class="even">
<td style="text-align: left;">3</td>
<td style="text-align: left;">Dress</td>
</tr>
<tr class="odd">
<td style="text-align: left;">4</td>
<td style="text-align: left;">Coat</td>
</tr>
<tr class="even">
<td style="text-align: left;">5</td>
<td style="text-align: left;">Sandal</td>
</tr>
<tr class="odd">
<td style="text-align: left;">6</td>
<td style="text-align: left;">Shirt</td>
</tr>
<tr class="even">
<td style="text-align: left;">7</td>
<td style="text-align: left;">Sneaker</td>
</tr>
<tr class="odd">
<td style="text-align: left;">8</td>
<td style="text-align: left;">Bag</td>
</tr>
<tr class="even">
<td style="text-align: left;">9</td>
<td style="text-align: left;">Ankle boot</td>
</tr>
</tbody>
</table>
<aside class="notes">
<p>The images in the Fashion-MNIST dataset are labeled with one of the shown classes. The numeric label is the target of the model. This is a multi-class classification problem.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="lab-exercise-mnist-digits" class="slide level2">
<h2>Lab Exercise: MNIST Digits</h2>
<p>Another clean dataset: handwritten digits for classification (0-9). Similar 28x28 grayscale format, one digit per image.</p>

<img data-src="04_res/digits.png" class="r-stretch quarto-figure-center"><p class="caption">Handwritten Digits</p><aside class="notes">
<p>We’ll also work with the MNIST digits dataset. This dataset contains handwritten digits that we’ll classify as 0 through 9. This is also a very clean dataset with one digit per image. Both Fashion-MNIST and MNIST are benchmarks in machine learning for image classification, ideal for introductory labs.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="your-turn" class="slide level2">
<h2>Your Turn!</h2>
<p>Get hands-on experience with image classification datasets.</p>
</section>
<section id="images-and-videos-foundations-for-ece-ml" class="slide level2">
<h2>Images and Videos: Foundations for ECE ML</h2>
<p>Images and video processing are integral to many ML applications in ECE.</p>
<aside class="notes">
<p>In this unit we will move away from machine learning for a bit and instead talk about images and videos. Why images and videos? Image and video processing is actually very common in machine learning applications. Can you think of any examples of images or video processing in machine learning? Some ideas include: facial recognition, classification, converting video to a textual description (story), analyzing video for suspicious movements, disease detection in medical images, crop yield estimates based on aerial photos of fields. The list goes on and on. There are many applications of image and video processing in machine learnings, especially relevant for ECE fields like robotics, embedded systems, and computer vision.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="what-is-an-image-really" class="slide level2">
<h2>What is an Image, Really?</h2>
<p>At its core, an image is a grid of pixels. Each pixel is a sampled data point representing color at a specific location.</p>
<aside class="notes">
<p>Let’s think for a second. What is an image actually? You likely know that an image is a grid of pixels. And each pixel represents a single color point in the image. But how is that pixel encoded? From an ECE perspective, this means understanding analog-to-digital conversion and spatial sampling.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="image-encodings" class="slide level2">
<h2>Image Encodings</h2>
<p>Not all pixels are encoded in the same way. Various encoding schemes exist, influencing data representation and processing.</p>
<aside class="notes">
<p>Not all pixels are encoded in the same way. There are actually quite a few different encodings for images. Choosing the right encoding can significantly impact memory usage, processing speed, and model performance.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="grayscale-vs.-color-images" class="slide level2">
<h2>Grayscale vs.&nbsp;Color Images</h2>
<p>A primary distinction: single-channel grayscale or multi-channel color. This choice affects feature count and information richness.</p>

<img data-src="04_res/color-vs-gray.png" class="r-stretch quarto-figure-center"><p class="caption">Color vs Grayscale Car</p><aside class="notes">
<p>One of the first distinctions to be made is if the image is made up pixels on a “gray scale” or if the image is made from a larger spectrum of colors. In this example you can see that the image on the left has many colors, including some reds while the image on the right is limited to black, white, and the grays in between. What does this mean for the encoding? This decision impacts the computational load and the type of features a model can extract.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="grayscale-pixel-ranges" class="slide level2">
<h2>Grayscale Pixel Ranges</h2>
<p>Grayscale values can range from: - Integers: <code>[0, 255]</code> (8-bit unsigned integer) - Floats: <code>[0.0, 1.0]</code> (normalized for neural networks)</p>

<img data-src="04_res/grayscale.png" class="r-stretch quarto-figure-center"><p class="caption">Grayscale Car</p><aside class="notes">
<p>We’ll start with the simplest format, grayscale. Grayscale images have a single numeric value representing each pixel in the image. But what are those numbers? Typically they range from 0 to 255 if they are integers or 0.0 to 1.0 if they are floating point values. Even with grayscale images, it is important to know the range of values for the input pixels. If the pixels range from 0.0 to 1.0 then many models will be able to more easily train on the images due to better gradient flow properties. If the values are integers between 0 and 255, then it is typically a good idea to divide the values by 255.0 in order to bring them into the 0.0 to 1.0 range that neural networks prefer to help models learn more quickly. This normalization is a common pre-processing step in ECE ML applications.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="interactive-pixel-normalizer" class="slide level2">
<h2>Interactive Pixel Normalizer</h2>
<p>Normalize a pixel value from <code>[0, 255]</code> to <code>[0.0, 1.0]</code> or vice versa.</p>
<div class="columns">
<div class="column" style="width:60%;">
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code hidden" id="cb2" data-startfrom="311" data-source-offset="-0"><pre class="sourceCode numberSource js number-lines code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 310;"><span id="cb2-311"><a></a>viewof pixel_value_0_255 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">0</span><span class="op">,</span> <span class="dv">255</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="dv">128</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="dv">1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Input Pixel Value (0-255):"</span>})<span class="op">;</span></span>
<span id="cb2-312"><a></a>viewof normalization_type <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">select</span>([<span class="st">"Normalize to [0.0, 1.0]"</span><span class="op">,</span> <span class="st">"Scale to [0, 255] (from [0.0, 1.0])"</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="st">"Normalize to [0.0, 1.0]"</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Operation:"</span>})<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-2-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-2-2" data-nodetype="declaration">

</div>
</div>
</div>
</div>
</div><div class="column" style="width:40%;">
<div>
<div id="pyodide-2" class="exercise-cell">

</div>
<script type="pyodide-2-contents">
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
</script>
</div>
</div></div>
<aside class="notes">
<p>Demonstrating pixel normalization is vital for ECE students. It shows how raw sensor data (e.g., 0-255 from an ADC) is often transformed into a range amenable to neural network training, which is typically floating-point values between 0 and 1. This interactive element allows them to see the direct mapping.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="color-image-encodings-a-spectrum" class="slide level2">
<h2>Color Image Encodings: A Spectrum</h2>
<p>From RGB to CMYK, many color spaces exist. Each offers a different way to represent color, impacting applications from display technology to printing.</p>

<img data-src="04_res/color.png" class="r-stretch quarto-figure-center"><p class="caption">Color Car</p><aside class="notes">
<p>Grayscale images typically are one of the two encodings that we mentioned. It is of course important to know which encoding your images are in since a model expects inputs to be on the same scale. However, converting between [0.0, 1.0] and [0, 255] is fairly trivial. Color images introduce an entirely new level of encoding complexity. There are scores of encodings for color images. One of the more common ones that you might have seen is RGB. RGB stands for “red”, “green”, “blue”. With these three colors you can make scores of other colors. With RGB encoding a value, typically between 0 and 255 (though sometimes between 0.0 and 1.0), you can combine the colors to create a rainbow of possibility. With RGB you have three numeric values for each pixel. This triples the size of your inputs! But that is not all. There is RGBA, which takes our red, green, and blue and adds an “alpha” channel which represents the opacity of the pixel. Opacity? Typically when we think of an image, we think about only seeing that image. But what if we put an image under the image we were looking at? If there were no opacity, then we’d only see the topmost image. If there is opacity (think transparency) then we would see a little bit of the underlying image too. The alpha channel, also typically between 0 and 255 or 0.0 and 1.0, manages how “see-through” our pixel is. But why RGB? Why not BRG or GBR or any other ordering? It turns out that there are other orderings, one of the more common being BGR. This was a common encoding in early digital cameras for hardware reasons that aren’t relevant to our topic. Just know that color order can change, and you need to make sure that your inputs for training and predicting have the same encodings. Of course, this begs the question. Are reds, greens, blues, and maybe alphas the only way to encode color? Of course not! There are other schemes such as CMYK, which stands for cyan, magenta, yellow, and black. Encodings aren’t complicated individually, but the number and variety of image encodings can be difficult to work with. Know your inputs!</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="modifying-images-encoding-transformations" class="slide level2">
<h2>Modifying Images: Encoding Transformations</h2>
<p>Python libraries like OpenCV <code>(cv)</code> and NumPy (for array operations) facilitate these transformations.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a></a><span class="im">import</span> cv2</span>
<span id="cb3-2"><a></a></span>
<span id="cb3-3"><a></a><span class="co"># Convert from BGR encoding (common in OpenCV) to RGB </span></span>
<span id="cb3-4"><a></a>image <span class="op">=</span> cv2.cvtColor(image, cv2.COLOR_BGR2RGB)</span>
<span id="cb3-5"><a></a></span>
<span id="cb3-6"><a></a><span class="co"># Normalize pixel values from [0, 255] to [0.0, 1.0]</span></span>
<span id="cb3-7"><a></a>image <span class="op">=</span> image <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb3-8"><a></a></span>
<span id="cb3-9"><a></a><span class="co"># Scale pixel values from [0.0, 1.0] back to [0, 255]</span></span>
<span id="cb3-10"><a></a>image <span class="op">=</span> (image <span class="op">*</span> <span class="dv">255</span>).astype(<span class="bu">int</span>) </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<aside class="notes">
<p>We talked about image encodings and how it is important to feed your model images encoded in the same way. In order to do this you have a few options. If you are converting encodings, you can use the OpenCV <code>cvtColor</code> function to do the conversion for you. If you are scaling the encodings you can use simple Python expressions with NumPy arrays. These are standard operations in many ECE signal processing pipelines.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="modifying-images-resizing" class="slide level2">
<h2>Modifying Images: Resizing</h2>
<p>Resizing images is often necessary to match model input requirements. Care must be taken to avoid distortion.</p>

<img data-src="04_res/resize.png" class="r-stretch quarto-figure-center"><p class="caption">Resized Running Shoe</p><aside class="notes">
<p>But what about scaling/resizing an image? Some models have a scaling layer as an early step, but not all do. Also, you may want more control of your image when you scale it. In the example in this slide we simply scaled the image down to a size that presumptively the model expects. We could have also padded it into a proportional square or rectangle before resizing. Resizing algorithms can involve various interpolation methods (e.g., nearest-neighbor, bilinear, bicubic), which are key considerations in image processing.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="modifying-images-padding" class="slide level2">
<h2>Modifying Images: Padding</h2>
<p>Padding helps maintain aspect ratio and prevents image distortion during resizing. Useful for ensuring all images conform to a fixed input size.</p>

<img data-src="04_res/pad.png" class="r-stretch quarto-figure-center"><p class="caption">Padded Running Shoe</p><aside class="notes">
<p>But you don’t always just want to blindly resize an image. That might distort it. In some cases you’ll want to pad an image with whatever the background color is and then resize it in order to avoid distorting the image. To do this, you must find the number of pixels to add to the height of the image and divide those pixels across the top and bottom of the image. You must do the same for the left and right of the image. This is a common pre-processing technique, especially in image recognition with CNNs.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="modifying-images-centering" class="slide level2">
<h2>Modifying Images: Centering</h2>
<p>Advanced techniques can find and center the focal object. Algorithms like Canny edge detection help pinpoint important features.</p>

<img data-src="04_res/lines.png" class="r-stretch quarto-figure-center"><p class="caption">Car with Edge Detection Lines</p><aside class="notes">
<p>If we have images with a predictable solid background, we can actually perform more complex processing and try to find the center of the focal object. Using algorithms like the Canny algorithm, we can find the key strokes and make our image, hone in on them, and find the center of our image. We can then pad around that. This involves control systems and image analysis, direct applications for ECE students.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="modifying-images-rotation" class="slide level2">
<h2>Modifying Images: Rotation</h2>
<p>Image augmentation often involves various transformations, including rotation, to increase data diversity and model robustness.</p>

<img data-src="04_res/upside-down-color.png" class="r-stretch quarto-figure-center"><p class="caption">Rotated Color Car</p><aside class="notes">
<p>Other image manipulation tricks involve rotations. You can augment an image by spinning it around. Data augmentation like rotation helps improve model generalization by exposing it to varied orientations of objects without needing more real-world data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="other-image-augmentations" class="slide level2">
<h2>Other Image Augmentations</h2>
<p>What other image augmentations can you imagine?</p>
<ul>
<li><strong>Cutting/Mixup:</strong> Splicing parts of images to create new training examples.</li>
<li><strong>Dropout:</strong> Randomly removing portions of an image during training.</li>
<li><strong>Generative Models:</strong> Creating synthetic data to augment existing datasets.</li>
<li><strong>Color Jittering:</strong> Randomly adjusting brightness, contrast, saturation.</li>
</ul>
<aside class="notes">
<p>What are some other image augmentations that you can think of? Some others include: ‘cutting’: where images of the same class are spliced together to increase the size of the training data set. ‘drop out’: where portions of training images are removed during training. Generating fake data based from a model that is then used to train another model. There are many more strategies and none are right for every model. You have to experiment. Data augmentation is a powerful technique to prevent overfitting and improve model performance in data-scarce scenarios, a critical aspect of practical ML deployment in ECE systems.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="understanding-video-data" class="slide level2">
<h2>Understanding Video Data</h2>
<p>For our purposes, video is simply a <strong>sequence of images (frames)</strong>.</p>
<p>Consider: - <strong>Frame Rate (FPS):</strong> Number of images per second (e.g., 30 fps, 60 fps). - <strong>Change between frames:</strong> Varies greatly based on content.</p>
<aside class="notes">
<p>This section also talks about video. We are going to greatly simplify video. If you think about a video feed, it has images, it has sound, it might have captions and other optional features. For our case in this course, video will simply be a series of images and we will treat it as such. Much of the video we watch is 30 “frames per second” (fps) or 60 fps. Think of each of these frames as a still image. Now think about how much changes between frames in 1/30th of a second? Well, it depends on what type of video you are processing. Are you watching the spray from a sneeze? Are you watching ice melt? Video might simply be a “series of images” for our purposes, but you still need to consider what you are modelling. This understanding is key for ECE applications in real-time video processing, such as autonomous vehicles or surveillance.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="your-turn-1" class="slide level2">
<h2>Your Turn</h2>
<p>It’s time to practice working with images and video. This involves multiple labs: 1. Image processing with PIL (Pillow). 2. Image processing with OpenCV. 3. Video processing (extracting frames from video).</p>
<p>These labs build towards a project on classifying items in a video. Have fun!</p>
</section>
<section id="saving-and-loading-models" class="slide level2">
<h2>Saving and Loading Models</h2>
<p>In real-world applications, models are typically saved after training and loaded for deployment. This enables reusability, fine-tuning, and sharing.</p>
<aside class="notes">
<p>So far in this course, we have built models and used them immediately. In practice, you’ll find that you need to save your models and load them for use later. You’ll also find models published online that you can load and start using immediately or use as a warm start for training your own model. This concept of model lifecycle management is crucial for ECE engineers deploying ML solutions on embedded systems or cloud platforms.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="pickling-scikit-learn-models" class="slide level2">
<h2>Pickling scikit-learn Models</h2>
<p>Standard Python’s <code>pickle</code> module can serialize (save) and deserialize (load) scikit-learn models.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Saving a model</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a></a><span class="im">import</span> pickle</span>
<span id="cb4-2"><a></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb4-3"><a></a><span class="co"># Assume 'model' is a trained scikit-learn model</span></span>
<span id="cb4-4"><a></a>model <span class="op">=</span> LogisticRegression() <span class="co"># Placeholder for a real trained model</span></span>
<span id="cb4-5"><a></a><span class="co"># ... train model ...</span></span>
<span id="cb4-6"><a></a></span>
<span id="cb4-7"><a></a>model_file <span class="op">=</span> <span class="st">'my_model.pkl'</span></span>
<span id="cb4-8"><a></a></span>
<span id="cb4-9"><a></a><span class="cf">with</span> <span class="bu">open</span>(model_file, <span class="st">'wb'</span>) <span class="im">as</span> output:</span>
<span id="cb4-10"><a></a>    pickle.dump(model, output, pickle.HIGHEST_PROTOCOL)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div><div class="column" style="width:50%;">
<p><strong>Loading and using a model</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a></a><span class="im">import</span> pickle</span>
<span id="cb5-2"><a></a>model_file <span class="op">=</span> <span class="st">'my_model.pkl'</span></span>
<span id="cb5-3"><a></a></span>
<span id="cb5-4"><a></a><span class="cf">with</span> <span class="bu">open</span>(model_file, <span class="st">'rb'</span>) <span class="im">as</span> <span class="bu">input</span>:</span>
<span id="cb5-5"><a></a>    model_restored <span class="op">=</span> pickle.load(<span class="bu">input</span>)</span>
<span id="cb5-6"><a></a></span>
<span id="cb5-7"><a></a><span class="co"># Example: make a prediction (dummy input)</span></span>
<span id="cb5-8"><a></a><span class="bu">print</span>(model_restored.predict([[<span class="dv">45</span>, <span class="dv">34</span>, <span class="dv">2</span>]])) <span class="co"># Placeholder</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div></div>
<aside class="notes">
<p>For models created using scikit-learn, we can use standard Python pickling to persist and reload the model. <code>pickle.HIGHEST_PROTOCOL</code> ensures compatibility and efficiency. This method is straightforward for simpler models, but attention to library versions is important for compatibility.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="saving-and-loading-keras-models" class="slide level2">
<h2>Saving and Loading Keras Models</h2>
<p>Keras (built on TensorFlow) provides native functions for saving and loading models in its own format or H5.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Saving a Keras model</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb6-2"><a></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb6-3"><a></a><span class="co"># Assume 'model' is a trained Keras model</span></span>
<span id="cb6-4"><a></a>model <span class="op">=</span> keras.Sequential([keras.layers.Dense(<span class="dv">1</span>, input_shape<span class="op">=</span>(<span class="dv">3</span>,))]) <span class="co"># Placeholder</span></span>
<span id="cb6-5"><a></a></span>
<span id="cb6-6"><a></a>tf.keras.models.save_model(</span>
<span id="cb6-7"><a></a>    model, <span class="st">'my_model.tf'</span></span>
<span id="cb6-8"><a></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div><div class="column" style="width:50%;">
<p><strong>Loading a Keras model</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb7-2"><a></a></span>
<span id="cb7-3"><a></a>loaded_model <span class="op">=</span> tf.keras.models.load_model(</span>
<span id="cb7-4"><a></a>    <span class="st">'my_model.tf'</span></span>
<span id="cb7-5"><a></a>)</span>
<span id="cb7-6"><a></a><span class="co"># Example: Summarize the loaded model</span></span>
<span id="cb7-7"><a></a>loaded_model.summary()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div></div>
<aside class="notes">
<p>Keras-based models can be saved and loaded using the <code>save_model</code> and <code>load_model</code> functions. By default the models are in a TensorFlow-specific format, which is optimized for TensorFlow’s execution graphs. However, the models can also be saved as H5, which is another popular file format for storing models, often more portable. This is particularly relevant for ECE engineers deploying Keras models on various hardware accelerators.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="loading-frozen-graphs-historical-context" class="slide level2">
<h2>Loading Frozen Graphs: Historical Context</h2>
<p>“Frozen graphs” refer to TensorFlow 1.x models saved as a single <code>.pb</code> file. They represent a static computation graph with embedded weights.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Loading the Graph Definition</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb8-2"><a></a><span class="im">import</span> os</span>
<span id="cb8-3"><a></a></span>
<span id="cb8-4"><a></a>frozen_graph_path <span class="op">=</span> os.path.join(<span class="st">"path"</span>, <span class="st">'frozen_inference_graph.pb'</span>)</span>
<span id="cb8-5"><a></a></span>
<span id="cb8-6"><a></a><span class="cf">with</span> tf.io.gfile.GFile(frozen_graph_path, <span class="st">"rb"</span>) <span class="im">as</span> f:</span>
<span id="cb8-7"><a></a>    graph_def <span class="op">=</span> tf.compat.v1.GraphDef()</span>
<span id="cb8-8"><a></a>    loaded <span class="op">=</span> graph_def.ParseFromString(f.read())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div><div class="column" style="width:50%;">
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>This process uses <code>tf.compat.v1</code>, indicating compatibility for older TensorFlow 1.x models within a TensorFlow 2.x environment.</p>
</div>
</div>
</div>
</div></div>
<aside class="notes">
<p>There is also the concept of freezing graphs. Some models, such as the one we’re going to use in this lab and in our next project, are distributed in this manner. In order to “unfreeze” a graph, you must first load the graph into a <code>GraphDef</code> object. Notice that this is using a TensorFlow version 1 compatibility layer object. This process is useful for loading models built in TensorFlow version 1, which many legacy systems in ECE might still use.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="loading-frozen-graphs-wrapping-for-tf2" class="slide level2">
<h2>Loading Frozen Graphs: Wrapping for TF2</h2>
<p>TensorFlow 1.x used lazy execution; TensorFlow 2.x uses eager execution. <code>tf.compat.v1.wrap_function</code> bridges this gap for seamless integration.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a></a><span class="kw">def</span> wrap_graph(graph_def, inputs, outputs, print_graph<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb9-2"><a></a>    wrapped <span class="op">=</span> tf.compat.v1.wrap_function(</span>
<span id="cb9-3"><a></a>        <span class="kw">lambda</span>: tf.compat.v1.import_graph_def(graph_def, name<span class="op">=</span><span class="st">""</span>), [])</span>
<span id="cb9-4"><a></a></span>
<span id="cb9-5"><a></a>    <span class="co"># Prune the graph to only include specified inputs and outputs</span></span>
<span id="cb9-6"><a></a>    <span class="cf">return</span> wrapped.prune(</span>
<span id="cb9-7"><a></a>        tf.nest.map_structure(wrapped.graph.as_graph_element, inputs),</span>
<span id="cb9-8"><a></a>        tf.nest.map_structure(wrapped.graph.as_graph_element, outputs))</span>
<span id="cb9-9"><a></a>  </span>
<span id="cb9-10"><a></a><span class="co"># Example usage:</span></span>
<span id="cb9-11"><a></a>model <span class="op">=</span> wrap_graph(graph_def<span class="op">=</span>graph_def,</span>
<span id="cb9-12"><a></a>                   inputs<span class="op">=</span>[<span class="st">"image_tensor:0"</span>], <span class="co"># Name of the input tensor</span></span>
<span id="cb9-13"><a></a>                   outputs<span class="op">=</span>[<span class="st">"detection_boxes:0"</span>, <span class="st">"detection_scores:0"</span>]) <span class="co"># Example output tensors</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<aside class="notes">
<p>The programming models of TensorFlow 1 and 2 are quite a bit different. TensorFlow 1 used lazy execution (defining the graph first, executing later) while TensorFlow 2 uses eager execution (operations execute immediately). In order to bridge the gap in these execution models, we need to wrap our TensorFlow version 1 graph. This wrapping makes the TF1 graph behave like a TF2 callable function, simplifying its use in modern workflows. This is a common challenge for ECE engineers maintaining or upgrading ML infrastructure.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="loading-frozen-graphs-using-the-wrapped-model" class="slide level2">
<h2>Loading Frozen Graphs: Using the Wrapped Model</h2>
<p>Once wrapped, the frozen graph can be used like any other TensorFlow 2.x callable.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a></a><span class="co"># Assuming 'tensor' is a pre-processed image tensor</span></span>
<span id="cb10-2"><a></a>predictions <span class="op">=</span> model(tensor) </span>
<span id="cb10-3"><a></a></span>
<span id="cb10-4"><a></a><span class="co"># 'predictions' would contain the outputs defined earlier, e.g.,</span></span>
<span id="cb10-5"><a></a><span class="co"># predictions["detection_boxes:0"], predictions["detection_scores:0"]</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<aside class="notes">
<p>And now we can use the model as a function. We pass it in tensor objects and get predictions back. This demonstrates how backward compatibility is handled, enabling ECE professionals to leverage older, pre-trained models.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="your-turn-2" class="slide level2">
<h2>Your Turn</h2>
<p>Practice saving and loading models using different frameworks to understand their practical deployment.</p>
</section>
<section id="video-processing-project-identifying-cars-in-a-video" class="slide level2">
<h2>Video Processing Project: Identifying Cars in a Video</h2>
<p>Combine image processing, pre-trained models, and video manipulation to perform object detection in real-time video.</p>

<img data-src="04_res/boxes.png" class="r-stretch quarto-figure-center"><p class="caption">Cars with Bounding Boxes</p><aside class="notes">
<p>We are about to combine many of the skills we’ve learned over the past few units. We will take a video file and a pre-trained model and build bounding boxes around items in each frame of the video. This is a complex project directly applicable to ECE fields like autonomous vehicles, traffic monitoring, and surveillance systems.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="review-image-data-representation" class="slide level2">
<h2>Review: Image Data Representation</h2>
<p>How is image data typically represented and stored? What are the features?</p>
<aside class="notes">
<p><strong>Exercise (5 minutes)</strong> Have students discuss the fact that images are simply pixels. There are different ways to represent pixels, but it’s common to use RGB values that each range from 0 to 255. Each pixel is a feature. Remind them that image data can be challenging to work with because it is often very large. For example, a 12 megapixel image has 36,000,000 features. You may mention that grayscale is one way to cut down on the number of features. This review reinforces the foundational concepts before tackling the project.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="review-python-libraries-for-imagevideo" class="slide level2">
<h2>Review: Python Libraries for Image/Video</h2>
<p>What Python libraries have we used for image and video manipulation?</p>
<aside class="notes">
<p><strong>Exercise (5 minutes)</strong> Have students discuss the labs they completed using PIL (Pillow) and OpenCV. These are industry-standard libraries, and knowing their application is crucial for ECE students in computer vision.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="review-classification-with-image-data" class="slide level2">
<h2>Review: Classification with Image Data</h2>
<p>How have you performed classification tasks with image data?</p>
<aside class="notes">
<p><strong>Exercise (5 minutes)</strong> Have students discuss the fact that they used TensorFlow to train a simple classification model for the Fashion MNIST dataset. This was a prefabricated dataset that was relatively small, so it was possible to train a simple model locally and in a reasonable amount of time. For larger classification tasks, we discussed using pre-built models (specifically those stored in the TensorFlow detection model zoo). This connects past labs to the current project and future complex applications.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="video-processing-project-overview" class="slide level2">
<h2>Video Processing Project Overview</h2>
<p>Process video frame-by-frame, applying a pre-trained object detection model. Visualize detections by drawing bounding boxes around identified objects.</p>

<img data-src="04_res/boxes.png" class="r-stretch quarto-figure-center"><p class="caption">Video Frame with Bounding Boxes</p><aside class="notes">
<p>Now let’s talk about the project for today! Here you can see a single frame of a video showing a road with a bunch of cars. A machine learning model, like the one you will use, has identified many of the cars in the image and labeled them as “car.” One was strangely labeled as a cell phone. Clearly models are not perfect. In this project we will process a video frame-by-frame and create bounding boxes around items found in those images by the third-party model. This real-world scenario demonstrates the power and limitations of current ML models.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="your-turn-3" class="slide level2">
<h2>Your Turn</h2>
<p>This lab will exercise many of your Python and modeling skills. Let’s get started!</p>
</section>
<section id="classification-gone-wrong" class="slide level2">
<h2>Classification Gone Wrong</h2>
<p>Even the most sophisticated models can make mistakes or exhibit biases. Understanding these failures is as important as understanding successes.</p>
<aside class="notes">
<p><strong>Note to facilitator:</strong> This is an activity best used during the Classification lessons in Track 4. It focuses on small group discussions and involves limited setup and facilitation by the instructor. It would best be taught in about 35 minutes, but it could be stretched to 45 minutes depending on the number of groups and how long you want to allow for discussion. This section emphasizes ethical considerations in ML, crucial for ECE students developing responsible AI systems.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="what-do-you-see-first" class="slide level2">
<h2>What Do You See First?</h2>
<p>Perception can be ambiguous. Similarly, ML models can misinterpret data, leading to serious issues.</p>

<img data-src="04_res/classificationgonewrong1.jpeg" class="r-stretch quarto-figure-center"><p class="caption">Duck-Rabbit Ambiguous Illusion</p><aside class="notes">
<p>When you look at this, what do you see first? Raise your hand if it’s a duck you see first. Raise your hand if it’s a rabbit you see first. You’ve likely seen similar ambiguous images, images where one thing jumps out at you at first, and it takes a little longer to see something else. This phenomenon is likely based on humans and their individual schemas. But models can make the same kinds of errors, which can lead to some serious issues. Today we’ll discuss three separate scenarios of classifications that did not go as planned and the impact they caused.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="framing-harmful-classifications" class="slide level2">
<h2>Framing Harmful Classifications</h2>
<ul>
<li>Often, bias is unintentional, not malicious.</li>
<li>However, errors can be difficult to fix once deployed.</li>
<li>Mistakes can have long-lasting, negative repercussions on individuals or groups.</li>
</ul>
<aside class="notes">
<p>Most people don’t set out to make a harmful or biased system; they set out to create a model that does one thing, but it ends up having other effects that could be unintended. This happens frequently. You can search for many, many examples of machine learning projects gone awry, some more serious than others. When these mistakes occur, they’re often very difficult to remedy because they require collecting new data, retraining a model, etc. If your dataset was biased to begin with, then focusing your data collection on only one type of data to remedy your initial issue may lead to another type of bias. These mistakes can have a negative and far-reaching impact on people or entire groups. Understanding the societal impact of ML is an ethical imperative for ECE professionals.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="group-activity" class="slide level2">
<h2>Group Activity</h2>
<p>Divide into groups (1, 2, 3) and each read a corresponding article. Be prepared to discuss the implications of “classification gone wrong.”</p>

<img data-src="04_res/classificationgonewrong2.png" class="r-stretch quarto-figure-center"><p class="caption">Group Collaboration Image</p><aside class="notes">
<p>Let’s break into groups by counting off 1, 2, and 3. Group 1, you will read the corresponding article. The same goes for groups 2 and 3.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="group-discussion-prompts" class="slide level2 scrollable">
<h2>Group Discussion Prompts</h2>
<ol type="1">
<li>Read the article as a group (10 minutes).</li>
<li>Discuss the following questions (15 minutes):
<ul>
<li>What was the original intent of this model/system?</li>
<li>Describe the bias with the model that led to this problem.</li>
<li>What was the cause of this problem?</li>
<li>Are there other mistakes that this model/system could make with this same root problem?</li>
<li>What are the short-term and long-term impacts of this?</li>
</ul></li>
<li>Each group will present to the class (5 minutes/group).</li>
</ol>
<aside class="notes">
<p>Each group will read their article and discuss these questions. After reading and your group discussion, each group will present to the class. You may have one member of your group present or several. When planning this 5-minute presentation, please allow one minute for questions. <strong>Give students a 5-minute warning to begin building a short presentation for the rest of the class.</strong> You can give students easel boards or posters to use as an aid to present or have them just use notes to verbally present. This encourages critical thinking and ethical analysis, vital skills for ECE graduates.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="class-discussion" class="slide level2">
<h2>Class Discussion</h2>
<ul>
<li>What was most surprising or alarming about these examples of bias?</li>
<li>How can situations like this be prevented in future ML system designs?</li>
<li>What is the real-world impact of these mistakes?</li>
<li>What does this mean regarding your responsibility as professionals in ECE ML?</li>
</ul>
<aside class="notes">
<p><strong>Note:</strong> If time permits, open up the discussion to the entire class after each group has presented. Encourage more follow-up questions and/or reflections of the questions listed on the slide. This final discussion brings together the ethical and technical aspects of ML, preparing ECE students for real-world challenges.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>


<script type="pyodide-data">
eyJvcHRpb25zIjp7ImVudiI6eyJQTE9UTFlfUkVOREVSRVIiOiJwbG90bHlfbWltZXR5cGUifSwiaW5kZXhVUkwiOiJodHRwczovL2Nkbi5qc2RlbGl2ci5uZXQvcHlvZGlkZS92MC4yNy4wL2Z1bGwvIn0sInBhY2thZ2VzIjp7InBrZ3MiOlsicHlvZGlkZV9odHRwIiwibWljcm9waXAiLCJpcHl0aG9uIiwibnVtcHkiLCJwbG90bHkiLCJuYmZvcm1hdCJdfX0=
</script>
<script type="ojs-module-contents">
{"contents":[{"cellName":"pyodide-2","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_2 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-2-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-2-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_2 = pyodideOjs.process(_pyodide_editor_2, {pixel_value_0_255, normalization_type});\n"},{"cellName":"pyodide-1","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_1 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-1-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-1-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_1 = pyodideOjs.process(_pyodide_editor_1, {image_width, image_height, color_channels});\n"},{"cellName":"pyodide-prelude","inline":false,"methodName":"interpretQuiet","source":"pyodideOjs = {\n  const {\n    PyodideEvaluator,\n    PyodideEnvironmentManager,\n    setupPython,\n    startPyodideWorker,\n    b64Decode,\n    collapsePath,\n  } = window._exercise_ojs_runtime;\n\n  const statusContainer = document.getElementById(\"exercise-loading-status\");\n  const indicatorContainer = document.getElementById(\"exercise-loading-indicator\");\n  indicatorContainer.classList.remove(\"d-none\");\n\n  let statusText = document.createElement(\"div\")\n  statusText.classList = \"exercise-loading-details\";\n  statusText = statusContainer.appendChild(statusText);\n  statusText.textContent = `Initialise`;\n\n  // Hoist indicator out from final slide when running under reveal\n  const revealStatus = document.querySelector(\".reveal .exercise-loading-indicator\");\n  if (revealStatus) {\n    revealStatus.remove();\n    document.querySelector(\".reveal > .slides\").appendChild(revealStatus);\n  }\n\n  // Make any reveal slides with live cells scrollable\n  document.querySelectorAll(\".reveal .exercise-cell\").forEach((el) => {\n    el.closest('section.slide').classList.add(\"scrollable\");\n  })\n\n  // Pyodide supplemental data and options\n  const dataContent = document.querySelector(`script[type=\\\"pyodide-data\\\"]`).textContent;\n  const data = JSON.parse(b64Decode(dataContent));\n\n  // Grab list of resources to be downloaded\n  const filesContent = document.querySelector(`script[type=\\\"vfs-file\\\"]`).textContent;\n  const files = JSON.parse(b64Decode(filesContent));\n\n  let pyodidePromise = (async () => {\n    statusText.textContent = `Downloading Pyodide`;\n    const pyodide = await startPyodideWorker(data.options);\n\n    statusText.textContent = `Downloading package: micropip`;\n    await pyodide.loadPackage(\"micropip\");\n    const micropip = await pyodide.pyimport(\"micropip\");\n    await data.packages.pkgs.map((pkg) => () => {\n      statusText.textContent = `Downloading package: ${pkg}`;\n      return micropip.install(pkg);\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n    await micropip.destroy();\n\n    // Download and install resources\n    await files.map((file) => async () => {\n      const name = file.substring(file.lastIndexOf('/') + 1);\n      statusText.textContent = `Downloading resource: ${name}`;\n      const response = await fetch(file);\n      if (!response.ok) {\n        throw new Error(`Can't download \\`${file}\\`. Error ${response.status}: \"${response.statusText}\".`);\n      }\n      const data = await response.arrayBuffer();\n\n      // Store URLs in the cwd without any subdirectory structure\n      if (file.includes(\"://\")) {\n        file = name;\n      }\n\n      // Collapse higher directory structure\n      file = collapsePath(file);\n\n      // Create directory tree, ignoring \"directory exists\" VFS errors\n      const parts = file.split('/').slice(0, -1);\n      let path = '';\n      while (parts.length > 0) {\n        path += parts.shift() + '/';\n        try {\n          await pyodide.FS.mkdir(path);\n        } catch (e) {\n          if (e.name !== \"ErrnoError\") throw e;\n          if (e.errno !== 20) {\n            const errorTextPtr = await pyodide._module._strerror(e.errno);\n            const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n            throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n          }\n        }\n      }\n\n      // Write this file to the VFS\n      try {\n        return await pyodide.FS.writeFile(file, new Uint8Array(data));\n      } catch (e) {\n        if (e.name !== \"ErrnoError\") throw e;\n        const errorTextPtr = await pyodide._module._strerror(e.errno);\n        const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n        throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n      }\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n\n    statusText.textContent = `Pyodide environment setup`;\n    await setupPython(pyodide);\n\n    statusText.remove();\n    if (statusContainer.children.length == 0) {\n      statusContainer.parentNode.remove();\n    }\n    return pyodide;\n  })().catch((err) => {\n    statusText.style.color = \"var(--exercise-editor-hl-er, #AD0000)\";\n    statusText.textContent = err.message;\n    //indicatorContainer.querySelector(\".spinner-grow\").classList.add(\"d-none\");\n    throw err;\n  });\n\n  // Keep track of initial OJS block render\n  const renderedOjs = {};\n\n  const process = async (context, inputs) => {\n    const pyodide = await pyodidePromise;\n    const evaluator = new PyodideEvaluator(pyodide, context);\n    await evaluator.process(inputs);\n    return evaluator.container;\n  }\n\n  return {\n    pyodidePromise,\n    renderedOjs,\n    process,\n  };\n}\n"}]}
</script>
<div id="exercise-loading-indicator" class="exercise-loading-indicator d-none d-flex align-items-center gap-2">
<div id="exercise-loading-status" class="d-flex gap-2">

</div>
<div class="spinner-grow spinner-grow-sm">

</div>
</div>
<script type="vfs-file">
W10=
</script>
</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="../qrjs_pics/unsoed_logo.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://imron-slide.vercel.app">irosyadi-2025</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script type="ojs-module-contents">
    eyJjb250ZW50cyI6W3sibWV0aG9kTmFtZSI6ImludGVycHJldCIsImNlbGxOYW1lIjoib2pzLWNlbGwtMSIsImlubGluZSI6ZmFsc2UsInNvdXJjZSI6InZpZXdvZiBpbWFnZV93aWR0aCA9IElucHV0cy5yYW5nZShbMTAsIDIwMDBdLCB7dmFsdWU6IDE5MjAsIHN0ZXA6IDEwLCBsYWJlbDogXCJJbWFnZSBXaWR0aCAocGl4ZWxzKTpcIn0pO1xudmlld29mIGltYWdlX2hlaWdodCA9IElucHV0cy5yYW5nZShbMTAsIDIwMDBdLCB7dmFsdWU6IDE5MjAsIHN0ZXA6IDEwLCBsYWJlbDogXCJJbWFnZSBIZWlnaHQgKHBpeGVscyk6XCJ9KTtcbnZpZXdvZiBjb2xvcl9jaGFubmVscyA9IElucHV0cy5zZWxlY3QoW1wiMSAoR3JheXNjYWxlKVwiLCBcIjMgKFJHQi9CR1IpXCIsIFwiNCAoUkdCQS9CR1JBKVwiXSwge3ZhbHVlOiBcIjMgKFJHQi9CR1IpXCIsIGxhYmVsOiBcIkNvbG9yIENoYW5uZWxzOlwifSk7XG4ifSx7Im1ldGhvZE5hbWUiOiJpbnRlcnByZXQiLCJjZWxsTmFtZSI6Im9qcy1jZWxsLTIiLCJpbmxpbmUiOmZhbHNlLCJzb3VyY2UiOiJ2aWV3b2YgcGl4ZWxfdmFsdWVfMF8yNTUgPSBJbnB1dHMucmFuZ2UoWzAsIDI1NV0sIHt2YWx1ZTogMTI4LCBzdGVwOiAxLCBsYWJlbDogXCJJbnB1dCBQaXhlbCBWYWx1ZSAoMC0yNTUpOlwifSk7XG52aWV3b2Ygbm9ybWFsaXphdGlvbl90eXBlID0gSW5wdXRzLnNlbGVjdChbXCJOb3JtYWxpemUgdG8gWzAuMCwgMS4wXVwiLCBcIlNjYWxlIHRvIFswLCAyNTVdIChmcm9tIFswLjAsIDEuMF0pXCJdLCB7dmFsdWU6IFwiTm9ybWFsaXplIHRvIFswLjAsIDEuMF1cIiwgbGFiZWw6IFwiT3BlcmF0aW9uOlwifSk7XG4ifSx7Im1ldGhvZE5hbWUiOiJpbnRlcnByZXRRdWlldCIsInNvdXJjZSI6InNoaW55SW5wdXQoJ2ltYWdlX3dpZHRoJykifSx7Im1ldGhvZE5hbWUiOiJpbnRlcnByZXRRdWlldCIsInNvdXJjZSI6InNoaW55SW5wdXQoJ2ltYWdlX2hlaWdodCcpIn0seyJtZXRob2ROYW1lIjoiaW50ZXJwcmV0UXVpZXQiLCJzb3VyY2UiOiJzaGlueUlucHV0KCdjb2xvcl9jaGFubmVscycpIn0seyJtZXRob2ROYW1lIjoiaW50ZXJwcmV0UXVpZXQiLCJzb3VyY2UiOiJzaGlueUlucHV0KCdwaXhlbF92YWx1ZV8wXzI1NScpIn0seyJtZXRob2ROYW1lIjoiaW50ZXJwcmV0UXVpZXQiLCJzb3VyY2UiOiJzaGlueUlucHV0KCdub3JtYWxpemF0aW9uX3R5cGUnKSJ9XX0=
    </script>
    <script type="module">
    if (window.location.protocol === "file:") { alert("The OJS runtime does not work with file:// URLs. Please use a web server to view this document."); }
    window._ojs.paths.runtimeToDoc = "../../amli";
    window._ojs.paths.runtimeToRoot = "../..";
    window._ojs.paths.docToRoot = "..";
    window._ojs.selfContained = false;
    window._ojs.runtime.interpretFromScriptTags();
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>