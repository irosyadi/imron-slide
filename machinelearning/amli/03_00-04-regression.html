<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/quarto-contrib/live-runtime/live-runtime.js" type="module"></script>
<link href="../site_libs/quarto-contrib/live-runtime/live-runtime.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.25">

  <meta name="author" content="Imron Rosyadi">
  <title>Machine Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-b0356e9119c1bdfd0db189d130feb51c.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script type="module" src="../site_libs/quarto-ojs/quarto-ojs-runtime.js"></script>
  <link href="../site_libs/quarto-ojs/quarto-ojs.css" rel="stylesheet">
  <meta name="mermaid-theme" content="neutral">
  <script src="../site_libs/quarto-diagram/mermaid.min.js"></script>
  <script src="../site_libs/quarto-diagram/mermaid-init.js"></script>
  <link href="../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Machine Learning</h1>
  <p class="subtitle">03 Regression: Fundamentals, Implementation, and Evaluation</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Imron Rosyadi 
</div>
</div>
</div>

</section>
<section>
<section id="introduction-of-regression" class="title-slide slide level1 center">
<h1>00. Introduction of Regression</h1>

</section>
<section id="regression" class="slide level2">
<h2>Regression</h2>

<img data-src="03_res/regression1.jpg" class="r-stretch quarto-figure-center"><p class="caption">Regression Introduction Image</p><aside class="notes">
<p>Has anyone seen a crime show or heard of an investigation where they used a footprint to determine a suspect’s height? It’s a tactic frequently mentioned in connection to forensics, but does it actually work? Let’s try it out!</p>
<p><em>Exercise (15 minutes):</em> Use your own shoe size and height as the suspect’s, and tell students to keep in mind that US men’s size equals US women’s size - 2. Don’t tell them the suspect’s height, but tell them the suspect’s shoe size and tell them that their task will be to guess the suspect’s height.</p>
<p>Split the students into groups of ~6 each and give each group a sheet of graph paper.</p>
<p>Ask the groups to plot each group member’s shoe size on the x-axis and height on the y-axis. What do they think is the suspect’s height based on the suspect’s shoe size?</p>
<p>Then have the groups share data, so each has a plot of the whole class’s information. Make another guess per group. Does anyone come close?</p>
<p>They should theoretically have better guesses with more data, but shoe size might not actually be well correlated to height, so they might not.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="mathematical-model" class="slide level2">
<h2>Mathematical Model</h2>

<img data-src="03_res/regression2.png" class="r-stretch quarto-figure-center"><p class="caption">Mathematical Model for Linear Regression</p><aside class="notes">
<p>Linear regression has a simple goal: to find a straight line that best fits a set of data. This model is foundational in many ECE applications, such as predicting sensor outputs or analyzing circuit performance based on input parameters.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="features-go-in-targets-come-out" class="slide level2">
<h2>Features Go In, Targets Come Out</h2>

<img data-src="03_res/regression3.png" class="r-stretch quarto-figure-center"><p class="caption">Features and Targets in Machine Learning</p><aside class="notes">
<p>Recall that the equation for a line is <code>y = m * x + b</code>, where <code>x</code> denotes our input variable and <code>y</code> is our output. In the case of machine learning, <code>x</code> represents input features and <code>y</code> represents target outputs. For example, if we were trying to forecast the power consumption of a device from its operating frequency, the operating frequency would be the input feature, and the power consumption would be the target output.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="what-is-the-machine-learning" class="slide level2">
<h2>What is the Machine “Learning?”</h2>

<img data-src="03_res/regression4.png" class="r-stretch quarto-figure-center"><p class="caption">Weights and Biases in Linear Regression</p><aside class="notes">
<p>Using the data, simple linear regression “learns” two values. The first is <code>m</code>, which you may have called “slope” and which we’ll refer to as a “weight / coefficient.” This represents how much a change in the feature value (<code>x</code>) should affect our prediction (<code>y</code>). In other words, a 1 unit increase in <code>x</code> yields an <code>m</code> unit change in <code>y</code>.</p>
<p>The second is <code>b</code>, which you may have called an “intercept” and which we’ll refer to as a “bias.” The bias represents the prediction we would make if our input features are all zero. For example, you may expect an electronic component to draw some baseline current even with zero input signal; this baseline would be analogous to the bias.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="multiple-features" class="slide level2">
<h2>Multiple Features</h2>

<img data-src="03_res/regression5.png" class="r-stretch quarto-figure-center"><p class="caption">Multiple Features in Regression</p><aside class="notes">
<p>Realistically, the performance of an ECE system might depend on several factors (e.g., current drawn depends on voltage, temperature, and load resistance). Now, our model needs to learn three weights (one for each input feature) and one bias.</p>
<p>The concept of weights and biases is important to most machine learning models, even complex neural networks. The model uses data to learn how each input feature affects the output and it learns a bias to linearly shift its predictions to fit the data. This is like shifting a y-intercept.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="machine-learning-process" class="slide level2 scrollable">
<h2>Machine Learning Process</h2>
<ol type="1">
<li>Infer/Predict/Forecast</li>
<li>Calculate Error/Loss/Cost</li>
<li>Train/Learn (Update parameters)</li>
<li>Iterate/Repeat (until some stopping condition)</li>
</ol>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>This iterative cycle is fundamental to how most machine learning algorithms “learn” from data.</p>
</div>
</div>
</div>
<aside class="notes">
<p>But how do we “learn” the weights and biases? Typically, in machine learning we use the following iterative process.</p>
<ul>
<li>Given an input value, we forecast (or guess) the potential target value.</li>
<li>We calculate the error (or difference) between the actual target value and the target we guessed.</li>
<li>We update the weights and biases to produce a guess that is closer to the actual target.</li>
<li>We iterate. That is, we repeat these steps until some stopping condition. (The stopping condition could be a small enough error, or that the error is no longer changing between iterations.)</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="machine-learning-process-flow" class="slide level2">
<h2>Machine Learning Process Flow</h2>
<div class="cell" data-reveal="true" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<pre class="mermaid mermaid-js">graph TD
    A[Start] --&gt; B{Data Input};
    B --&gt; C[Predict Target];
    C --&gt; D[Compare to Actual Target];
    D --&gt; E[Calculate Error/Loss];
    E --&gt; F["Update Model Parameters &lt;br&gt; (Weights &amp; Bias)"];
    F --&gt; G{Stopping Condition Met?};
    G -- No --&gt; C;
    G -- Yes --&gt; H[Model Converged];
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<aside class="notes">
<p>This flowchart visually represents the iterative machine learning process. Data is fed in, a prediction is made, the error is calculated, parameters are updated, and the cycle repeats until the model achieves a satisfactory level of accuracy or reaches a predefined limit.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="predict-the-selling-price-of-a-house" class="slide level2">
<h2>Predict the Selling Price of a House</h2>

<img data-src="03_res/regression12.png" class="r-stretch quarto-figure-center"><p class="caption">House Price Data</p><aside class="notes">
<p>Here are four data points. The feature (x-value) is square footage of a house (in thousands of square feet), and the target (y-value) is the price of the house (in thousands of dollars). We will use this simple example to trace the machine learning process.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="predict-the-price-of-a-house-using-the-machine-learning-process" class="slide level2">
<h2>Predict the Price of a House Using the Machine Learning Process</h2>

<img data-src="03_res/regression13.png" class="r-stretch quarto-figure-center"><p class="caption">House Price Scatter Plot</p><aside class="notes">
<p>Here is a different depiction of the same four data points. It is a simple scatter plot, where the x-axis is the size of a house (our feature), and on the y-axis we have the price of the house (our target).</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="predict-the-price-of-a-house-using-the-machine-learning-process-1" class="slide level2">
<h2>Predict the Price of a House Using the Machine Learning Process</h2>

<img data-src="03_res/regression14.png" class="r-stretch quarto-figure-center"><p class="caption">Initial Guess for Line</p><aside class="notes">
<p>To begin the iterative machine learning process, we make an initial guess at the weights and biases. In this case, we have one weight, <code>m</code>, and one bias, <code>b</code>. Glancing at the data (but not agonizing too hard), we make a simple guess: <code>b = 160</code> and <code>m = 1</code>.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="predict-the-price-of-a-house-using-the-machine-learning-process-2" class="slide level2">
<h2>Predict the Price of a House Using the Machine Learning Process</h2>

<img data-src="03_res/regression15.png" class="r-stretch quarto-figure-center"><p class="caption">Line from Initial Guess</p><aside class="notes">
<p>We now have an initial guess for our model’s parameters, and we use them to create the linear equation <code>y = mx + b</code>. This line represents our initial prediction model.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="predict-the-price-of-a-house-using-the-machine-learning-process-3" class="slide level2">
<h2>Predict the Price of a House Using the Machine Learning Process</h2>

<img data-src="03_res/regression16.png" class="r-stretch quarto-figure-center"><p class="caption">Predicted Values Example</p><aside class="notes">
<p>Now, we use this line to forecast predicted output values. For each point in our training data set <code>(x_k, y_k)</code>, we calculate <code>y_pred = m(x_k) + b</code>. These are the predictions the model makes given its current parameters.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="predict-the-price-of-a-house-using-the-machine-learning-process-4" class="slide level2">
<h2>Predict the Price of a House Using the Machine Learning Process</h2>

<img data-src="03_res/regression17.png" class="r-stretch quarto-figure-center"><p class="caption">Actual vs Forecasted Values</p><aside class="notes">
<p>Here we have the actual target outputs (blue dots) and the forecasted outputs that came from our model (purple points on the line). We’ve completed the infer/predict/forecast step. Notice the vertical distance between the blue dots and the purple points; these represent the errors.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="predict-the-price-of-a-house-using-the-machine-learning-process-5" class="slide level2">
<h2>Predict the Price of a House Using the Machine Learning Process</h2>

<img data-src="03_res/regression18.png" class="r-stretch quarto-figure-center"><p class="caption">Error Calculation Illustration</p><aside class="notes">
<p>Now we move onto step 2, which is to compute error/loss/cost. We calculate the error between the actual target values, and the forecasted values. The metric we use to calculate this error can be simple Euclidean distance, but there are other measures as well. We will talk about error/cost functions in a minute, but for now it’s okay to think of the vertical distance between the actual value and the forecasted value.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="predict-the-price-of-a-house-using-the-machine-learning-process-6" class="slide level2">
<h2>Predict the Price of a House Using the Machine Learning Process</h2>

<img data-src="03_res/regression19.png" class="r-stretch quarto-figure-center"><p class="caption">Updated Line Illustration</p><aside class="notes">
<p>Finally, we update the weight and bias such that we reduce the error. The machine learning algorithm adjusts <code>m</code> and <code>b</code> in a way that minimizes the overall error. This leads to a new, better-fitting line. Now we have new <code>m</code> and <code>b</code> values, and we start at step 1 using these new parameters (predict, calculate error, update, repeat). This iterative refinement continues until the model converges or a stopping condition is met.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="interactive-tuning-our-regression-model" class="slide level2">
<h2>Interactive: Tuning Our Regression Model</h2>
<p>Adjust the weight (slope) and bias (intercept) to fit the data and minimize the Mean Squared Error (MSE).</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code hidden" id="cb1" data-startfrom="223" data-source-offset="-0"><pre class="sourceCode numberSource js number-lines code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 222;"><span id="cb1-223"><a></a>viewof weight_m <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="op">-</span><span class="dv">10</span><span class="op">,</span> <span class="dv">10</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="dv">1</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Weight (m)"</span>})<span class="op">;</span></span>
<span id="cb1-224"><a></a>viewof bias_b <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">100</span><span class="op">,</span> <span class="dv">200</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="dv">160</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="dv">1</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Bias (b)"</span>})<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-2" data-nodetype="declaration">

</div>
</div>
</div>
</div>
<div>
<div id="pyodide-1" class="exercise-cell">

</div>
<script type="pyodide-1-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWUsImlucHV0IjpbIndlaWdodF9tIiwiYmlhc19iIl0sIm1heC1saW5lcyI6MTB9LCJjb2RlIjoiaW1wb3J0IHBsb3RseS5ncmFwaF9vYmplY3RzIGFzIGdvXG5pbXBvcnQgbnVtcHkgYXMgbnBcblxuIyBTYW1wbGUgZGF0YSBmcm9tIGhvdXNpbmcgZXhhbXBsZSAoaW4gdGhvdXNhbmRzKVxueF9kYXRhID0gbnAuYXJyYXkoWzEuMCwgMS41LCAyLjAsIDIuNV0pXG55X2RhdGEgPSBucC5hcnJheShbMTYxLCAxNjIsIDE2MywgMTY0XSlcblxuIyBDYWxjdWxhdGUgcHJlZGljdGVkIHkgdmFsdWVzIGJhc2VkIG9uIGN1cnJlbnQgbSBhbmQgYlxueV9wcmVkID0gd2VpZ2h0X20gKiB4X2RhdGEgKyBiaWFzX2JcblxuIyBDYWxjdWxhdGUgTWVhbiBTcXVhcmVkIEVycm9yXG5tc2UgPSBucC5tZWFuKCh5X2RhdGEgLSB5X3ByZWQpKioyKVxuXG4jIENyZWF0ZSBQbG90bHkgZmlndXJlXG5maWcgPSBnby5GaWd1cmUoKVxuXG4jIEFkZCB0cmFjZSBmb3Igb3JpZ2luYWwgZGF0YSBwb2ludHNcbmZpZy5hZGRfdHJhY2UoZ28uU2NhdHRlcihcbiAgICB4PXhfZGF0YSxcbiAgICB5PXlfZGF0YSxcbiAgICBtb2RlPSdtYXJrZXJzJyxcbiAgICBuYW1lPSdBY3R1YWwgRGF0YScsXG4gICAgbWFya2VyPWRpY3Qoc2l6ZT0xMCwgY29sb3I9J2JsdWUnKVxuKSlcblxuIyBBZGQgdHJhY2UgZm9yIHRoZSByZWdyZXNzaW9uIGxpbmVcbmZpZy5hZGRfdHJhY2UoZ28uU2NhdHRlcihcbiAgICB4PXhfZGF0YSxcbiAgICB5PXlfcHJlZCxcbiAgICBtb2RlPSdsaW5lcycsXG4gICAgbmFtZT1mJ1ByZWRpY3RlZCBMaW5lIChtPXt3ZWlnaHRfbTouMmZ9LCBiPXtiaWFzX2I6LjJmfSknLFxuICAgIGxpbmU9ZGljdCh3aWR0aD0zLCBjb2xvcj0ncmVkJylcbikpXG5cbiMgVXBkYXRlIGxheW91dFxuZmlnLnVwZGF0ZV9sYXlvdXQoXG4gICAgeGF4aXNfdGl0bGU9XCJIb3VzZSBTaXplICgxMDAwIHNxIGZ0KVwiLFxuICAgIHlheGlzX3RpdGxlPVwiSG91c2UgUHJpY2UgKDEwMDBzICQpXCIsXG4gICAgdGl0bGU9ZlwiUmVncmVzc2lvbiBMaW5lIGFuZCBEYXRhIChNU0U6IHttc2U6LjJmfSlcIixcbiAgICBob3Zlcm1vZGU9XCJ4IHVuaWZpZWRcIixcbiAgICB3aWR0aD04MDAsXG4gICAgaGVpZ2h0PTQ1MCxcbiAgICBtYXJnaW49ZGljdChsPTAsIHI9MCwgYj0wLCB0PTApLFxuICAgIHRlbXBsYXRlPVwicGxvdGx5X3doaXRlXCJcbilcblxuIyBEaXNwbGF5IHRoZSBmaWd1cmVcbmZpZyJ9
</script>
</div>
<aside class="notes">
<p>This interactive plot demonstrates how drastically changing the <code>weight (m)</code> and <code>bias (b)</code> affects the regression line and, consequently, the Mean Squared Error. Try to find the optimal <code>m</code> and <code>b</code> that result in the lowest MSE. For this simple dataset, the optimal values are <code>m=2</code> and <code>b=159</code>, which yield an MSE of <code>0.00</code>. This exercise visually reinforces the goal of regression: minimizing the error between predicted and actual values by adjusting model parameters.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="errorlosscost-functions" class="slide level2">
<h2>Error/Loss/Cost Functions</h2>

<img data-src="03_res/regression20.png" class="r-stretch quarto-figure-center"><p class="caption">Common Loss Functions</p><aside class="notes">
<p>Now let’s look at a few common loss/cost functions. Remember we use these functions to determine the error that results from a particular set of weights and biases. These are not the only loss functions, but they are very common.</p>
<ul>
<li><p><strong>L1 Loss (Least Absolute Deviations or LAE):</strong> L1 is resistant to outliers in the data (i.e.&nbsp;robust). If your data has outliers that can be ignored, then L1 is a good choice. If it is important to pay attention to any and all outliers, the method of least squares is a better choice.</p></li>
<li><p><strong>L2 Loss (Least Squares):</strong> Generally, L2 loss is preferred to L1, but when outliers are present in the data, then L2 may not perform well. The reason for this is because we are squaring the difference between the actual target and the predicted target. So if the error is large (in the case of an extreme outlier), then the error function will overcompensate.</p></li>
<li><p><strong>Mean Squared Error (MSE):</strong> MSE is the average of the squared differences between predicted targets and actual targets. Due to squaring, predictions which are far away from actual values are penalized heavily in comparison to less deviated predictions (similar to L2). MSE also has nice mathematical properties which make it easier to calculate gradients, which are used to update the model parameters (weights and biases). It’s widely used in ECE for tasks like signal reconstruction error or system identification.</p></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="housing-example" class="slide level2">
<h2>Housing Example</h2>

<img data-src="03_res/regression21.png" class="r-stretch quarto-figure-center"><p class="caption">Housing Example Data Table</p><aside class="notes">
<p>Let’s practice calculating each of these loss functions for the data in the housing example, with the initial model <code>y = 70x + 100000</code>.</p>
<p><em>Exercise (15 minutes):</em> Have students work in small groups to calculate the loss functions based on the data in the table. It may be helpful to write the loss functions on the board at this point or flip back to the slide with the loss functions and allow students to write them down.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="housing-example-l1-loss" class="slide level2">
<h2>Housing Example (L1 Loss)</h2>

<img data-src="03_res/housingexample2.png" class="r-stretch quarto-figure-center"><p class="caption">L1 Loss Calculation</p><aside class="notes">
<p>The L1 Loss, also known as Mean Absolute Error (MAE), for our initial model <code>y = 7x + 100000</code> for the housing data is 1.0. This is the sum of the absolute differences between the actual and predicted values.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="housing-example-l2-loss" class="slide level2">
<h2>Housing Example (L2 Loss)</h2>

<img data-src="03_res/housingexample3.png" class="r-stretch quarto-figure-center"><p class="caption">L2 Loss Calculation</p><aside class="notes">
<p>The L2 Loss for our initial model <code>y = 7x + 100000</code> for the housing data is …. This is the sum of the squared differences between the actual and predicted values. Notice how squaring penalizes larger errors more significantly.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="housing-example-mse" class="slide level2">
<h2>Housing Example (MSE)</h2>

<img data-src="03_res/housingexample4.png" class="r-stretch quarto-figure-center"><p class="caption">MSE Calculation</p><aside class="notes">
<p>The Mean Squared Error (MSE) for our initial model <code>y = 7x + 100000</code> for the housing data is <strong>…</strong>. This is the average of the squared differences. This metric is a cornerstone for optimizing many ECE-related ML models due to its smooth, differentiable nature.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="computer-vs.-human-jobs" class="slide level2">
<h2>Computer vs.&nbsp;Human Jobs</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>The computer’s job:</strong></p>
<ul>
<li>Start with an arbitrary guess of parameters.</li>
<li>Tweak these parameters to reduce loss.</li>
<li>The less the loss is changing, the less the value should be tweaked.</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>The human’s job:</strong></p>
<ul>
<li>Choose the <strong>learning rate</strong>, a constant value which scales how far we tweak the value during each iteration.</li>
</ul>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>Learning rate is a <strong>hyperparameter</strong> - not a parameter in the actual model.</p>
</div>
</div>
</div>
</div></div>
<aside class="notes">
<p>A hyperparameter is not a parameter in the model. In other words, it’s not a weight or bias. It is a value that is chosen by the machine learning specialist that controls how the algorithm “learns” the model parameters. This is a subtle but important distinction. In ECE terms, it’s like choosing the gain in a control loop: it’s part of the controller’s design, not a variable being optimized by the system itself during operation.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="gradient-descent" class="slide level2">
<h2>Gradient Descent</h2>

<img data-src="03_res/regression9.gif" class="r-stretch quarto-figure-center"><p class="caption">Gradient Descent Illustration</p><aside class="notes">
<p>How does the model “iteratively” update its parameters? We can think of our goal as an optimization problem, where we’d like to optimize (minimize) a loss function. Machine learning models then use an “optimizer,” an algorithm to perform that optimization.</p>
<p>The most common optimizer is gradient descent, where the model starts by picking random values for each parameter. It then changes each in the direction that reduces loss the most. On each iteration or “step,” the model should get closer to the minimal loss until it “converges,” or reaches a point where the loss isn’t changing much between steps. (Usually this is based on some threshold, like the loss function changing by less than 0.001 between steps.) Since this isn’t a closed-form solution, gradient descent isn’t guaranteed to converge to the absolute lowest loss possible. There are more sophisticated optimizers that can sometimes do better.</p>
<p>You can control gradient descent by choosing the learning rate, which determines how much you tweak each parameter on each step. We call this a hyperparameter: a value you can change to change model performance, but one that isn’t “learned” by the model. This is analogous to tuning PID controller gains in an ECE system to optimize stability and response time.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="linear-algebra-notation-for-ymxb" class="slide level2">
<h2>Linear Algebra Notation for <span class="math inline">\(y=mx+b\)</span></h2>

<img data-src="03_res/regression25.png" class="r-stretch quarto-figure-center"><p class="caption">Linear Algebra Notation for Single Feature</p><aside class="notes">
<p>Here <span class="math inline">\(\theta_0\)</span> is the bias and <span class="math inline">\(\theta_1\)</span> is the weight (i.e.&nbsp;<span class="math inline">\(\theta_0 = b\)</span> and <span class="math inline">\(\theta_1 = m\)</span>). This notation is helpful in ECE when dealing with system identification or control, allowing for a compact representation of input-output relationships.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="linear-algebra-notation-for-ymxb-1" class="slide level2">
<h2>Linear Algebra Notation for <span class="math inline">\(y=mx+b\)</span></h2>

<img data-src="03_res/regression26.png" class="r-stretch quarto-figure-center"><p class="caption">Compact Linear Algebra Notation</p><aside class="notes">
<p>Using matrix/vector notation we can rewrite the equation of the line more compactly as <span class="math inline">\(\theta^T X\)</span>. This compact form is efficient for computations, especially when dealing with large datasets or multiple features, and simplifies theoretical analysis.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="multiple-regression-i.e.-multiple-features" class="slide level2">
<h2>Multiple Regression (i.e.&nbsp;Multiple Features)</h2>

<img data-src="03_res/regression27.png" class="r-stretch quarto-figure-center"><p class="caption">Multiple Features in Multiple Regression</p><aside class="notes">
<p>This notational convenience can be extended to regression with multiple features. Recall our example from before where energy is a function of coffee, time of day, and temperature. In ECE, this could be predicting system latency based on processor clock speed, memory usage, and ambient temperature.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="multiple-regression-notation" class="slide level2">
<h2>Multiple Regression Notation</h2>

<img data-src="03_res/regression28.png" class="r-stretch quarto-figure-center"><p class="caption">Multiple Regression Compact Notation</p><aside class="notes">
<p>Again, we can use <span class="math inline">\(\theta^T X\)</span> to represent the regression equation, even with multiple features. Here, <span class="math inline">\(X\)</span> becomes a vector of features for each data point, and <span class="math inline">\(\theta\)</span> becomes a vector of corresponding weights (coefficients) and bias. This generalizes the single-feature case efficiently.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="closed-form-exact-solution" class="slide level2">
<h2>Closed Form, Exact Solution</h2>
<p><span class="math display">\[ \theta = (X^{T} \cdot X)^{-1} \cdot X^{T} \cdot y \]</span></p>
<ul>
<li>Good for small datasets</li>
<li>Finds optimal solution</li>
<li>Can be computationally expensive</li>
<li>Requires an invertible matrix</li>
</ul>
<aside class="notes">
<p>How does the model actually “learn” those values? Through linear algebra, there is an exact (closed form) solution. All you need to do is plug in your <span class="math inline">\(X\)</span> (features) and <span class="math inline">\(y\)</span> (targets) values, and calculate to get your weight and bias values (<span class="math inline">\(\theta\)</span>).</p>
<p><span class="math inline">\(X\)</span> is an <span class="math inline">\(m \times n\)</span> matrix. <span class="math inline">\(X^T X\)</span> is invertible if and only if <span class="math inline">\(m \leq n\)</span> and <code>rank(X) = m</code>. This method is an elegant mathematical solution, typically used when computational resources aren’t a concern or for smaller, well-behaved datasets. In ECE, direct matrix methods are common in signal processing and control for static or well-defined systems.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="batched-data" class="slide level2">
<h2>Batched Data</h2>
<p>Break data into smaller batches.</p>
<ul>
<li>We’ll use a new batch on each learning step.</li>
<li>New hyperparameter <strong>batch size</strong> controls how much data is used for each learning step.</li>
</ul>

<img data-src="03_res/regression10.png" class="r-stretch quarto-figure-center"><p class="caption">Batched Data for Training</p><aside class="notes">
<p>Another important hyperparameter is batch size. While you could perform gradient descent based on your full dataset every step, it may require too much memory, and take longer to converge. To combat both, we split the data into smaller batches. On each step, we’ll use a new batch to update parameters. You can control how large these batches are. This is particularly relevant in ECE for real-time processing or embedded systems with limited memory, where processing data in chunks is necessary.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="hyperparameters-we-care-about" class="slide level2">
<h2>Hyperparameters We Care About</h2>

<img data-src="03_res/regression11.png" class="r-stretch quarto-figure-center"><p class="caption">Hyperparameter Tuning Guidelines</p><aside class="notes">
<p>After setting up a model, you may find you need to perform “hyperparameter tuning” to achieve better results. Different problems work well with different combinations of hyperparameter values. You’ll often need to experiment or “tune” those combinations. Here are some rough guidelines for potential problems with learning rate and batch size that might suggest increasing or decreasing their values. This tuning process is similar to empirically optimizing parameters in hardware or control systems to meet specific performance requirements.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

</section></section>
<section>
<section id="introduction-to-scikit-learn" class="title-slide slide level1 center">
<h1>01. Introduction to scikit-learn</h1>

</section>
<section id="scikit-learn" class="slide level2">
<h2><a href="https://scikit-learn.org">scikit-Learn</a></h2>
<aside class="notes">
<p>Scikit-learn.org is the primary website for the scikit-learn project. Here you will find information pertaining to scikit-learn, including instructions on installation, documentation, and even the project source code.</p>
<p><em>Let’s take a few moments to look around the project website.</em></p>
<p><em>Exercise (10 minutes)</em>: Either navigate to scikit-learn.org on your own computer and present your computer screen to the students (recommended), or ask them to open their laptops to scikit-learn.org. Take the time to point out the following elements on the website: * The classification, regression, clustering, dimensionality reduction, model selection, and preprocessing sections on the main page. These represent core groupings of features provided by scikit-learn. * The top-page navigation with links on how to install the toolkit, documentation, and examples. * The banner on the upper right corner that says “Fork me on GitHub.” This leads to the source code. * When you click the ‘Documentation’ drop-down in the upper navigation, it tells you the current stable version and has a <em>link to ‘All available versions.’</em> Tell students to be sure to check the version of scikit-learn they’re working with once they start the lab. * The ‘Examples’ linked in the top navigation are not just API usage examples; they also contain some interesting machine learning insights.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="datasets" class="slide level2">
<h2>Datasets</h2>
<aside class="notes">
<p>Scikit-learn comes with support for acquiring and generating datasets. The library even comes packaged with some datasets that are commonly used for exploring new models. Let’s look at some of the ways you can acquire data with scikit-learn: loading built-in datasets, fetching external ones, and generating synthetic data. This versatility makes it ideal for testing ECE algorithms against various data contexts without needing complex data acquisition setups initially.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="loading" class="slide level2">
<h2>Loading</h2>
<div>
<div id="pyodide-2" class="exercise-cell">

</div>
<script type="pyodide-2-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWV9LCJjb2RlIjoiZnJvbSBza2xlYXJuLmRhdGFzZXRzIGltcG9ydCBsb2FkX2lyaXNcbmltcG9ydCBwYW5kYXMgYXMgcGRcblxuIyBMb2FkIHRoZSBkYXRhc2V0XG5pcmlzX2RhdGEgPSBsb2FkX2lyaXMoKVxuXG4jIENvbnZlcnQgdG8gRGF0YUZyYW1lXG5kZiA9IHBkLkRhdGFGcmFtZShkYXRhPWlyaXNfZGF0YS5kYXRhLCBjb2x1bW5zPWlyaXNfZGF0YS5mZWF0dXJlX25hbWVzKVxuXG4jIEFkZCB0YXJnZXQgbGFiZWxzXG5kZlsndGFyZ2V0J10gPSBpcmlzX2RhdGEudGFyZ2V0XG5kZlsndGFyZ2V0X25hbWUnXSA9IGRmWyd0YXJnZXQnXS5hcHBseShsYW1iZGEgeDogaXJpc19kYXRhLnRhcmdldF9uYW1lc1t4XSlcblxuIyBEaXNwbGF5IHRoZSBEYXRhRnJhbWVcbnByaW50KGRmLmhlYWQoKSkgICMgU2hvd3MgdGhlIGZpcnN0IDUgcm93cyJ9
</script>
</div>
<aside class="notes">
<p>Scikit-learn has a few datasets that are installed alongside the library. To access these datasets, you can rely on <code>load</code> functions like the <code>load_iris</code> function shown in this example. These are small, classic datasets ideal for quick demos and code verification.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="fetching" class="slide level2">
<h2>Fetching</h2>
<div>
<div id="pyodide-3" class="exercise-cell">

</div>
<script type="pyodide-3-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWV9LCJjb2RlIjoiZnJvbSBza2xlYXJuLmRhdGFzZXRzIGltcG9ydCBmZXRjaF9jYWxpZm9ybmlhX2hvdXNpbmdcbmltcG9ydCBwYW5kYXMgYXMgcGRcblxuIyBMb2FkIHRoZSBkYXRhc2V0XG5ob3VzaW5nX2RhdGEgPSBmZXRjaF9jYWxpZm9ybmlhX2hvdXNpbmcoKVxuXG4jIENvbnZlcnQgdG8gRGF0YUZyYW1lXG5kZiA9IHBkLkRhdGFGcmFtZShkYXRhPWhvdXNpbmdfZGF0YS5kYXRhLCBjb2x1bW5zPWhvdXNpbmdfZGF0YS5mZWF0dXJlX25hbWVzKVxuXG4jIEFkZCB0YXJnZXQgY29sdW1uIChtZWRpYW4gaG91c2UgdmFsdWUpXG5kZlsndGFyZ2V0J10gPSBob3VzaW5nX2RhdGEudGFyZ2V0XG5cbiMgRGlzcGxheSB0aGUgZmlyc3QgZmV3IHJvd3NcbnByaW50KGRmLmhlYWQoKSkifQ==
</script>
</div>
<aside class="notes">
<p>Some common datasets aren’t installed alongside scikit-learn, but the library does know how to access them. For these datasets, we use <code>fetch</code> functions, which pull the dataset down from the internet if necessary. This allows access to larger, more realistic datasets relevant to diverse engineering problems.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="generating" class="slide level2">
<h2>Generating</h2>
<div>
<div id="pyodide-4" class="exercise-cell">

</div>
<script type="pyodide-4-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWV9LCJjb2RlIjoiZnJvbSBza2xlYXJuLmRhdGFzZXRzIGltcG9ydCBtYWtlX3JlZ3Jlc3Npb25cblxuZmVhdHVyZXMsIHRhcmdldHMgPSBtYWtlX3JlZ3Jlc3Npb24obl9zYW1wbGVzPTEwLCBuX2ZlYXR1cmVzPTEsIHJhbmRvbV9zdGF0ZT00MikifQ==
</script>
</div>
<aside class="notes">
<p>Finally, sometimes it makes more sense to generate a dataset from scratch. This is particularly useful in ECE for simulating system responses, creating test cases for control algorithms, or understanding model behavior under idealized conditions. For this, we can use one of the many generator functions provided by scikit-learn, such as <code>make_regression</code> for linear regression tasks.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="bunches" class="slide level2">
<h2>Bunches</h2>
<aside class="notes">
<p><code>Bunch</code> objects are scikit-learn objects that are often used to store datasets. If you find yourself using a <code>load</code> or <code>fetch</code> method, you’ll likely encounter a <code>Bunch</code> object. The lab for this lesson provides more details on <code>Bunch</code> objects and explores the data stored within them. You’ll encounter data that is composed of named features, as well as target values paired with sets of features.</p>
<p>For the most part, we will convert scikit-learn <code>Bunch</code> objects into Pandas DataFrame objects or TensorFlow dataset objects. The aforementioned objects are more easily integrated with the methods and frameworks we will cover in this course.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="estimators" class="slide level2">
<h2>Estimators</h2>
<div>
<div id="pyodide-5" class="exercise-cell">

</div>
<script type="pyodide-5-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWUsIm1heC1saW5lcyI6MTV9LCJjb2RlIjoiZnJvbSBza2xlYXJuLmRhdGFzZXRzIGltcG9ydCBtYWtlX3JlZ3Jlc3Npb25cbmZyb20gc2tsZWFybi5saW5lYXJfbW9kZWwgaW1wb3J0IExpbmVhclJlZ3Jlc3Npb25cbmltcG9ydCBtYXRwbG90bGliLnB5cGxvdCBhcyBwbHRcblxuIyBHZW5lcmF0ZSBzeW50aGV0aWMgcmVncmVzc2lvbiBkYXRhXG5mZWF0dXJlcywgdGFyZ2V0cyA9IG1ha2VfcmVncmVzc2lvbihuX3NhbXBsZXM9MTAwLCBuX2ZlYXR1cmVzPTEsIG5vaXNlPTEwLCByYW5kb21fc3RhdGU9NDIpXG5cbiMgQ3JlYXRlIGFuZCB0cmFpbiB0aGUgbW9kZWxcbnJlZ3Jlc3Npb24gPSBMaW5lYXJSZWdyZXNzaW9uKClcbnJlZ3Jlc3Npb24uZml0KGZlYXR1cmVzLCB0YXJnZXRzKVxuXG4jIE1ha2UgcHJlZGljdGlvbnNcbnByZWRpY3Rpb25zID0gcmVncmVzc2lvbi5wcmVkaWN0KGZlYXR1cmVzKVxuXG4jIFZpc3VhbGl6ZSB0aGUgcmVzdWx0c1xucGx0LnNjYXR0ZXIoZmVhdHVyZXMsIHRhcmdldHMsIGNvbG9yPSdibHVlJywgbGFiZWw9J0FjdHVhbCcpXG5wbHQucGxvdChmZWF0dXJlcywgcHJlZGljdGlvbnMsIGNvbG9yPSdyZWQnLCBsYWJlbD0nUHJlZGljdGVkJylcbnBsdC54bGFiZWwoJ0ZlYXR1cmUnKVxucGx0LnlsYWJlbCgnVGFyZ2V0JylcbnBsdC50aXRsZSgnU2ltcGxlIExpbmVhciBSZWdyZXNzaW9uJylcbnBsdC5sZWdlbmQoKVxucGx0LnNob3coKSJ9
</script>
</div>
<aside class="notes">
<p>Most of the models in scikit-learn are considered <em>estimators</em>. An estimator is expected to implement two methods: <code>fit</code> and <code>predict</code>.</p>
<ul>
<li><code>fit</code> is used to train the model. At a minimum, it is passed the feature data used to train the model. In supervised models, it is also passed the target data.</li>
<li><code>predict</code> is used to get predictions from the model. This method is passed features and returns target predictions.</li>
</ul>
<p>This consistent API design simplifies the process of swapping out different models for a given ECE problem, allowing engineers to quickly experiment with various ML approaches.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="transformers" class="slide level2">
<h2>Transformers</h2>
<div>
<div id="pyodide-6" class="exercise-cell">

</div>
<script type="pyodide-6-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWUsIm1heC1saW5lcyI6MTV9LCJjb2RlIjoiZnJvbSBza2xlYXJuLmRhdGFzZXRzIGltcG9ydCBtYWtlX3JlZ3Jlc3Npb25cbmZyb20gc2tsZWFybi5wcmVwcm9jZXNzaW5nIGltcG9ydCBNaW5NYXhTY2FsZXJcbmltcG9ydCBwYW5kYXMgYXMgcGRcblxuIyBHZW5lcmF0ZSBzeW50aGV0aWMgZGF0YVxuZmVhdHVyZXMsIHRhcmdldHMgPSBtYWtlX3JlZ3Jlc3Npb24obl9zYW1wbGVzPTEwMCwgbl9mZWF0dXJlcz0yLCBub2lzZT01LCByYW5kb21fc3RhdGU9NDIpXG5cbiMgQ29udmVydCB0byBEYXRhRnJhbWUgZm9yIGJldHRlciByZWFkYWJpbGl0eVxuZGYgPSBwZC5EYXRhRnJhbWUoZmVhdHVyZXMsIGNvbHVtbnM9WydGZWF0dXJlIDEnLCAnRmVhdHVyZSAyJ10pXG5cbiMgQXBwbHkgTWluTWF4U2NhbGVyXG50cmFuc2Zvcm1lciA9IE1pbk1heFNjYWxlcigpXG50cmFuc2Zvcm1lci5maXQoZmVhdHVyZXMpXG5mZWF0dXJlc19zY2FsZWQgPSB0cmFuc2Zvcm1lci50cmFuc2Zvcm0oZmVhdHVyZXMpXG5cbiMgQ29udmVydCBzY2FsZWQgZmVhdHVyZXMgdG8gRGF0YUZyYW1lXG5kZl9zY2FsZWQgPSBwZC5EYXRhRnJhbWUoZmVhdHVyZXNfc2NhbGVkLCBjb2x1bW5zPVsnRmVhdHVyZSAxIChzY2FsZWQpJywgJ0ZlYXR1cmUgMiAoc2NhbGVkKSddKVxuXG4jIERpc3BsYXkgb3JpZ2luYWwgYW5kIHNjYWxlZCBmZWF0dXJlcyBzaWRlIGJ5IHNpZGVcbnByaW50KHBkLmNvbmNhdChbZGYsIGRmX3NjYWxlZF0sIGF4aXM9MSkuaGVhZCgpKSJ9
</script>
</div>
<aside class="notes">
<p>In practice, it is rare that you will get perfectly clean data that is ready to feed into your model for training. Most of the time you will need to perform some type of cleaning or preprocessing on the data first. This is especially true for ECE datasets, which might come from various sensors requiring normalization, filtering, or feature scaling.</p>
<p><em>Transformers</em> implement <code>fit</code> and <code>transform</code> methods. The <code>fit</code> method calculates parameters necessary to perform the data transformation (e.g., min/max values for scaling). <code>transform</code> actually applies the transformation. There is a convenience <code>fit_transform</code> method that performs both fitting and transformation in one method call.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="pipelines" class="slide level2">
<h2>Pipelines</h2>
<div>
<div id="pyodide-7" class="exercise-cell">

</div>
<script type="pyodide-7-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWUsIm1heC1saW5lcyI6MTV9LCJjb2RlIjoiZnJvbSBza2xlYXJuLmRhdGFzZXRzIGltcG9ydCBtYWtlX3JlZ3Jlc3Npb25cbmZyb20gc2tsZWFybi5waXBlbGluZSBpbXBvcnQgUGlwZWxpbmVcbmZyb20gc2tsZWFybi5wcmVwcm9jZXNzaW5nIGltcG9ydCBNaW5NYXhTY2FsZXJcbmZyb20gc2tsZWFybi5saW5lYXJfbW9kZWwgaW1wb3J0IExpbmVhclJlZ3Jlc3Npb25cbmltcG9ydCBtYXRwbG90bGliLnB5cGxvdCBhcyBwbHRcblxuIyBHZW5lcmF0ZSBzeW50aGV0aWMgcmVncmVzc2lvbiBkYXRhXG5mZWF0dXJlcywgdGFyZ2V0cyA9IG1ha2VfcmVncmVzc2lvbihcbiAgICBuX3NhbXBsZXM9MTAsIG5fZmVhdHVyZXM9MSwgXG4gICAgcmFuZG9tX3N0YXRlPTQyLCBub2lzZT01LjBcbilcblxuIyBEZWZpbmUgcGlwZWxpbmUgd2l0aCBzY2FsaW5nIGFuZCByZWdyZXNzaW9uXG5waXBlbGluZSA9IFBpcGVsaW5lKFtcbiAgICAoJ3NjYWxlJywgTWluTWF4U2NhbGVyKCkpLFxuICAgICgncmVncmVzc2lvbicsIExpbmVhclJlZ3Jlc3Npb24oKSlcbl0pXG5cbiMgRml0IHRoZSBwaXBlbGluZVxucGlwZWxpbmUuZml0KGZlYXR1cmVzLCB0YXJnZXRzKVxuXG4jIFByZWRpY3QgdXNpbmcgdGhlIHBpcGVsaW5lXG5wcmVkaWN0aW9ucyA9IHBpcGVsaW5lLnByZWRpY3QoZmVhdHVyZXMpXG5cbiMgVmlzdWFsaXplIHJlc3VsdHNcbnBsdC5zY2F0dGVyKGZlYXR1cmVzLCB0YXJnZXRzLCBjb2xvcj0nYmx1ZScsIGxhYmVsPSdBY3R1YWwnKVxucGx0LnBsb3QoZmVhdHVyZXMsIHByZWRpY3Rpb25zLCBjb2xvcj0ncmVkJywgbGFiZWw9J1ByZWRpY3RlZCcpXG5wbHQueGxhYmVsKCdGZWF0dXJlJylcbnBsdC55bGFiZWwoJ1RhcmdldCcpXG5wbHQudGl0bGUoJ1BpcGVsaW5lOiBNaW5NYXhTY2FsZXIgKyBMaW5lYXJSZWdyZXNzaW9uJylcbnBsdC5sZWdlbmQoKVxucGx0LnNob3coKSJ9
</script>
</div>
<aside class="notes">
<p>It isn’t a coincidence that transformers have <code>fit</code> and <code>transform</code> methods and that models have <code>fit</code> methods. The common interface across classes allows scikit-learn to create <em>pipelines</em> for data processing and model building.</p>
<p>A pipeline is simply a series of transformers, often with an estimator at the end. This allows for a streamlined, organized workflow, ensuring consistent data preprocessing steps are applied before training and prediction, which is critical for reproducible results in ECE experiments.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="metrics" class="slide level2">
<h2>Metrics</h2>
<div>
<div id="pyodide-8" class="exercise-cell">

</div>
<script type="pyodide-8-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWV9LCJjb2RlIjoiZnJvbSBza2xlYXJuLm1ldHJpY3MgaW1wb3J0IG1lYW5fc3F1YXJlZF9lcnJvclxuXG5tZWFuX3NxdWFyZWRfZXJyb3IodGFyZ2V0cywgcHJlZGljdGlvbnMpIn0=
</script>
</div>
<aside class="notes">
<p>Scikit-learn also comes with many functions for measuring model performance, located in the <code>metrics</code> package.</p>
<p>In this case, we are calculating the mean squared error. (In the Introduction to Regression lesson, you saw L1, L2, and MSE). These metrics are crucial for quantifying the success of an ML model in ECE applications, e.g., evaluating the accuracy of a power prediction model or the precision of a control algorithm.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="your-turn" class="slide level2">
<h2>Your Turn</h2>
<aside class="notes">
<p>It may be helpful to scroll through the lab associated with this unit and point out to the students the key ideas we covered here. Now, let’s apply these scikit-learn concepts in a practical lab.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

</section></section>
<section>
<section id="linear-regression-with-scikit-learn" class="title-slide slide level1 center">
<h1>02. Linear Regression with scikit-learn</h1>
<aside class="notes">
<p>We’ve learned about linear regression, and we’ve learned about scikit-learn. In this unit we’re going to perform a linear regression using the scikit-learn toolkit, focusing on practical implementation for ECE problems.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="linear-regression" class="slide level2">
<h2>Linear Regression</h2>

<img data-src="03_res/linearregressionwithscikit1.gif" class="r-stretch quarto-figure-center"><p class="caption">Linear Regression Fit Animation</p><aside class="notes">
<p>Remember that linear regression involves fitting a straight line to a dataset. Most of the time, the line doesn’t fit perfectly for all data points. You can see in this illustration, the blue data points, the regression line, and then the red lines between the data points and the regression line. The red lines indicate the “error.” There are many ways to measure this error that we’ll talk about in detail in a future unit. This GIF helps visualize the goal of minimizing these errors.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="scikit-learn-using-linear-algebra" class="slide level2">
<h2>scikit-learn: Using Linear Algebra</h2>
<div>
<div id="pyodide-9" class="exercise-cell">

</div>
<script type="pyodide-9-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWV9LCJjb2RlIjoiZnJvbSBza2xlYXJuLmxpbmVhcl9tb2RlbCBpbXBvcnQgTGluZWFyUmVncmVzc2lvblxuZnJvbSBza2xlYXJuLmRhdGFzZXRzIGltcG9ydCBtYWtlX3JlZ3Jlc3Npb25cblxuIyBHZW5lcmF0ZSBzeW50aGV0aWMgcmVncmVzc2lvbiBkYXRhXG5YLCB5ID0gbWFrZV9yZWdyZXNzaW9uKG5fc2FtcGxlcz0xMDAsIG5fZmVhdHVyZXM9MSwgbm9pc2U9MTAsIHJhbmRvbV9zdGF0ZT00MilcblxuIyBDcmVhdGUgYW5kIGZpdCB0aGUgbW9kZWxcbmxpbl9yZWcgPSBMaW5lYXJSZWdyZXNzaW9uKClcbmxpbl9yZWcuZml0KFgsIHkpXG5cbiMgRGlzcGxheSBjb2VmZmljaWVudHMgYW5kIGludGVyY2VwdFxucHJpbnQoXCJDb2VmZmljaWVudDpcIiwgbGluX3JlZy5jb2VmXylcbnByaW50KFwiSW50ZXJjZXB0OlwiLCBsaW5fcmVnLmludGVyY2VwdF8pIn0=
</script>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>This is not a learning algorithm that iteratively optimizes. It’s a direct, closed-form solution.</p>
</div>
</div>
</div>
<aside class="notes">
<p>To perform linear regression in scikit-learn <em>without iterative learning</em>, we use the <code>LinearRegression</code> class from the <code>linear_model</code> package. As you can see in this example, performing the regression is as simple as instantiating the class and then calling the <code>fit()</code> method. The model then directly calculates the coefficient and intercept for the linear equation using the closed-form solution we discussed earlier. This is efficient for smaller datasets or when <code>GDRegressor</code> convergence is not desired / needed.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="optimizers" class="slide level2 scrollable">
<h2>Optimizers</h2>
<ol type="1">
<li>(Batch) Gradient Descent</li>
<li>Stochastic Gradient Descent (SGD)</li>
<li>Mini-Batch Gradient Descent</li>
</ol>
<aside class="notes">
<p>Recall that our overall goal is to learn parameters that minimize a particular cost/loss function. There are many ways to perform this optimization, but gradient descent is a very popular choice. At a high level, we use the gradient (i.e.&nbsp;the derivative/slope) of the cost function to determine the direction to adjust the parameters. In other words, if we want to get to the bottom of the hill, we walk in the direction of the steepest downward slope.</p>
<ul>
<li><strong>Batch Gradient Descent:</strong> The entire dataset is used to calculate the gradient during each iteration of training. This can be computationally expensive for large datasets.</li>
<li><strong>Stochastic Gradient Descent (SGD):</strong> We randomly choose one data point from our training set to compute the gradient at each iteration (i.e.&nbsp;we use a batch-size of 1). This introduces more noise but can be much faster per iteration.</li>
<li><strong>Mini-Batch Gradient Descent:</strong> A middle ground between batch and stochastic gradient descent, using a fixed number of training samples (greater than 1, but less than the entire dataset) to compute the gradient during each iteration. This combines the speed of SGD with some stability.</li>
</ul>
<p>These various methods mirror iterative optimization techniques used in ECE for adaptive systems or circuit design (e.g., tuning filters).</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="scikit-learn-stochastic-gradient-descent" class="slide level2">
<h2>scikit-learn: Stochastic Gradient Descent</h2>
<div>
<div id="pyodide-10" class="exercise-cell">

</div>
<script type="pyodide-10-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWV9LCJjb2RlIjoiZnJvbSBza2xlYXJuLmxpbmVhcl9tb2RlbCBpbXBvcnQgU0dEUmVncmVzc29yXG5mcm9tIHNrbGVhcm4uZGF0YXNldHMgaW1wb3J0IG1ha2VfcmVncmVzc2lvblxuZnJvbSBza2xlYXJuLnByZXByb2Nlc3NpbmcgaW1wb3J0IFN0YW5kYXJkU2NhbGVyXG5cbiMgR2VuZXJhdGUgc3ludGhldGljIHJlZ3Jlc3Npb24gZGF0YVxuWCwgeSA9IG1ha2VfcmVncmVzc2lvbihuX3NhbXBsZXM9MTAwLCBuX2ZlYXR1cmVzPTEsIG5vaXNlPTE1LCByYW5kb21fc3RhdGU9NDIpXG5cbiMgU2NhbGUgZmVhdHVyZXMgZm9yIGJldHRlciBTR0QgcGVyZm9ybWFuY2VcbnNjYWxlciA9IFN0YW5kYXJkU2NhbGVyKClcblhfc2NhbGVkID0gc2NhbGVyLmZpdF90cmFuc2Zvcm0oWClcblxuIyBDcmVhdGUgYW5kIGZpdCB0aGUgU0dEUmVncmVzc29yXG5zZ2RfcmVnID0gU0dEUmVncmVzc29yKG1heF9pdGVyPTEwMDAsIHRvbD0xZS0zLCByYW5kb21fc3RhdGU9NDIpXG5zZ2RfcmVnLmZpdChYX3NjYWxlZCwgeSlcblxuIyBEaXNwbGF5IGNvZWZmaWNpZW50cyBhbmQgaW50ZXJjZXB0XG5wcmludChcIkNvZWZmaWNpZW50OlwiLCBzZ2RfcmVnLmNvZWZfKVxucHJpbnQoXCJJbnRlcmNlcHQ6XCIsIHNnZF9yZWcuaW50ZXJjZXB0XykifQ==
</script>
</div>
<aside class="notes">
<p>Using stochastic gradient descent looks strikingly similar to using <code>LinearRegression</code>. This is no accident. Scikit-learn’s API is very consistent, simplifying learning and application.</p>
<p>In this example, we load the data into memory, perform SGD (<code>fit</code> method), and then print out the coefficient and intercept.</p>
<p>Note that this might not be the <em>absolute optimal</em> coefficient and intercept; it’s the best one that the SGD algorithm found after running through its epochs and iterations, within its convergence criteria. This is a common characteristic of iterative optimization in ECE.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="scikit-learn-sgd-hyperparameters" class="slide level2">
<h2>scikit-learn: SGD Hyperparameters</h2>
<div>
<div id="pyodide-11" class="exercise-cell">

</div>
<script type="pyodide-11-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWV9LCJjb2RlIjoiZnJvbSBza2xlYXJuLmxpbmVhcl9tb2RlbCBpbXBvcnQgU0dEUmVncmVzc29yXG5cbnNnZF9yZWcgPSBTR0RSZWdyZXNzb3IoXG4gICAgbWF4X2l0ZXI9MTAwMDAwLFxuICAgIG5faXRlcl9ub19jaGFuZ2U9MTAsXG4gICAgdG9sPTFlLTQsXG4gICAgbGVhcm5pbmdfcmF0ZT0nYWRhcHRpdmUnLFxuKVxuc2dkX3JlZy5maXQoWCwgeSlcbnNnZF9yZWcuY29lZl8sIHNnZF9yZWcuaW50ZXJjZXB0XyJ9
</script>
</div>
<aside class="notes">
<p>There aren’t really any hyperparameters to tune for <code>LinearRegression</code> (the closed-form solution). <code>SGDRegressor</code>, however, has many hyperparameters that can be tuned. You can see some of those hyperparameters in use here:</p>
<ul>
<li><code>max_iter</code>: The maximum number of passes over the training data (epochs). Sometimes training more can improve performance.</li>
<li><code>n_iter_no_change</code>: Manages “early stopping.” If loss doesn’t meaningfully improve for this many iterations, training stops.</li>
<li><code>tol</code>: The tolerance for loss improvement to trigger early stopping.</li>
<li><code>learning_rate</code>: Affects how much the weights are adjusted during each step. ‘adaptive’ means it adjusts dynamically. Different learning rate schedules are crucial in ECE control systems for stability or convergence speed.</li>
</ul>
<p>There are many more hyperparameters that can be found in the <code>SGDRegressor</code> documentation, allowing for fine-grained control over the optimization process, similar to tuning intricate analog circuits.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="scikit-learn-sgd-partial_fit" class="slide level2">
<h2>scikit-learn: SGD <code>partial_fit</code></h2>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> SGDRegressor</span>
<span id="cb2-2"><a></a></span>
<span id="cb2-3"><a></a>sgd_reg <span class="op">=</span> SGDRegressor()</span>
<span id="cb2-4"><a></a>sgd_reg.partial_fit(X_1, y_1)</span>
<span id="cb2-5"><a></a>sgd_reg.partial_fit(X_2, y_2)</span>
<span id="cb2-6"><a></a>...</span>
<span id="cb2-7"><a></a>sgd_reg.partial_fit(X_n, y_n)</span>
<span id="cb2-8"><a></a>sgd_reg.coef_, sgd_reg.intercept_</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div>
<div id="pyodide-12" class="exercise-cell">

</div>
<script type="pyodide-12-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWUsIm1heC1saW5lcyI6MTV9LCJjb2RlIjoiZnJvbSBza2xlYXJuLmxpbmVhcl9tb2RlbCBpbXBvcnQgU0dEUmVncmVzc29yXG5mcm9tIHNrbGVhcm4uZGF0YXNldHMgaW1wb3J0IG1ha2VfcmVncmVzc2lvblxuZnJvbSBza2xlYXJuLnByZXByb2Nlc3NpbmcgaW1wb3J0IFN0YW5kYXJkU2NhbGVyXG5pbXBvcnQgbnVtcHkgYXMgbnBcblxuIyBHZW5lcmF0ZSBzeW50aGV0aWMgcmVncmVzc2lvbiBkYXRhXG5YLCB5ID0gbWFrZV9yZWdyZXNzaW9uKG5fc2FtcGxlcz0xMDAsIG5fZmVhdHVyZXM9MSwgbm9pc2U9MTUsIHJhbmRvbV9zdGF0ZT00MilcblxuIyBTY2FsZSBmZWF0dXJlcyBmb3IgYmV0dGVyIFNHRCBwZXJmb3JtYW5jZVxuc2NhbGVyID0gU3RhbmRhcmRTY2FsZXIoKVxuWF9zY2FsZWQgPSBzY2FsZXIuZml0X3RyYW5zZm9ybShYKVxuXG4jIFNwbGl0IGRhdGEgaW50byA1IG1pbmktYmF0Y2hlc1xuWF9iYXRjaGVzID0gbnAuYXJyYXlfc3BsaXQoWF9zY2FsZWQsIDUpXG55X2JhdGNoZXMgPSBucC5hcnJheV9zcGxpdCh5LCA1KVxuXG4jIEluaXRpYWxpemUgU0dEUmVncmVzc29yXG5zZ2RfcmVnID0gU0dEUmVncmVzc29yKG1heF9pdGVyPTEsIHRvbD1Ob25lLCBsZWFybmluZ19yYXRlPSdjb25zdGFudCcsIGV0YTA9MC4wMSwgcmFuZG9tX3N0YXRlPTQyKVxuXG4jIFBlcmZvcm0gaW5jcmVtZW50YWwgdHJhaW5pbmdcbmZvciBYX2JhdGNoLCB5X2JhdGNoIGluIHppcChYX2JhdGNoZXMsIHlfYmF0Y2hlcyk6XG4gICAgc2dkX3JlZy5wYXJ0aWFsX2ZpdChYX2JhdGNoLCB5X2JhdGNoKVxuXG4jIERpc3BsYXkgbGVhcm5lZCBwYXJhbWV0ZXJzXG5wcmludChcIkNvZWZmaWNpZW50OlwiLCBzZ2RfcmVnLmNvZWZfKVxucHJpbnQoXCJJbnRlcmNlcHQ6XCIsIHNnZF9yZWcuaW50ZXJjZXB0XykifQ==
</script>
</div>
<aside class="notes">
<p>Another capability of the <code>SGDRegressor</code> is the ability to <em>partially train</em> the model. This can be useful if your data doesn’t fit into memory, or if you’re working with streaming data. You can continually call <code>partial_fit</code> with subsets of the full dataset.</p>
<p>This is highly relevant in ECE for embedded systems, real-time control, or sensor networks where data arrives continuously and cannot be stored entirely. <code>partial_fit</code> allows the model to learn incrementally.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="loss" class="slide level2">
<h2>Loss</h2>
<h3 id="mean-squared-error">Mean Squared Error</h3>
<p><span class="math display">\[ MSE = \frac{1}{n} \sum_{n=1}^{n}(y_{i} - \hat{y_{i}})^{2} \]</span></p>
<aside class="notes">
<p>We’ll get into loss and different ways to measure it in later units. For this unit we’ll calculate loss using the mean squared error. The mean squared error is the measure of the values that our model predicts vs.&nbsp;what the values actually are. The differences are calculated, squared to get rid of negatives, and summed so that the average squared error can be found. This measure is crucial for quantifying the performance of ECE systems, from signal reconstruction errors to predictive modeling accuracy.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="trainvalidate-test" class="slide level2">
<h2>Train/Validate, Test</h2>

<img data-src="03_res/train_validate_test.png" class="r-stretch quarto-figure-center"><p class="caption">Train, Validate, Test Split</p><aside class="notes">
<p>This lab will also be the first time we’ll need to split our data for model training.</p>
<p>When we train a model, we could use all of the data that we have. When we do that, however, we risk <em>overfitting</em> the model to our data, and we lose the ability to test our model on “new” data it hasn’t seen. The model might become really good at making predictions that look like the data that it has already seen, but really bad at generalizing.</p>
<p>For this reason we typically hold out some of the data and don’t use it to train the model at all. We keep this “test set” of data and use it only to evaluate the model after training has completed. We pass the trained models the features in the test set, get the predictions from the model, and then calculate the difference between the predictions and the actual values.</p>
<p>The concept of a validation set is also important. The validation set is used during training to let the optimizer evaluate the model. The loss calculated with the validation set directly affects decisions the model makes. This practice is akin to rigorous testing of an ECE prototype against a test bench and then against real-world scenarios before deployment.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="trainvalidate-test-validate" class="slide level2">
<h2>Train/Validate, Test, Validate</h2>

<img data-src="03_res/double-validate.png" class="r-stretch quarto-figure-center"><p class="caption">Double Validation Process</p><aside class="notes">
<p>The holdout data story gets more complicated when hyperparameter tuning is involved. When you tune hyperparameters, you’ll still have the same training and validating data available during model fitting. Then you’ll use your test data to see how well the model generalizes. With that said, if you then change hyperparameters and test again, you risk over-tuning hyperparameters to the test data set.</p>
<p>In order to prevent this, many data scientists also keep another holdout dataset called the <em>validation dataset</em> (often called a “holdout validation set” or “dev set” to distinguish from the training-time validation set). This dataset is used for one final check after you have selected hyperparameters and before final deployment. This multi-layered validation ensures the model’s robustness, similar to multiple stages of verification and validation in ECE product lifecycle management.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="your-turn-1" class="slide level2">
<h2>Your Turn</h2>
<aside class="notes">
<p>Let’s now build a few different linear regression models using scikit-learn in our lab exercise. You’ll apply these concepts to practical ECE datasets.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

</section></section>
<section>
<section id="regression-quality" class="title-slide slide level1 center">
<h1>03. Regression Quality</h1>
<aside class="notes">
<p>So far in this course, we have spent some time building and testing regression models. But how can we measure how good these models are? In this unit we will examine a few of the ways we can measure and graph the results of a regression model in order to better understand the quality of the model, which is paramount in ECE for ensuring system reliability and performance.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="coefficient-of-determination-r2" class="slide level2">
<h2>Coefficient of Determination (<span class="math inline">\(R^2\)</span>)</h2>
<p><span class="math display">\[ SS_{res} = \sum_{i}(y_i - \hat{y_i})^2 \]</span> <span class="math display">\[ \bar{y} = \frac{1}{n}\sum_{i=1}^{n}y_{i} \]</span> <span class="math display">\[ SS_{tot} = \sum_{i}(y_{i}-\bar{y})^2 \]</span> <span class="math display">\[ R^{2} = 1 - \frac{SS_{res}}{SS_{tot}} \]</span></p>
<aside class="notes">
<p>The coefficient of determination, denoted <span class="math inline">\(R^2\)</span>, is one of the most important metrics in regression. It tells us how much of the data is “explained” by the model. This is very useful in ECE to understand how well a model captures the underlying physics or behavior of an electronic component or system.</p>
<p>Before we can define the metric itself, we need to define a few other key terms: * <strong>Residual sum of squares (<span class="math inline">\(SS_{res}\)</span>):</strong> The summation of the square of every difference between the target value <span class="math inline">\(y_i\)</span> and the predicted value <span class="math inline">\(\hat{y_i}\)</span>. * <strong>Total sum of squares (<span class="math inline">\(SS_{tot}\)</span>):</strong> The sum of the squared differences between each value <span class="math inline">\(y_i\)</span> and their mean <span class="math inline">\(\bar{y}\)</span>.</p>
<p>Given these, we can calculate <span class="math inline">\(R^2\)</span>. The <span class="math inline">\(R^2\)</span> score measures how well the actual variance from <span class="math inline">\(x\)</span>-values to <span class="math inline">\(y\)</span>-values is represented in the variance between the <span class="math inline">\(x\)</span>-values and the predicted <span class="math inline">\(\hat{y}\)</span>-values.</p>
<p>Typically, this score ranges from 0 to 1, where 0 is bad and 1 is a perfect mapping. However, the score can also be negative. This happens if a simple horizontal line through the mean <span class="math inline">\(\bar{y}\)</span> performs better than your regression, meaning your model is truly terrible and needs serious revision. For values in the range 0-1, interpreting the <span class="math inline">\(R^2\)</span> is more subjective. The closer to 0, the worse your model is at fitting the data. And generally, the closer to 1, the better. But you also don’t want to overfit, which we’ll discuss later.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="mean-squared-error-mse" class="slide level2">
<h2>Mean Squared Error (MSE)</h2>
<p><span class="math display">\[ MSE = \frac{1}{n} \sum_{n=1}^{n}(y_{i} - \hat{y_{i}})^{2} \]</span></p>
<aside class="notes">
<p>The mean squared error is the measure of the values that our model predicts vs.&nbsp;what the values actually are. The differences are calculated, squared to get rid of negatives, and summed so that the average squared error can be found.</p>
<p>What is a good MSE value? The answer really depends how perfect you want your model to be. The values of MSE are a little difficult to interpret, though. Since they are the square of the error, the units don’t match the units of the target in our model (e.g., if predicting voltage in volts, MSE is in volts-squared). This is fine for machines training the model, but it’s nearly impossible for an engineer to intuitively reason about afterward.</p>
<p>There is a solution, though.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="root-mean-squared-error-rmse" class="slide level2">
<h2>Root Mean Squared Error (RMSE)</h2>
<p><span class="math display">\[ RMSE = \sqrt{\frac{1}{n} \sum_{n=1}^{n}(y_{i} - \hat{y_{i}})^{2}} \]</span></p>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>RMSE returns the error in the original units of the target variable for easier interpretation.</p>
</div>
</div>
</div>
<aside class="notes">
<p>The root mean squared error is simply the square root of the mean squared error. It adjusts the units of the error back to the units of the target, which makes model quality much easier to reason about directly.</p>
<p>For instance, if you have a model that predicted the output voltage of a power supply, and it had an RMSE of 0.1V, that would tell an ECE engineer that, on average, the predictions are off by 0.1 volts. If the target voltage was 5V, this error might be acceptable, but if it was 0.5V, it might be too high. This direct interpretability is why RMSE is often preferred when discussing model accuracy in engineering contexts.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="mean-absolute-error-mae" class="slide level2">
<h2>Mean Absolute Error (MAE)</h2>
<p><span class="math display">\[ MAE = \frac{1}{n} \sum_{n=1}^{n}(|y_{i} - \hat{y_{i}}|) \]</span></p>
<aside class="notes">
<p>Mean absolute error is calculated similarly to mean squared error. Instead of using squaring to remove negative values, the absolute value of the difference in actual and predicted values is taken.</p>
<p>A benefit of MAE is that the units remain the same as that of the target, similar to RMSE.</p>
<p>The primary difference in MAE and MSE/RMSE is that MSE/RMSE squares the error, which gives larger differences in value and much higher error scores, effectively giving extra penalty to really bad predictions (outliers). MAE treats all errors linearly. In ECE, choosing between MAE and RMSE often depends on whether large errors should be heavily penalized (RMSE) or if all errors are considered equally important magnitude-wise (MAE).</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="predicted-vs.-actual-plots" class="slide level2">
<h2>Predicted vs.&nbsp;Actual Plots</h2>

<img data-src="03_res/predicted_vs_actual.png" class="r-stretch quarto-figure-center"><p class="caption">Good Predicted vs Actual Plot</p><aside class="notes">
<p>There are numerous ways to visualize regression predictions, but one of the most basic and intuitive is the “predicted vs.&nbsp;actual” plot. Here, we plot the actual target values on one axis and the model’s predicted values for those targets on the other. A perfect model would have all points lying directly on the <code>y=x</code> line.</p>
<p>In this case, the data points scatter pretty evenly around the prediction-to-actual line (i.e., the line y=x, where the actual and prediction are equal). This indicates a good fit without significant bias.</p>
<p>So what does a bad plot look like?</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="predicted-vs.-actual-plots-positive-bias" class="slide level2">
<h2>Predicted vs.&nbsp;Actual Plots (Positive Bias)</h2>

<img data-src="03_res/predicted_vs_actual_positive_bias.png" class="r-stretch quarto-figure-center"><p class="caption">Predicted vs Actual Plot with Positive Bias</p><aside class="notes">
<p>Now we have a situation where there is an obvious bias. All predictions are consistently higher than the actual values (lying below the <code>y=x</code> line, meaning Predicted &gt; Actual). This suggests the model needs to be adjusted to make smaller predictions across the board. Such biases are critical to identify in ECE, as they could indicate systematic errors in sensor calibration or system modeling.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="residual-plots" class="slide level2">
<h2>Residual Plots</h2>

<img data-src="03_res/residual.png" class="r-stretch quarto-figure-center"><p class="caption">Good Residual Plot</p><aside class="notes">
<p>Another helpful visualization tool is plotting the regression <em>residuals</em>. As a reminder, residuals are the difference between the actual values and the predicted values (<code>y - y_pred</code>).</p>
<p>We plot residuals on the y-axis against the predicted values on the x-axis and draw a horizontal line through <code>y=0</code>. Ideally, residuals should be randomly scattered around zero, indicating no systematic pattern in the errors, and that the model has captured the underlying relationship well.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="residual-plots-bias-example" class="slide level2">
<h2>Residual Plots (Bias Example)</h2>

<img data-src="03_res/residual_bias.png" class="r-stretch quarto-figure-center"><p class="caption">Residual Plot with Bias</p><aside class="notes">
<p>Cases where our predictions were too low appear above the zero line (positive residuals). Cases where our predictions were too high appear below the zero line (negative residuals).</p>
<p>In the “predicted vs.&nbsp;actual” section above, we plotted a case where there was a large positive bias in our predictions (model consistently overpredicts). Plotting the same biased data on a residual plot shows all of the residuals falling below the zero line. This clear pattern in residuals indicates a systemic issue with the model, which an ECE engineer would seek to diagnose and correct.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="your-turn-2" class="slide level2">
<h2>Your Turn</h2>
<aside class="notes">
<p>Let’s now move on to the lab portion of the unit. In this lab, you’ll create and interpret various measures and visualizations of regression model quality. This is a practical skill for any ECE professional working with data-driven systems.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

</section></section>
<section>
<section id="polynomial-regression-overfitting" class="title-slide slide level1 center">
<h1>04. Polynomial Regression &amp; Overfitting</h1>

</section>
<section id="outline" class="slide level2 scrollable">
<h2>Outline</h2>
<ol type="1">
<li>Recall <strong>Linear Regression</strong></li>
<li><strong>Polynomial Regression</strong>: What is it and how is it different (or not so different)?</li>
<li>Caution! <strong>Dangers</strong> of polynomial regression!</li>
</ol>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p>Understanding when and how to use polynomial regression is key to modeling complex system behaviors in ECE.</p>
</div>
</div>
</div>
</section>
<section class="slide level2">


<img data-src="03_res/polynomialregression1.gif" class="r-stretch quarto-figure-center"><p class="caption">Linear Regression Fit Animation</p><aside class="notes">
<p>We have seen linear regression and have implemented it using scikit-learn. We found the line of best fit using optimizers such as gradient descent. Though linear regression can be a powerful predictive tool, it isn’t appropriate for all types of regression problems, especially when the underlying relationship isn’t truly linear, as often happens with non-linear components or systems in ECE.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">


<img data-src="03_res/polynomialregression2.png" class="r-stretch quarto-figure-center"><p class="caption">Dataset for Polynomial Regression</p><aside class="notes">
<p>Take a look at this dataset for a few seconds. See if you can find a good spot to place a straight line. Does a linear relationship seem to adequately describe these points? This kind of data might arise from, for example, the efficiency curve of a power amplifier.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">


<img data-src="03_res/polynomialregression3.png" class="r-stretch quarto-figure-center"><p class="caption">Linear Fit on Non-linear Data</p><aside class="notes">
<p>This is the line that the closed-form solution of linear regression would create. It doesn’t look like a very good fit, does it? The R-squared score for this line is actually 0.228, indicating a poor explanation of variance. Clearly, a straight line is insufficient here.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">


<img data-src="03_res/polynomialregression4.png" class="r-stretch quarto-figure-center"><p class="caption">Polynomial Fit on Non-linear Data</p><aside class="notes">
<p>We can see that if we introduce a <em>polynomial line</em> (in this case, a 2nd degree polynomial, a quadratic curve), we get a much better looking fit. The R-squared score is now 0.790, which is significantly better. This demonstrates how polynomial regression can capture non-linear relationships, commonly encountered in the real-world performance characteristics of ECE devices.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="polynomial-equations" class="slide level2">
<h2>Polynomial Equations</h2>

<img data-src="03_res/polynomialregression5.png" class="r-stretch quarto-figure-center"><p class="caption">Examples of Polynomial Equations</p><aside class="notes">
<p>Here are a few examples of polynomial equations. The topmost is the linear equation we are used to (degree 1). The next is commonly called a quadratic equation (degree 2). The third is a cubic equation (degree 3). The number of factors (or the ‘degree’ of the polynomial) you can add to the equation is theoretically unbounded, though you’ll pay a computational expense for polynomials of higher degree and also increase the risk of overfitting, which we’ll discuss soon.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">


<img data-src="03_res/polynomialregression6.png" class="r-stretch quarto-figure-center"><p class="caption">What is the Original Curve?</p><aside class="notes">
<p><em>Exercise (3 minutes):</em> Ask students to turn to a partner next to them and discuss the two questions on the slide: “What kind of curve do you think generated this data?” and “Did the polynomial regression model find the ‘true’ curve?”</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">


<img data-src="03_res/polynomialregression7.png" class="r-stretch quarto-figure-center"><p class="caption">Original Polynomial Curve</p><aside class="notes">
<p>This dataset was actually randomly generated based on a polynomial equation with some random noise. You can see the original polynomial line drawn in green on the chart. Our predicted line in red isn’t perfect by any means. You can see a much sharper slope at both ends, but it is still much better than a straight-line fit. This highlights that models aim to approximate, not necessarily perfectly replicate, the true underlying function.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">


<img data-src="03_res/polynomialregression8.png" class="r-stretch quarto-figure-center"><p class="caption">Polynomial Regression Process</p><aside class="notes">
<p>If we understand the process behind polynomial regression, we can see that it is not so different from linear regression. Ultimately, in order to reduce error in our model, we are still attempting to find the most accurate weights and biases we can find. The underlying optimization principles remain the same; we are just working with a richer set of features.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="polynomial-regression-linear-regression" class="slide level2">
<h2>Polynomial Regression = Linear Regression</h2>
<p>Turn the original polynomial regression problem into a polynomial regression problem with multiple features.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<svg width="960" height="480" viewbox="0.00 0.00 1193.27 65.20" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style="; max-width: none; max-height: none; display: block; margin: auto auto auto auto">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 61.2)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-61.2 1189.27,-61.2 1189.27,4 -4,4"></polygon>
<!-- Input_x -->
<g id="node1" class="node">
<title>Input_x</title>
<polygon fill="lightblue" stroke="black" points="127.94,-46.6 0.02,-46.6 0.02,-10.6 127.94,-10.6 127.94,-46.6"></polygon>
<text text-anchor="middle" x="63.98" y="-24.4" font-family="Times,serif" font-size="14.00">Original Feature (x)</text>
</g>
<!-- Transformation -->
<g id="node2" class="node">
<title>Transformation</title>
<polygon fill="lightblue" stroke="black" points="377.5,-49.2 251.43,-49.2 251.43,-8 377.5,-8 377.5,-49.2"></polygon>
<text text-anchor="middle" x="314.46" y="-32.8" font-family="Times,serif" font-size="14.00">Polynomial Feature</text>
<text text-anchor="middle" x="314.46" y="-16" font-family="Times,serif" font-size="14.00">Transformation</text>
</g>
<!-- Input_x&#45;&gt;Transformation -->
<g id="edge1" class="edge">
<title>Input_x-&gt;Transformation</title>
<path fill="none" stroke="black" d="M128,-28.6C162.42,-28.6 205.19,-28.6 240.99,-28.6"></path>
<polygon fill="black" stroke="black" points="241.37,-32.1 251.37,-28.6 241.37,-25.1 241.37,-32.1"></polygon>
<text text-anchor="middle" x="189.7" y="-32.8" font-family="Times,serif" font-size="14.00">Applies powers</text>
</g>
<!-- Features_X_prime -->
<g id="node3" class="node">
<title>Features_X_prime</title>
<path fill="lightgreen" stroke="black" d="M655.65,-52.08C655.65,-54.96 624.4,-57.3 585.92,-57.3 547.44,-57.3 516.19,-54.96 516.19,-52.08 516.19,-52.08 516.19,-5.12 516.19,-5.12 516.19,-2.24 547.44,0.1 585.92,0.1 624.4,0.1 655.65,-2.24 655.65,-5.12 655.65,-5.12 655.65,-52.08 655.65,-52.08"></path>
<path fill="none" stroke="black" d="M655.65,-52.08C655.65,-49.2 624.4,-46.86 585.92,-46.86 547.44,-46.86 516.19,-49.2 516.19,-52.08"></path>
<text text-anchor="middle" x="585.92" y="-32.8" font-family="Times,serif" font-size="14.00">Transformed Features</text>
<text text-anchor="middle" x="585.92" y="-16" font-family="Times,serif" font-size="14.00">([1, x, x^2, x^3, ...])</text>
</g>
<!-- Transformation&#45;&gt;Features_X_prime -->
<g id="edge2" class="edge">
<title>Transformation-&gt;Features_X_prime</title>
<path fill="none" stroke="black" d="M377.62,-28.6C415.73,-28.6 464.89,-28.6 505.77,-28.6"></path>
<polygon fill="black" stroke="black" points="505.97,-32.1 515.97,-28.6 505.97,-25.1 505.97,-32.1"></polygon>
<text text-anchor="middle" x="446.89" y="-32.8" font-family="Times,serif" font-size="14.00">New Features (X')</text>
</g>
<!-- Linear_Regression_Model -->
<g id="node4" class="node">
<title>Linear_Regression_Model</title>
<polygon fill="lightyellow" stroke="black" points="939.03,-49.2 778.5,-49.2 778.5,-8 939.03,-8 939.03,-49.2"></polygon>
<text text-anchor="middle" x="858.76" y="-32.8" font-family="Times,serif" font-size="14.00">Linear Regression Model</text>
<text text-anchor="middle" x="858.76" y="-16" font-family="Times,serif" font-size="14.00">(w_0, w_1, w_2, w_3, ...)</text>
</g>
<!-- Features_X_prime&#45;&gt;Linear_Regression_Model -->
<g id="edge3" class="edge">
<title>Features_X_prime-&gt;Linear_Regression_Model</title>
<path fill="none" stroke="black" d="M655.61,-28.6C689.85,-28.6 731.67,-28.6 768.38,-28.6"></path>
<polygon fill="black" stroke="black" points="768.58,-32.1 778.58,-28.6 768.58,-25.1 768.58,-32.1"></polygon>
<text text-anchor="middle" x="717.08" y="-32.8" font-family="Times,serif" font-size="14.00">Input for fitting</text>
</g>
<!-- Output_y_hat -->
<g id="node5" class="node">
<title>Output_y_hat</title>
<polygon fill="lightblue" stroke="black" points="1185.18,-46.6 1032.53,-46.6 1032.53,-10.6 1185.18,-10.6 1185.18,-46.6"></polygon>
<text text-anchor="middle" x="1108.86" y="-24.4" font-family="Times,serif" font-size="14.00">Predicted Target (y_hat)</text>
</g>
<!-- Linear_Regression_Model&#45;&gt;Output_y_hat -->
<g id="edge4" class="edge">
<title>Linear_Regression_Model-&gt;Output_y_hat</title>
<path fill="none" stroke="black" d="M938.94,-28.6C965.48,-28.6 995.17,-28.6 1022.18,-28.6"></path>
<polygon fill="black" stroke="black" points="1022.38,-32.1 1032.38,-28.6 1022.38,-25.1 1022.38,-32.1"></polygon>
<text text-anchor="middle" x="985.67" y="-32.8" font-family="Times,serif" font-size="14.00">Prediction</text>
</g>
</g>
</svg>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<aside class="notes">
<p>To find the weights and biases for a polynomial regression model, we recast the problem as a <em>multivariate linear regression problem</em>. This is a key insight: We are not changing the linear regression algorithm itself. Instead, we are changing the input features.</p>
<p>For example, if we want a order-2 polynomial <span class="math inline">\(y = w_0 + w_1 x + w_2 x^2\)</span>, we take our original feature <code>x</code> and create <em>new features</em>: <code>x_1 = x</code> and <code>x_2 = x^2</code>. Then, the problem becomes a linear regression <code>y = w_0 + w_1 x_1 + w_2 x_2</code>. The machine learning algorithm still learns linear weights for these new, transformed features. This technique of feature engineering is very important in ECE for transforming raw sensor data into more meaningful inputs for ML models.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="overfitting" class="slide level2">
<h2>Overfitting</h2>
<aside class="notes">
<p>We will now explore the potential dangers of using higher-order polynomial fits. While they can capture non-linear relationships, they pose a significant risk of ‘overfitting’ the data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">


<img data-src="03_res/polynomialregression9.png" class="r-stretch quarto-figure-center"><p class="caption">Dataset for Overfitting Example</p><aside class="notes">
<p>What could possibly go wrong if we use a polynomial model to fit the following data? Brainstorm some ideas. Consider how the curve behaves between the observed data points, and imagine how it might generalize to new, unseen data points. This is a common pitfall in ECE when modeling complex systems with too many parameters.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">


<img data-src="03_res/polynomialregression10.png" class="r-stretch quarto-figure-center"><p class="caption">Overfitting Demonstration</p><aside class="notes">
<p>Imagine we introduce one more data point to the right (in a region where curve B does not pass through, perhaps above the second valley). It is clear that curve B is <em>not able to generalize</em> to new data points. It fits the existing points almost perfectly, but its behavior in between and outside these points is erratic. This is a classic example of overfitting.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="overfitting-analogy-clothing-fit" class="slide level2">
<h2>Overfitting Analogy: Clothing Fit</h2>

<img data-src="03_res/polynomialregression11.jpg" class="r-stretch quarto-figure-center"><p class="caption">Well-fitting shirt</p><aside class="notes">
<p>Let’s think of overfitting by looking into clothing. Here, we have a person wearing a reasonably well-fitting shirt. It’s comfortable, covers well, and fits a typical body shape, allowing for some variation without being loose. This is like a well-generalized model.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">


<img data-src="03_res/polynomialregression12.jpg" class="r-stretch quarto-figure-center"><p class="caption">Overfitted clothing</p><aside class="notes">
<p>Clothing can be a little more form-fitting, or even custom-tailored. This reduces the number of people that it will fit perfectly. This is an analogy for <em>overfitting</em>. You can think of it like a custom-tailored suit of armor that fits only one person perfectly; it’s extremely specific to the current data but fails to generalize.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">


<img data-src="03_res/polynomialregression13.jpg" class="r-stretch quarto-figure-center"><p class="caption">Underfitted clothing</p><aside class="notes">
<p>There is the other extreme, where you make clothing so loose that just about anyone could wear it. This would be <em>underfitting</em>. The model is too simple to capture the underlying patterns, like a garment that’s too generic to even properly fit its intended wearer.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">


<img data-src="03_res/polynomialregression14.jpg" class="r-stretch quarto-figure-center"><p class="caption">Just right fit</p><aside class="notes">
<p>Most of the time, you probably just want a simple, midsized, unisex t-shirt of a given style. It’s not perfectly tailored but fits a large range of people reasonably well. This represents a model that generalizes well: it captures the essential patterns without being overly specific to the training data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">


<img data-src="03_res/polynomialregression15.png" class="r-stretch quarto-figure-center"><p class="caption">Illustration of Overfitting Regression</p><aside class="notes">
<p>So how does this apply to real data? Here is an illustration of overfitting a regression model. You can see how the regression line perfectly fits almost every data point on the training data, but notice its wild oscillations between points and at the edges. This model will likely have very poor predictive performance for new, unseen data points, even if they closely resemble the training distribution.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="how-do-we-avoid-this" class="slide level2">
<h2>How Do We Avoid This?</h2>
<aside class="notes">
<p>Given the problem of a polynomial fitting data too closely (overfitting), how would you avoid it? <em>Give students some time to throw out some ideas.</em> This is a crucial design challenge in ECE when trying to build robust and generalizable ML systems for signal processing, control, or embedded intelligence.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="avoiding-overfitting" class="slide level2">
<h2>Avoiding Overfitting</h2>
<ul>
<li>Simpler polynomial</li>
<li>More training data</li>
<li>Dropping out some training data (e.g., regularization)</li>
<li>Overfitting penalties (regularization)</li>
</ul>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p><strong>Regularization</strong> is a key technique to manage overfitting by adding penalties to model complexity.</p>
</div>
</div>
</div>
<aside class="notes">
<p>Here are some of the most common ways to avoid overfitting:</p>
<ul>
<li><strong>Using a polynomial function with fewer degrees:</strong> If your model is introducing enough curvature to cross most training data points, then use fewer degrees in your polynomial. This reduces model complexity.</li>
<li><strong>More training data:</strong> As your dataset grows in size, it will likely also grow in diversity and create a model that is less overfitted because it sees more variations. This can be challenging for ECE systems where data collection is expensive.</li>
<li><strong>Dropping out some training data (or features):</strong> Sometimes removing less informative features or artificially simplifying the input can help.</li>
<li><strong>Overfitting penalties (regularization):</strong> There are strategies for adding penalties to the model that make even a high-degree polynomial less likely to overfit. Some common strategies are called Lasso, Ridge, and ElasticNet. We’ll look at each of these more closely, and you’ll experiment with them in your lab exercise. Regularization can be seen as introducing a “cost” for excessive model complexity.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="regularization" class="slide level2">
<h2>Regularization</h2>
<aside class="notes">
<p>Regularization is a method of “shrinking” the coefficients (weights) in the learned equation. This process discourages overly complex models that fit the noise in the training data rather than the underlying signal. There are many types of regularizers, but we will look at the most common ones here: Lasso, Ridge, and ElasticNet. These methods are indispensable in ECE for developing robust models, especially in areas like predictive maintenance or system fault diagnosis where generalization is critical.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="recall-mean-squared-error" class="slide level2">
<h2>Recall: Mean Squared Error</h2>

<img data-src="03_res/polynomialregression20.png" class="r-stretch quarto-figure-center"><p class="caption">Mean Squared Error Formula Breakdown</p><aside class="notes">
<p>As a reminder, this is the equation of a common loss function, the mean squared error. Let’s break down its components in the context of our linear algebra notation:</p>
<ul>
<li><strong>Line 1:</strong> <code>(y_true - y_pred)</code> is the error between the true target and the predicted target from the model.</li>
<li><strong>Line 2:</strong> Recall that <code>y_pred</code> came from a linear regression equation, which can be written in matrix notation as <code>X * theta</code>.</li>
<li><strong>Line 3:</strong> Completing the matrix multiplication and writing the multivariate regression formula using detailed notation:
<ul>
<li><code>n</code> = number of rows in the training data</li>
<li><code>p</code> = number of coefficients in the equation (also number of features + bias term)</li>
<li><code>y_true</code> = true target value</li>
<li><code>theta_0</code> = intercept (bias)</li>
<li><code>theta_j</code> = coefficients (weights) of the polynomial equation</li>
<li><code>x_i</code> = feature values</li>
</ul></li>
</ul>
<p>This foundation helps us understand how regularization will modify this loss function.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="lasso-l1-regularization" class="slide level2">
<h2>Lasso (L1) Regularization</h2>

<img data-src="03_res/polynomialregression17.png" class="r-stretch quarto-figure-center"><p class="caption">Lasso (L1) Regularization Formula</p><aside class="notes">
<p>What does it mean to shrink coefficients? It effectively means to increase the value of the loss function as the coefficients get larger.</p>
<p>Lasso is L1 regression. This means that it adds the sum of the <em>absolute values</em> of the coefficients (multiplied by a regularization parameter <span class="math inline">\(\lambda\)</span>) to the original MSE loss function. We can see that by adding <code>lambda * |theta_j|</code> (a positive number), the cost function is always slightly larger than the regular MSE loss function for any non-zero <code>theta_j</code>. This regularization term forces the model to make <code>theta_j</code> values smaller.</p>
<p>Smaller coefficients make the model “more linear” or less sensitive to specific features. For example, in a high-degree polynomial, if <code>theta_2</code> and <code>theta_3</code> become very small, their terms have less impact on the curve.</p>
<p>LAGSO is an acronym for “Least Absolute Shrinkage and Selection Operator.”</p>
<p>Due to the absolute value function, L1 regularization has a unique property: it can drive some coefficients <em>exactly to zero</em>. This means L1 regularization can perform <em>feature selection</em>, effectively identifying and ignoring less important features. This is highly useful in ECE for reducing model complexity and identifying critical system parameters.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="ridge-l2-regularization" class="slide level2">
<h2>Ridge (L2) Regularization</h2>

<img data-src="03_res/polynomialregression18.png" class="r-stretch quarto-figure-center"><p class="caption">Ridge (L2) Regularization Formula</p><aside class="notes">
<p>Ridge regularization looks similar to Lasso, but instead of appending the sum of absolute values of coefficients to the loss function, it appends the sum of their <em>squares</em>. This is also multiplied by a regularization parameter <span class="math inline">\(\lambda\)</span>.</p>
<p>Ridge regularization also shrinks coefficients towards zero, but it tends to shrink them proportionally. Unlike Lasso, L2 regularization rarely drives coefficients exactly to zero; it prefers to reduce their magnitude without fully eliminating them.</p>
<p>Since the L2 norm (sum of squares) is differentiable, problems using this method can be solved efficiently by gradient descent, making it computationally attractive. In ECE, Ridge regression helps stabilize models in the presence of multicollinearity (highly correlated features), which is common in sensor data fusion.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="elasticnet-l1-l2" class="slide level2">
<h2>ElasticNet (L1 + L2)</h2>
<aside class="notes">
<p>ElasticNet is a regularization technique that combines both L1 (Lasso) and L2 (Ridge) penalties. It includes both the sum of the absolute values of coefficients and the sum of their squares in the loss function.</p>
<p>This combination allows ElasticNet to benefit from both methods: it can perform feature selection (like Lasso) by shrinking some coefficients to zero, while also handling groups of correlated features gracefully (like Ridge). This makes ElasticNet a robust choice in many real-world ECE applications where datasets might have many features, some of which are correlated, and model sparsity is desired.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="which-regularization-is-best" class="slide level2">
<h2>Which Regularization Is Best?</h2>
<div class="callout callout-caution callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Caution</strong></p>
</div>
<div class="callout-content">
<p>The choice of regularization often depends on the specific dataset and problem. Experimentation is key!</p>
</div>
</div>
</div>
<aside class="notes">
<p>It depends on the specific characteristics of your data and your modeling goals:</p>
<ul>
<li><strong>L1 regularization (Lasso):</strong>
<ul>
<li>Can drive coefficients to zero, and it tends to produce a <em>sparse model</em> (i.e., many coefficients are zero).</li>
<li>Its unique ability to eliminate irrelevant features makes it useful for <em>feature selection</em>.</li>
</ul></li>
<li><strong>L2 regularization (Ridge):</strong>
<ul>
<li>Is less likely to drive coefficients exactly to zero; it shrinks them proportionally.</li>
<li>Instead, it tends to produce a <em>more dense model</em> (most coefficients are small but non-zero).</li>
<li>Effective at handling <em>multicollinearity</em> (highly correlated features).</li>
</ul></li>
<li><strong>ElasticNet:</strong>
<ul>
<li>As a compromise between L1 and L2, it often works well in situations where there are multiple correlated features or when feature selection is important.</li>
</ul></li>
</ul>
<p>It is probably worth experimenting with each method to see which works best for your particular model, considering performance, interpretability, and computational resources, especially in diverse ECE problem domains.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="your-turn-3" class="slide level2">
<h2>Your Turn</h2>
<aside class="notes">
<p>Now that we’ve covered polynomial regression and regularization techniques, it’s time to apply these concepts in a hands-on lab. You’ll experiment with fitting complex curves and battling overfitting, essential skills for robust system modeling in ECE.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>


<script type="pyodide-data">
eyJvcHRpb25zIjp7ImVudiI6eyJQTE9UTFlfUkVOREVSRVIiOiJwbG90bHlfbWltZXR5cGUifSwiaW5kZXhVUkwiOiJodHRwczovL2Nkbi5qc2RlbGl2ci5uZXQvcHlvZGlkZS92MC4yNy4wL2Z1bGwvIn0sInBhY2thZ2VzIjp7InBrZ3MiOlsicHlvZGlkZV9odHRwIiwibWljcm9waXAiLCJpcHl0aG9uIiwibnVtcHkiLCJwbG90bHkiLCJuYmZvcm1hdCIsInNjaWtpdC1sZWFybiIsInBhbmRhcyIsIm1hdHBsb3RsaWIiXX19
</script>
<script type="ojs-module-contents">
{"contents":[{"cellName":"pyodide-12","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_12 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-12-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-12-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_12 = pyodideOjs.process(_pyodide_editor_12, {});\n"},{"cellName":"pyodide-11","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_11 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-11-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-11-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_11 = pyodideOjs.process(_pyodide_editor_11, {});\n"},{"cellName":"pyodide-10","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_10 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-10-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-10-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_10 = pyodideOjs.process(_pyodide_editor_10, {});\n"},{"cellName":"pyodide-9","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_9 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-9-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-9-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_9 = pyodideOjs.process(_pyodide_editor_9, {});\n"},{"cellName":"pyodide-8","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_8 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-8-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-8-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_8 = pyodideOjs.process(_pyodide_editor_8, {});\n"},{"cellName":"pyodide-7","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_7 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-7-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-7-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_7 = pyodideOjs.process(_pyodide_editor_7, {});\n"},{"cellName":"pyodide-6","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_6 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-6-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-6-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_6 = pyodideOjs.process(_pyodide_editor_6, {});\n"},{"cellName":"pyodide-5","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_5 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-5-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-5-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_5 = pyodideOjs.process(_pyodide_editor_5, {});\n"},{"cellName":"pyodide-4","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_4 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-4-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-4-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_4 = pyodideOjs.process(_pyodide_editor_4, {});\n"},{"cellName":"pyodide-3","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_3 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-3-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-3-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_3 = pyodideOjs.process(_pyodide_editor_3, {});\n"},{"cellName":"pyodide-2","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_2 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-2-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-2-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_2 = pyodideOjs.process(_pyodide_editor_2, {});\n"},{"cellName":"pyodide-1","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_1 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-1-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-1-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_1 = pyodideOjs.process(_pyodide_editor_1, {weight_m, bias_b});\n"},{"cellName":"pyodide-prelude","inline":false,"methodName":"interpretQuiet","source":"pyodideOjs = {\n  const {\n    PyodideEvaluator,\n    PyodideEnvironmentManager,\n    setupPython,\n    startPyodideWorker,\n    b64Decode,\n    collapsePath,\n  } = window._exercise_ojs_runtime;\n\n  const statusContainer = document.getElementById(\"exercise-loading-status\");\n  const indicatorContainer = document.getElementById(\"exercise-loading-indicator\");\n  indicatorContainer.classList.remove(\"d-none\");\n\n  let statusText = document.createElement(\"div\")\n  statusText.classList = \"exercise-loading-details\";\n  statusText = statusContainer.appendChild(statusText);\n  statusText.textContent = `Initialise`;\n\n  // Hoist indicator out from final slide when running under reveal\n  const revealStatus = document.querySelector(\".reveal .exercise-loading-indicator\");\n  if (revealStatus) {\n    revealStatus.remove();\n    document.querySelector(\".reveal > .slides\").appendChild(revealStatus);\n  }\n\n  // Make any reveal slides with live cells scrollable\n  document.querySelectorAll(\".reveal .exercise-cell\").forEach((el) => {\n    el.closest('section.slide').classList.add(\"scrollable\");\n  })\n\n  // Pyodide supplemental data and options\n  const dataContent = document.querySelector(`script[type=\\\"pyodide-data\\\"]`).textContent;\n  const data = JSON.parse(b64Decode(dataContent));\n\n  // Grab list of resources to be downloaded\n  const filesContent = document.querySelector(`script[type=\\\"vfs-file\\\"]`).textContent;\n  const files = JSON.parse(b64Decode(filesContent));\n\n  let pyodidePromise = (async () => {\n    statusText.textContent = `Downloading Pyodide`;\n    const pyodide = await startPyodideWorker(data.options);\n\n    statusText.textContent = `Downloading package: micropip`;\n    await pyodide.loadPackage(\"micropip\");\n    const micropip = await pyodide.pyimport(\"micropip\");\n    await data.packages.pkgs.map((pkg) => () => {\n      statusText.textContent = `Downloading package: ${pkg}`;\n      return micropip.install(pkg);\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n    await micropip.destroy();\n\n    // Download and install resources\n    await files.map((file) => async () => {\n      const name = file.substring(file.lastIndexOf('/') + 1);\n      statusText.textContent = `Downloading resource: ${name}`;\n      const response = await fetch(file);\n      if (!response.ok) {\n        throw new Error(`Can't download \\`${file}\\`. Error ${response.status}: \"${response.statusText}\".`);\n      }\n      const data = await response.arrayBuffer();\n\n      // Store URLs in the cwd without any subdirectory structure\n      if (file.includes(\"://\")) {\n        file = name;\n      }\n\n      // Collapse higher directory structure\n      file = collapsePath(file);\n\n      // Create directory tree, ignoring \"directory exists\" VFS errors\n      const parts = file.split('/').slice(0, -1);\n      let path = '';\n      while (parts.length > 0) {\n        path += parts.shift() + '/';\n        try {\n          await pyodide.FS.mkdir(path);\n        } catch (e) {\n          if (e.name !== \"ErrnoError\") throw e;\n          if (e.errno !== 20) {\n            const errorTextPtr = await pyodide._module._strerror(e.errno);\n            const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n            throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n          }\n        }\n      }\n\n      // Write this file to the VFS\n      try {\n        return await pyodide.FS.writeFile(file, new Uint8Array(data));\n      } catch (e) {\n        if (e.name !== \"ErrnoError\") throw e;\n        const errorTextPtr = await pyodide._module._strerror(e.errno);\n        const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n        throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n      }\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n\n    statusText.textContent = `Pyodide environment setup`;\n    await setupPython(pyodide);\n\n    statusText.remove();\n    if (statusContainer.children.length == 0) {\n      statusContainer.parentNode.remove();\n    }\n    return pyodide;\n  })().catch((err) => {\n    statusText.style.color = \"var(--exercise-editor-hl-er, #AD0000)\";\n    statusText.textContent = err.message;\n    //indicatorContainer.querySelector(\".spinner-grow\").classList.add(\"d-none\");\n    throw err;\n  });\n\n  // Keep track of initial OJS block render\n  const renderedOjs = {};\n\n  const process = async (context, inputs) => {\n    const pyodide = await pyodidePromise;\n    const evaluator = new PyodideEvaluator(pyodide, context);\n    await evaluator.process(inputs);\n    return evaluator.container;\n  }\n\n  return {\n    pyodidePromise,\n    renderedOjs,\n    process,\n  };\n}\n"}]}
</script>
<div id="exercise-loading-indicator" class="exercise-loading-indicator d-none d-flex align-items-center gap-2">
<div id="exercise-loading-status" class="d-flex gap-2">

</div>
<div class="spinner-grow spinner-grow-sm">

</div>
</div>
<script type="vfs-file">
W10=
</script>
</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="../qrjs_pics/unsoed_logo.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://imron-slide.vercel.app">irosyadi-2025</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script type="ojs-module-contents">
    eyJjb250ZW50cyI6W3sibWV0aG9kTmFtZSI6ImludGVycHJldCIsImNlbGxOYW1lIjoib2pzLWNlbGwtMSIsImlubGluZSI6ZmFsc2UsInNvdXJjZSI6InZpZXdvZiB3ZWlnaHRfbSA9IElucHV0cy5yYW5nZShbLTEwLCAxMF0sIHt2YWx1ZTogMSwgc3RlcDogMC4xLCBsYWJlbDogXCJXZWlnaHQgKG0pXCJ9KTtcbnZpZXdvZiBiaWFzX2IgPSBJbnB1dHMucmFuZ2UoWzEwMCwgMjAwXSwge3ZhbHVlOiAxNjAsIHN0ZXA6IDEsIGxhYmVsOiBcIkJpYXMgKGIpXCJ9KTtcbiJ9LHsibWV0aG9kTmFtZSI6ImludGVycHJldFF1aWV0Iiwic291cmNlIjoic2hpbnlJbnB1dCgnd2VpZ2h0X20nKSJ9LHsibWV0aG9kTmFtZSI6ImludGVycHJldFF1aWV0Iiwic291cmNlIjoic2hpbnlJbnB1dCgnYmlhc19iJykifV19
    </script>
    <script type="module">
    if (window.location.protocol === "file:") { alert("The OJS runtime does not work with file:// URLs. Please use a web server to view this document."); }
    window._ojs.paths.runtimeToDoc = "../../amli";
    window._ojs.paths.runtimeToRoot = "../..";
    window._ojs.paths.docToRoot = "..";
    window._ojs.selfContained = false;
    window._ojs.runtime.interpretFromScriptTags();
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>