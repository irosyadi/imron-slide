<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/quarto-contrib/live-runtime/live-runtime.js" type="module"></script>
<link href="../site_libs/quarto-contrib/live-runtime/live-runtime.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.25">

  <meta name="author" content="Imron Rosyadi">
  <title>Machine Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-b0356e9119c1bdfd0db189d130feb51c.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script type="module" src="../site_libs/quarto-ojs/quarto-ojs-runtime.js"></script>
  <link href="../site_libs/quarto-ojs/quarto-ojs.css" rel="stylesheet">
  <meta name="mermaid-theme" content="neutral">
  <script src="../site_libs/quarto-diagram/mermaid.min.js"></script>
  <script src="../site_libs/quarto-diagram/mermaid-init.js"></script>
  <link href="../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Machine Learning</h1>
  <p class="subtitle">5. Deep Learning: CNN, RNN, NLP</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Imron Rosyadi 
</div>
</div>
</div>

</section>
<section>
<section id="convolutional-neural-network" class="title-slide slide level1 center">
<h1>Convolutional Neural Network</h1>

</section>
<section id="the-biological-inspiration-visual-cortex" class="slide level2">
<h2>The Biological Inspiration: Visual Cortex</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>In the 1960s, research by David Hubel and Torsten Wiesel revealed how the visual cortex processes visual information.</p>
<ul>
<li>Neurons respond to specific regions of the visual field.<br>
</li>
<li>Each neuron has a “receptive field.”<br>
</li>
<li>Spatially close neurons have similar, overlapping receptive fields.<br>
</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/conNN02.jpg"></p>
<aside class="notes">
<p>Like neural networks, convolutional neural networks were inspired by biology.</p>
<p>In the 1960s, David Hubel and Torsten Wiesel showed that the visual cortex in cats and monkeys contain neurons that fire individually in response to small regions in the field of view.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="receptive-fields-and-image-formation" class="slide level2">
<h2>Receptive Fields and Image Formation</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Our visual system integrates information from these small receptive fields.</p>
<p>This process forms the complete images we perceive.</p>
<p>This biological mechanism provided a key inspiration for <strong>Convolutional Neural Networks (CNNs)</strong>.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/conNN03.jpg"></p>
<aside class="notes">
<p>For a given neuron, the visual space that affects whether or not that neuron will fire is known as its “receptive field.”</p>
<p>Neurons that are spatially close together often have similar and overlapping receptive fields.</p>
<p>Our eyes and brains then take the information from each of these small receptive fields and meld them together into the images that we see.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="introduction-to-convolutional-neural-networks-cnns" class="slide level2">
<h2>Introduction to Convolutional Neural Networks (CNNs)</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Inspired by the visual cortex, CNNs emerged in the 1980s.</p>
<p>CNNs are specialized neural networks for processing structured grid data, like images.</p>
<p>They incorporate unique layer types:</p>
<ul>
<li><strong>Convolutional Layers</strong><br>
</li>
<li><strong>Downsampling Layers</strong><br>
</li>
<li><strong>Pooling Layers</strong><br>
</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/conNN01.png"></p>
<aside class="notes">
<p>In the 1980s researchers were inspired by the visual cortex and used these ideas to create convolutional neural networks.</p>
<p>A convolutional neural network is simply a neural network with additional (or different) types of layers. There are convolutional layers, downsampling layers, and pooling layers.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="cnn-architecture-flexibility" class="slide level2">
<h2>CNN Architecture Flexibility</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>The power of CNNs lies in their flexible architecture.</p>
<p>Different combinations and ordering of layers lead to varied results.</p>
<p>This allows for optimization based on the specific task.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/conNN05.jpg"></p>
<aside class="notes">
<p>You can stack different numbers of these layers in various orders to achieve different results during training.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="recall-the-perceptron" class="slide level2">
<h2>Recall: The Perceptron</h2>
<p>The simplest building block of a typical neural network.</p>
<p>A basic computational unit that processes inputs to produce an output.</p>

<img data-src="05_res/conNN13.png" class="r-stretch"><aside class="notes">
<p>Recall the simplest building block for a typical neural network: the perceptron.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="issues-with-multi-layer-perceptrons-plain-anns-for-images" class="slide level2">
<h2>Issues with Multi-Layer Perceptrons (Plain ANNs) for Images</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Sensitivity to Input Changes:</strong></p>
<ul>
<li>Small shifts in image data can drastically alter learned parameters.<br>
</li>
<li>E.g., a cat’s position in an image should not change its recognition.<br>
</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/conNN15.png"></p>
<aside class="notes">
<p>If we are dealing with image data, small and often insignificant changes to the training data can yield large and often incorrect changes to the learned parameters in the model.</p>
<p>For example, consider a problem where you want to identify a cat in an image. If the cat is translated to a different part of the image, then the model will adjust different weights to recognize the cat. But the cat being on the left or right of an image isn’t really a defining feature of a cat, right? We’d prefer to recognize things like ears, fur, etc.</p>
<p>CNNs help us solve this problem.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="cnns-a-solution-for-image-data" class="slide level2">
<h2>CNNs: A Solution for Image Data</h2>
<p>CNNs introduce specialized layers to address the limitations of plain ANNs for image processing.</p>
<p>The processed output then feeds into a fully connected neural network.</p>

<img data-src="05_res/conNN14.png" class="r-stretch"><aside class="notes">
<p>In a convolutional neural network, we first feed our data into convolutional, downsampling, and pooling layers. The results are then fed into a fully connected neural network like we have seen before.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="convolution-analyzing-pixel-influence" class="slide level2">
<h2>Convolution: Analyzing Pixel Influence</h2>
<p>A fundamental operation in CNNs.</p>
<p>It uses a <strong>filter</strong> (also called a <strong>kernel</strong>, <strong>mask</strong>, or <strong>convolution matrix</strong>) to analyze the influence of nearby pixels.</p>
</section>
<section id="convolution-example-input-image" class="slide level2">
<h2>Convolution Example: Input Image</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Consider a simple image: a rectangle with two shaded halves.</p>
<p>Pixel intensities are represented numerically.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/conNN16.png"></p>
<aside class="notes">
<p>Let’s look at a simple example. Imagine we have the image on the left. It’s just a rectangle with two halves shaded different colors.</p>
<p>The intensity of each pixel is recorded on the right. This is how we typically work with image data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="convolution-example-blurring-filter" class="slide level2">
<h2>Convolution Example: Blurring Filter</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>We apply a 3x3 filter designed to create a blurring effect.</p>
<p>Each element in the filter has a specific weight.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/conNN17.png"></p>
<aside class="notes">
<p>We’ll apply this 3x3 filter to the image.</p>
<p>It’s a filter that adds a blurring effect.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="convolution-example-filter-application" class="slide level2">
<h2>Convolution Example: Filter Application</h2>
<p>The filter is applied by sliding it over the image.</p>
<p>At each position, it computes a new pixel value.</p>

<img data-src="05_res/conNN18.png" class="r-stretch"><aside class="notes">
<p>We’ll apply this 3x3 filter to the image.</p>
<p>It’s a filter that adds a blurring effect.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="convolution-example-calculating-a-new-pixel-value" class="slide level2 scrollable">
<h2>Convolution Example: Calculating a New Pixel Value</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>To calculate the new value for a pixel:</p>
<ol type="1">
<li>Center the filter on the target pixel.<br>
</li>
<li>Multiply corresponding values from the filter and the image.<br>
</li>
<li>Sum the products.<br>
</li>
</ol>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/conNN19.png"></p>
<aside class="notes">
<p>Let’s use the 3x3 filter to calculate the new value for this pixel.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="convolution-example-result-of-blurring" class="slide level2">
<h2>Convolution Example: Result of Blurring</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>The new pixel value is an average of its neighbors.</p>
<p>This averaging effect reduces sharp intensity changes, leading to blurring.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/conNN20.png"></p>
<aside class="notes">
<p>First we think of centering the filter on the pixel. Then we multiply the values in the filter by the values in the image. And finally, we add up the result.</p>
<p>As you can see, the new pixel value is slightly lower than 100, but it’s higher than 50. So the intensity is getting muted a little. This is because our filter is averaging the intensity of all the pixels around the center point. That is why this filter results in a blurring effect.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="handling-edges-padding" class="slide level2">
<h2>Handling Edges: Padding</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>When the filter reaches the image edge, padding is often used.</p>
<p>Commonly, the original image is padded with zeros around its borders.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/conNN21.png"></p>
<aside class="notes">
<p>You may be wondering what happens if we’re at the edge. There are different ways to handle this. But it’s common to pad the original image with 0’s around the edges. That way, those values drop out in the average.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="edge-handling-relevant-filter-area" class="slide level2">
<h2>Edge Handling: Relevant Filter Area</h2>
<p>The padding ensures the filter can be centered on edge pixels.</p>
<p>Only the part of the filter overlapping the original image contributes to the sum.</p>

<img data-src="05_res/conNN22.png" class="r-stretch"><aside class="notes">
<p>Here you can see that we only used the part of the filter that is relevant to the image.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="interactive-convolution-demo" class="slide level2">
<h2>Interactive Convolution Demo</h2>
<p>Explore how different filter values can affect a simple 3x3 image.</p>
<p>Adjust the filter weights and observe the output.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code hidden" id="cb1" data-startfrom="325" data-source-offset="-0"><pre class="sourceCode numberSource js number-lines code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 324;"><span id="cb1-325"><a></a>viewof p1 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">0</span><span class="op">,</span> <span class="dv">1</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="fl">0.11</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.01</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Top-Left Weight"</span>})<span class="op">;</span></span>
<span id="cb1-326"><a></a>viewof p2 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">0</span><span class="op">,</span> <span class="dv">1</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="fl">0.11</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.01</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Top-Middle Weight"</span>})<span class="op">;</span></span>
<span id="cb1-327"><a></a>viewof p3 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">0</span><span class="op">,</span> <span class="dv">1</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="fl">0.11</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.01</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Top-Right Weight"</span>})<span class="op">;</span></span>
<span id="cb1-328"><a></a>viewof p4 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">0</span><span class="op">,</span> <span class="dv">1</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="fl">0.11</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.01</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Mid-Left Weight"</span>})<span class="op">;</span></span>
<span id="cb1-329"><a></a>viewof p5 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">0</span><span class="op">,</span> <span class="dv">1</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="fl">0.11</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.01</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Center Weight"</span>})<span class="op">;</span></span>
<span id="cb1-330"><a></a>viewof p6 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">0</span><span class="op">,</span> <span class="dv">1</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="fl">0.11</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.01</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Mid-Right Weight"</span>})<span class="op">;</span></span>
<span id="cb1-331"><a></a>viewof p7 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">0</span><span class="op">,</span> <span class="dv">1</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="fl">0.11</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.01</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Bottom-Left Weight"</span>})<span class="op">;</span></span>
<span id="cb1-332"><a></a>viewof p8 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">0</span><span class="op">,</span> <span class="dv">1</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="fl">0.11</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.01</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Bottom-Middle Weight"</span>})<span class="op">;</span></span>
<span id="cb1-333"><a></a>viewof p9 <span class="op">=</span> Inputs<span class="op">.</span><span class="fu">range</span>([<span class="dv">0</span><span class="op">,</span> <span class="dv">1</span>]<span class="op">,</span> {<span class="dt">value</span><span class="op">:</span> <span class="fl">0.11</span><span class="op">,</span> <span class="dt">step</span><span class="op">:</span> <span class="fl">0.01</span><span class="op">,</span> <span class="dt">label</span><span class="op">:</span> <span class="st">"Bottom-Right Weight"</span>})<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-1" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-2" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-3" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-4" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-5" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-6" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-7" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-8" data-nodetype="declaration">

</div>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<div id="ojs-cell-1-9" data-nodetype="declaration">

</div>
</div>
</div>
</div>
<div>
<div id="pyodide-1" class="exercise-cell">

</div>
<script type="pyodide-1-contents">
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
</script>
</div>
</section>
<section id="line-detectors-gx-and-gy-kernels" class="slide level2">
<h2>Line Detectors: Gx and Gy Kernels</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>These kernels are designed to detect sharp intensity changes, indicating lines or edges.</p>
<ul>
<li><span class="math inline">\(G_x\)</span>: Detects vertical lines.<br>
</li>
<li><span class="math inline">\(G_y\)</span>: Detects horizontal lines.<br>
</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/conNN23.png"></p>
<aside class="notes">
<p>Here are two very common kernels that can be used to detect lines in an image.</p>
<p>Overall the goal is to detect sharp changes in intensity. Let’s see how this works by doing an example with G<span class="math inline">\(_{x}\)</span>.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="line-detector-example-input-image" class="slide level2">
<h2>Line Detector Example: Input Image</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>An image with a vertical line where shading changes.</p>
<p>We will apply the <span class="math inline">\(G_x\)</span> kernel to detect this line.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/conNN24.png"></p>
<aside class="notes">
<p>On the left we have an image that is similar to the previous example. There is a line down the center, where the shading changes color. Let’s see if the kernel G<span class="math inline">\(_{x}\)</span> can detect this line.</p>
<p>Calculate the pixel on the right.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="line-detector-example-first-pixel" class="slide level2">
<h2>Line Detector Example: First Pixel</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Applying <span class="math inline">\(G_x\)</span> to the first 3x3 block yields 0.</p>
<p>No intensity change within this block.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/conNN25.png"></p>
<aside class="notes">
<p>We get 0. There are no changes in intensity in the 3x3 block that is highlighted in the original image.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="line-detector-example-shifting-right" class="slide level2">
<h2>Line Detector Example: Shifting Right</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Shifting the <span class="math inline">\(G_x\)</span> kernel one pixel to the right.</p>
<p>The kernel now straddles the intensity change.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/conNN26.png"></p>
<aside class="notes">
<p>Now let’s move one pixel to the right.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="line-detector-example-detecting-the-edge" class="slide level2">
<h2>Line Detector Example: Detecting the Edge</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>The calculation results in a non-zero value (<span class="math inline">\(200/9\)</span>).</p>
<p>This indicates the presence of a vertical edge.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/conNN27.png"></p>
<aside class="notes">
<p>We get 200/9.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="line-detector-example-further-shift" class="slide level2">
<h2>Line Detector Example: Further Shift</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Shifting the <span class="math inline">\(G_x\)</span> kernel one more pixel to the right.</p>
<p>The kernel is now fully on the right side of the edge.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/conNN28.png"></p>
<aside class="notes">
<p>Again move one pixel to the right.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="line-detector-example-another-non-zero" class="slide level2">
<h2>Line Detector Example: Another Non-Zero</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>The calculation results in <span class="math inline">\(300/9\)</span>.</p>
<p>This continues to highlight the edge region.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/conNN29.png"></p>
<aside class="notes">
<p>We get 300/9.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="line-detector-example-past-the-edge" class="slide level2">
<h2>Line Detector Example: Past the Edge</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Shifting the <span class="math inline">\(G_x\)</span> kernel one final pixel to the right.</p>
<p>The kernel is now past the intensity change.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/conNN30.png"></p>
<aside class="notes">
<p>Finally, let’s move one more pixel to the right.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="line-detector-example-back-to-zero" class="slide level2">
<h2>Line Detector Example: Back to Zero</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>The calculation returns 0 again.</p>
<p>This demonstrates that <span class="math inline">\(G_x\)</span> effectively detects vertical lines by identifying sharp horizontal intensity transitions.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/conNN31.png"></p>
<aside class="notes">
<p>And again we get 0.</p>
<p>Thus, we see that a vertical line was detected when the intensity changed in the original image.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="cnns-learn-features" class="slide level2">
<h2>CNNs Learn Features</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>In CNNs, the kernel values are <em>learned</em> during training.</p>
<p>The network automatically identifies important features.</p>
<p>We don’t explicitly tell the model what to look for (e.g., “vertical lines”).</p>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/conNN14.png"></p>
<aside class="notes">
<p>This type of convolution happens in the convolutional layers of a neural network. The values in the kernels are parameters that will be learned during training. Thus, the specific features in the images that the kernels are testing for is something that the model “learns.” In other words, you don’t say “Hey model, test for vertical lines.” Instead, the model identifies the features that are important to test for.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="pooling" class="slide level2 scrollable">
<h2>Pooling</h2>
<p>A downsampling technique often applied after convolution.</p>
<p><strong>Objective:</strong> Reduce data size without losing critical information.</p>
<ol type="1">
<li><strong>Window Size:</strong> Select a region (e.g., 2x2 or 3x3).<br>
</li>
<li><strong>Stride:</strong> Define movement step (e.g., 2 pixels).<br>
</li>
<li><strong>Window Movement:</strong> Slide the window across filtered images.<br>
</li>
<li><strong>Value Selection:</strong> Take the maximum (Max Pooling) or average (Average Pooling) value within each window.</li>
</ol>
<aside class="notes">
<p>Pooling is a type of downsampling that often occurs after convolution. The goal is, without losing much information, to reduce the size of the training data before it goes into the fully connected network.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="hyperparameters-in-cnns" class="slide level2">
<h2>Hyperparameters in CNNs</h2>
<p>While CNNs learn many parameters, users define several key hyperparameters:</p>
<p><strong>Convolution:</strong><br>
- Number of filters (features)<br>
- Size of filters</p>
<p><strong>Pooling:</strong><br>
- Window size<br>
- Stride</p>
<p><strong>Fully Connected Layers:</strong><br>
- Number of nodes</p>
<p>Also, the number and order of each layer type.</p>
<aside class="notes">
<p>While a convolutional neural network learns MANY parameters, there are also several hyperparameters that are chosen by the user. Here are the main ones. But as with our previous neural networks, the user can also choose the optimizer, activation function, etc.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="your-turn-cnns-in-practice" class="slide level2">
<h2>Your Turn: CNNs in Practice</h2>
<p>In the lab, you will build and experiment with a Convolutional Neural Network.</p>
<p>You’ll apply these concepts to a practical image classification task.</p>
<aside class="notes">
<p>Now it’s your turn to build a CNN in the lab.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

</section></section>
<section>
<section id="recurrent-neural-networks-rnns" class="title-slide slide level1 center">
<h1>Recurrent Neural Networks (RNNs)</h1>

</section>
<section id="beyond-feedforward-introducing-rnns" class="slide level2">
<h2>Beyond Feedforward: Introducing RNNs</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Previous deep neural networks were primarily <strong>feedforward</strong>.</p>
<ul>
<li>Data flows in one direction.<br>
</li>
<li>Weights adjusted via backpropagation.</li>
</ul>
<p><strong>Recurrent Neural Networks (RNNs)</strong> introduce a new dynamic.</p>
<ul>
<li>Not strictly feedforward.<br>
</li>
<li>Designed for sequential data.<br>
</li>
</ul>
</div><div class="column" style="width:50%;">
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>RNNs excel in tasks where the order of data points is crucial, such as time series or natural language.</p>
</div>
</div>
</div>
</div></div>
<aside class="notes">
<p>We have encountered numerous deep neural networks throughout this course. In previous tracks we’ve covered, training data flowed through the network (feedforward), and then adjustments were made to the weights in the network from the last layer through the first (backpropagation).</p>
<p>Just to name a few, we’ve used dense layers, dropout layers, convolutional layers, and pooling layers. In this unit we will learn about recurrent neural networks, which are not strictly feedforward networks.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="the-feedforward-neuron" class="slide level2">
<h2>The Feedforward Neuron</h2>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Receives inputs from the previous layer.<br>
</li>
<li>Multiplies inputs by weights, adds bias.<br>
</li>
<li>Passes sum through an activation function.<br>
</li>
<li>Output goes to the next layer.<br>
</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/ff_neuron.png"></p>
<aside class="notes">
<p>Here we see a typical feedforward neuron. Depending on the size of the layers before and after, one or more weights feed into the neuron. These are multiplied by the bias, summed, and then passed through an activation function. The resultant value is then passed to the nodes in the next layer of the network.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="the-recurrent-neuron" class="slide level2">
<h2>The Recurrent Neuron</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>A feedforward neuron with a crucial addition:</p>
<ul>
<li>Its output feeds back into its own inputs.<br>
</li>
<li>This creates a “memory” over time.<br>
</li>
<li>Allows processing of sequential data.<br>
</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/r_neuron.png"></p>
<aside class="notes">
<p>Here is a recurrent neuron. You can see that the recurrent neuron looks a lot like a feedforward neuron, except that it also feeds its output back into its inputs.</p>
<p>Imagine we have a fully connected layer that is 10 nodes wide before this neuron. In a typical feedforward, fully-connected network, we would expect 10 inputs, one for each of the nodes in the previous layer. In this case we’ll actually have 11 inputs: one for each of the nodes in the previous layer and the output of the node itself.</p>
<p>What does this do? This gives the neuron memory over time. It allows you to pass a series of data points into the network over time.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="unrolling-a-recurrent-neuron-over-time" class="slide level2">
<h2>Unrolling a Recurrent Neuron Over Time</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Visualizing the data flow:</p>
<ul>
<li>Starts with a seeded input (often zero).<br>
</li>
<li>At each time step, it processes current input and its own previous output.<br>
</li>
<li>Passes data to the next layer and forward in time to itself.<br>
</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/unrolled.png"></p>
<aside class="notes">
<p>This is what it would look like to “unroll” the flow of data through a recurrent neuron over time. You can see that it typically starts out with a seeded input value of zero for its backfeed. At each point in the series, the neuron both passes data to the next layer of neurons and passes data forward in time to itself the next time it fires.</p>
<p>Also note that we’re looking at a single neuron in a layer with one input and one output. In reality you’ll have wide layers, so imagine multiple recurrent nodes side by side, each with multiple inputs and outputs.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="the-long-short-term-memory-lstm-neuron" class="slide level2">
<h2>The Long Short-Term Memory (LSTM) Neuron</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Addresses the “short memory” problem of standard RNNs.</p>
<ul>
<li>Passes two weights back to itself:
<ul>
<li>Long-term memory<br>
</li>
<li>Short-term memory<br>
</li>
</ul></li>
<li>Uses “gates” (forget, input, output) to control information flow.<br>
</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/lstm_neuron.png"></p>
<aside class="notes">
<p>With a typical recurrent neural network, the network tends to have a very short memory. As the sequences passing through the network get longer, the network forgets what it first saw. There have been a few strategies to get around this, one of which is the “long short term memory” (LSTM) neuron.</p>
<p>On this slide you can see a very simplified LSTM cell. If you look at the horizontal center, you can see the standard neuron: X-in, y-out. However, instead of having a single feedback like a standard recurrent neuron, this neuron passes two weights back to itself. One represents the long-term member, and the other represents the short-term member.</p>
<p>You can see that the short-term state gets mixed with the weights in a set of activation functions labelled A1 through A4. The outputs of these functions, as well as the long-term state, then get passed through a series of gates that ultimately lead to the output of a new y, c, and h value.</p>
<p>The numbered gates in order are:<br>
1. The forget gate<br>
2. The input gate<br>
3. Addition of the forget and input gate<br>
4. The output gate</p>
<p>LSTM cells are often higher-performing than standard recurrent cells. They also often train faster than standard recurrent cells.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="other-recurrent-neuron-types" class="slide level2">
<h2>Other Recurrent Neuron Types</h2>
<ul>
<li><strong>Gated Recurrent Unit (GRU) Neuron:</strong>
<ul>
<li>Simpler than LSTM, with a single feedback channel managing both short- and long-term state.<br>
</li>
<li>Often performs comparably to LSTMs with fewer parameters.<br>
</li>
</ul></li>
<li><strong>Convolutional Neurons:</strong>
<ul>
<li>Can also be adapted for sequence tasks.<br>
</li>
<li>Effective in identifying local patterns within sequences.</li>
</ul></li>
</ul>
<aside class="notes">
<p>The LSTM cell is pretty complex. There is an alternative called the “Gated Recurrent Unit” (GRU) neuron. The GRU has a single feedback channel that manages both short- and long-term state.</p>
<p>Another type of neuron that performs very well for sequence tasks alongside, or in place of, LSTM and GRU neurons are convolutional neurons. We’ll see a convolutional neuron in action in our lab.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="what-are-rnns-good-for" class="slide level2">
<h2>What Are RNNs Good For?</h2>
<p>RNNs excel in tasks involving sequential data:</p>
<ul>
<li><strong>Language Translation</strong><br>
</li>
<li><strong>Sequence Prediction</strong> (e.g., stock prices, weather)<br>
</li>
<li><strong>Sequence Generation</strong> (e.g., music, text)<br>
</li>
<li><strong>Tagging</strong> (e.g., video annotation)<br>
</li>
<li><strong>Summarization</strong> (e.g., text summarization)</li>
</ul>
<p>… and many more!</p>
<aside class="notes">
<p>Recurrent neural networks are useful for solving a variety of problems. They are commonly used on problems where there is a sequence of data that needs to be processed. For example, they can convert from one language to another. They are also useful in sequence prediction. For example, you might use an RNN to try to predict stock prices or temperatures over time. Since they also work so well with sequences, they can be used to generate sequences such as musical notes or strokes on a canvas. They can also process data such as video and “tag” those videos. Another application is summarization. A RNN can ingest a large amount of text and create a summary about that text.</p>
<p>There are many many more applications of RNNs.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="sequence-prediction" class="slide level2">
<h2>Sequence Prediction</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>RNNs are particularly strong in sequence prediction.</p>
<p>Unlike earlier models, RNNs inherently consider the temporal dependency of data.</p>
<p>Previous models often assumed time-independent data.</p>
</div><div class="column" style="width:50%;">
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>For example, predicting future sensor readings from past measurements in an ECE system.</p>
</div>
</div>
</div>
</div></div>
<aside class="notes">
<p>One application that RNNs are particularly good at is sequence prediction. Our lab will actually be a sequence prediction lab.</p>
<p>So far in this course, all of our predictions have assumed the data are time-independent. That is, we could shuffle around all the data points and prediction points in time, and nothing would change.</p>
<p>For example, consider the model we used earlier to predict height from shoe size. The dataset we used was all the students in the class. Now, if we shuffle around all those students, and take the data points in a different order, the model doesn’t change.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="time-series-data" class="slide level2">
<h2>Time Series Data</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Time series data</strong> is an ordered set of data points indexed by time.</p>
<p>The inherent ordering makes it ideal for RNNs.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/time_series.png"></p>
<aside class="notes">
<p>In this unit, we will look at sequence prediction. In sequence prediction the input data is an <em>ordered</em> set of data, most commonly a time series. A time series is a set of data where the index is a date. Since dates have an inherent ordering, time series are ordered data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="what-are-we-predicting" class="slide level2">
<h2>What Are We Predicting?</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Sequence prediction aims to forecast future values based on historical data.</p>
<p>Example: Predicting the next quarter’s performance from a year of data.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/time_series_2.png"></p>
<aside class="notes">
<p>Sequence prediction is about predicting what happens next in a sequence. For example, if we have a year’s worth of data, we may want to know what happens in Q1 of the next year.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="rnns-for-sequence-prediction-a-new-tool" class="slide level2">
<h2>RNNs for Sequence Prediction: A New Tool</h2>
<ul>
<li><strong>Traditional Approach:</strong> Statistical methods (e.g., ARIMA, Markov chains)
<ul>
<li>Often involve numerous assumptions.<br>
</li>
</ul></li>
<li><strong>Machine Learning &amp; RNNs:</strong> A largely non-parametric approach.
<ul>
<li>Data “speaks for itself,” fewer assumptions.<br>
</li>
<li>Requires more data for optimal performance.</li>
</ul></li>
</ul>
<aside class="notes">
<p>The standard approach to sequence prediction for several years was a statistical one (like Markov chains or ARIMA).</p>
<p><em>There may not be a need to go into detail, but you could mention Markov Chains or ARIMA time series forecasting. Suffice to say, these approaches often require a lot of assumptions, such as a transition matrix of probabilities, or a normal distribution of noise.</em></p>
<p>RNNs allow the data to “speak for themselves.” Using an RNN is a largely non-parametric approach. The downside is that RNNs usually need more data to make good predictions.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="examples-of-sequence-prediction" class="slide level2">
<h2>Examples of Sequence Prediction</h2>
<aside class="notes">
<p><em>Walk students through the following examples. These are just a few examples. There are many more, so feel free to elaborate on these and/or add others.</em></p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="example-stock-price-prediction" class="slide level2">
<h2>Example: Stock Price Prediction</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Predicting future stock prices based on historical market data.</p>
<p>A complex task due to market volatility, but RNNs can capture trends.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/stock_paper.png"></p>
<aside class="notes">
<p>One of the most common examples of sequence prediction is predicting stock prices. Stock prices are notoriously volatile, but a lot of people are involved in the practice of trying to predict them. There are entire industries based on this practice.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="example-weather-forecasting" class="slide level2">
<h2>Example: Weather Forecasting</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Forecasting weather patterns based on past meteorological data.</p>
<p>While complex, RNNs can identify subtle temporal dependencies.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/rain_forecast.png"></p>
<aside class="notes">
<p>Predicting the weather based on previous days of weather could also be an important application of sequence prediction. While most meteorological systems use a parametric approach based on input data (such as air pressure, cloud cover, etc.), a sequence prediction model can go surprisingly far.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="example-predicting-passenger-traffic" class="slide level2">
<h2>Example: Predicting Passenger Traffic</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Predicting daily traveler numbers at a train station.</p>
<p>RNNs can learn seasonality (weekdays vs.&nbsp;weekends, holidays).</p>
<p>Requires sufficient historical data to capture these patterns.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/train_station.jpg"></p>
<aside class="notes">
<p>You may want to predict the number of travelers at a train station on a given day, given the previous data of how many travelers were there each day. RNNs pick up on things like varieties of seasonality (e.g., weekday vs weekend, holiday season) and noise.</p>
<p>However, especially for time series with seasonality, we need to have enough data. For example, if we only have data for October and November, we won’t do very well at predicting December because it is a holiday month; we would ideally have data for December of the previous year (if not multiple years).</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="your-turn-rnns-for-vibration-prediction" class="slide level2">
<h2>Your Turn: RNNs for Vibration Prediction</h2>
<p>In the lab, you will use an RNN to predict a sequence of vibration readings from an engine.</p>
<p>You’ll apply TensorFlow with Keras to build, test, and tune your model.</p>
<aside class="notes">
<p>In the lab we’ll use a recurrent neural network to predict a sequence of vibration readings from an engine. We’ll see how to apply TensorFlow with Keras to build, test, and tune your model.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

</section></section>
<section>
<section id="natural-language-processing-nlp" class="title-slide slide level1 center">
<h1>Natural Language Processing (NLP)</h1>

</section>
<section id="what-is-natural-language-processing" class="slide level2">
<h2>What is Natural Language Processing?</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>The interaction between computers and human (natural) language.</p>
<p>Enables computers to understand, interpret, and generate human language.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/open-book.jpg"></p>
<aside class="notes">
<p>So what is natural language processing?</p>
<p>What are some applications of NLP in your everyday life? <em>Prompt the group to respond.</em></p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="nlp-applications-in-everyday-life" class="slide level2">
<h2>NLP Applications in Everyday Life</h2>
<ul>
<li><strong>Autocorrect &amp; Predictive Text</strong><br>
</li>
<li><strong>Translation Services</strong> (e.g., Google Translate)<br>
</li>
<li><strong>Parsing Text</strong> (e.g., extracting information)<br>
</li>
<li><strong>Chatbots &amp; Virtual Assistants</strong><br>
</li>
<li><strong>Question Answering Systems</strong><br>
</li>
<li><strong>Speech Recognition</strong></li>
</ul>
<p>… and so much more!</p>
<aside class="notes">
<p>Here are some examples of what is considered natural language processing. You have likely interacted with systems that perform these tasks before.</p>
<p>There is some argument regarding whether speech recognition should actually be considered NLP. It is possible to convert sound waves into words without actually understanding what those words are. This is technically “processing” of natural language, but it falls short of “Natural Language Understanding.” However, many speech recognition systems actually attempt to understand the speech in order to correctly predict ambiguous words like “there,” “their,” and “they’re.”</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="character-vs.-word-level-models" class="slide level2">
<h2>Character vs.&nbsp;Word-Level Models</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Character-Level Models:</strong><br>
- Process text one character at a time.<br>
- Can handle out-of-vocabulary words and typos.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/character-model.png"></p>
<aside class="notes">
<p>Models can process text at different levels. For example, you’ll see some language generation models that use a character-by-character approach such as the RNN shown in this slide.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="character-vs.-word-level-models-cont." class="slide level2">
<h2>Character vs.&nbsp;Word-Level Models (Cont.)</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Word-Level Models:</strong><br>
- Process text one word at a time.<br>
- More common for English and similar languages.<br>
- Often faster to train and perform well.</p>
<p><strong>Which is better?</strong> Depends on the language and use case.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/word-model.png"></p>
<aside class="notes">
<p>Here is a word-based model. It looks structurally like the character-based model except that it works at the word level.</p>
<p>Which is better?</p>
<p>It depends. For some languages and use cases, the character-based approach works well. In practice, you see more word-based models, especially for English and similar languages. The models typically perform well and are quicker to train than character-based models.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="text-processing-regular-expressions-regex" class="slide level2">
<h2>Text Processing: Regular Expressions (Regex)</h2>
<p>A powerful tool for pattern matching in strings.</p>
<p>Used for extracting, validating, or modifying text.</p>
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: left;">regex</th>
<th style="text-align: left;">matches</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>[wW]ood</code></td>
<td style="text-align: left;"><strong>w</strong>ood, <strong>W</strong>ood</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>beg.n</code></td>
<td style="text-align: left;">beg<strong>i</strong>n, beg<strong>u</strong>n, beg<strong>3</strong>n</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>o+h</code></td>
<td style="text-align: left;"><strong>o</strong>h, <strong>ooooo</strong>h</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>[^a-zA-Z]</code></td>
<td style="text-align: left;">a single non-alpha character</td>
</tr>
</tbody>
</table>
<aside class="notes">
<p>Before machine learning, NLP problems were usually solved by pattern matching. Even now, these text processing techniques can be very important in processing messy natural language.</p>
<p>Regular expressions are widely used in text processing. Imagine needing to extract all the email addresses from a block of text or remove prefixes/suffixes from a word. A regex defines a pattern that is used to match certain character combinations, following a set of rules. In this table we show a few examples of pattern matching rules:<br>
* “.” matches any single character<br>
* “+” matches 1 or more of the previous character<br>
* “[^...]” negates the rest of the pattern in the brackets</p>
<p>Regex rules can be very powerful but also very complex. Many guides exist for effectively using regexes: https://www.rexegg.com/regex-quickstart.html</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="text-processing-minimum-edit-distance" class="slide level2">
<h2>Text Processing: Minimum Edit Distance</h2>
<p>Also known as <strong>Levenshtein distance</strong>.</p>
<ul>
<li>Measures the minimum number of single-character edits (insertions, deletions, substitutions) required to change one string into another.<br>
</li>
<li>Crucial for autocorrect, spell checkers, and evaluating text generation systems.</li>
</ul>

<img data-src="05_res/distance.png" class="r-stretch"><aside class="notes">
<p>Another important concept for text processing is minimum edit distance, also called Levenshtein distance. This is especially useful for autocorrect tools and evaluating systems that generate language, e.g., translation. There are many open source Python implementations of this metric that you can use.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="feature-extraction-in-nlp" class="slide level2">
<h2>Feature Extraction in NLP</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Transforming raw text into informative numerical features.</p>
<ul>
<li><strong>N-grams:</strong> Consider sequences of <code>n</code> words.
<ul>
<li>Captures context beyond single words (e.g., “not horrible”).<br>
</li>
</ul></li>
<li><strong>TFIDF (Term Frequency-Inverse Document Frequency):</strong>
<ul>
<li>Determines word importance in a document.<br>
</li>
<li>Discounts common words like “the” or “and.”<br>
</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p>N-grams help capture local word order, while TFIDF helps identify unique and important terms.</p>
</div>
</div>
</div>
</div></div>
<aside class="notes">
<p>Before neural networks, the first step in NLP was “feature extraction,” or transforming raw text into informative features. The idea is that just the individual words in a text do not fully capture the meaning of the text.</p>
<p>One very common feature extraction technique is n-grams, which consider n-word sequences instead of just individual words. In the original sentence “that movie was not horrible,” the word “horrible” may cause a model to predict very strong negative emotion. But, if we extract bigrams (2-grams), then we would correctly pair “not horrible,” which is a much milder emotion.</p>
<p>Another common technique is TFIDF, which calculates how important a word is to a text. This often has the effect of ignoring more common words like “the” and letting the model focus on more unique words in the text.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="language-modeling-bag-of-words" class="slide level2">
<h2>Language Modeling: Bag-of-Words</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Simplest language modeling approach.</p>
<p>Treats a sentence as an unordered collection of words.</p>
<p><strong>Example:</strong> “I love love loved it!” and “I HATED it :-(”<br>
- Meaning can often be inferred without word order.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/bag-of-words.png"></p>
<aside class="notes">
<p>To build models for NLP tasks, we must have some notion of how words fit together into sentences and text. Language modeling refers to determining how likely a certain sentence is. The simplest language modeling approach is a bag-of-words: treat a sentence like an unordered collection (set) of words.</p>
<p>Take an example movie review, “I love love loved it!”, and another, “I HATED it :-(”. As humans, we could deduce which review corresponded to a positive sentiment and which review corresponded to a negative sentiment, even if we looked at these sentences out of order (e.g., “it! I loved love love” and “HATED :-( I it”). So bag-of-words is like saying, “I’m pretty sure I can glean the meaning of sentences, with words in any order, so why bother keeping track of the order? Sounds like more work to me.”</p>
<p>But can you think of an example or two where this strategy would fail? Especially consider if you’re trying to predict more than just two sentiments (“good” and “bad”). <em>Prompt class for discussion.</em></p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="language-modeling-sequential-words" class="slide level2">
<h2>Language Modeling: Sequential Words</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>While bag-of-words is effective for some tasks (e.g., spam filtering), word order is crucial for complex NLP.</p>
<p><strong>Sequential approaches</strong> preserve word order.</p>
<p>This is where <strong>Recurrent Neural Networks (RNNs)</strong> become indispensable.</p>
</div><div class="column" style="width:50%;">
<p><img data-src="05_res/sequential-words.png"></p>
<aside class="notes">
<p>Bag-of-words approaches are surprisingly successful on many tasks (email spam filter, sentiment analysis) and are less computationally intensive.</p>
<p>But fundamentally we know that the order of words matters. Harder NLP tasks build upon sequential approaches, which preserve the order of words in a text. This is exactly what RNNs are useful for. Recurrent Neural Networks handle this well.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div></div>
</section>
<section id="the-nlp-processing-pipeline" class="slide level2">
<h2>The NLP Processing Pipeline</h2>
<p>Understanding the typical flow for NLP tasks:</p>
<div class="cell" data-reveal="true" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<pre class="mermaid mermaid-js">graph TD
    A[Raw Text] --&gt; B{Feature Extraction / Embeddings};
    B --&gt; C[Machine Learning Model];
    C --&gt; D[Supervised Task (e.g., Classification, Generation)];
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<aside class="notes">
<p>The typical process for an NLP task is:<br>
1. Raw text<br>
2. Transform to feature vectors (either through feature extraction or embeddings)<br>
3. Run through some model<br>
4. Perform supervised task</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="your-turn-nlp-lab" class="slide level2">
<h2>Your Turn: NLP Lab</h2>
<p>In the lab, you will perform sentiment analysis on reviews.</p>
<p>You will then build a classifier to determine authorship (e.g., Jane Austen vs.&nbsp;Charles Dickens).</p>
<aside class="notes">
<p>In this lab we will perform sentiment analysis on reviews as an example. After that we’ll write a classifier that determines if a piece of text was written by Jane Austen or Charles Dickens.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="conclusion-and-qa" class="slide level2">
<h2>Conclusion and Q&amp;A</h2>
<p>We’ve explored Convolutional Neural Networks for image processing, Recurrent Neural Networks for sequential data, and key concepts in Natural Language Processing.</p>
<p>These deep learning techniques are fundamental to many modern ECE applications.</p>
<p>What questions do you have about these powerful tools and their applications in engineering?</p>


<script type="pyodide-data">
eyJvcHRpb25zIjp7ImVudiI6eyJQTE9UTFlfUkVOREVSRVIiOiJwbG90bHlfbWltZXR5cGUifSwiaW5kZXhVUkwiOiJodHRwczovL2Nkbi5qc2RlbGl2ci5uZXQvcHlvZGlkZS92MC4yNy4wL2Z1bGwvIn0sInBhY2thZ2VzIjp7InBrZ3MiOlsicHlvZGlkZV9odHRwIiwibWljcm9waXAiLCJpcHl0aG9uIiwibnVtcHkiLCJwbG90bHkiLCJuYmZvcm1hdCJdfX0=
</script>
<script type="ojs-module-contents">
{"contents":[{"cellName":"pyodide-1","inline":false,"methodName":"interpret","source":"_pyodide_value_1 = {\n  const { highlightPython, b64Decode} = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-1-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  // Default evaluation configuration\n  const options = Object.assign({\n    id: \"pyodide-1-contents\",\n    echo: true,\n    output: true\n  }, block.attr);\n\n  // Evaluate the provided Python code\n  const result = pyodideOjs.process({code: block.code, options}, {p1, p2, p3, p4, p5, p6, p7, p8, p9});\n\n  // Early yield while we wait for the first evaluation and render\n  if (options.output && !(\"1\" in pyodideOjs.renderedOjs)) {\n    const container = document.createElement(\"div\");\n    const spinner = document.createElement(\"div\");\n\n    if (options.echo) {\n      // Show output as highlighted source\n      const preElem = document.createElement(\"pre\");\n      container.className = \"sourceCode\";\n      preElem.className = \"sourceCode python\";\n      preElem.appendChild(highlightPython(block.code));\n      spinner.className = \"spinner-grow spinner-grow-sm m-2 position-absolute top-0 end-0\";\n      preElem.appendChild(spinner);\n      container.appendChild(preElem);\n    } else {\n      spinner.className = \"spinner-border spinner-border-sm\";\n      container.appendChild(spinner);\n    }\n\n    yield container;\n  }\n\n  pyodideOjs.renderedOjs[\"1\"] = true;\n  yield await result;\n}\n"},{"cellName":"pyodide-prelude","inline":false,"methodName":"interpretQuiet","source":"pyodideOjs = {\n  const {\n    PyodideEvaluator,\n    PyodideEnvironmentManager,\n    setupPython,\n    startPyodideWorker,\n    b64Decode,\n    collapsePath,\n  } = window._exercise_ojs_runtime;\n\n  const statusContainer = document.getElementById(\"exercise-loading-status\");\n  const indicatorContainer = document.getElementById(\"exercise-loading-indicator\");\n  indicatorContainer.classList.remove(\"d-none\");\n\n  let statusText = document.createElement(\"div\")\n  statusText.classList = \"exercise-loading-details\";\n  statusText = statusContainer.appendChild(statusText);\n  statusText.textContent = `Initialise`;\n\n  // Hoist indicator out from final slide when running under reveal\n  const revealStatus = document.querySelector(\".reveal .exercise-loading-indicator\");\n  if (revealStatus) {\n    revealStatus.remove();\n    document.querySelector(\".reveal > .slides\").appendChild(revealStatus);\n  }\n\n  // Make any reveal slides with live cells scrollable\n  document.querySelectorAll(\".reveal .exercise-cell\").forEach((el) => {\n    el.closest('section.slide').classList.add(\"scrollable\");\n  })\n\n  // Pyodide supplemental data and options\n  const dataContent = document.querySelector(`script[type=\\\"pyodide-data\\\"]`).textContent;\n  const data = JSON.parse(b64Decode(dataContent));\n\n  // Grab list of resources to be downloaded\n  const filesContent = document.querySelector(`script[type=\\\"vfs-file\\\"]`).textContent;\n  const files = JSON.parse(b64Decode(filesContent));\n\n  let pyodidePromise = (async () => {\n    statusText.textContent = `Downloading Pyodide`;\n    const pyodide = await startPyodideWorker(data.options);\n\n    statusText.textContent = `Downloading package: micropip`;\n    await pyodide.loadPackage(\"micropip\");\n    const micropip = await pyodide.pyimport(\"micropip\");\n    await data.packages.pkgs.map((pkg) => () => {\n      statusText.textContent = `Downloading package: ${pkg}`;\n      return micropip.install(pkg);\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n    await micropip.destroy();\n\n    // Download and install resources\n    await files.map((file) => async () => {\n      const name = file.substring(file.lastIndexOf('/') + 1);\n      statusText.textContent = `Downloading resource: ${name}`;\n      const response = await fetch(file);\n      if (!response.ok) {\n        throw new Error(`Can't download \\`${file}\\`. Error ${response.status}: \"${response.statusText}\".`);\n      }\n      const data = await response.arrayBuffer();\n\n      // Store URLs in the cwd without any subdirectory structure\n      if (file.includes(\"://\")) {\n        file = name;\n      }\n\n      // Collapse higher directory structure\n      file = collapsePath(file);\n\n      // Create directory tree, ignoring \"directory exists\" VFS errors\n      const parts = file.split('/').slice(0, -1);\n      let path = '';\n      while (parts.length > 0) {\n        path += parts.shift() + '/';\n        try {\n          await pyodide.FS.mkdir(path);\n        } catch (e) {\n          if (e.name !== \"ErrnoError\") throw e;\n          if (e.errno !== 20) {\n            const errorTextPtr = await pyodide._module._strerror(e.errno);\n            const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n            throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n          }\n        }\n      }\n\n      // Write this file to the VFS\n      try {\n        return await pyodide.FS.writeFile(file, new Uint8Array(data));\n      } catch (e) {\n        if (e.name !== \"ErrnoError\") throw e;\n        const errorTextPtr = await pyodide._module._strerror(e.errno);\n        const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n        throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n      }\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n\n    statusText.textContent = `Pyodide environment setup`;\n    await setupPython(pyodide);\n\n    statusText.remove();\n    if (statusContainer.children.length == 0) {\n      statusContainer.parentNode.remove();\n    }\n    return pyodide;\n  })().catch((err) => {\n    statusText.style.color = \"var(--exercise-editor-hl-er, #AD0000)\";\n    statusText.textContent = err.message;\n    //indicatorContainer.querySelector(\".spinner-grow\").classList.add(\"d-none\");\n    throw err;\n  });\n\n  // Keep track of initial OJS block render\n  const renderedOjs = {};\n\n  const process = async (context, inputs) => {\n    const pyodide = await pyodidePromise;\n    const evaluator = new PyodideEvaluator(pyodide, context);\n    await evaluator.process(inputs);\n    return evaluator.container;\n  }\n\n  return {\n    pyodidePromise,\n    renderedOjs,\n    process,\n  };\n}\n"}]}
</script>
<div id="exercise-loading-indicator" class="exercise-loading-indicator d-none d-flex align-items-center gap-2">
<div id="exercise-loading-status" class="d-flex gap-2">

</div>
<div class="spinner-grow spinner-grow-sm">

</div>
</div>
<script type="vfs-file">
W10=
</script>
</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="../qrjs_pics/unsoed_logo.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://imron-slide.vercel.app">irosyadi-2025</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script type="ojs-module-contents">
    eyJjb250ZW50cyI6W3sibWV0aG9kTmFtZSI6ImludGVycHJldCIsImNlbGxOYW1lIjoib2pzLWNlbGwtMSIsImlubGluZSI6ZmFsc2UsInNvdXJjZSI6InZpZXdvZiBwMSA9IElucHV0cy5yYW5nZShbMCwgMV0sIHt2YWx1ZTogMC4xMSwgc3RlcDogMC4wMSwgbGFiZWw6IFwiVG9wLUxlZnQgV2VpZ2h0XCJ9KTtcbnZpZXdvZiBwMiA9IElucHV0cy5yYW5nZShbMCwgMV0sIHt2YWx1ZTogMC4xMSwgc3RlcDogMC4wMSwgbGFiZWw6IFwiVG9wLU1pZGRsZSBXZWlnaHRcIn0pO1xudmlld29mIHAzID0gSW5wdXRzLnJhbmdlKFswLCAxXSwge3ZhbHVlOiAwLjExLCBzdGVwOiAwLjAxLCBsYWJlbDogXCJUb3AtUmlnaHQgV2VpZ2h0XCJ9KTtcbnZpZXdvZiBwNCA9IElucHV0cy5yYW5nZShbMCwgMV0sIHt2YWx1ZTogMC4xMSwgc3RlcDogMC4wMSwgbGFiZWw6IFwiTWlkLUxlZnQgV2VpZ2h0XCJ9KTtcbnZpZXdvZiBwNSA9IElucHV0cy5yYW5nZShbMCwgMV0sIHt2YWx1ZTogMC4xMSwgc3RlcDogMC4wMSwgbGFiZWw6IFwiQ2VudGVyIFdlaWdodFwifSk7XG52aWV3b2YgcDYgPSBJbnB1dHMucmFuZ2UoWzAsIDFdLCB7dmFsdWU6IDAuMTEsIHN0ZXA6IDAuMDEsIGxhYmVsOiBcIk1pZC1SaWdodCBXZWlnaHRcIn0pO1xudmlld29mIHA3ID0gSW5wdXRzLnJhbmdlKFswLCAxXSwge3ZhbHVlOiAwLjExLCBzdGVwOiAwLjAxLCBsYWJlbDogXCJCb3R0b20tTGVmdCBXZWlnaHRcIn0pO1xudmlld29mIHA4ID0gSW5wdXRzLnJhbmdlKFswLCAxXSwge3ZhbHVlOiAwLjExLCBzdGVwOiAwLjAxLCBsYWJlbDogXCJCb3R0b20tTWlkZGxlIFdlaWdodFwifSk7XG52aWV3b2YgcDkgPSBJbnB1dHMucmFuZ2UoWzAsIDFdLCB7dmFsdWU6IDAuMTEsIHN0ZXA6IDAuMDEsIGxhYmVsOiBcIkJvdHRvbS1SaWdodCBXZWlnaHRcIn0pO1xuIn0seyJtZXRob2ROYW1lIjoiaW50ZXJwcmV0UXVpZXQiLCJzb3VyY2UiOiJzaGlueUlucHV0KCdwMScpIn0seyJtZXRob2ROYW1lIjoiaW50ZXJwcmV0UXVpZXQiLCJzb3VyY2UiOiJzaGlueUlucHV0KCdwMicpIn0seyJtZXRob2ROYW1lIjoiaW50ZXJwcmV0UXVpZXQiLCJzb3VyY2UiOiJzaGlueUlucHV0KCdwMycpIn0seyJtZXRob2ROYW1lIjoiaW50ZXJwcmV0UXVpZXQiLCJzb3VyY2UiOiJzaGlueUlucHV0KCdwNCcpIn0seyJtZXRob2ROYW1lIjoiaW50ZXJwcmV0UXVpZXQiLCJzb3VyY2UiOiJzaGlueUlucHV0KCdwNScpIn0seyJtZXRob2ROYW1lIjoiaW50ZXJwcmV0UXVpZXQiLCJzb3VyY2UiOiJzaGlueUlucHV0KCdwNicpIn0seyJtZXRob2ROYW1lIjoiaW50ZXJwcmV0UXVpZXQiLCJzb3VyY2UiOiJzaGlueUlucHV0KCdwNycpIn0seyJtZXRob2ROYW1lIjoiaW50ZXJwcmV0UXVpZXQiLCJzb3VyY2UiOiJzaGlueUlucHV0KCdwOCcpIn0seyJtZXRob2ROYW1lIjoiaW50ZXJwcmV0UXVpZXQiLCJzb3VyY2UiOiJzaGlueUlucHV0KCdwOScpIn1dfQ==
    </script>
    <script type="module">
    if (window.location.protocol === "file:") { alert("The OJS runtime does not work with file:// URLs. Please use a web server to view this document."); }
    window._ojs.paths.runtimeToDoc = "../../amli";
    window._ojs.paths.runtimeToRoot = "../..";
    window._ojs.paths.docToRoot = "..";
    window._ojs.selfContained = false;
    window._ojs.runtime.interpretFromScriptTags();
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>